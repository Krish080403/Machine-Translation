{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Train Test and Validation based on Stratified Sampling**"
      ],
      "metadata": {
        "id": "JyGCiKl2YFGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdwcQxDHXUHD"
      },
      "outputs": [],
      "source": [
        "from transformers import M2M100Tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the M2M100 tokenizer\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "file_path = \"/content/34.871k.csv\"  # Replace with your file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Fill missing sentences with empty strings to avoid TypeError\n",
        "for col in ['HINDI', 'MUNDARI']:\n",
        "    df[col] = df[col].fillna(\"\")\n",
        "\n",
        "# Function to count the number of tokens in a sentence\n",
        "def count_tokens(sentence):\n",
        "    # ensure input is string\n",
        "    text = str(sentence)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return len(tokens)\n",
        "\n",
        "# Apply the token count function to both columns\n",
        "df['Token_Count_Hindi'] = df['HINDI'].apply(count_tokens)\n",
        "df['Token_Count_Mundari'] = df['MUNDARI'].apply(count_tokens)\n",
        "\n",
        "# Compute the average token count between the two languages\n",
        "df['Average_Token_Count'] = df[['Token_Count_Hindi', 'Token_Count_Mundari']].mean(axis=1)\n",
        "\n",
        "# Sort by average token count\n",
        "df_sorted = df.sort_values(by='Average_Token_Count').reset_index(drop=True)\n",
        "\n",
        "# Perform KMeans clustering into 15 clusters based on the average token count\n",
        "kmeans = KMeans(n_clusters=15, random_state=42)\n",
        "df_sorted['Cluster'] = kmeans.fit_predict(df_sorted[['Average_Token_Count']])\n",
        "\n",
        "# Initialize DataFrames for train, test, and validation sets\n",
        "test_data = pd.DataFrame()\n",
        "validation_data = pd.DataFrame()\n",
        "train_data = pd.DataFrame()\n",
        "\n",
        "# For each cluster, split into 10% validation, then 5% of remaining as test, rest as train\n",
        "for cluster in range(15):\n",
        "    cluster_data = df_sorted[df_sorted['Cluster'] == cluster]\n",
        "\n",
        "    # 10% -> validation\n",
        "    validation_samples = cluster_data.sample(frac=0.10, random_state=42)\n",
        "    remaining_after_val = cluster_data.drop(validation_samples.index)\n",
        "\n",
        "    # 5% of the remaining -> test\n",
        "    test_samples = remaining_after_val.sample(frac=0.05, random_state=42)\n",
        "    train_samples = remaining_after_val.drop(test_samples.index)\n",
        "\n",
        "    # Append to overall sets\n",
        "    validation_data = pd.concat([validation_data, validation_samples])\n",
        "    test_data = pd.concat([test_data, test_samples])\n",
        "    train_data = pd.concat([train_data, train_samples])\n",
        "\n",
        "# Drop helper columns before saving\n",
        "cols_to_drop = ['Token_Count_Hindi', 'Token_Count_Mundari', 'Average_Token_Count', 'Cluster']\n",
        "validation_data = validation_data.drop(columns=cols_to_drop)\n",
        "test_data       = test_data.drop(columns=cols_to_drop)\n",
        "train_data      = train_data.drop(columns=cols_to_drop)\n",
        "\n",
        "# Save the splits to Excel files\n",
        "test_data.to_excel(\"test_data.xlsx\", index=False)\n",
        "train_data.to_excel(\"train_data.xlsx\", index=False)\n",
        "validation_data.to_excel(\"validation_data.xlsx\", index=False)\n"
      ]
    }
  ]
}