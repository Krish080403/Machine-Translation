2024-09-04 06:59:23 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Version2/checkpoint1.2B', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/Version1/checkpoint1.2B', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2B', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2B', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='hi-mr', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2B', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2B', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='hi-mr', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-09-04 06:59:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-09-04 06:59:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-09-04 06:59:23 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-09-04 06:59:31 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-09-04 06:59:31 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-09-04 06:59:31 | INFO | fairseq_cli.train | model: TransformerModel
2024-09-04 06:59:31 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-09-04 06:59:31 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-09-04 06:59:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-09-04 06:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-09-04 06:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14113.65625Mb; avail=240948.453125Mb
2024-09-04 06:59:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 06:59:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:hi-mr': 1}
2024-09-04 06:59:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hi-mr src_langtok: 128036; tgt_langtok: 128063
2024-09-04 06:59:31 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.hi
2024-09-04 06:59:31 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.mr
2024-09-04 06:59:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin valid hi-mr 3313 examples
2024-09-04 06:59:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-09-04 06:59:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-09-04 06:59:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 06:59:33 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-09-04 06:59:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 06:59:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-09-04 06:59:33 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-09-04 06:59:33 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/Version1/checkpoint1.2B: optimizer, lr scheduler, meters, dataloader will be reset
2024-09-04 06:59:33 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/Version1/checkpoint1.2B
2024-09-04 06:59:33 | INFO | fairseq.trainer | No existing checkpoint found /home/krish/content/Version1/checkpoint1.2B
2024-09-04 06:59:33 | INFO | fairseq.trainer | loading train data for epoch 1
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7544.56640625Mb; avail=247509.5390625Mb
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:hi-mr': 1}
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hi-mr src_langtok: 128036; tgt_langtok: 128063
2024-09-04 06:59:33 | INFO | fairseq.data.data_utils | loaded 2,034 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/train.hi-mr.hi
2024-09-04 06:59:33 | INFO | fairseq.data.data_utils | loaded 2,034 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/train.hi-mr.mr
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin train hi-mr 2034 examples
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:hi-mr', 2034)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:hi-mr', 1.0)]
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000455
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7544.5625Mb; avail=247509.5390625Mb
2024-09-04 06:59:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000040
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000498
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7544.5625Mb; avail=247509.5390625Mb
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7544.5625Mb; avail=247509.5390625Mb
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000565
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001382
2024-09-04 06:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7544.5625Mb; avail=247509.5390625Mb
2024-09-04 06:59:33 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-09-04 06:59:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 06:59:33 | INFO | fairseq.trainer | begin training epoch 1
2024-09-04 06:59:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 06:59:38 | INFO | train_inner | epoch 001:      2 / 21 loss=17.717, nll_loss=17.721, ppl=216038, wps=2086.1, ups=0.43, wpb=4524, bsz=56, num_updates=2, lr=2.4e-08, gnorm=9.019, train_wall=5, gb_free=14.5, wall=5
2024-09-04 06:59:43 | INFO | train_inner | epoch 001:      4 / 21 loss=17.723, nll_loss=17.729, ppl=217194, wps=1578.1, ups=0.35, wpb=4539, bsz=116, num_updates=4, lr=4.8e-08, gnorm=8.336, train_wall=6, gb_free=12.6, wall=11
2024-09-04 06:59:48 | INFO | train_inner | epoch 001:      6 / 21 loss=17.648, nll_loss=17.635, ppl=203578, wps=1701.1, ups=0.39, wpb=4332.5, bsz=104, num_updates=6, lr=7.2e-08, gnorm=8.93, train_wall=5, gb_free=12.9, wall=16
2024-09-04 07:00:33 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Version2/checkpoint1.2B', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/Version1/checkpoint1.2B', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2B', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2B', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='hi-mr', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2B', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2B', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='hi-mr', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-09-04 07:00:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-09-04 07:00:33 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-09-04 07:00:34 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-09-04 07:00:42 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-09-04 07:00:42 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-09-04 07:00:42 | INFO | fairseq_cli.train | model: TransformerModel
2024-09-04 07:00:42 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-09-04 07:00:42 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-09-04 07:00:42 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-09-04 07:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-09-04 07:00:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14104.7109375Mb; avail=240957.3984375Mb
2024-09-04 07:00:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 07:00:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:hi-mr': 1}
2024-09-04 07:00:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hi-mr src_langtok: 128036; tgt_langtok: 128063
2024-09-04 07:00:42 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.hi
2024-09-04 07:00:42 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.mr
2024-09-04 07:00:42 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin valid hi-mr 3313 examples
2024-09-04 07:00:43 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-09-04 07:00:43 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-09-04 07:00:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 07:00:43 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-09-04 07:00:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 07:00:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-09-04 07:00:43 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-09-04 07:00:43 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/Version1/checkpoint1.2B: optimizer, lr scheduler, meters, dataloader will be reset
2024-09-04 07:00:43 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/Version1/checkpoint1.2B
2024-09-04 07:00:43 | INFO | fairseq.trainer | No existing checkpoint found /home/krish/content/Version1/checkpoint1.2B
2024-09-04 07:00:43 | INFO | fairseq.trainer | loading train data for epoch 1
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7527.1640625Mb; avail=247526.94140625Mb
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:hi-mr': 1}
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hi-mr src_langtok: 128036; tgt_langtok: 128063
2024-09-04 07:00:43 | INFO | fairseq.data.data_utils | loaded 2,034 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/train.hi-mr.hi
2024-09-04 07:00:43 | INFO | fairseq.data.data_utils | loaded 2,034 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/train.hi-mr.mr
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin train hi-mr 2034 examples
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:hi-mr', 2034)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:hi-mr', 1.0)]
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000463
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7527.16015625Mb; avail=247526.94140625Mb
2024-09-04 07:00:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000043
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000511
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7527.16015625Mb; avail=247526.94140625Mb
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7527.16015625Mb; avail=247526.94140625Mb
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000560
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001384
2024-09-04 07:00:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7527.16015625Mb; avail=247526.94140625Mb
2024-09-04 07:00:43 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-09-04 07:00:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:00:43 | INFO | fairseq.trainer | begin training epoch 1
2024-09-04 07:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:00:48 | INFO | train_inner | epoch 001:      2 / 21 loss=17.717, nll_loss=17.721, ppl=216038, wps=1833.4, ups=0.38, wpb=4524, bsz=56, num_updates=2, lr=2.4e-08, gnorm=9.019, train_wall=5, gb_free=14.5, wall=5
2024-09-04 07:00:54 | INFO | train_inner | epoch 001:      4 / 21 loss=17.723, nll_loss=17.729, ppl=217194, wps=1642.5, ups=0.36, wpb=4539, bsz=116, num_updates=4, lr=4.8e-08, gnorm=8.336, train_wall=6, gb_free=12.6, wall=11
2024-09-04 07:01:00 | INFO | train_inner | epoch 001:      6 / 21 loss=17.648, nll_loss=17.635, ppl=203578, wps=1441.5, ups=0.33, wpb=4332.5, bsz=104, num_updates=6, lr=7.2e-08, gnorm=8.93, train_wall=6, gb_free=12.9, wall=17
2024-09-04 07:01:06 | INFO | train_inner | epoch 001:      8 / 21 loss=17.681, nll_loss=17.676, ppl=209405, wps=1606, ups=0.35, wpb=4598, bsz=92, num_updates=8, lr=9.6e-08, gnorm=8.849, train_wall=6, gb_free=12.1, wall=23
2024-09-04 07:01:11 | INFO | train_inner | epoch 001:     10 / 21 loss=17.692, nll_loss=17.69, ppl=211459, wps=2031, ups=0.4, wpb=5028, bsz=96, num_updates=10, lr=1.2e-07, gnorm=8.556, train_wall=5, gb_free=11, wall=27
2024-09-04 07:01:16 | INFO | train_inner | epoch 001:     12 / 21 loss=17.641, nll_loss=17.626, ppl=202266, wps=1511.7, ups=0.36, wpb=4225, bsz=84, num_updates=12, lr=1.44e-07, gnorm=8.696, train_wall=6, gb_free=15.7, wall=33
2024-09-04 07:01:22 | INFO | train_inner | epoch 001:     14 / 21 loss=17.565, nll_loss=17.53, ppl=189315, wps=1626.2, ups=0.35, wpb=4712.5, bsz=132, num_updates=14, lr=1.68e-07, gnorm=8.658, train_wall=6, gb_free=13.2, wall=39
2024-09-04 07:01:27 | INFO | train_inner | epoch 001:     16 / 21 loss=17.539, nll_loss=17.498, ppl=185079, wps=1977.2, ups=0.43, wpb=4562, bsz=112, num_updates=16, lr=1.92e-07, gnorm=8.107, train_wall=5, gb_free=14.7, wall=43
2024-09-04 07:01:32 | INFO | train_inner | epoch 001:     18 / 21 loss=17.472, nll_loss=17.414, ppl=174614, wps=1691.7, ups=0.34, wpb=4930.5, bsz=84, num_updates=18, lr=2.16e-07, gnorm=8.73, train_wall=6, gb_free=12.6, wall=49
2024-09-04 07:01:37 | INFO | train_inner | epoch 001:     20 / 21 loss=17.396, nll_loss=17.318, ppl=163372, wps=1876, ups=0.48, wpb=3886, bsz=97, num_updates=20, lr=2.4e-07, gnorm=7.959, train_wall=4, gb_free=13.3, wall=53
2024-09-04 07:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=8015.640625Mb; avail=247030.4765625Mb
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000769
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8015.640625Mb; avail=247030.4765625Mb
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012850
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8015.640625Mb; avail=247030.4765625Mb
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011398
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025435
2024-09-04 07:01:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=8015.640625Mb; avail=247030.4765625Mb
2024-09-04 07:01:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.343 | nll_loss 17.253 | ppl 156188 | wps 4691.3 | wpb 2350.9 | bsz 94.7 | num_updates 21
2024-09-04 07:01:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 21 updates
2024-09-04 07:01:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 1 @ 21 updates, score 17.343) (writing took 49.71243111882359 seconds)
2024-09-04 07:02:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-09-04 07:02:46 | INFO | train | epoch 001 | loss 17.601 | nll_loss 17.576 | ppl 195336 | wps 759.3 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 21 | lr 2.52e-07 | gnorm 8.584 | train_wall 56 | gb_free 11.7 | wall 123
2024-09-04 07:02:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:02:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:02:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:02:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000741
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9062.27734375Mb; avail=245983.8125Mb
2024-09-04 07:02:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000594
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9062.27734375Mb; avail=245983.8125Mb
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9062.27734375Mb; avail=245983.8125Mb
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000184
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001124
2024-09-04 07:02:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9062.27734375Mb; avail=245983.8125Mb
2024-09-04 07:02:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:02:46 | INFO | fairseq.trainer | begin training epoch 2
2024-09-04 07:02:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:02:55 | INFO | train_inner | epoch 002:      1 / 21 loss=17.368, nll_loss=17.284, ppl=159574, wps=122.1, ups=0.03, wpb=4786.5, bsz=124, num_updates=22, lr=2.64e-07, gnorm=8.507, train_wall=11, gb_free=12, wall=132
2024-09-04 07:03:00 | INFO | train_inner | epoch 002:      3 / 21 loss=17.266, nll_loss=17.155, ppl=145947, wps=1816.7, ups=0.37, wpb=4967.5, bsz=84, num_updates=24, lr=2.88e-07, gnorm=8.49, train_wall=5, gb_free=13.6, wall=137
2024-09-04 07:03:06 | INFO | train_inner | epoch 002:      5 / 21 loss=17.193, nll_loss=17.062, ppl=136813, wps=1784.5, ups=0.39, wpb=4597, bsz=80, num_updates=26, lr=3.12e-07, gnorm=7.971, train_wall=5, gb_free=12.7, wall=142
2024-09-04 07:03:10 | INFO | train_inner | epoch 002:      7 / 21 loss=17.151, nll_loss=17.01, ppl=131966, wps=1941.9, ups=0.43, wpb=4515.5, bsz=68, num_updates=28, lr=3.36e-07, gnorm=7.825, train_wall=5, gb_free=12.2, wall=147
2024-09-04 07:03:16 | INFO | train_inner | epoch 002:      9 / 21 loss=17.022, nll_loss=16.848, ppl=117947, wps=1542.3, ups=0.33, wpb=4678, bsz=104, num_updates=30, lr=3.6e-07, gnorm=7.933, train_wall=6, gb_free=11.4, wall=153
2024-09-04 07:03:22 | INFO | train_inner | epoch 002:     11 / 21 loss=16.954, nll_loss=16.761, ppl=111077, wps=1747.9, ups=0.35, wpb=4944.5, bsz=104, num_updates=32, lr=3.84e-07, gnorm=7.23, train_wall=6, gb_free=13.1, wall=159
2024-09-04 07:03:26 | INFO | train_inner | epoch 002:     13 / 21 loss=16.843, nll_loss=16.62, ppl=100739, wps=1917.9, ups=0.48, wpb=4025, bsz=69, num_updates=34, lr=4.08e-07, gnorm=7.216, train_wall=4, gb_free=11.1, wall=163
2024-09-04 07:03:32 | INFO | train_inner | epoch 002:     15 / 21 loss=16.763, nll_loss=16.519, ppl=93928.6, wps=1574.4, ups=0.34, wpb=4665.5, bsz=128, num_updates=36, lr=4.32e-07, gnorm=6.834, train_wall=6, gb_free=14, wall=169
2024-09-04 07:03:36 | INFO | train_inner | epoch 002:     17 / 21 loss=16.698, nll_loss=16.436, ppl=88644, wps=1808.5, ups=0.45, wpb=4035, bsz=80, num_updates=38, lr=4.56e-07, gnorm=6.513, train_wall=4, gb_free=16.5, wall=173
2024-09-04 07:03:42 | INFO | train_inner | epoch 002:     19 / 21 loss=16.668, nll_loss=16.397, ppl=86298.8, wps=1782.5, ups=0.4, wpb=4472.5, bsz=132, num_updates=40, lr=4.8e-07, gnorm=6.111, train_wall=5, gb_free=12.8, wall=178
2024-09-04 07:03:47 | INFO | train_inner | epoch 002:     21 / 21 loss=16.559, nll_loss=16.258, ppl=78387.6, wps=1668.9, ups=0.36, wpb=4657.5, bsz=88, num_updates=42, lr=5.04e-07, gnorm=5.962, train_wall=6, gb_free=14.4, wall=184
2024-09-04 07:03:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9115.4296875Mb; avail=245930.64453125Mb
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000655
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9115.4296875Mb; avail=245930.64453125Mb
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012710
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9115.4296875Mb; avail=245930.64453125Mb
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011218
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024952
2024-09-04 07:03:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9115.42578125Mb; avail=245930.64453125Mb
2024-09-04 07:04:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.269 | nll_loss 15.886 | ppl 60576.7 | wps 5025.3 | wpb 2350.9 | bsz 94.7 | num_updates 42 | best_loss 16.269
2024-09-04 07:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 42 updates
2024-09-04 07:04:03 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:05:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 2 @ 42 updates, score 16.269) (writing took 62.01262482162565 seconds)
2024-09-04 07:05:05 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-09-04 07:05:05 | INFO | train | epoch 002 | loss 16.936 | nll_loss 16.737 | ppl 109228 | wps 687.7 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 42 | lr 5.04e-07 | gnorm 7.266 | train_wall 61 | gb_free 14.4 | wall 262
2024-09-04 07:05:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:05:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:05:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:05:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000711
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10784.21875Mb; avail=244261.85546875Mb
2024-09-04 07:05:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000635
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10784.21875Mb; avail=244261.85546875Mb
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10784.21875Mb; avail=244261.85546875Mb
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000182
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001172
2024-09-04 07:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10784.21875Mb; avail=244261.85546875Mb
2024-09-04 07:05:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:05:05 | INFO | fairseq.trainer | begin training epoch 3
2024-09-04 07:05:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:05:10 | INFO | train_inner | epoch 003:      2 / 21 loss=16.438, nll_loss=16.111, ppl=70764.7, wps=106.7, ups=0.02, wpb=4434, bsz=160, num_updates=44, lr=5.28e-07, gnorm=6.148, train_wall=5, gb_free=12.5, wall=267
2024-09-04 07:05:15 | INFO | train_inner | epoch 003:      4 / 21 loss=16.329, nll_loss=15.966, ppl=64001, wps=1614.8, ups=0.39, wpb=4163, bsz=84, num_updates=46, lr=5.52e-07, gnorm=5.339, train_wall=5, gb_free=16.5, wall=272
2024-09-04 07:05:20 | INFO | train_inner | epoch 003:      6 / 21 loss=16.292, nll_loss=15.917, ppl=61867.2, wps=1732.2, ups=0.47, wpb=3699, bsz=45, num_updates=48, lr=5.76e-07, gnorm=5.431, train_wall=4, gb_free=14.5, wall=277
2024-09-04 07:05:26 | INFO | train_inner | epoch 003:      8 / 21 loss=16.11, nll_loss=15.694, ppl=53004.9, wps=1630.5, ups=0.33, wpb=4995.5, bsz=152, num_updates=50, lr=6e-07, gnorm=5.267, train_wall=6, gb_free=10.8, wall=283
2024-09-04 07:05:31 | INFO | train_inner | epoch 003:     10 / 21 loss=16.123, nll_loss=15.711, ppl=53647.6, wps=1648.3, ups=0.37, wpb=4466, bsz=80, num_updates=52, lr=6.24e-07, gnorm=4.721, train_wall=5, gb_free=11.8, wall=288
2024-09-04 07:05:36 | INFO | train_inner | epoch 003:     12 / 21 loss=15.994, nll_loss=15.546, ppl=47855, wps=1957, ups=0.41, wpb=4790, bsz=104, num_updates=54, lr=6.48e-07, gnorm=4.543, train_wall=5, gb_free=11.7, wall=293
2024-09-04 07:05:42 | INFO | train_inner | epoch 003:     14 / 21 loss=15.945, nll_loss=15.486, ppl=45892.8, wps=1640.2, ups=0.32, wpb=5092, bsz=88, num_updates=56, lr=6.72e-07, gnorm=4.336, train_wall=6, gb_free=10.3, wall=299
2024-09-04 07:05:47 | INFO | train_inner | epoch 003:     16 / 21 loss=15.861, nll_loss=15.382, ppl=42714.7, wps=1842.1, ups=0.42, wpb=4359.5, bsz=76, num_updates=58, lr=6.96e-07, gnorm=4.225, train_wall=5, gb_free=11.9, wall=304
2024-09-04 07:05:53 | INFO | train_inner | epoch 003:     18 / 21 loss=15.821, nll_loss=15.331, ppl=41225.3, wps=1667, ups=0.35, wpb=4818, bsz=124, num_updates=60, lr=7.2e-07, gnorm=4.115, train_wall=6, gb_free=12.7, wall=310
2024-09-04 07:05:58 | INFO | train_inner | epoch 003:     20 / 21 loss=15.717, nll_loss=15.201, ppl=37655.2, wps=2071.5, ups=0.42, wpb=4914, bsz=80, num_updates=62, lr=7.44e-07, gnorm=3.976, train_wall=5, gb_free=12.2, wall=314
2024-09-04 07:06:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10824.90625Mb; avail=244221.125Mb
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000755
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10824.90625Mb; avail=244221.125Mb
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012706
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10824.90625Mb; avail=244221.125Mb
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011237
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025114
2024-09-04 07:06:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10824.90625Mb; avail=244221.125Mb
2024-09-04 07:06:12 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Version2/checkpoint1.2_Mr-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/Version1/checkpoint1.2_Mr-Hi', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2_Mr-Hi', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2_Mr-Hi', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-09-04 07:06:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-09-04 07:06:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-09-04 07:06:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-09-04 07:06:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 15.415 | nll_loss 14.816 | ppl 28834.8 | wps 4749.7 | wpb 2350.9 | bsz 94.7 | num_updates 63 | best_loss 15.415
2024-09-04 07:06:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 63 updates
2024-09-04 07:06:17 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:06:21 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-09-04 07:06:21 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-09-04 07:06:21 | INFO | fairseq_cli.train | model: TransformerModel
2024-09-04 07:06:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-09-04 07:06:21 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-09-04 07:06:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-09-04 07:06:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-09-04 07:06:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17831.04296875Mb; avail=237214.9375Mb
2024-09-04 07:06:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 07:06:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:mr-hi': 1}
2024-09-04 07:06:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-09-04 07:06:21 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.mr
2024-09-04 07:06:21 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.hi
2024-09-04 07:06:21 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin valid mr-hi 3313 examples
2024-09-04 07:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:07:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 3 @ 63 updates, score 15.415) (writing took 67.19341467227787 seconds)
2024-09-04 07:07:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-09-04 07:07:24 | INFO | train | epoch 003 | loss 16.038 | nll_loss 15.603 | ppl 49769.3 | wps 689.1 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 63 | lr 7.56e-07 | gnorm 4.766 | train_wall 54 | gb_free 13.1 | wall 401
2024-09-04 07:07:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:07:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:07:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:07:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000607
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13741.5546875Mb; avail=241304.515625Mb
2024-09-04 07:07:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000068
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000595
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13741.5546875Mb; avail=241304.515625Mb
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13741.5546875Mb; avail=241304.515625Mb
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000175
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001077
2024-09-04 07:07:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13741.5546875Mb; avail=241304.515625Mb
2024-09-04 07:07:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:07:24 | INFO | fairseq.trainer | begin training epoch 4
2024-09-04 07:07:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:07:27 | INFO | train_inner | epoch 004:      1 / 21 loss=15.655, nll_loss=15.125, ppl=35724, wps=102.1, ups=0.02, wpb=4546, bsz=68, num_updates=64, lr=7.68e-07, gnorm=3.749, train_wall=5, gb_free=10.8, wall=404
2024-09-04 07:07:32 | INFO | train_inner | epoch 004:      3 / 21 loss=15.566, nll_loss=15.013, ppl=33064.3, wps=1522.1, ups=0.4, wpb=3849, bsz=105, num_updates=66, lr=7.92e-07, gnorm=4.004, train_wall=5, gb_free=18.2, wall=409
2024-09-04 07:07:32 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Version2/checkpoint1.2_Mr-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/Version1/checkpoint1.2_Mr-Hi', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2_Mr-Hi', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2_Mr-Hi', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-09-04 07:07:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-09-04 07:07:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-09-04 07:07:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-09-04 07:07:36 | INFO | train_inner | epoch 004:      5 / 21 loss=15.595, nll_loss=15.049, ppl=33905.4, wps=1920.3, ups=0.44, wpb=4399.5, bsz=84, num_updates=68, lr=8.16e-07, gnorm=3.767, train_wall=5, gb_free=13.7, wall=413
2024-09-04 07:07:41 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-09-04 07:07:41 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-09-04 07:07:41 | INFO | fairseq_cli.train | model: TransformerModel
2024-09-04 07:07:41 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-09-04 07:07:41 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-09-04 07:07:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-09-04 07:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-09-04 07:07:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20763.625Mb; avail=234282.38671875Mb
2024-09-04 07:07:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 07:07:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:mr-hi': 1}
2024-09-04 07:07:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-09-04 07:07:41 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.mr
2024-09-04 07:07:41 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.hi
2024-09-04 07:07:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin valid mr-hi 3313 examples
2024-09-04 07:07:41 | INFO | train_inner | epoch 004:      7 / 21 loss=15.52, nll_loss=14.955, ppl=31762.2, wps=1972.5, ups=0.39, wpb=5074.5, bsz=92, num_updates=70, lr=8.4e-07, gnorm=3.386, train_wall=5, gb_free=12, wall=418
2024-09-04 07:07:42 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-09-04 07:07:42 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-09-04 07:07:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 07:07:42 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-09-04 07:07:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 07:07:42 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-09-04 07:07:42 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-09-04 07:07:47 | INFO | train_inner | epoch 004:      9 / 21 loss=15.352, nll_loss=14.744, ppl=27433.1, wps=1672.1, ups=0.36, wpb=4621.5, bsz=96, num_updates=72, lr=8.64e-07, gnorm=3.323, train_wall=6, gb_free=12.6, wall=424
2024-09-04 07:07:52 | INFO | train_inner | epoch 004:     11 / 21 loss=15.512, nll_loss=14.944, ppl=31526.2, wps=1648.8, ups=0.38, wpb=4317, bsz=56, num_updates=74, lr=8.88e-07, gnorm=3.524, train_wall=5, gb_free=13.4, wall=429
2024-09-04 07:07:58 | INFO | train_inner | epoch 004:     13 / 21 loss=15.268, nll_loss=14.641, ppl=25548, wps=1703.7, ups=0.35, wpb=4930, bsz=180, num_updates=76, lr=9.12e-07, gnorm=3.808, train_wall=6, gb_free=11.6, wall=435
2024-09-04 07:08:03 | INFO | train_inner | epoch 004:     15 / 21 loss=15.305, nll_loss=14.685, ppl=26343.4, wps=1578.1, ups=0.37, wpb=4250.5, bsz=80, num_updates=78, lr=9.36e-07, gnorm=3.288, train_wall=5, gb_free=12.9, wall=440
2024-09-04 07:08:09 | INFO | train_inner | epoch 004:     17 / 21 loss=15.241, nll_loss=14.604, ppl=24909.9, wps=1798.6, ups=0.37, wpb=4806, bsz=56, num_updates=80, lr=9.6e-07, gnorm=3.038, train_wall=5, gb_free=11.4, wall=446
2024-09-04 07:08:15 | INFO | train_inner | epoch 004:     19 / 21 loss=15.24, nll_loss=14.604, ppl=24901, wps=1608.6, ups=0.33, wpb=4832.5, bsz=100, num_updates=82, lr=9.84e-07, gnorm=2.983, train_wall=6, gb_free=12.8, wall=452
2024-09-04 07:08:20 | INFO | train_inner | epoch 004:     21 / 21 loss=15.117, nll_loss=14.45, ppl=22387.4, wps=1503.9, ups=0.35, wpb=4324.5, bsz=124, num_updates=84, lr=1.008e-06, gnorm=3.623, train_wall=6, gb_free=12.9, wall=457
2024-09-04 07:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13806.125Mb; avail=241239.890625Mb
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000748
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13806.125Mb; avail=241239.890625Mb
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012843
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13806.125Mb; avail=241239.890625Mb
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011256
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025220
2024-09-04 07:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13806.6171875Mb; avail=241239.3984375Mb
2024-09-04 07:08:38 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 14.873 | nll_loss 14.131 | ppl 17944.7 | wps 4735.5 | wpb 2350.9 | bsz 94.7 | num_updates 84 | best_loss 14.873
2024-09-04 07:08:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 84 updates
2024-09-04 07:08:38 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:08:41 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Version2/checkpoint1.2B_Mr-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/Version1/checkpoint1.2B_Mr-Hi', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2B_Mr-Hi', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Version2/checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/Version1/checkpoint1.2B_Mr-Hi', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Version2/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-09-04 07:08:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-09-04 07:08:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-09-04 07:08:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-09-04 07:08:49 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-09-04 07:08:49 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-09-04 07:08:49 | INFO | fairseq_cli.train | model: TransformerModel
2024-09-04 07:08:49 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-09-04 07:08:49 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-09-04 07:08:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-09-04 07:08:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-09-04 07:08:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36304.66015625Mb; avail=218741.35546875Mb
2024-09-04 07:08:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 07:08:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:mr-hi': 1}
2024-09-04 07:08:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-09-04 07:08:49 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.mr
2024-09-04 07:08:49 | INFO | fairseq.data.data_utils | loaded 3,313 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/valid.hi-mr.hi
2024-09-04 07:08:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin valid mr-hi 3313 examples
2024-09-04 07:08:50 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-09-04 07:08:50 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-09-04 07:08:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 07:08:50 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-09-04 07:08:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-09-04 07:08:50 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-09-04 07:08:50 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-09-04 07:08:50 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/Version1/checkpoint1.2B_Mr-Hi: optimizer, lr scheduler, meters, dataloader will be reset
2024-09-04 07:08:50 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/Version1/checkpoint1.2B_Mr-Hi
2024-09-04 07:08:50 | INFO | fairseq.trainer | No existing checkpoint found /home/krish/content/Version1/checkpoint1.2B_Mr-Hi
2024-09-04 07:08:50 | INFO | fairseq.trainer | loading train data for epoch 1
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31336.0859375Mb; avail=223702.046875Mb
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:mr-hi': 1}
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-09-04 07:08:50 | INFO | fairseq.data.data_utils | loaded 2,034 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/train.hi-mr.mr
2024-09-04 07:08:50 | INFO | fairseq.data.data_utils | loaded 2,034 examples from: /home/krish/content/Version2/wmt22_spm/wmt22_bin/train.hi-mr.hi
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Version2/wmt22_spm/wmt22_bin train mr-hi 2034 examples
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:mr-hi', 2034)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:mr-hi', 1.0)]
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000456
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31336.08203125Mb; avail=223702.046875Mb
2024-09-04 07:08:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000036
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000494
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31336.08203125Mb; avail=223702.046875Mb
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000025
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31336.08203125Mb; avail=223702.046875Mb
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000534
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001336
2024-09-04 07:08:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31335.58984375Mb; avail=223702.5390625Mb
2024-09-04 07:08:51 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-09-04 07:08:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:08:51 | INFO | fairseq.trainer | begin training epoch 1
2024-09-04 07:08:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:08:57 | INFO | train_inner | epoch 001:      2 / 16 loss=17.629, nll_loss=17.611, ppl=200149, wps=1558.4, ups=0.33, wpb=4314.5, bsz=104, num_updates=2, lr=2.4e-08, gnorm=6.975, train_wall=6, gb_free=9, wall=6
2024-09-04 07:09:02 | INFO | train_inner | epoch 001:      4 / 16 loss=17.579, nll_loss=17.548, ppl=191627, wps=1485.3, ups=0.36, wpb=4163.5, bsz=104, num_updates=4, lr=4.8e-08, gnorm=6.654, train_wall=6, gb_free=10, wall=12
2024-09-04 07:09:08 | INFO | train_inner | epoch 001:      6 / 16 loss=17.585, nll_loss=17.557, ppl=192822, wps=1471.2, ups=0.34, wpb=4270.5, bsz=180, num_updates=6, lr=7.2e-08, gnorm=7.473, train_wall=6, gb_free=10.3, wall=18
2024-09-04 07:09:13 | INFO | train_inner | epoch 001:      8 / 16 loss=17.534, nll_loss=17.493, ppl=184510, wps=1445.5, ups=0.39, wpb=3680.5, bsz=141, num_updates=8, lr=9.6e-08, gnorm=7.206, train_wall=5, gb_free=8.6, wall=23
2024-09-04 07:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:09:19 | INFO | train_inner | epoch 001:     10 / 16 loss=17.582, nll_loss=17.552, ppl=192143, wps=1520.4, ups=0.33, wpb=4556.5, bsz=164, num_updates=10, lr=1.2e-07, gnorm=7.065, train_wall=6, gb_free=9.3, wall=29
2024-09-04 07:09:25 | INFO | train_inner | epoch 001:     12 / 16 loss=17.545, nll_loss=17.505, ppl=185984, wps=1567.6, ups=0.34, wpb=4671.5, bsz=96, num_updates=12, lr=1.44e-07, gnorm=6.54, train_wall=6, gb_free=9.1, wall=35
2024-09-04 07:09:31 | INFO | train_inner | epoch 001:     14 / 16 loss=17.507, nll_loss=17.458, ppl=180105, wps=1549.6, ups=0.34, wpb=4529.5, bsz=160, num_updates=14, lr=1.68e-07, gnorm=6.92, train_wall=6, gb_free=9.7, wall=41
2024-09-04 07:09:36 | INFO | train_inner | epoch 001:     16 / 16 loss=17.494, nll_loss=17.441, ppl=177983, wps=1541.1, ups=0.42, wpb=3705, bsz=68, num_updates=16, lr=1.92e-07, gnorm=6.38, train_wall=5, gb_free=14.4, wall=45
2024-09-04 07:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14599.41796875Mb; avail=240430.640625Mb
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14599.66015625Mb; avail=240430.39453125Mb
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012844
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14599.66015625Mb; avail=240430.39453125Mb
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011203
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025161
2024-09-04 07:09:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14599.66015625Mb; avail=240430.39453125Mb
2024-09-04 07:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 4 @ 84 updates, score 14.873) (writing took 61.48163729254156 seconds)
2024-09-04 07:09:39 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-09-04 07:09:39 | INFO | train | epoch 004 | loss 15.378 | nll_loss 14.777 | ppl 28083.6 | wps 709.1 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 84 | lr 1.008e-06 | gnorm 3.482 | train_wall 56 | gb_free 12.9 | wall 536
2024-09-04 07:09:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:09:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:09:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:09:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000729
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14627.58203125Mb; avail=240402.4296875Mb
2024-09-04 07:09:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000622
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14627.58203125Mb; avail=240402.4296875Mb
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14627.58203125Mb; avail=240402.4296875Mb
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000178
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001129
2024-09-04 07:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14627.58203125Mb; avail=240402.4296875Mb
2024-09-04 07:09:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:09:39 | INFO | fairseq.trainer | begin training epoch 5
2024-09-04 07:09:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:09:45 | INFO | train_inner | epoch 005:      2 / 21 loss=15.108, nll_loss=14.436, ppl=22159.7, wps=102.5, ups=0.02, wpb=4310, bsz=76, num_updates=86, lr=1.032e-06, gnorm=3.183, train_wall=5, gb_free=12.1, wall=541
2024-09-04 07:09:50 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 17.355 | nll_loss 17.268 | ppl 157865 | wps 3909.3 | wpb 2070.5 | bsz 122.7 | num_updates 16
2024-09-04 07:09:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 16 updates
2024-09-04 07:09:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:09:50 | INFO | train_inner | epoch 005:      4 / 21 loss=15.09, nll_loss=14.412, ppl=21806, wps=1672.6, ups=0.35, wpb=4819, bsz=100, num_updates=88, lr=1.056e-06, gnorm=2.848, train_wall=6, gb_free=13.2, wall=547
2024-09-04 07:09:56 | INFO | train_inner | epoch 005:      6 / 21 loss=14.97, nll_loss=14.266, ppl=19701.3, wps=1706.5, ups=0.37, wpb=4578, bsz=104, num_updates=90, lr=1.08e-06, gnorm=2.853, train_wall=5, gb_free=13, wall=553
2024-09-04 07:10:01 | INFO | train_inner | epoch 005:      8 / 21 loss=14.927, nll_loss=14.211, ppl=18962.7, wps=1791.2, ups=0.4, wpb=4487.5, bsz=60, num_updates=92, lr=1.104e-06, gnorm=2.768, train_wall=5, gb_free=13, wall=558
2024-09-04 07:10:06 | INFO | train_inner | epoch 005:     10 / 21 loss=14.931, nll_loss=14.216, ppl=19028.5, wps=1657.9, ups=0.38, wpb=4380.5, bsz=96, num_updates=94, lr=1.128e-06, gnorm=2.721, train_wall=5, gb_free=13.6, wall=563
2024-09-04 07:10:12 | INFO | train_inner | epoch 005:     12 / 21 loss=14.844, nll_loss=14.108, ppl=17658.3, wps=1478, ups=0.32, wpb=4563, bsz=128, num_updates=96, lr=1.152e-06, gnorm=3.218, train_wall=6, gb_free=12.6, wall=569
2024-09-04 07:10:17 | INFO | train_inner | epoch 005:     14 / 21 loss=14.971, nll_loss=14.267, ppl=19716.3, wps=1969.3, ups=0.39, wpb=5047, bsz=76, num_updates=98, lr=1.176e-06, gnorm=2.635, train_wall=5, gb_free=12.6, wall=574
2024-09-04 07:10:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:10:23 | INFO | train_inner | epoch 005:     16 / 21 loss=14.911, nll_loss=14.192, ppl=18722.5, wps=1587.1, ups=0.34, wpb=4636, bsz=108, num_updates=100, lr=1.2e-06, gnorm=2.651, train_wall=6, gb_free=13.2, wall=580
2024-09-04 07:10:29 | INFO | train_inner | epoch 005:     18 / 21 loss=14.661, nll_loss=13.881, ppl=15090.4, wps=1596.9, ups=0.35, wpb=4564.5, bsz=124, num_updates=102, lr=1.224e-06, gnorm=2.771, train_wall=6, gb_free=13.6, wall=586
2024-09-04 07:10:34 | INFO | train_inner | epoch 005:     20 / 21 loss=14.724, nll_loss=13.96, ppl=15936.1, wps=1834.4, ups=0.36, wpb=5031.5, bsz=132, num_updates=104, lr=1.248e-06, gnorm=2.619, train_wall=5, gb_free=12.3, wall=591
2024-09-04 07:10:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16658.56640625Mb; avail=238371.40234375Mb
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000747
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16658.56640625Mb; avail=238371.89453125Mb
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012979
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16658.56640625Mb; avail=238371.40234375Mb
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011345
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025463
2024-09-04 07:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16658.80078125Mb; avail=238371.66015625Mb
2024-09-04 07:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 1 @ 16 updates, score 17.355) (writing took 46.95631869044155 seconds)
2024-09-04 07:10:37 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-09-04 07:10:37 | INFO | train | epoch 001 | loss 17.558 | nll_loss 17.522 | ppl 188226 | wps 618.6 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 16 | lr 1.92e-07 | gnorm 6.902 | train_wall 45 | gb_free 14.4 | wall 106
2024-09-04 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000780
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16677.453125Mb; avail=238352.5078125Mb
2024-09-04 07:10:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000584
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16677.453125Mb; avail=238352.5078125Mb
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16677.453125Mb; avail=238352.5078125Mb
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000172
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001103
2024-09-04 07:10:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16677.453125Mb; avail=238352.5078125Mb
2024-09-04 07:10:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:10:37 | INFO | fairseq.trainer | begin training epoch 2
2024-09-04 07:10:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:10:43 | INFO | train_inner | epoch 002:      2 / 16 loss=17.4, nll_loss=17.324, ppl=164105, wps=123.9, ups=0.03, wpb=4149, bsz=156, num_updates=18, lr=2.16e-07, gnorm=7.254, train_wall=6, gb_free=9.8, wall=112
2024-09-04 07:10:53 | INFO | train_inner | epoch 002:      4 / 16 loss=17.443, nll_loss=17.378, ppl=170294, wps=877.5, ups=0.19, wpb=4585.5, bsz=120, num_updates=20, lr=2.4e-07, gnorm=5.961, train_wall=10, gb_free=12.5, wall=123
2024-09-04 07:10:55 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 14.49 | nll_loss 13.655 | ppl 12902 | wps 4439.1 | wpb 2350.9 | bsz 94.7 | num_updates 105 | best_loss 14.49
2024-09-04 07:10:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 105 updates
2024-09-04 07:10:55 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:10:58 | INFO | train_inner | epoch 002:      6 / 16 loss=17.352, nll_loss=17.264, ppl=157384, wps=1418.7, ups=0.39, wpb=3676.5, bsz=77, num_updates=22, lr=2.64e-07, gnorm=6.543, train_wall=5, gb_free=13.7, wall=128
2024-09-04 07:11:05 | INFO | train_inner | epoch 002:      8 / 16 loss=17.225, nll_loss=17.106, ppl=141070, wps=1480.6, ups=0.33, wpb=4518, bsz=192, num_updates=24, lr=2.88e-07, gnorm=7.133, train_wall=6, gb_free=9.8, wall=134
2024-09-04 07:11:10 | INFO | train_inner | epoch 002:     10 / 16 loss=17.283, nll_loss=17.177, ppl=148152, wps=1525.3, ups=0.38, wpb=3966.5, bsz=96, num_updates=26, lr=3.12e-07, gnorm=5.918, train_wall=5, gb_free=10.7, wall=139
2024-09-04 07:11:16 | INFO | train_inner | epoch 002:     12 / 16 loss=17.091, nll_loss=16.938, ppl=125553, wps=1493.6, ups=0.33, wpb=4534.5, bsz=196, num_updates=28, lr=3.36e-07, gnorm=6.739, train_wall=6, gb_free=11.3, wall=145
2024-09-04 07:11:22 | INFO | train_inner | epoch 002:     14 / 16 loss=17.102, nll_loss=16.951, ppl=126726, wps=1489.3, ups=0.35, wpb=4270, bsz=84, num_updates=30, lr=3.6e-07, gnorm=6.191, train_wall=6, gb_free=10.3, wall=151
2024-09-04 07:11:27 | INFO | train_inner | epoch 002:     16 / 16 loss=17.035, nll_loss=16.867, ppl=119510, wps=1463.4, ups=0.35, wpb=4191.5, bsz=96, num_updates=32, lr=3.84e-07, gnorm=5.931, train_wall=6, gb_free=9.8, wall=157
2024-09-04 07:11:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28158.98828125Mb; avail=226870.9921875Mb
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000619
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28158.98828125Mb; avail=226870.9921875Mb
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012500
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28158.49609375Mb; avail=226871.484375Mb
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011141
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024640
2024-09-04 07:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28158.49609375Mb; avail=226870.9921875Mb
2024-09-04 07:11:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 16.757 | nll_loss 16.518 | ppl 93853.2 | wps 3844.1 | wpb 2070.5 | bsz 122.7 | num_updates 32 | best_loss 16.757
2024-09-04 07:11:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 32 updates
2024-09-04 07:11:42 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:11:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:12:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 5 @ 105 updates, score 14.49) (writing took 77.86240273620933 seconds)
2024-09-04 07:12:13 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-09-04 07:12:13 | INFO | train | epoch 005 | loss 14.907 | nll_loss 14.187 | ppl 18649.9 | wps 623.1 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 105 | lr 1.26e-06 | gnorm 2.836 | train_wall 57 | gb_free 15 | wall 690
2024-09-04 07:12:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:12:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:12:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:12:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000624
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29223.453125Mb; avail=225806.54296875Mb
2024-09-04 07:12:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000588
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29223.453125Mb; avail=225806.54296875Mb
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29223.453125Mb; avail=225806.54296875Mb
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001062
2024-09-04 07:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29223.453125Mb; avail=225806.54296875Mb
2024-09-04 07:12:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:12:13 | INFO | fairseq.trainer | begin training epoch 6
2024-09-04 07:12:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:12:16 | INFO | train_inner | epoch 006:      1 / 21 loss=14.683, nll_loss=13.907, ppl=15366.1, wps=67.6, ups=0.02, wpb=3434.5, bsz=61, num_updates=106, lr=1.272e-06, gnorm=2.76, train_wall=5, gb_free=12.2, wall=693
2024-09-04 07:12:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:12:21 | INFO | train_inner | epoch 006:      3 / 21 loss=14.759, nll_loss=14.002, ppl=16410.6, wps=2001.4, ups=0.41, wpb=4908, bsz=64, num_updates=108, lr=1.296e-06, gnorm=2.443, train_wall=5, gb_free=13.4, wall=698
2024-09-04 07:12:25 | INFO | train_inner | epoch 006:      5 / 21 loss=14.676, nll_loss=13.898, ppl=15263.8, wps=1858.2, ups=0.47, wpb=3941.5, bsz=65, num_updates=110, lr=1.32e-06, gnorm=2.57, train_wall=4, gb_free=12.3, wall=702
2024-09-04 07:12:30 | INFO | train_inner | epoch 006:      7 / 21 loss=14.619, nll_loss=13.826, ppl=14525.5, wps=1864.4, ups=0.38, wpb=4926, bsz=124, num_updates=112, lr=1.344e-06, gnorm=2.494, train_wall=5, gb_free=12.9, wall=707
2024-09-04 07:12:36 | INFO | train_inner | epoch 006:      9 / 21 loss=14.521, nll_loss=13.707, ppl=13370, wps=1598, ups=0.37, wpb=4291, bsz=148, num_updates=114, lr=1.368e-06, gnorm=2.84, train_wall=5, gb_free=12.1, wall=713
2024-09-04 07:12:41 | INFO | train_inner | epoch 006:     11 / 21 loss=14.554, nll_loss=13.747, ppl=13747.3, wps=1982.3, ups=0.42, wpb=4694.5, bsz=108, num_updates=116, lr=1.392e-06, gnorm=2.406, train_wall=5, gb_free=12.5, wall=717
2024-09-04 07:12:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 2 @ 32 updates, score 16.757) (writing took 62.98991642333567 seconds)
2024-09-04 07:12:45 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-09-04 07:12:45 | INFO | train | epoch 002 | loss 17.239 | nll_loss 17.123 | ppl 142770 | wps 530.6 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 32 | lr 3.84e-07 | gnorm 6.459 | train_wall 50 | gb_free 9.8 | wall 234
2024-09-04 07:12:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:12:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:12:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:12:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000679
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13930.45703125Mb; avail=241099.53125Mb
2024-09-04 07:12:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000615
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13930.45703125Mb; avail=241099.53125Mb
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13930.45703125Mb; avail=241099.53125Mb
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000161
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001094
2024-09-04 07:12:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13930.45703125Mb; avail=241099.53125Mb
2024-09-04 07:12:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:12:45 | INFO | fairseq.trainer | begin training epoch 3
2024-09-04 07:12:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:12:46 | INFO | train_inner | epoch 006:     13 / 21 loss=14.564, nll_loss=13.76, ppl=13876.7, wps=1691.3, ups=0.39, wpb=4349, bsz=76, num_updates=118, lr=1.416e-06, gnorm=2.327, train_wall=5, gb_free=11.8, wall=723
2024-09-04 07:12:50 | INFO | train_inner | epoch 003:      2 / 16 loss=16.934, nll_loss=16.74, ppl=109431, wps=92.6, ups=0.02, wpb=3829.5, bsz=120, num_updates=34, lr=4.08e-07, gnorm=6.016, train_wall=5, gb_free=14.8, wall=240
2024-09-04 07:12:50 | INFO | train_inner | epoch 006:     15 / 21 loss=14.636, nll_loss=13.851, ppl=14774.2, wps=1847.7, ups=0.42, wpb=4359.5, bsz=68, num_updates=120, lr=1.44e-06, gnorm=2.558, train_wall=5, gb_free=14.7, wall=727
2024-09-04 07:12:55 | INFO | train_inner | epoch 003:      4 / 16 loss=16.94, nll_loss=16.748, ppl=110099, wps=1414.4, ups=0.43, wpb=3272.5, bsz=49, num_updates=36, lr=4.32e-07, gnorm=5.54, train_wall=5, gb_free=14.5, wall=244
2024-09-04 07:12:57 | INFO | train_inner | epoch 006:     17 / 21 loss=14.435, nll_loss=13.598, ppl=12402.7, wps=1719.2, ups=0.33, wpb=5278.5, bsz=128, num_updates=122, lr=1.464e-06, gnorm=2.424, train_wall=6, gb_free=11.2, wall=733
2024-09-04 07:13:01 | INFO | train_inner | epoch 003:      6 / 16 loss=16.795, nll_loss=16.566, ppl=97048.6, wps=1437.2, ups=0.33, wpb=4385.5, bsz=124, num_updates=38, lr=4.56e-07, gnorm=5.169, train_wall=6, gb_free=9.6, wall=250
2024-09-04 07:13:02 | INFO | train_inner | epoch 006:     19 / 21 loss=14.551, nll_loss=13.744, ppl=13722.3, wps=1528.6, ups=0.36, wpb=4271.5, bsz=84, num_updates=124, lr=1.488e-06, gnorm=2.315, train_wall=6, gb_free=12.8, wall=739
2024-09-04 07:13:06 | INFO | train_inner | epoch 003:      8 / 16 loss=16.789, nll_loss=16.557, ppl=96435.9, wps=1608, ups=0.35, wpb=4600.5, bsz=120, num_updates=40, lr=4.8e-07, gnorm=5.126, train_wall=6, gb_free=11.2, wall=256
2024-09-04 07:13:08 | INFO | train_inner | epoch 006:     21 / 21 loss=14.407, nll_loss=13.565, ppl=12116.5, wps=1705.9, ups=0.35, wpb=4811, bsz=104, num_updates=126, lr=1.512e-06, gnorm=2.322, train_wall=6, gb_free=12, wall=745
2024-09-04 07:13:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13994.76171875Mb; avail=241035.18359375Mb
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000730
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13994.76171875Mb; avail=241035.18359375Mb
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012814
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13994.76171875Mb; avail=241035.18359375Mb
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011257
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025209
2024-09-04 07:13:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13994.76171875Mb; avail=241035.18359375Mb
2024-09-04 07:13:12 | INFO | train_inner | epoch 003:     10 / 16 loss=16.768, nll_loss=16.531, ppl=94665.3, wps=1530.4, ups=0.37, wpb=4132, bsz=100, num_updates=42, lr=5.04e-07, gnorm=5.112, train_wall=5, gb_free=11, wall=261
2024-09-04 07:13:18 | INFO | train_inner | epoch 003:     12 / 16 loss=16.583, nll_loss=16.301, ppl=80721.1, wps=1544.4, ups=0.33, wpb=4683.5, bsz=172, num_updates=44, lr=5.28e-07, gnorm=4.97, train_wall=6, gb_free=9.4, wall=267
2024-09-04 07:13:24 | INFO | train_inner | epoch 003:     14 / 16 loss=16.571, nll_loss=16.283, ppl=79766.1, wps=1528.7, ups=0.33, wpb=4676, bsz=132, num_updates=46, lr=5.52e-07, gnorm=4.433, train_wall=6, gb_free=8.7, wall=274
2024-09-04 07:13:24 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 14.241 | nll_loss 13.346 | ppl 10409 | wps 4913.8 | wpb 2350.9 | bsz 94.7 | num_updates 126 | best_loss 14.241
2024-09-04 07:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 126 updates
2024-09-04 07:13:24 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:13:30 | INFO | train_inner | epoch 003:     16 / 16 loss=16.412, nll_loss=16.085, ppl=69522.7, wps=1435.1, ups=0.33, wpb=4312, bsz=200, num_updates=48, lr=5.76e-07, gnorm=5.433, train_wall=6, gb_free=9.5, wall=280
2024-09-04 07:13:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18953.26171875Mb; avail=236076.640625Mb
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18953.75390625Mb; avail=236076.1484375Mb
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012802
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18953.75390625Mb; avail=236076.1484375Mb
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011158
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025060
2024-09-04 07:13:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18953.75390625Mb; avail=236076.1484375Mb
2024-09-04 07:13:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 16.127 | nll_loss 15.718 | ppl 53917.1 | wps 3834.7 | wpb 2070.5 | bsz 122.7 | num_updates 48 | best_loss 16.127
2024-09-04 07:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 48 updates
2024-09-04 07:13:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:14:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:14:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 6 @ 126 updates, score 14.241) (writing took 63.21590994298458 seconds)
2024-09-04 07:14:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-09-04 07:14:28 | INFO | train | epoch 006 | loss 14.573 | nll_loss 13.77 | ppl 13973.1 | wps 709.2 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 126 | lr 1.512e-06 | gnorm 2.471 | train_wall 55 | gb_free 12 | wall 824
2024-09-04 07:14:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:14:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:14:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:14:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001438
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34102.7578125Mb; avail=220922.203125Mb
2024-09-04 07:14:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000124
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001290
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34108.171875Mb; avail=220924.171875Mb
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34103.25Mb; avail=220922.203125Mb
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000283
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002090
2024-09-04 07:14:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34106.6953125Mb; avail=220918.7578125Mb
2024-09-04 07:14:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:14:28 | INFO | fairseq.trainer | begin training epoch 7
2024-09-04 07:14:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:14:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:14:34 | INFO | train_inner | epoch 007:      2 / 21 loss=14.41, nll_loss=13.568, ppl=12141, wps=111.2, ups=0.02, wpb=4770, bsz=92, num_updates=128, lr=1.536e-06, gnorm=2.239, train_wall=6, gb_free=11.7, wall=831
2024-09-04 07:14:38 | INFO | train_inner | epoch 007:      4 / 21 loss=14.422, nll_loss=13.584, ppl=12275.8, wps=1856.3, ups=0.43, wpb=4330, bsz=76, num_updates=130, lr=1.56e-06, gnorm=2.262, train_wall=5, gb_free=13.8, wall=835
2024-09-04 07:14:44 | INFO | train_inner | epoch 007:      6 / 21 loss=14.353, nll_loss=13.497, ppl=11558.2, wps=1689.2, ups=0.35, wpb=4818, bsz=116, num_updates=132, lr=1.584e-06, gnorm=2.227, train_wall=6, gb_free=14.6, wall=841
2024-09-04 07:14:49 | INFO | train_inner | epoch 007:      8 / 21 loss=14.365, nll_loss=13.514, ppl=11695, wps=2015.2, ups=0.41, wpb=4887, bsz=104, num_updates=134, lr=1.608e-06, gnorm=2.235, train_wall=5, gb_free=13.1, wall=846
2024-09-04 07:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 3 @ 48 updates, score 16.127) (writing took 69.41156579367816 seconds)
2024-09-04 07:14:54 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-09-04 07:14:54 | INFO | train | epoch 003 | loss 16.712 | nll_loss 16.461 | ppl 90208.1 | wps 524.5 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 48 | lr 5.76e-07 | gnorm 5.225 | train_wall 45 | gb_free 9.5 | wall 363
2024-09-04 07:14:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:14:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:14:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:14:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000785
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17026.1484375Mb; avail=237999.75390625Mb
2024-09-04 07:14:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000056
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000620
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17026.1484375Mb; avail=237999.75390625Mb
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17026.1484375Mb; avail=237999.75390625Mb
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000186
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001140
2024-09-04 07:14:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17026.1484375Mb; avail=237999.75390625Mb
2024-09-04 07:14:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:14:54 | INFO | fairseq.trainer | begin training epoch 4
2024-09-04 07:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:14:54 | INFO | train_inner | epoch 007:     10 / 21 loss=14.372, nll_loss=13.521, ppl=11759.1, wps=1595.4, ups=0.36, wpb=4391.5, bsz=80, num_updates=136, lr=1.632e-06, gnorm=2.283, train_wall=5, gb_free=13.2, wall=851
2024-09-04 07:15:00 | INFO | train_inner | epoch 004:      2 / 16 loss=16.38, nll_loss=16.044, ppl=67567.1, wps=93.4, ups=0.02, wpb=4180.5, bsz=112, num_updates=50, lr=6e-07, gnorm=4.155, train_wall=6, gb_free=10.6, wall=369
2024-09-04 07:15:00 | INFO | train_inner | epoch 007:     12 / 21 loss=14.385, nll_loss=13.537, ppl=11889.2, wps=1580.4, ups=0.36, wpb=4437.5, bsz=88, num_updates=138, lr=1.656e-06, gnorm=2.173, train_wall=6, gb_free=14.5, wall=857
2024-09-04 07:15:05 | INFO | train_inner | epoch 004:      4 / 16 loss=16.42, nll_loss=16.092, ppl=69859.6, wps=1467.1, ups=0.41, wpb=3611, bsz=89, num_updates=52, lr=6.24e-07, gnorm=4.29, train_wall=5, gb_free=10.2, wall=374
2024-09-04 07:15:06 | INFO | train_inner | epoch 007:     14 / 21 loss=14.152, nll_loss=13.246, ppl=9715.48, wps=1637.9, ups=0.35, wpb=4731.5, bsz=164, num_updates=140, lr=1.68e-06, gnorm=2.608, train_wall=6, gb_free=12.6, wall=863
2024-09-04 07:15:10 | INFO | train_inner | epoch 004:      6 / 16 loss=16.313, nll_loss=15.959, ppl=63680.5, wps=1534.9, ups=0.38, wpb=4081, bsz=104, num_updates=54, lr=6.48e-07, gnorm=4.009, train_wall=5, gb_free=9.7, wall=379
2024-09-04 07:15:11 | INFO | train_inner | epoch 007:     16 / 21 loss=14.256, nll_loss=13.378, ppl=10648.8, wps=1836.7, ups=0.41, wpb=4441, bsz=112, num_updates=142, lr=1.704e-06, gnorm=2.608, train_wall=5, gb_free=13, wall=867
2024-09-04 07:15:16 | INFO | train_inner | epoch 007:     18 / 21 loss=14.234, nll_loss=13.349, ppl=10434, wps=1430.1, ups=0.38, wpb=3718, bsz=69, num_updates=144, lr=1.728e-06, gnorm=2.292, train_wall=5, gb_free=12.2, wall=873
2024-09-04 07:15:21 | INFO | train_inner | epoch 007:     20 / 21 loss=14.342, nll_loss=13.485, ppl=11465.4, wps=1955.8, ups=0.39, wpb=4960, bsz=80, num_updates=146, lr=1.752e-06, gnorm=2.112, train_wall=5, gb_free=13, wall=878
2024-09-04 07:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13193.29296875Mb; avail=241832.60546875Mb
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13193.29296875Mb; avail=241832.60546875Mb
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012895
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13193.29296875Mb; avail=241832.60546875Mb
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011314
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025340
2024-09-04 07:15:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13193.29296875Mb; avail=241832.60546875Mb
2024-09-04 07:15:25 | INFO | train_inner | epoch 004:      8 / 16 loss=16.34, nll_loss=15.991, ppl=65106.5, wps=513.3, ups=0.13, wpb=3945, bsz=80, num_updates=56, lr=6.72e-07, gnorm=3.656, train_wall=15, gb_free=14.7, wall=395
2024-09-04 07:15:31 | INFO | train_inner | epoch 004:     10 / 16 loss=16.046, nll_loss=15.623, ppl=50473.6, wps=1488.4, ups=0.33, wpb=4469, bsz=208, num_updates=58, lr=6.96e-07, gnorm=4.221, train_wall=6, gb_free=9, wall=401
2024-09-04 07:15:37 | INFO | train_inner | epoch 004:     12 / 16 loss=16.168, nll_loss=15.775, ppl=56060.3, wps=1558.4, ups=0.34, wpb=4586, bsz=116, num_updates=60, lr=7.2e-07, gnorm=3.467, train_wall=6, gb_free=10.6, wall=407
2024-09-04 07:15:41 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 14.059 | nll_loss 13.12 | ppl 8904.06 | wps 4590.2 | wpb 2350.9 | bsz 94.7 | num_updates 147 | best_loss 14.059
2024-09-04 07:15:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 147 updates
2024-09-04 07:15:41 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:15:43 | INFO | train_inner | epoch 004:     14 / 16 loss=16.172, nll_loss=15.779, ppl=56237.1, wps=1534.6, ups=0.33, wpb=4705.5, bsz=112, num_updates=62, lr=7.44e-07, gnorm=3.168, train_wall=6, gb_free=7.7, wall=413
2024-09-04 07:15:49 | INFO | train_inner | epoch 004:     16 / 16 loss=15.906, nll_loss=15.445, ppl=44614.4, wps=1403.3, ups=0.33, wpb=4313.5, bsz=196, num_updates=64, lr=7.68e-07, gnorm=3.938, train_wall=6, gb_free=9.1, wall=419
2024-09-04 07:15:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19503.64453125Mb; avail=235522.25Mb
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000644
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19505.12109375Mb; avail=235520.7734375Mb
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012730
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19533.66796875Mb; avail=235491.734375Mb
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011194
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024941
2024-09-04 07:15:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19560.73828125Mb; avail=235465.15625Mb
2024-09-04 07:16:04 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 15.659 | nll_loss 15.112 | ppl 35408.1 | wps 3842.2 | wpb 2070.5 | bsz 122.7 | num_updates 64 | best_loss 15.659
2024-09-04 07:16:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 64 updates
2024-09-04 07:16:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:16:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:16:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:16:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 7 @ 147 updates, score 14.059) (writing took 69.00005174335092 seconds)
2024-09-04 07:16:50 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-09-04 07:16:50 | INFO | train | epoch 007 | loss 14.331 | nll_loss 13.47 | ppl 11345.2 | wps 669.4 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 147 | lr 1.764e-06 | gnorm 2.297 | train_wall 56 | gb_free 12.4 | wall 967
2024-09-04 07:16:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:16:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:16:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:16:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000653
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18858.703125Mb; avail=236171.28125Mb
2024-09-04 07:16:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000590
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18858.703125Mb; avail=236171.28125Mb
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18858.703125Mb; avail=236171.28125Mb
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001089
2024-09-04 07:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18858.703125Mb; avail=236171.28125Mb
2024-09-04 07:16:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:16:51 | INFO | fairseq.trainer | begin training epoch 8
2024-09-04 07:16:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:16:53 | INFO | train_inner | epoch 008:      1 / 21 loss=14.261, nll_loss=13.384, ppl=10687, wps=105.6, ups=0.02, wpb=4868.5, bsz=80, num_updates=148, lr=1.776e-06, gnorm=2.131, train_wall=5, gb_free=14, wall=970
2024-09-04 07:16:59 | INFO | train_inner | epoch 008:      3 / 21 loss=14.173, nll_loss=13.274, ppl=9905.97, wps=1593.9, ups=0.36, wpb=4433, bsz=84, num_updates=150, lr=1.8e-06, gnorm=2.215, train_wall=6, gb_free=12.4, wall=976
2024-09-04 07:17:04 | INFO | train_inner | epoch 008:      5 / 21 loss=14.178, nll_loss=13.279, ppl=9941.14, wps=1580.5, ups=0.35, wpb=4475, bsz=104, num_updates=152, lr=1.824e-06, gnorm=2.273, train_wall=6, gb_free=14.2, wall=981
2024-09-04 07:17:09 | INFO | train_inner | epoch 008:      7 / 21 loss=14.312, nll_loss=13.449, ppl=11184.4, wps=1598.9, ups=0.38, wpb=4157.5, bsz=76, num_updates=154, lr=1.848e-06, gnorm=2.231, train_wall=5, gb_free=12.6, wall=986
2024-09-04 07:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 4 @ 64 updates, score 15.659) (writing took 68.30713703576475 seconds)
2024-09-04 07:17:12 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-09-04 07:17:12 | INFO | train | epoch 004 | loss 16.21 | nll_loss 15.828 | ppl 58159.7 | wps 490.6 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 64 | lr 7.68e-07 | gnorm 3.863 | train_wall 55 | gb_free 9.1 | wall 502
2024-09-04 07:17:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:17:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:17:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:17:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000695
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18903.90625Mb; avail=236126.03515625Mb
2024-09-04 07:17:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000806
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18903.90625Mb; avail=236126.03515625Mb
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18903.90625Mb; avail=236126.03515625Mb
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000170
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001363
2024-09-04 07:17:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18903.90234375Mb; avail=236126.03515625Mb
2024-09-04 07:17:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:17:12 | INFO | fairseq.trainer | begin training epoch 5
2024-09-04 07:17:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:17:15 | INFO | train_inner | epoch 008:      9 / 21 loss=14.167, nll_loss=13.268, ppl=9867.41, wps=1613.6, ups=0.34, wpb=4683, bsz=76, num_updates=156, lr=1.872e-06, gnorm=2.261, train_wall=6, gb_free=12.2, wall=992
2024-09-04 07:17:18 | INFO | train_inner | epoch 005:      2 / 16 loss=16.084, nll_loss=15.667, ppl=52011.4, wps=95.9, ups=0.02, wpb=4230.5, bsz=80, num_updates=66, lr=7.92e-07, gnorm=3.013, train_wall=5, gb_free=13.9, wall=507
2024-09-04 07:17:21 | INFO | train_inner | epoch 008:     11 / 21 loss=14.102, nll_loss=13.185, ppl=9311.8, wps=1563.9, ups=0.35, wpb=4497, bsz=116, num_updates=158, lr=1.896e-06, gnorm=2.22, train_wall=6, gb_free=12.8, wall=998
2024-09-04 07:17:23 | INFO | train_inner | epoch 005:      4 / 16 loss=16.113, nll_loss=15.703, ppl=53324.2, wps=1539.7, ups=0.37, wpb=4163.5, bsz=84, num_updates=68, lr=8.16e-07, gnorm=2.97, train_wall=5, gb_free=12.2, wall=513
2024-09-04 07:17:26 | INFO | train_inner | epoch 008:     13 / 21 loss=14.206, nll_loss=13.318, ppl=10211.9, wps=1976.2, ups=0.4, wpb=4896, bsz=80, num_updates=160, lr=1.92e-06, gnorm=2.077, train_wall=5, gb_free=13.8, wall=1003
2024-09-04 07:17:29 | INFO | train_inner | epoch 005:      6 / 16 loss=15.854, nll_loss=15.379, ppl=42605.7, wps=1495.1, ups=0.34, wpb=4419, bsz=148, num_updates=70, lr=8.4e-07, gnorm=2.902, train_wall=6, gb_free=10.2, wall=518
2024-09-04 07:17:32 | INFO | train_inner | epoch 008:     15 / 21 loss=14.187, nll_loss=13.295, ppl=10049.5, wps=1590.4, ups=0.34, wpb=4733.5, bsz=88, num_updates=162, lr=1.944e-06, gnorm=2.083, train_wall=6, gb_free=14.7, wall=1009
2024-09-04 07:17:35 | INFO | train_inner | epoch 005:      8 / 16 loss=15.872, nll_loss=15.398, ppl=43177.8, wps=1529.2, ups=0.32, wpb=4753.5, bsz=120, num_updates=72, lr=8.64e-07, gnorm=2.69, train_wall=6, gb_free=10.4, wall=525
2024-09-04 07:17:37 | INFO | train_inner | epoch 008:     17 / 21 loss=14.095, nll_loss=13.178, ppl=9266.54, wps=1770.8, ups=0.39, wpb=4590, bsz=108, num_updates=164, lr=1.968e-06, gnorm=2.281, train_wall=5, gb_free=13.4, wall=1014
2024-09-04 07:17:40 | INFO | train_inner | epoch 005:     10 / 16 loss=15.912, nll_loss=15.449, ppl=44727.3, wps=1509, ups=0.4, wpb=3790, bsz=120, num_updates=74, lr=8.88e-07, gnorm=3.114, train_wall=5, gb_free=9.3, wall=530
2024-09-04 07:17:43 | INFO | train_inner | epoch 008:     19 / 21 loss=14.103, nll_loss=13.187, ppl=9327.2, wps=1796.3, ups=0.37, wpb=4879.5, bsz=120, num_updates=166, lr=1.992e-06, gnorm=2.126, train_wall=5, gb_free=11, wall=1019
2024-09-04 07:17:46 | INFO | train_inner | epoch 005:     12 / 16 loss=15.72, nll_loss=15.209, ppl=37887.4, wps=1492.9, ups=0.34, wpb=4407.5, bsz=132, num_updates=76, lr=9.12e-07, gnorm=2.749, train_wall=6, gb_free=9.9, wall=536
2024-09-04 07:17:48 | INFO | train_inner | epoch 008:     21 / 21 loss=13.937, nll_loss=12.979, ppl=8072.72, wps=1435.9, ups=0.36, wpb=3984.5, bsz=121, num_updates=168, lr=2.016e-06, gnorm=2.907, train_wall=6, gb_free=12.9, wall=1025
2024-09-04 07:17:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18948.4453125Mb; avail=236081.453125Mb
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000754
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18948.4453125Mb; avail=236081.453125Mb
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012747
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18948.4453125Mb; avail=236081.453125Mb
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011322
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025202
2024-09-04 07:17:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18948.4453125Mb; avail=236081.453125Mb
2024-09-04 07:17:52 | INFO | train_inner | epoch 005:     14 / 16 loss=15.757, nll_loss=15.255, ppl=39109.4, wps=1559.1, ups=0.34, wpb=4606, bsz=140, num_updates=78, lr=9.36e-07, gnorm=2.619, train_wall=6, gb_free=11.5, wall=542
2024-09-04 07:17:57 | INFO | train_inner | epoch 005:     16 / 16 loss=15.445, nll_loss=14.863, ppl=29797.1, wps=1341.1, ups=0.38, wpb=3521.5, bsz=193, num_updates=80, lr=9.6e-07, gnorm=4.147, train_wall=5, gb_free=9.1, wall=547
2024-09-04 07:17:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18971.25Mb; avail=236058.60546875Mb
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000744
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18971.7421875Mb; avail=236058.11328125Mb
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012964
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18971.7421875Mb; avail=236058.11328125Mb
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011266
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025371
2024-09-04 07:17:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18971.7421875Mb; avail=236058.11328125Mb
2024-09-04 07:18:05 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 13.928 | nll_loss 12.958 | ppl 7958.3 | wps 4716.5 | wpb 2350.9 | bsz 94.7 | num_updates 168 | best_loss 13.928
2024-09-04 07:18:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 168 updates
2024-09-04 07:18:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:18:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 15.35 | nll_loss 14.715 | ppl 26901.9 | wps 3838.7 | wpb 2070.5 | bsz 122.7 | num_updates 80 | best_loss 15.35
2024-09-04 07:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 80 updates
2024-09-04 07:18:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:18:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:18:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:19:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 8 @ 168 updates, score 13.928) (writing took 63.08921506907791 seconds)
2024-09-04 07:19:08 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-09-04 07:19:08 | INFO | train | epoch 008 | loss 14.15 | nll_loss 13.246 | ppl 9715.1 | wps 693.6 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 168 | lr 2.016e-06 | gnorm 2.259 | train_wall 57 | gb_free 12.9 | wall 1105
2024-09-04 07:19:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:19:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:19:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:19:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000766
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23924.39453125Mb; avail=231105.47265625Mb
2024-09-04 07:19:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000619
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23924.39453125Mb; avail=231105.47265625Mb
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23924.39453125Mb; avail=231105.47265625Mb
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000206
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001194
2024-09-04 07:19:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23924.39453125Mb; avail=231105.47265625Mb
2024-09-04 07:19:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:19:08 | INFO | fairseq.trainer | begin training epoch 9
2024-09-04 07:19:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:19:14 | INFO | train_inner | epoch 009:      2 / 21 loss=14.097, nll_loss=13.181, ppl=9284.18, wps=107.1, ups=0.02, wpb=4581, bsz=80, num_updates=170, lr=2.04e-06, gnorm=2.041, train_wall=5, gb_free=13.1, wall=1111
2024-09-04 07:19:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 5 @ 80 updates, score 15.35) (writing took 67.38181626331061 seconds)
2024-09-04 07:19:19 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-09-04 07:19:19 | INFO | train | epoch 005 | loss 15.85 | nll_loss 15.373 | ppl 42423.7 | wps 533.7 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 80 | lr 9.6e-07 | gnorm 3.026 | train_wall 45 | gb_free 9.1 | wall 629
2024-09-04 07:19:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:19:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:19:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:19:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000712
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23971.05859375Mb; avail=231058.796875Mb
2024-09-04 07:19:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000608
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23971.05859375Mb; avail=231058.796875Mb
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23971.05859375Mb; avail=231058.796875Mb
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001095
2024-09-04 07:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23971.05859375Mb; avail=231058.796875Mb
2024-09-04 07:19:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:19:19 | INFO | fairseq.trainer | begin training epoch 6
2024-09-04 07:19:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:19:19 | INFO | train_inner | epoch 009:      4 / 21 loss=14.032, nll_loss=13.1, ppl=8777.65, wps=1667, ups=0.35, wpb=4729.5, bsz=92, num_updates=172, lr=2.064e-06, gnorm=2.082, train_wall=6, gb_free=14.2, wall=1116
2024-09-04 07:19:24 | INFO | train_inner | epoch 009:      6 / 21 loss=14.051, nll_loss=13.123, ppl=8923.48, wps=1872.5, ups=0.41, wpb=4592, bsz=84, num_updates=174, lr=2.088e-06, gnorm=2.18, train_wall=5, gb_free=13.3, wall=1121
2024-09-04 07:19:25 | INFO | train_inner | epoch 006:      2 / 16 loss=15.634, nll_loss=15.099, ppl=35100, wps=102.2, ups=0.02, wpb=4482, bsz=144, num_updates=82, lr=9.84e-07, gnorm=2.657, train_wall=6, gb_free=8.7, wall=634
2024-09-04 07:19:29 | INFO | train_inner | epoch 009:      8 / 21 loss=14.007, nll_loss=13.07, ppl=8597.79, wps=1543.4, ups=0.41, wpb=3732.5, bsz=49, num_updates=176, lr=2.112e-06, gnorm=2.243, train_wall=5, gb_free=12.9, wall=1126
2024-09-04 07:19:31 | INFO | train_inner | epoch 006:      4 / 16 loss=15.583, nll_loss=15.035, ppl=33570.8, wps=1600.1, ups=0.34, wpb=4690, bsz=172, num_updates=84, lr=1.008e-06, gnorm=2.584, train_wall=6, gb_free=9.3, wall=640
2024-09-04 07:19:35 | INFO | train_inner | epoch 009:     10 / 21 loss=13.791, nll_loss=12.798, ppl=7119.76, wps=1450.1, ups=0.35, wpb=4163.5, bsz=148, num_updates=178, lr=2.136e-06, gnorm=2.476, train_wall=6, gb_free=12.8, wall=1132
2024-09-04 07:19:36 | INFO | train_inner | epoch 006:      6 / 16 loss=15.568, nll_loss=15.015, ppl=33108.2, wps=1457.6, ups=0.4, wpb=3682.5, bsz=144, num_updates=86, lr=1.032e-06, gnorm=3.237, train_wall=5, gb_free=13.8, wall=645
2024-09-04 07:19:40 | INFO | train_inner | epoch 009:     12 / 21 loss=13.953, nll_loss=13.002, ppl=8202.26, wps=1654.1, ups=0.37, wpb=4465.5, bsz=88, num_updates=180, lr=2.16e-06, gnorm=2.117, train_wall=5, gb_free=11.3, wall=1137
2024-09-04 07:19:41 | INFO | train_inner | epoch 006:      8 / 16 loss=15.654, nll_loss=15.124, ppl=35711.8, wps=1513.4, ups=0.41, wpb=3687, bsz=101, num_updates=88, lr=1.056e-06, gnorm=2.596, train_wall=5, gb_free=10.6, wall=650
2024-09-04 07:19:45 | INFO | train_inner | epoch 009:     14 / 21 loss=14.08, nll_loss=13.16, ppl=9151.25, wps=2099.1, ups=0.4, wpb=5218, bsz=88, num_updates=182, lr=2.184e-06, gnorm=2.032, train_wall=5, gb_free=12.6, wall=1142
2024-09-04 07:19:51 | INFO | train_inner | epoch 009:     16 / 21 loss=13.934, nll_loss=12.977, ppl=8064.68, wps=1686.5, ups=0.37, wpb=4584.5, bsz=124, num_updates=184, lr=2.208e-06, gnorm=2.116, train_wall=5, gb_free=12.5, wall=1148
2024-09-04 07:19:56 | INFO | train_inner | epoch 009:     18 / 21 loss=14.03, nll_loss=13.099, ppl=8773.55, wps=1903.2, ups=0.4, wpb=4757, bsz=76, num_updates=186, lr=2.232e-06, gnorm=2.128, train_wall=5, gb_free=12.2, wall=1153
2024-09-04 07:19:56 | INFO | train_inner | epoch 006:     10 / 16 loss=15.491, nll_loss=14.919, ppl=30973, wps=529.3, ups=0.13, wpb=4152, bsz=136, num_updates=90, lr=1.08e-06, gnorm=2.485, train_wall=16, gb_free=7.8, wall=666
2024-09-04 07:20:01 | INFO | train_inner | epoch 009:     20 / 21 loss=13.818, nll_loss=12.833, ppl=7296.35, wps=1597.6, ups=0.35, wpb=4568.5, bsz=144, num_updates=188, lr=2.256e-06, gnorm=2.262, train_wall=6, gb_free=14.4, wall=1158
2024-09-04 07:20:03 | INFO | train_inner | epoch 006:     12 / 16 loss=15.474, nll_loss=14.896, ppl=30493, wps=1437, ups=0.33, wpb=4344.5, bsz=116, num_updates=92, lr=1.104e-06, gnorm=2.35, train_wall=6, gb_free=8.8, wall=672
2024-09-04 07:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16585.5234375Mb; avail=238444.37109375Mb
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16585.5234375Mb; avail=238444.37109375Mb
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012787
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16585.5234375Mb; avail=238444.37109375Mb
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011274
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025184
2024-09-04 07:20:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16585.5234375Mb; avail=238444.37109375Mb
2024-09-04 07:20:09 | INFO | train_inner | epoch 006:     14 / 16 loss=15.656, nll_loss=15.126, ppl=35768.9, wps=1536.3, ups=0.33, wpb=4639, bsz=92, num_updates=94, lr=1.128e-06, gnorm=2.199, train_wall=6, gb_free=8.2, wall=678
2024-09-04 07:20:14 | INFO | train_inner | epoch 006:     16 / 16 loss=15.49, nll_loss=14.918, ppl=30949.2, wps=1469, ups=0.35, wpb=4214.5, bsz=112, num_updates=96, lr=1.152e-06, gnorm=2.246, train_wall=6, gb_free=8.9, wall=684
2024-09-04 07:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16612.79296875Mb; avail=238417.0625Mb
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000624
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16612.79296875Mb; avail=238417.0625Mb
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012753
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16612.79296875Mb; avail=238417.0625Mb
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011105
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024864
2024-09-04 07:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16612.79296875Mb; avail=238417.0625Mb
2024-09-04 07:20:20 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 13.769 | nll_loss 12.761 | ppl 6939.89 | wps 4950.7 | wpb 2350.9 | bsz 94.7 | num_updates 189 | best_loss 13.769
2024-09-04 07:20:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 189 updates
2024-09-04 07:20:20 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:20:29 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 15.108 | nll_loss 14.41 | ppl 21771.3 | wps 3849.9 | wpb 2070.5 | bsz 122.7 | num_updates 96 | best_loss 15.108
2024-09-04 07:20:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 96 updates
2024-09-04 07:20:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:21:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:21:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:21:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 6 @ 96 updates, score 15.108) (writing took 69.21340254228562 seconds)
2024-09-04 07:21:38 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-09-04 07:21:38 | INFO | train | epoch 006 | loss 15.569 | nll_loss 15.017 | ppl 33153.3 | wps 488.5 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 96 | lr 1.152e-06 | gnorm 2.544 | train_wall 55 | gb_free 8.9 | wall 767
2024-09-04 07:21:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:21:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:21:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:21:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000652
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21077.66015625Mb; avail=233952.32421875Mb
2024-09-04 07:21:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000578
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21077.66015625Mb; avail=233952.32421875Mb
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21077.66015625Mb; avail=233952.32421875Mb
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000185
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001073
2024-09-04 07:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21077.66015625Mb; avail=233952.32421875Mb
2024-09-04 07:21:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:21:38 | INFO | fairseq.trainer | begin training epoch 7
2024-09-04 07:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:21:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 9 @ 189 updates, score 13.769) (writing took 78.51090967748314 seconds)
2024-09-04 07:21:39 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-09-04 07:21:39 | INFO | train | epoch 009 | loss 13.984 | nll_loss 13.04 | ppl 8423.43 | wps 636.3 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 189 | lr 2.268e-06 | gnorm 2.162 | train_wall 55 | gb_free 12.7 | wall 1256
2024-09-04 07:21:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:21:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:21:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:21:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000630
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21114.73828125Mb; avail=233915.203125Mb
2024-09-04 07:21:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000058
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000578
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.73828125Mb; avail=233915.203125Mb
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.73828125Mb; avail=233915.203125Mb
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001058
2024-09-04 07:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21114.73828125Mb; avail=233915.203125Mb
2024-09-04 07:21:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:21:39 | INFO | fairseq.trainer | begin training epoch 10
2024-09-04 07:21:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:21:41 | INFO | train_inner | epoch 010:      1 / 21 loss=13.992, nll_loss=13.052, ppl=8493.65, wps=88.3, ups=0.02, wpb=4422, bsz=76, num_updates=190, lr=2.28e-06, gnorm=2.077, train_wall=5, gb_free=13.8, wall=1258
2024-09-04 07:21:43 | INFO | train_inner | epoch 007:      2 / 16 loss=15.546, nll_loss=14.989, ppl=32528.8, wps=98.1, ups=0.02, wpb=4370.5, bsz=100, num_updates=98, lr=1.176e-06, gnorm=2.264, train_wall=5, gb_free=14.5, wall=773
2024-09-04 07:21:47 | INFO | train_inner | epoch 010:      3 / 21 loss=13.807, nll_loss=12.821, ppl=7236.44, wps=1739.5, ups=0.38, wpb=4617.5, bsz=108, num_updates=192, lr=2.304e-06, gnorm=2.145, train_wall=5, gb_free=14, wall=1264
2024-09-04 07:21:48 | INFO | train_inner | epoch 007:      4 / 16 loss=15.491, nll_loss=14.919, ppl=30981.7, wps=1480.1, ups=0.46, wpb=3219, bsz=93, num_updates=100, lr=1.2e-06, gnorm=2.781, train_wall=4, gb_free=9.9, wall=777
2024-09-04 07:21:52 | INFO | train_inner | epoch 010:      5 / 21 loss=13.903, nll_loss=12.94, ppl=7860.93, wps=1696.1, ups=0.37, wpb=4629.5, bsz=88, num_updates=194, lr=2.328e-06, gnorm=2.078, train_wall=5, gb_free=14.1, wall=1269
2024-09-04 07:21:53 | INFO | train_inner | epoch 007:      6 / 16 loss=15.343, nll_loss=14.734, ppl=27247.4, wps=1462.3, ups=0.36, wpb=4070, bsz=108, num_updates=102, lr=1.224e-06, gnorm=2.049, train_wall=6, gb_free=9.7, wall=783
2024-09-04 07:21:58 | INFO | train_inner | epoch 010:      7 / 21 loss=13.72, nll_loss=12.711, ppl=6704.37, wps=1541.8, ups=0.33, wpb=4666.5, bsz=128, num_updates=196, lr=2.352e-06, gnorm=2.282, train_wall=6, gb_free=11.3, wall=1275
2024-09-04 07:21:59 | INFO | train_inner | epoch 007:      8 / 16 loss=15.322, nll_loss=14.709, ppl=26773.8, wps=1526.9, ups=0.34, wpb=4449.5, bsz=140, num_updates=104, lr=1.248e-06, gnorm=2.112, train_wall=6, gb_free=9.4, wall=789
2024-09-04 07:22:03 | INFO | train_inner | epoch 010:      9 / 21 loss=13.937, nll_loss=12.983, ppl=8094.96, wps=1970, ups=0.41, wpb=4817.5, bsz=80, num_updates=198, lr=2.376e-06, gnorm=1.983, train_wall=5, gb_free=11.4, wall=1280
2024-09-04 07:22:05 | INFO | train_inner | epoch 007:     10 / 16 loss=15.016, nll_loss=14.324, ppl=20509.9, wps=1410.5, ups=0.33, wpb=4298.5, bsz=244, num_updates=106, lr=1.272e-06, gnorm=3.158, train_wall=6, gb_free=9.3, wall=795
2024-09-04 07:22:09 | INFO | train_inner | epoch 010:     11 / 21 loss=13.801, nll_loss=12.812, ppl=7190.69, wps=1684.2, ups=0.35, wpb=4851.5, bsz=104, num_updates=200, lr=2.4e-06, gnorm=2.054, train_wall=6, gb_free=12, wall=1286
2024-09-04 07:22:11 | INFO | train_inner | epoch 007:     12 / 16 loss=15.445, nll_loss=14.863, ppl=29793.9, wps=1540.4, ups=0.34, wpb=4543, bsz=96, num_updates=108, lr=1.296e-06, gnorm=1.932, train_wall=6, gb_free=12.1, wall=801
2024-09-04 07:22:13 | INFO | train_inner | epoch 010:     13 / 21 loss=13.92, nll_loss=12.964, ppl=7990.03, wps=1923.4, ups=0.47, wpb=4108.5, bsz=68, num_updates=202, lr=2.424e-06, gnorm=2.139, train_wall=4, gb_free=12.6, wall=1290
2024-09-04 07:22:17 | INFO | train_inner | epoch 007:     14 / 16 loss=15.316, nll_loss=14.7, ppl=26607.2, wps=1555.6, ups=0.33, wpb=4674.5, bsz=120, num_updates=110, lr=1.32e-06, gnorm=1.962, train_wall=6, gb_free=9.9, wall=807
2024-09-04 07:22:19 | INFO | train_inner | epoch 010:     15 / 21 loss=13.877, nll_loss=12.91, ppl=7698.5, wps=1477.8, ups=0.37, wpb=3953.5, bsz=61, num_updates=204, lr=2.448e-06, gnorm=2.217, train_wall=5, gb_free=12.3, wall=1296
2024-09-04 07:22:23 | INFO | train_inner | epoch 007:     16 / 16 loss=15.319, nll_loss=14.704, ppl=26684.8, wps=1451.2, ups=0.34, wpb=4266.5, bsz=116, num_updates=112, lr=1.344e-06, gnorm=1.998, train_wall=6, gb_free=12.5, wall=813
2024-09-04 07:22:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21168.05859375Mb; avail=233861.83984375Mb
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000747
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21168.55078125Mb; avail=233861.34765625Mb
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012698
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21168.55078125Mb; avail=233861.34765625Mb
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011301
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025125
2024-09-04 07:22:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21168.55078125Mb; avail=233861.34765625Mb
2024-09-04 07:22:23 | INFO | train_inner | epoch 010:     17 / 21 loss=13.799, nll_loss=12.812, ppl=7193.2, wps=1938, ups=0.41, wpb=4706, bsz=92, num_updates=206, lr=2.472e-06, gnorm=2.056, train_wall=5, gb_free=11.2, wall=1300
2024-09-04 07:22:29 | INFO | train_inner | epoch 010:     19 / 21 loss=13.55, nll_loss=12.5, ppl=5791.9, wps=1791.1, ups=0.39, wpb=4611.5, bsz=160, num_updates=208, lr=2.496e-06, gnorm=2.249, train_wall=5, gb_free=14, wall=1306
2024-09-04 07:22:34 | INFO | train_inner | epoch 010:     21 / 21 loss=13.844, nll_loss=12.87, ppl=7486.1, wps=1712.2, ups=0.35, wpb=4906, bsz=96, num_updates=210, lr=2.52e-06, gnorm=1.993, train_wall=6, gb_free=12.6, wall=1311
2024-09-04 07:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21205.328125Mb; avail=233824.4453125Mb
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000714
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21205.328125Mb; avail=233824.4453125Mb
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.013030
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21205.328125Mb; avail=233824.4453125Mb
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011160
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025280
2024-09-04 07:22:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21205.8203125Mb; avail=233823.953125Mb
2024-09-04 07:22:37 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 14.941 | nll_loss 14.207 | ppl 18912 | wps 3834.6 | wpb 2070.5 | bsz 122.7 | num_updates 112 | best_loss 14.941
2024-09-04 07:22:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 112 updates
2024-09-04 07:22:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:22:52 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 13.625 | nll_loss 12.576 | ppl 6106.48 | wps 4691 | wpb 2350.9 | bsz 94.7 | num_updates 210 | best_loss 13.625
2024-09-04 07:22:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 210 updates
2024-09-04 07:22:52 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:23:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:23:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:23:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 7 @ 112 updates, score 14.941) (writing took 59.799402757547796 seconds)
2024-09-04 07:23:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-09-04 07:23:37 | INFO | train | epoch 007 | loss 15.346 | nll_loss 14.738 | ppl 27323.5 | wps 567.8 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 112 | lr 1.344e-06 | gnorm 2.282 | train_wall 45 | gb_free 12.5 | wall 887
2024-09-04 07:23:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:23:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:23:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:23:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000753
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23259.6015625Mb; avail=231769.8046875Mb
2024-09-04 07:23:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000740
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23254.1875Mb; avail=231775.7109375Mb
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23255.171875Mb; avail=231773.7421875Mb
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000172
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001272
2024-09-04 07:23:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23258.125Mb; avail=231771.28125Mb
2024-09-04 07:23:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:23:37 | INFO | fairseq.trainer | begin training epoch 8
2024-09-04 07:23:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:23:43 | INFO | train_inner | epoch 008:      2 / 16 loss=15.314, nll_loss=14.698, ppl=26582.7, wps=98.6, ups=0.03, wpb=3920.5, bsz=116, num_updates=114, lr=1.368e-06, gnorm=2.152, train_wall=5, gb_free=12.5, wall=892
2024-09-04 07:23:48 | INFO | train_inner | epoch 008:      4 / 16 loss=15.221, nll_loss=14.582, ppl=24525.6, wps=1540.8, ups=0.35, wpb=4346.5, bsz=124, num_updates=116, lr=1.392e-06, gnorm=1.931, train_wall=6, gb_free=9.4, wall=898
2024-09-04 07:23:54 | INFO | train_inner | epoch 008:      6 / 16 loss=15.069, nll_loss=14.391, ppl=21482.2, wps=1470.5, ups=0.33, wpb=4508.5, bsz=160, num_updates=118, lr=1.416e-06, gnorm=1.911, train_wall=6, gb_free=8.6, wall=904
2024-09-04 07:23:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 10 @ 210 updates, score 13.625) (writing took 67.33081927988678 seconds)
2024-09-04 07:23:59 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-09-04 07:23:59 | INFO | train | epoch 010 | loss 13.821 | nll_loss 12.838 | ppl 7323.01 | wps 682.7 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 210 | lr 2.52e-06 | gnorm 2.119 | train_wall 55 | gb_free 12.6 | wall 1396
2024-09-04 07:23:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:23:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:23:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:23:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000708
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23331.96875Mb; avail=231697.8828125Mb
2024-09-04 07:23:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000597
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23331.96875Mb; avail=231697.8828125Mb
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23331.96875Mb; avail=231697.8828125Mb
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000180
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001087
2024-09-04 07:23:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23331.96875Mb; avail=231697.8828125Mb
2024-09-04 07:23:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:23:59 | INFO | fairseq.trainer | begin training epoch 11
2024-09-04 07:23:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:24:04 | INFO | train_inner | epoch 011:      2 / 21 loss=13.593, nll_loss=12.554, ppl=6012.28, wps=106.1, ups=0.02, wpb=4771, bsz=136, num_updates=212, lr=2.544e-06, gnorm=2.288, train_wall=5, gb_free=11.6, wall=1401
2024-09-04 07:24:10 | INFO | train_inner | epoch 011:      4 / 21 loss=13.732, nll_loss=12.727, ppl=6779.69, wps=1715.2, ups=0.37, wpb=4585, bsz=80, num_updates=214, lr=2.568e-06, gnorm=2.087, train_wall=5, gb_free=14.7, wall=1407
2024-09-04 07:24:10 | INFO | train_inner | epoch 008:      8 / 16 loss=15.283, nll_loss=14.661, ppl=25905.6, wps=592, ups=0.13, wpb=4692.5, bsz=124, num_updates=120, lr=1.44e-06, gnorm=2.016, train_wall=16, gb_free=11.9, wall=920
2024-09-04 07:24:15 | INFO | train_inner | epoch 011:      6 / 21 loss=13.745, nll_loss=12.744, ppl=6858.1, wps=1649.8, ups=0.37, wpb=4402, bsz=76, num_updates=216, lr=2.592e-06, gnorm=2.018, train_wall=5, gb_free=15.1, wall=1412
2024-09-04 07:24:16 | INFO | train_inner | epoch 008:     10 / 16 loss=15.226, nll_loss=14.588, ppl=24622.4, wps=1567, ups=0.35, wpb=4541.5, bsz=108, num_updates=122, lr=1.464e-06, gnorm=1.877, train_wall=6, gb_free=10.7, wall=925
2024-09-04 07:24:20 | INFO | train_inner | epoch 011:      8 / 21 loss=13.764, nll_loss=12.766, ppl=6966.51, wps=1640, ups=0.37, wpb=4385, bsz=84, num_updates=218, lr=2.616e-06, gnorm=1.929, train_wall=5, gb_free=14.7, wall=1417
2024-09-04 07:24:22 | INFO | train_inner | epoch 008:     12 / 16 loss=15.049, nll_loss=14.366, ppl=21110, wps=1495.1, ups=0.35, wpb=4266, bsz=144, num_updates=124, lr=1.488e-06, gnorm=1.877, train_wall=6, gb_free=9.9, wall=931
2024-09-04 07:24:26 | INFO | train_inner | epoch 011:     10 / 21 loss=13.649, nll_loss=12.62, ppl=6296.27, wps=1749.5, ups=0.38, wpb=4656, bsz=112, num_updates=220, lr=2.64e-06, gnorm=2, train_wall=5, gb_free=12.2, wall=1423
2024-09-04 07:24:28 | INFO | train_inner | epoch 008:     14 / 16 loss=15.196, nll_loss=14.552, ppl=24021.7, wps=1483.1, ups=0.34, wpb=4323, bsz=104, num_updates=126, lr=1.512e-06, gnorm=1.826, train_wall=6, gb_free=10.9, wall=937
2024-09-04 07:24:31 | INFO | train_inner | epoch 011:     12 / 21 loss=13.567, nll_loss=12.516, ppl=5855.79, wps=1633.6, ups=0.35, wpb=4664.5, bsz=164, num_updates=222, lr=2.664e-06, gnorm=2.18, train_wall=6, gb_free=13.2, wall=1428
2024-09-04 07:24:33 | INFO | train_inner | epoch 008:     16 / 16 loss=14.915, nll_loss=14.199, ppl=18803.7, wps=1303.2, ups=0.4, wpb=3293, bsz=137, num_updates=128, lr=1.536e-06, gnorm=3.082, train_wall=5, gb_free=9.4, wall=942
2024-09-04 07:24:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17646.5703125Mb; avail=237383.32421875Mb
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000626
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17646.5703125Mb; avail=237383.32421875Mb
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012693
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17646.5703125Mb; avail=237383.32421875Mb
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011090
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024774
2024-09-04 07:24:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17646.5703125Mb; avail=237383.32421875Mb
2024-09-04 07:24:37 | INFO | train_inner | epoch 011:     14 / 21 loss=13.633, nll_loss=12.602, ppl=6218.88, wps=1716.5, ups=0.34, wpb=5022.5, bsz=92, num_updates=224, lr=2.688e-06, gnorm=2.028, train_wall=6, gb_free=13.8, wall=1434
2024-09-04 07:24:43 | INFO | train_inner | epoch 011:     16 / 21 loss=13.703, nll_loss=12.692, ppl=6618.14, wps=1623.9, ups=0.35, wpb=4656, bsz=76, num_updates=226, lr=2.712e-06, gnorm=2.03, train_wall=6, gb_free=16.4, wall=1440
2024-09-04 07:24:47 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 14.8 | nll_loss 14.033 | ppl 16767.8 | wps 3845.6 | wpb 2070.5 | bsz 122.7 | num_updates 128 | best_loss 14.8
2024-09-04 07:24:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 128 updates
2024-09-04 07:24:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:24:49 | INFO | train_inner | epoch 011:     18 / 21 loss=13.757, nll_loss=12.76, ppl=6935.12, wps=1646, ups=0.36, wpb=4635, bsz=56, num_updates=228, lr=2.736e-06, gnorm=2.039, train_wall=6, gb_free=13.7, wall=1445
2024-09-04 07:24:53 | INFO | train_inner | epoch 011:     20 / 21 loss=13.497, nll_loss=12.432, ppl=5524.19, wps=1934.7, ups=0.43, wpb=4474, bsz=132, num_updates=230, lr=2.76e-06, gnorm=2.108, train_wall=5, gb_free=12.6, wall=1450
2024-09-04 07:24:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32640.765625Mb; avail=222389.08984375Mb
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32641.2578125Mb; avail=222388.59765625Mb
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012641
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32641.2578125Mb; avail=222388.59765625Mb
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011305
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025041
2024-09-04 07:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32641.2578125Mb; avail=222388.59765625Mb
2024-09-04 07:25:12 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 13.463 | nll_loss 12.358 | ppl 5250.23 | wps 4709.8 | wpb 2350.9 | bsz 94.7 | num_updates 231 | best_loss 13.463
2024-09-04 07:25:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 231 updates
2024-09-04 07:25:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:25:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:25:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 8 @ 128 updates, score 14.8) (writing took 67.14329530484974 seconds)
2024-09-04 07:25:54 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-09-04 07:25:54 | INFO | train | epoch 008 | loss 15.166 | nll_loss 14.513 | ppl 23387 | wps 495.3 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 128 | lr 1.536e-06 | gnorm 2.084 | train_wall 55 | gb_free 9.4 | wall 1024
2024-09-04 07:25:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:25:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:25:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:25:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000685
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31176.5Mb; avail=223853.3984375Mb
2024-09-04 07:25:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000599
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31176.9921875Mb; avail=223852.90625Mb
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31176.9921875Mb; avail=223852.90625Mb
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000165
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001065
2024-09-04 07:25:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31176.9921875Mb; avail=223852.90625Mb
2024-09-04 07:25:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:25:54 | INFO | fairseq.trainer | begin training epoch 9
2024-09-04 07:25:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:26:00 | INFO | train_inner | epoch 009:      2 / 16 loss=14.876, nll_loss=14.15, ppl=18181.9, wps=98.6, ups=0.02, wpb=4322.5, bsz=176, num_updates=130, lr=1.56e-06, gnorm=2.271, train_wall=6, gb_free=9.7, wall=1030
2024-09-04 07:26:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:26:05 | INFO | train_inner | epoch 009:      4 / 16 loss=15.087, nll_loss=14.415, ppl=21843.9, wps=1522.5, ups=0.38, wpb=4000, bsz=116, num_updates=132, lr=1.584e-06, gnorm=1.805, train_wall=5, gb_free=10.7, wall=1035
2024-09-04 07:26:11 | INFO | train_inner | epoch 009:      6 / 16 loss=15.159, nll_loss=14.504, ppl=23235.7, wps=1524.4, ups=0.33, wpb=4555, bsz=112, num_updates=134, lr=1.608e-06, gnorm=1.777, train_wall=6, gb_free=10.1, wall=1041
2024-09-04 07:26:17 | INFO | train_inner | epoch 009:      8 / 16 loss=15.18, nll_loss=14.533, ppl=23698.4, wps=1542, ups=0.35, wpb=4432, bsz=100, num_updates=136, lr=1.632e-06, gnorm=1.795, train_wall=6, gb_free=8.9, wall=1047
2024-09-04 07:26:23 | INFO | train_inner | epoch 009:     10 / 16 loss=14.878, nll_loss=14.151, ppl=18187.8, wps=1429.1, ups=0.34, wpb=4178.5, bsz=164, num_updates=138, lr=1.656e-06, gnorm=2.064, train_wall=6, gb_free=12.2, wall=1053
2024-09-04 07:26:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 11 @ 231 updates, score 13.463) (writing took 73.8965851990506 seconds)
2024-09-04 07:26:26 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-09-04 07:26:26 | INFO | train | epoch 011 | loss 13.664 | nll_loss 12.641 | ppl 6386.77 | wps 649.7 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 231 | lr 2.772e-06 | gnorm 2.085 | train_wall 56 | gb_free 11.8 | wall 1543
2024-09-04 07:26:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:26:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:26:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:26:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000630
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15183.5078125Mb; avail=239846.42578125Mb
2024-09-04 07:26:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000618
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15183.5078125Mb; avail=239846.42578125Mb
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15183.5078125Mb; avail=239846.42578125Mb
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000185
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001152
2024-09-04 07:26:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15183.5078125Mb; avail=239846.42578125Mb
2024-09-04 07:26:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:26:26 | INFO | fairseq.trainer | begin training epoch 12
2024-09-04 07:26:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:26:28 | INFO | train_inner | epoch 009:     12 / 16 loss=15.102, nll_loss=14.436, ppl=22164.8, wps=1501, ups=0.37, wpb=4039.5, bsz=92, num_updates=140, lr=1.68e-06, gnorm=1.799, train_wall=5, gb_free=14.3, wall=1058
2024-09-04 07:26:29 | INFO | train_inner | epoch 012:      1 / 21 loss=13.554, nll_loss=12.505, ppl=5814.67, wps=83.5, ups=0.02, wpb=3985, bsz=65, num_updates=232, lr=2.784e-06, gnorm=2.204, train_wall=4, gb_free=13.7, wall=1546
2024-09-04 07:26:34 | INFO | train_inner | epoch 009:     14 / 16 loss=14.829, nll_loss=14.09, ppl=17441.9, wps=1444.5, ups=0.38, wpb=3761.5, bsz=145, num_updates=142, lr=1.704e-06, gnorm=1.986, train_wall=5, gb_free=9.5, wall=1063
2024-09-04 07:26:34 | INFO | train_inner | epoch 012:      3 / 21 loss=13.336, nll_loss=12.228, ppl=4798.16, wps=1580.2, ups=0.36, wpb=4430.5, bsz=152, num_updates=234, lr=2.808e-06, gnorm=2.153, train_wall=6, gb_free=12.5, wall=1551
2024-09-04 07:26:39 | INFO | train_inner | epoch 009:     16 / 16 loss=14.987, nll_loss=14.29, ppl=20028.2, wps=1574.7, ups=0.34, wpb=4602.5, bsz=112, num_updates=144, lr=1.728e-06, gnorm=1.723, train_wall=6, gb_free=11.3, wall=1069
2024-09-04 07:26:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:26:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15219.8671875Mb; avail=239810.0234375Mb
2024-09-04 07:26:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2024-09-04 07:26:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15219.8671875Mb; avail=239810.0234375Mb
2024-09-04 07:26:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.013109
2024-09-04 07:26:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15219.8671875Mb; avail=239810.0234375Mb
2024-09-04 07:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011187
2024-09-04 07:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025384
2024-09-04 07:26:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15220.359375Mb; avail=239809.53125Mb
2024-09-04 07:26:40 | INFO | train_inner | epoch 012:      5 / 21 loss=13.656, nll_loss=12.635, ppl=6361.61, wps=1547.7, ups=0.34, wpb=4594.5, bsz=80, num_updates=236, lr=2.832e-06, gnorm=1.974, train_wall=6, gb_free=12, wall=1557
2024-09-04 07:26:46 | INFO | train_inner | epoch 012:      7 / 21 loss=13.55, nll_loss=12.499, ppl=5789.19, wps=1714.7, ups=0.37, wpb=4667.5, bsz=96, num_updates=238, lr=2.856e-06, gnorm=1.974, train_wall=5, gb_free=13.5, wall=1563
2024-09-04 07:26:51 | INFO | train_inner | epoch 012:      9 / 21 loss=13.472, nll_loss=12.4, ppl=5403.51, wps=1936.9, ups=0.4, wpb=4797.5, bsz=100, num_updates=240, lr=2.88e-06, gnorm=1.968, train_wall=5, gb_free=12.4, wall=1568
2024-09-04 07:26:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 14.684 | nll_loss 13.889 | ppl 15165.8 | wps 3848.5 | wpb 2070.5 | bsz 122.7 | num_updates 144 | best_loss 14.684
2024-09-04 07:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 144 updates
2024-09-04 07:26:54 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:26:56 | INFO | train_inner | epoch 012:     11 / 21 loss=13.478, nll_loss=12.403, ppl=5415.65, wps=1683.9, ups=0.34, wpb=4916.5, bsz=116, num_updates=242, lr=2.904e-06, gnorm=1.922, train_wall=6, gb_free=12, wall=1573
2024-09-04 07:27:02 | INFO | train_inner | epoch 012:     13 / 21 loss=13.387, nll_loss=12.29, ppl=5008.13, wps=1553.7, ups=0.36, wpb=4326, bsz=104, num_updates=244, lr=2.928e-06, gnorm=2.248, train_wall=6, gb_free=11.8, wall=1579
2024-09-04 07:27:08 | INFO | train_inner | epoch 012:     15 / 21 loss=13.446, nll_loss=12.366, ppl=5278.19, wps=1627.4, ups=0.34, wpb=4789.5, bsz=88, num_updates=246, lr=2.952e-06, gnorm=1.991, train_wall=6, gb_free=12.5, wall=1585
2024-09-04 07:27:12 | INFO | train_inner | epoch 012:     17 / 21 loss=13.524, nll_loss=12.468, ppl=5666.22, wps=1857.5, ups=0.48, wpb=3882, bsz=53, num_updates=248, lr=2.976e-06, gnorm=2.084, train_wall=4, gb_free=16.9, wall=1589
2024-09-04 07:27:17 | INFO | train_inner | epoch 012:     19 / 21 loss=13.494, nll_loss=12.429, ppl=5513.52, wps=1906.5, ups=0.41, wpb=4661.5, bsz=80, num_updates=250, lr=3e-06, gnorm=1.997, train_wall=5, gb_free=14.3, wall=1594
2024-09-04 07:27:22 | INFO | train_inner | epoch 012:     21 / 21 loss=13.448, nll_loss=12.37, ppl=5294.87, wps=1612.3, ups=0.37, wpb=4380.5, bsz=92, num_updates=252, lr=3.024e-06, gnorm=2.024, train_wall=5, gb_free=12.5, wall=1599
2024-09-04 07:27:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32424.29296875Mb; avail=222605.54296875Mb
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000726
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32424.29296875Mb; avail=222605.05078125Mb
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012777
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32425.01953125Mb; avail=222604.81640625Mb
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011241
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025123
2024-09-04 07:27:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32425.01953125Mb; avail=222604.81640625Mb
2024-09-04 07:27:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:27:40 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 13.35 | nll_loss 12.191 | ppl 4677.3 | wps 4713.2 | wpb 2350.9 | bsz 94.7 | num_updates 252 | best_loss 13.35
2024-09-04 07:27:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 252 updates
2024-09-04 07:27:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:27:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 9 @ 144 updates, score 14.684) (writing took 56.495268679223955 seconds)
2024-09-04 07:27:50 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-09-04 07:27:50 | INFO | train | epoch 009 | loss 15.016 | nll_loss 14.325 | ppl 20529.7 | wps 583.1 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 144 | lr 1.728e-06 | gnorm 1.902 | train_wall 45 | gb_free 11.3 | wall 1140
2024-09-04 07:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000794
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29965.74609375Mb; avail=225063.60546875Mb
2024-09-04 07:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000722
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29967.22265625Mb; avail=225062.12890625Mb
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29967.71484375Mb; avail=225062.12890625Mb
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000181
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001246
2024-09-04 07:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29968.69921875Mb; avail=225061.14453125Mb
2024-09-04 07:27:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:27:50 | INFO | fairseq.trainer | begin training epoch 10
2024-09-04 07:27:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:27:56 | INFO | train_inner | epoch 010:      2 / 16 loss=14.823, nll_loss=14.083, ppl=17356, wps=107.4, ups=0.03, wpb=4125, bsz=160, num_updates=146, lr=1.752e-06, gnorm=2.275, train_wall=6, gb_free=9, wall=1146
2024-09-04 07:28:02 | INFO | train_inner | epoch 010:      4 / 16 loss=14.975, nll_loss=14.276, ppl=19835.2, wps=1576.3, ups=0.34, wpb=4700, bsz=124, num_updates=148, lr=1.776e-06, gnorm=1.679, train_wall=6, gb_free=10.1, wall=1152
2024-09-04 07:28:07 | INFO | train_inner | epoch 010:      6 / 16 loss=15.04, nll_loss=14.358, ppl=21002.1, wps=1412.5, ups=0.46, wpb=3089, bsz=61, num_updates=150, lr=1.8e-06, gnorm=2.098, train_wall=4, gb_free=21.1, wall=1156
2024-09-04 07:28:12 | INFO | train_inner | epoch 010:      8 / 16 loss=14.871, nll_loss=14.146, ppl=18124.5, wps=1553.6, ups=0.35, wpb=4431.5, bsz=144, num_updates=152, lr=1.824e-06, gnorm=1.741, train_wall=6, gb_free=13.9, wall=1162
2024-09-04 07:28:18 | INFO | train_inner | epoch 010:     10 / 16 loss=15.022, nll_loss=14.337, ppl=20701.5, wps=1540.4, ups=0.34, wpb=4548, bsz=112, num_updates=154, lr=1.848e-06, gnorm=1.71, train_wall=6, gb_free=8.8, wall=1168
2024-09-04 07:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:28:39 | INFO | train_inner | epoch 010:     12 / 16 loss=14.83, nll_loss=14.096, ppl=17507.5, wps=411.9, ups=0.1, wpb=4264, bsz=116, num_updates=156, lr=1.872e-06, gnorm=1.793, train_wall=21, gb_free=10.7, wall=1188
2024-09-04 07:28:45 | INFO | train_inner | epoch 010:     14 / 16 loss=14.583, nll_loss=13.785, ppl=14112.6, wps=1452.2, ups=0.33, wpb=4398, bsz=200, num_updates=158, lr=1.896e-06, gnorm=2.068, train_wall=6, gb_free=10.9, wall=1195
2024-09-04 07:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 12 @ 252 updates, score 13.35) (writing took 66.06029958743602 seconds)
2024-09-04 07:28:46 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-09-04 07:28:46 | INFO | train | epoch 012 | loss 13.479 | nll_loss 12.409 | ppl 5437.97 | wps 686.2 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 252 | lr 3.024e-06 | gnorm 2.034 | train_wall 56 | gb_free 12.5 | wall 1683
2024-09-04 07:28:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:28:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:28:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:28:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000752
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14893.23046875Mb; avail=240136.6484375Mb
2024-09-04 07:28:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000679
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14893.23046875Mb; avail=240136.6484375Mb
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14893.23046875Mb; avail=240136.6484375Mb
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000186
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001205
2024-09-04 07:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14893.23046875Mb; avail=240136.6484375Mb
2024-09-04 07:28:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:28:46 | INFO | fairseq.trainer | begin training epoch 13
2024-09-04 07:28:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:28:50 | INFO | train_inner | epoch 013:      2 / 21 loss=13.4, nll_loss=12.31, ppl=5078.47, wps=104.6, ups=0.02, wpb=4602, bsz=80, num_updates=254, lr=3.048e-06, gnorm=2.001, train_wall=5, gb_free=12.8, wall=1687
2024-09-04 07:28:51 | INFO | train_inner | epoch 010:     16 / 16 loss=14.939, nll_loss=14.233, ppl=19257.4, wps=1500.9, ups=0.35, wpb=4336, bsz=100, num_updates=160, lr=1.92e-06, gnorm=1.698, train_wall=6, gb_free=11.5, wall=1200
2024-09-04 07:28:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14918.01171875Mb; avail=240111.70703125Mb
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000641
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14918.01171875Mb; avail=240111.70703125Mb
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012596
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14918.01171875Mb; avail=240111.70703125Mb
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011286
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024875
2024-09-04 07:28:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14918.01171875Mb; avail=240111.70703125Mb
2024-09-04 07:28:56 | INFO | train_inner | epoch 013:      4 / 21 loss=13.323, nll_loss=12.207, ppl=4727.32, wps=1541.1, ups=0.39, wpb=3985.5, bsz=97, num_updates=256, lr=3.072e-06, gnorm=2.08, train_wall=5, gb_free=16.5, wall=1693
2024-09-04 07:29:01 | INFO | train_inner | epoch 013:      6 / 21 loss=13.264, nll_loss=12.133, ppl=4490.51, wps=1595.1, ups=0.35, wpb=4571, bsz=124, num_updates=258, lr=3.096e-06, gnorm=2.079, train_wall=6, gb_free=13.3, wall=1698
2024-09-04 07:29:05 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 14.581 | nll_loss 13.762 | ppl 13889.7 | wps 3862.7 | wpb 2070.5 | bsz 122.7 | num_updates 160 | best_loss 14.581
2024-09-04 07:29:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 160 updates
2024-09-04 07:29:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:29:07 | INFO | train_inner | epoch 013:      8 / 21 loss=13.442, nll_loss=12.364, ppl=5271.75, wps=1573.2, ups=0.37, wpb=4213, bsz=68, num_updates=260, lr=3.12e-06, gnorm=1.978, train_wall=5, gb_free=12.6, wall=1704
2024-09-04 07:29:12 | INFO | train_inner | epoch 013:     10 / 21 loss=13.181, nll_loss=12.03, ppl=4181.68, wps=1885.6, ups=0.39, wpb=4791.5, bsz=124, num_updates=262, lr=3.144e-06, gnorm=2.124, train_wall=5, gb_free=11, wall=1709
2024-09-04 07:29:17 | INFO | train_inner | epoch 013:     12 / 21 loss=13.262, nll_loss=12.133, ppl=4490.81, wps=1589.4, ups=0.36, wpb=4394, bsz=92, num_updates=264, lr=3.168e-06, gnorm=1.931, train_wall=6, gb_free=13.7, wall=1714
2024-09-04 07:29:22 | INFO | train_inner | epoch 013:     14 / 21 loss=13.24, nll_loss=12.11, ppl=4420.27, wps=2003.1, ups=0.44, wpb=4532.5, bsz=124, num_updates=266, lr=3.192e-06, gnorm=2.026, train_wall=5, gb_free=12.2, wall=1719
2024-09-04 07:29:28 | INFO | train_inner | epoch 013:     16 / 21 loss=13.286, nll_loss=12.167, ppl=4597.23, wps=1596.4, ups=0.35, wpb=4616.5, bsz=80, num_updates=268, lr=3.216e-06, gnorm=1.969, train_wall=6, gb_free=11.8, wall=1725
2024-09-04 07:29:32 | INFO | train_inner | epoch 013:     18 / 21 loss=13.475, nll_loss=12.405, ppl=5424.15, wps=2069.3, ups=0.43, wpb=4847, bsz=72, num_updates=270, lr=3.24e-06, gnorm=1.953, train_wall=5, gb_free=14.6, wall=1729
2024-09-04 07:29:38 | INFO | train_inner | epoch 013:     20 / 21 loss=13.242, nll_loss=12.102, ppl=4395.15, wps=1698, ups=0.35, wpb=4845, bsz=120, num_updates=272, lr=3.264e-06, gnorm=1.943, train_wall=6, gb_free=12.2, wall=1735
2024-09-04 07:29:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32067.33984375Mb; avail=222961.484375Mb
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32062.41796875Mb; avail=222966.8984375Mb
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012717
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32062.9140625Mb; avail=222966.40234375Mb
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011299
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025106
2024-09-04 07:29:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32064.8828125Mb; avail=222964.92578125Mb
2024-09-04 07:29:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:29:58 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 13.201 | nll_loss 11.984 | ppl 4050.91 | wps 4730.2 | wpb 2350.9 | bsz 94.7 | num_updates 273 | best_loss 13.201
2024-09-04 07:29:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 273 updates
2024-09-04 07:29:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 10 @ 160 updates, score 14.581) (writing took 62.12539192195982 seconds)
2024-09-04 07:30:07 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-09-04 07:30:07 | INFO | train | epoch 010 | loss 14.881 | nll_loss 14.159 | ppl 18296.3 | wps 495.2 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 160 | lr 1.92e-06 | gnorm 1.883 | train_wall 60 | gb_free 11.5 | wall 1277
2024-09-04 07:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000742
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26915.78515625Mb; avail=228114.05078125Mb
2024-09-04 07:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000638
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26917.26171875Mb; avail=228112.57421875Mb
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000047
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26917.26171875Mb; avail=228112.08203125Mb
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001156
2024-09-04 07:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26918.24609375Mb; avail=228111.58984375Mb
2024-09-04 07:30:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:30:07 | INFO | fairseq.trainer | begin training epoch 11
2024-09-04 07:30:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:30:13 | INFO | train_inner | epoch 011:      2 / 16 loss=14.858, nll_loss=14.132, ppl=17952.5, wps=103, ups=0.02, wpb=4228, bsz=100, num_updates=162, lr=1.944e-06, gnorm=1.68, train_wall=6, gb_free=10.5, wall=1282
2024-09-04 07:30:19 | INFO | train_inner | epoch 011:      4 / 16 loss=14.896, nll_loss=14.179, ppl=18553, wps=1598.9, ups=0.34, wpb=4649.5, bsz=128, num_updates=164, lr=1.968e-06, gnorm=1.61, train_wall=6, gb_free=8.7, wall=1288
2024-09-04 07:30:24 | INFO | train_inner | epoch 011:      6 / 16 loss=14.914, nll_loss=14.203, ppl=18865.9, wps=1567.4, ups=0.34, wpb=4559.5, bsz=96, num_updates=166, lr=1.992e-06, gnorm=1.705, train_wall=6, gb_free=9.2, wall=1294
2024-09-04 07:30:30 | INFO | train_inner | epoch 011:      8 / 16 loss=14.531, nll_loss=13.719, ppl=13485.8, wps=1424.4, ups=0.34, wpb=4154, bsz=192, num_updates=168, lr=2.016e-06, gnorm=2.155, train_wall=6, gb_free=12.1, wall=1300
2024-09-04 07:30:36 | INFO | train_inner | epoch 011:     10 / 16 loss=14.699, nll_loss=13.932, ppl=15631.2, wps=1465.6, ups=0.33, wpb=4436, bsz=140, num_updates=170, lr=2.04e-06, gnorm=1.811, train_wall=6, gb_free=9.2, wall=1306
2024-09-04 07:30:42 | INFO | train_inner | epoch 011:     12 / 16 loss=14.827, nll_loss=14.092, ppl=17465.2, wps=1486.3, ups=0.38, wpb=3875, bsz=100, num_updates=172, lr=2.064e-06, gnorm=1.642, train_wall=5, gb_free=12.5, wall=1311
2024-09-04 07:30:47 | INFO | train_inner | epoch 011:     14 / 16 loss=14.626, nll_loss=13.838, ppl=14640, wps=1495.4, ups=0.36, wpb=4173, bsz=164, num_updates=174, lr=2.088e-06, gnorm=1.789, train_wall=6, gb_free=9.5, wall=1317
2024-09-04 07:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:30:52 | INFO | train_inner | epoch 011:     16 / 16 loss=14.787, nll_loss=14.042, ppl=16869.9, wps=1451.1, ups=0.38, wpb=3816.5, bsz=97, num_updates=176, lr=2.112e-06, gnorm=1.692, train_wall=5, gb_free=10.3, wall=1322
2024-09-04 07:30:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21128.03125Mb; avail=233901.359375Mb
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000762
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21120.65625Mb; avail=233908.734375Mb
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012699
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21129.515625Mb; avail=233899.875Mb
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011099
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024924
2024-09-04 07:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21127.5546875Mb; avail=233901.8359375Mb
2024-09-04 07:31:07 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 14.469 | nll_loss 13.616 | ppl 12551.9 | wps 3847.6 | wpb 2070.5 | bsz 122.7 | num_updates 176 | best_loss 14.469
2024-09-04 07:31:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 176 updates
2024-09-04 07:31:07 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:31:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 13 @ 273 updates, score 13.201) (writing took 76.65020398143679 seconds)
2024-09-04 07:31:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-09-04 07:31:15 | INFO | train | epoch 013 | loss 13.307 | nll_loss 12.19 | ppl 4671.72 | wps 641.9 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 273 | lr 3.276e-06 | gnorm 2.005 | train_wall 55 | gb_free 12.7 | wall 1832
2024-09-04 07:31:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:31:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:31:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:31:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000647
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25714.4375Mb; avail=229315.4375Mb
2024-09-04 07:31:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000069
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000630
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25715.9140625Mb; avail=229313.9609375Mb
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25716.40625Mb; avail=229313.46875Mb
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000177
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001240
2024-09-04 07:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25716.8984375Mb; avail=229312.9765625Mb
2024-09-04 07:31:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:31:15 | INFO | fairseq.trainer | begin training epoch 14
2024-09-04 07:31:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:31:17 | INFO | train_inner | epoch 014:      1 / 21 loss=13.212, nll_loss=12.073, ppl=4307.88, wps=81.1, ups=0.02, wpb=4029, bsz=65, num_updates=274, lr=3.288e-06, gnorm=2.05, train_wall=5, gb_free=13.3, wall=1834
2024-09-04 07:31:22 | INFO | train_inner | epoch 014:      3 / 21 loss=13.287, nll_loss=12.167, ppl=4597.58, wps=1722, ups=0.39, wpb=4441.5, bsz=80, num_updates=276, lr=3.312e-06, gnorm=1.935, train_wall=5, gb_free=12.5, wall=1839
2024-09-04 07:31:28 | INFO | train_inner | epoch 014:      5 / 21 loss=13.221, nll_loss=12.078, ppl=4322.54, wps=1813.9, ups=0.39, wpb=4629, bsz=92, num_updates=278, lr=3.336e-06, gnorm=1.952, train_wall=5, gb_free=12.1, wall=1845
2024-09-04 07:31:33 | INFO | train_inner | epoch 014:      7 / 21 loss=13.215, nll_loss=12.072, ppl=4306.98, wps=1708.1, ups=0.37, wpb=4666, bsz=104, num_updates=280, lr=3.36e-06, gnorm=1.891, train_wall=5, gb_free=12.9, wall=1850
2024-09-04 07:31:38 | INFO | train_inner | epoch 014:      9 / 21 loss=13.333, nll_loss=12.227, ppl=4793.91, wps=1924.1, ups=0.41, wpb=4721.5, bsz=84, num_updates=282, lr=3.384e-06, gnorm=1.868, train_wall=5, gb_free=14.2, wall=1855
2024-09-04 07:31:43 | INFO | train_inner | epoch 014:     11 / 21 loss=13.12, nll_loss=11.951, ppl=3958.92, wps=1932.1, ups=0.42, wpb=4616, bsz=112, num_updates=284, lr=3.408e-06, gnorm=1.964, train_wall=5, gb_free=14, wall=1860
2024-09-04 07:31:48 | INFO | train_inner | epoch 014:     13 / 21 loss=13.308, nll_loss=12.2, ppl=4704.93, wps=1518.9, ups=0.36, wpb=4206, bsz=48, num_updates=286, lr=3.432e-06, gnorm=1.975, train_wall=6, gb_free=13.4, wall=1865
2024-09-04 07:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:31:54 | INFO | train_inner | epoch 014:     15 / 21 loss=12.789, nll_loss=11.531, ppl=2958.69, wps=1643.9, ups=0.36, wpb=4532.5, bsz=132, num_updates=288, lr=3.456e-06, gnorm=2.144, train_wall=6, gb_free=12.1, wall=1871
2024-09-04 07:31:59 | INFO | train_inner | epoch 014:     17 / 21 loss=13.062, nll_loss=11.88, ppl=3770.06, wps=1679.3, ups=0.37, wpb=4554, bsz=88, num_updates=290, lr=3.48e-06, gnorm=1.884, train_wall=5, gb_free=16.7, wall=1876
2024-09-04 07:32:05 | INFO | train_inner | epoch 014:     19 / 21 loss=13.1, nll_loss=11.933, ppl=3909.75, wps=1850.4, ups=0.37, wpb=4997, bsz=92, num_updates=292, lr=3.504e-06, gnorm=1.906, train_wall=5, gb_free=13.8, wall=1882
2024-09-04 07:32:11 | INFO | train_inner | epoch 014:     21 / 21 loss=12.845, nll_loss=11.596, ppl=3095.02, wps=1631.9, ups=0.33, wpb=4891.5, bsz=156, num_updates=294, lr=3.528e-06, gnorm=2.12, train_wall=6, gb_free=12.1, wall=1888
2024-09-04 07:32:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25350.078125Mb; avail=229679.26953125Mb
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000723
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25350.5703125Mb; avail=229679.26953125Mb
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012789
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25350.5703125Mb; avail=229679.26953125Mb
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011271
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025142
2024-09-04 07:32:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25350.5703125Mb; avail=229679.26953125Mb
2024-09-04 07:32:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 11 @ 176 updates, score 14.469) (writing took 66.58103103656322 seconds)
2024-09-04 07:32:13 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-09-04 07:32:13 | INFO | train | epoch 011 | loss 14.77 | nll_loss 14.02 | ppl 16617.7 | wps 537.2 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 176 | lr 2.112e-06 | gnorm 1.76 | train_wall 45 | gb_free 10.3 | wall 1403
2024-09-04 07:32:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:32:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:32:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:32:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000732
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25380.06640625Mb; avail=229649.65625Mb
2024-09-04 07:32:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000641
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25380.06640625Mb; avail=229649.65625Mb
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25380.06640625Mb; avail=229649.65625Mb
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000175
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001144
2024-09-04 07:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25380.06640625Mb; avail=229649.65625Mb
2024-09-04 07:32:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:32:13 | INFO | fairseq.trainer | begin training epoch 12
2024-09-04 07:32:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:32:19 | INFO | train_inner | epoch 012:      2 / 16 loss=14.72, nll_loss=13.958, ppl=15916.8, wps=105.2, ups=0.02, wpb=4564, bsz=120, num_updates=178, lr=2.136e-06, gnorm=1.616, train_wall=6, gb_free=10.4, wall=1409
2024-09-04 07:32:25 | INFO | train_inner | epoch 012:      4 / 16 loss=14.814, nll_loss=14.077, ppl=17278.1, wps=1501.3, ups=0.32, wpb=4630.5, bsz=112, num_updates=180, lr=2.16e-06, gnorm=1.542, train_wall=6, gb_free=9.4, wall=1415
2024-09-04 07:32:29 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 13.02 | nll_loss 11.753 | ppl 3452.57 | wps 4455.3 | wpb 2350.9 | bsz 94.7 | num_updates 294 | best_loss 13.02
2024-09-04 07:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 294 updates
2024-09-04 07:32:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:32:31 | INFO | train_inner | epoch 012:      6 / 16 loss=14.728, nll_loss=13.969, ppl=16031, wps=1555.8, ups=0.36, wpb=4329, bsz=116, num_updates=182, lr=2.184e-06, gnorm=1.61, train_wall=6, gb_free=10.4, wall=1420
2024-09-04 07:32:37 | INFO | train_inner | epoch 012:      8 / 16 loss=14.46, nll_loss=13.63, ppl=12677.2, wps=1458.1, ups=0.34, wpb=4247, bsz=184, num_updates=184, lr=2.208e-06, gnorm=2.014, train_wall=6, gb_free=9.8, wall=1426
2024-09-04 07:32:41 | INFO | train_inner | epoch 012:     10 / 16 loss=14.904, nll_loss=14.192, ppl=18717.6, wps=1402.9, ups=0.47, wpb=2981.5, bsz=45, num_updates=186, lr=2.232e-06, gnorm=1.778, train_wall=4, gb_free=15.2, wall=1431
2024-09-04 07:32:47 | INFO | train_inner | epoch 012:     12 / 16 loss=14.64, nll_loss=13.859, ppl=14861.9, wps=1574, ups=0.36, wpb=4422, bsz=124, num_updates=188, lr=2.256e-06, gnorm=1.633, train_wall=6, gb_free=11.4, wall=1436
2024-09-04 07:32:53 | INFO | train_inner | epoch 012:     14 / 16 loss=14.582, nll_loss=13.787, ppl=14130.7, wps=1460.7, ups=0.34, wpb=4344.5, bsz=140, num_updates=190, lr=2.28e-06, gnorm=1.704, train_wall=6, gb_free=9.5, wall=1442
2024-09-04 07:32:58 | INFO | train_inner | epoch 012:     16 / 16 loss=14.447, nll_loss=13.616, ppl=12551.2, wps=1513.5, ups=0.35, wpb=4373, bsz=176, num_updates=192, lr=2.304e-06, gnorm=1.782, train_wall=6, gb_free=11.2, wall=1448
2024-09-04 07:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42537.2890625Mb; avail=212492.49609375Mb
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000736
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42537.78125Mb; avail=212492.00390625Mb
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012643
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42537.78125Mb; avail=212492.00390625Mb
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011161
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024933
2024-09-04 07:32:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42537.2890625Mb; avail=212492.49609375Mb
2024-09-04 07:33:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:33:13 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 14.38 | nll_loss 13.501 | ppl 11592.9 | wps 3850.9 | wpb 2070.5 | bsz 122.7 | num_updates 192 | best_loss 14.38
2024-09-04 07:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 192 updates
2024-09-04 07:33:13 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:33:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 14 @ 294 updates, score 13.02) (writing took 62.39540724363178 seconds)
2024-09-04 07:33:31 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-09-04 07:33:31 | INFO | train | epoch 014 | loss 13.127 | nll_loss 11.962 | ppl 3990.4 | wps 700.9 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 294 | lr 3.528e-06 | gnorm 1.973 | train_wall 56 | gb_free 12.1 | wall 1968
2024-09-04 07:33:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:33:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:33:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:33:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000761
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40401.6875Mb; avail=214628.67578125Mb
2024-09-04 07:33:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000671
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40401.1953125Mb; avail=214628.67578125Mb
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40401.1953125Mb; avail=214628.67578125Mb
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000198
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001201
2024-09-04 07:33:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40401.1953125Mb; avail=214628.67578125Mb
2024-09-04 07:33:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:33:31 | INFO | fairseq.trainer | begin training epoch 15
2024-09-04 07:33:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:33:37 | INFO | train_inner | epoch 015:      2 / 21 loss=12.988, nll_loss=11.792, ppl=3545.68, wps=112.9, ups=0.02, wpb=4894.5, bsz=84, num_updates=296, lr=3.552e-06, gnorm=1.929, train_wall=6, gb_free=11.9, wall=1974
2024-09-04 07:33:42 | INFO | train_inner | epoch 015:      4 / 21 loss=13.101, nll_loss=11.935, ppl=3916.84, wps=1909.2, ups=0.43, wpb=4439.5, bsz=80, num_updates=298, lr=3.576e-06, gnorm=1.888, train_wall=5, gb_free=12.6, wall=1979
2024-09-04 07:33:47 | INFO | train_inner | epoch 015:      6 / 21 loss=12.832, nll_loss=11.587, ppl=3076.7, wps=1827.1, ups=0.42, wpb=4373, bsz=120, num_updates=300, lr=3.6e-06, gnorm=2.137, train_wall=5, gb_free=13.5, wall=1984
2024-09-04 07:33:52 | INFO | train_inner | epoch 015:      8 / 21 loss=13.044, nll_loss=11.864, ppl=3728.65, wps=1482.3, ups=0.38, wpb=3872.5, bsz=45, num_updates=302, lr=3.624e-06, gnorm=2.092, train_wall=5, gb_free=13.5, wall=1989
2024-09-04 07:33:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:33:56 | INFO | train_inner | epoch 015:     10 / 21 loss=13.168, nll_loss=12.019, ppl=4150.64, wps=2002, ups=0.45, wpb=4414, bsz=56, num_updates=304, lr=3.648e-06, gnorm=1.951, train_wall=4, gb_free=13.1, wall=1993
2024-09-04 07:34:02 | INFO | train_inner | epoch 015:     12 / 21 loss=13.029, nll_loss=11.836, ppl=3656.64, wps=1648.5, ups=0.37, wpb=4481.5, bsz=80, num_updates=306, lr=3.672e-06, gnorm=1.896, train_wall=5, gb_free=15.8, wall=1999
2024-09-04 07:34:08 | INFO | train_inner | epoch 015:     14 / 21 loss=12.778, nll_loss=11.516, ppl=2929.47, wps=1621.5, ups=0.32, wpb=5115.5, bsz=120, num_updates=308, lr=3.696e-06, gnorm=1.921, train_wall=6, gb_free=13.5, wall=2005
2024-09-04 07:34:13 | INFO | train_inner | epoch 015:     16 / 21 loss=12.877, nll_loss=11.646, ppl=3205.71, wps=1926.8, ups=0.4, wpb=4769.5, bsz=100, num_updates=310, lr=3.72e-06, gnorm=1.909, train_wall=5, gb_free=12.1, wall=2010
2024-09-04 07:34:18 | INFO | train_inner | epoch 015:     18 / 21 loss=12.751, nll_loss=11.487, ppl=2869.66, wps=1911.5, ups=0.43, wpb=4471.5, bsz=120, num_updates=312, lr=3.744e-06, gnorm=1.929, train_wall=5, gb_free=12.7, wall=2015
2024-09-04 07:34:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 12 @ 192 updates, score 14.38) (writing took 66.25435881968588 seconds)
2024-09-04 07:34:19 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-09-04 07:34:19 | INFO | train | epoch 012 | loss 14.654 | nll_loss 13.876 | ppl 15035.1 | wps 539.4 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 192 | lr 2.304e-06 | gnorm 1.71 | train_wall 45 | gb_free 11.2 | wall 1529
2024-09-04 07:34:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:34:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:34:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:34:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000626
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28657.37109375Mb; avail=226372.51171875Mb
2024-09-04 07:34:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000067
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000601
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28657.37109375Mb; avail=226372.51171875Mb
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28657.37109375Mb; avail=226372.51171875Mb
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000159
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001085
2024-09-04 07:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28657.37109375Mb; avail=226372.51171875Mb
2024-09-04 07:34:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:34:19 | INFO | fairseq.trainer | begin training epoch 13
2024-09-04 07:34:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:34:24 | INFO | train_inner | epoch 015:     20 / 21 loss=12.866, nll_loss=11.636, ppl=3183.02, wps=1658.9, ups=0.35, wpb=4779.5, bsz=120, num_updates=314, lr=3.768e-06, gnorm=1.933, train_wall=6, gb_free=14.5, wall=2020
2024-09-04 07:34:25 | INFO | train_inner | epoch 013:      2 / 16 loss=14.671, nll_loss=13.901, ppl=15292.8, wps=105.1, ups=0.02, wpb=4546, bsz=124, num_updates=194, lr=2.328e-06, gnorm=1.582, train_wall=6, gb_free=10.7, wall=1534
2024-09-04 07:34:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28698.91015625Mb; avail=226330.9296875Mb
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000729
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28699.40234375Mb; avail=226330.4375Mb
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012789
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28699.40234375Mb; avail=226330.4375Mb
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011188
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025083
2024-09-04 07:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28699.40234375Mb; avail=226330.4375Mb
2024-09-04 07:34:31 | INFO | train_inner | epoch 013:      4 / 16 loss=14.535, nll_loss=13.729, ppl=13577, wps=1517.7, ups=0.35, wpb=4360, bsz=132, num_updates=196, lr=2.352e-06, gnorm=1.578, train_wall=6, gb_free=9.5, wall=1540
2024-09-04 07:34:36 | INFO | train_inner | epoch 013:      6 / 16 loss=14.148, nll_loss=13.237, ppl=9654.5, wps=1442.3, ups=0.34, wpb=4236.5, bsz=212, num_updates=198, lr=2.376e-06, gnorm=2.156, train_wall=6, gb_free=10.4, wall=1546
2024-09-04 07:34:42 | INFO | train_inner | epoch 013:      8 / 16 loss=14.496, nll_loss=13.68, ppl=13127, wps=1528.7, ups=0.34, wpb=4476, bsz=124, num_updates=200, lr=2.4e-06, gnorm=1.637, train_wall=6, gb_free=8.1, wall=1552
2024-09-04 07:34:43 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 12.827 | nll_loss 11.508 | ppl 2913.3 | wps 4693.9 | wpb 2350.9 | bsz 94.7 | num_updates 315 | best_loss 12.827
2024-09-04 07:34:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 315 updates
2024-09-04 07:34:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:34:48 | INFO | train_inner | epoch 013:     10 / 16 loss=14.495, nll_loss=13.677, ppl=13094.3, wps=1432.9, ups=0.34, wpb=4237.5, bsz=136, num_updates=202, lr=2.424e-06, gnorm=1.752, train_wall=6, gb_free=10.3, wall=1558
2024-09-04 07:34:54 | INFO | train_inner | epoch 013:     12 / 16 loss=14.632, nll_loss=13.851, ppl=14775, wps=1495.9, ups=0.35, wpb=4280.5, bsz=108, num_updates=204, lr=2.448e-06, gnorm=1.573, train_wall=6, gb_free=8.6, wall=1563
2024-09-04 07:34:59 | INFO | train_inner | epoch 013:     14 / 16 loss=14.521, nll_loss=13.708, ppl=13380.4, wps=1426.7, ups=0.4, wpb=3534, bsz=105, num_updates=206, lr=2.472e-06, gnorm=1.68, train_wall=5, gb_free=8.5, wall=1568
2024-09-04 07:35:05 | INFO | train_inner | epoch 013:     16 / 16 loss=14.732, nll_loss=13.977, ppl=16126.7, wps=1515.4, ups=0.36, wpb=4221, bsz=76, num_updates=208, lr=2.496e-06, gnorm=1.544, train_wall=6, gb_free=9.4, wall=1574
2024-09-04 07:35:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43908.05078125Mb; avail=211121.875Mb
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000764
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43908.54296875Mb; avail=211121.3828125Mb
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012727
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43908.54296875Mb; avail=211121.3828125Mb
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011114
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025007
2024-09-04 07:35:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43908.54296875Mb; avail=211121.3828125Mb
2024-09-04 07:35:19 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 14.276 | nll_loss 13.355 | ppl 10475.8 | wps 3849.2 | wpb 2070.5 | bsz 122.7 | num_updates 208 | best_loss 14.276
2024-09-04 07:35:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 208 updates
2024-09-04 07:35:19 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:35:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:35:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 15 @ 315 updates, score 12.827) (writing took 72.61024369765073 seconds)
2024-09-04 07:35:56 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-09-04 07:35:56 | INFO | train | epoch 015 | loss 12.924 | nll_loss 11.707 | ppl 3343.01 | wps 661.6 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 315 | lr 3.78e-06 | gnorm 1.971 | train_wall 54 | gb_free 12.7 | wall 2113
2024-09-04 07:35:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:35:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:35:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:35:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000651
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41925.45703125Mb; avail=213104.46875Mb
2024-09-04 07:35:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000597
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41925.45703125Mb; avail=213104.46875Mb
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41925.45703125Mb; avail=213104.46875Mb
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001085
2024-09-04 07:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41925.45703125Mb; avail=213104.46875Mb
2024-09-04 07:35:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:35:56 | INFO | fairseq.trainer | begin training epoch 16
2024-09-04 07:35:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:35:58 | INFO | train_inner | epoch 016:      1 / 21 loss=12.765, nll_loss=11.505, ppl=2906.47, wps=94.3, ups=0.02, wpb=4472.5, bsz=132, num_updates=316, lr=3.792e-06, gnorm=2.04, train_wall=5, gb_free=12.1, wall=2115
2024-09-04 07:36:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:36:03 | INFO | train_inner | epoch 016:      3 / 21 loss=12.651, nll_loss=11.368, ppl=2643.25, wps=1851.9, ups=0.45, wpb=4150.5, bsz=108, num_updates=318, lr=3.816e-06, gnorm=2.262, train_wall=4, gb_free=14.9, wall=2120
2024-09-04 07:36:07 | INFO | train_inner | epoch 016:      5 / 21 loss=12.926, nll_loss=11.72, ppl=3372.76, wps=1961.4, ups=0.45, wpb=4360, bsz=49, num_updates=320, lr=3.84e-06, gnorm=2.092, train_wall=4, gb_free=11.8, wall=2124
2024-09-04 07:36:13 | INFO | train_inner | epoch 016:      7 / 21 loss=12.761, nll_loss=11.488, ppl=2873.21, wps=1597.9, ups=0.36, wpb=4432, bsz=96, num_updates=322, lr=3.864e-06, gnorm=2.241, train_wall=6, gb_free=12.8, wall=2130
2024-09-04 07:36:18 | INFO | train_inner | epoch 016:      9 / 21 loss=12.617, nll_loss=11.307, ppl=2534.46, wps=2015.5, ups=0.41, wpb=4915, bsz=148, num_updates=324, lr=3.888e-06, gnorm=2.159, train_wall=5, gb_free=13.8, wall=2135
2024-09-04 07:36:22 | INFO | train_inner | epoch 016:     11 / 21 loss=12.785, nll_loss=11.532, ppl=2961.37, wps=1960.5, ups=0.43, wpb=4529.5, bsz=92, num_updates=326, lr=3.912e-06, gnorm=2.141, train_wall=5, gb_free=16.7, wall=2139
2024-09-04 07:36:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 13 @ 208 updates, score 14.276) (writing took 66.76640758384019 seconds)
2024-09-04 07:36:26 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-09-04 07:36:26 | INFO | train | epoch 013 | loss 14.53 | nll_loss 13.722 | ppl 13508.7 | wps 535.4 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 208 | lr 2.496e-06 | gnorm 1.688 | train_wall 45 | gb_free 9.4 | wall 1655
2024-09-04 07:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000792
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29941.56640625Mb; avail=225088.31640625Mb
2024-09-04 07:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000612
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29941.56640625Mb; avail=225088.31640625Mb
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29941.56640625Mb; avail=225088.31640625Mb
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000199
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001140
2024-09-04 07:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29941.56640625Mb; avail=225088.31640625Mb
2024-09-04 07:36:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:36:26 | INFO | fairseq.trainer | begin training epoch 14
2024-09-04 07:36:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:36:28 | INFO | train_inner | epoch 016:     13 / 21 loss=12.654, nll_loss=11.365, ppl=2638.13, wps=1627, ups=0.35, wpb=4673, bsz=108, num_updates=328, lr=3.936e-06, gnorm=1.969, train_wall=6, gb_free=12.9, wall=2145
2024-09-04 07:36:32 | INFO | train_inner | epoch 014:      2 / 16 loss=14.591, nll_loss=13.798, ppl=14242, wps=106.9, ups=0.02, wpb=4666, bsz=120, num_updates=210, lr=2.52e-06, gnorm=1.48, train_wall=6, gb_free=10.3, wall=1661
2024-09-04 07:36:33 | INFO | train_inner | epoch 016:     15 / 21 loss=12.688, nll_loss=11.417, ppl=2734.62, wps=1827.8, ups=0.38, wpb=4873.5, bsz=80, num_updates=330, lr=3.96e-06, gnorm=1.977, train_wall=5, gb_free=13.1, wall=2150
2024-09-04 07:36:37 | INFO | train_inner | epoch 014:      4 / 16 loss=14.633, nll_loss=13.852, ppl=14783, wps=1495.8, ups=0.42, wpb=3564, bsz=77, num_updates=212, lr=2.544e-06, gnorm=1.571, train_wall=5, gb_free=11.9, wall=1666
2024-09-04 07:36:39 | INFO | train_inner | epoch 016:     17 / 21 loss=12.496, nll_loss=11.162, ppl=2291.15, wps=1579.4, ups=0.34, wpb=4697.5, bsz=112, num_updates=332, lr=3.984e-06, gnorm=2.02, train_wall=6, gb_free=12, wall=2156
2024-09-04 07:36:42 | INFO | train_inner | epoch 014:      6 / 16 loss=14.474, nll_loss=13.65, ppl=12853, wps=1463.5, ups=0.35, wpb=4142.5, bsz=112, num_updates=214, lr=2.568e-06, gnorm=1.597, train_wall=6, gb_free=10.8, wall=1672
2024-09-04 07:36:45 | INFO | train_inner | epoch 016:     19 / 21 loss=12.72, nll_loss=11.444, ppl=2785.9, wps=1702.8, ups=0.39, wpb=4420, bsz=88, num_updates=334, lr=4.008e-06, gnorm=1.981, train_wall=5, gb_free=12.9, wall=2162
2024-09-04 07:36:48 | INFO | train_inner | epoch 014:      8 / 16 loss=14.397, nll_loss=13.553, ppl=12019.5, wps=1534.7, ups=0.33, wpb=4709, bsz=148, num_updates=216, lr=2.592e-06, gnorm=1.551, train_wall=6, gb_free=11, wall=1678
2024-09-04 07:36:49 | INFO | train_inner | epoch 016:     21 / 21 loss=12.717, nll_loss=11.44, ppl=2777.94, wps=1987, ups=0.44, wpb=4547.5, bsz=96, num_updates=336, lr=4.032e-06, gnorm=2.046, train_wall=5, gb_free=13.8, wall=2166
2024-09-04 07:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29987.1484375Mb; avail=225042.69140625Mb
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29987.1484375Mb; avail=225042.69140625Mb
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012730
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29987.1484375Mb; avail=225042.69140625Mb
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011241
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025087
2024-09-04 07:36:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29987.640625Mb; avail=225042.19921875Mb
2024-09-04 07:36:54 | INFO | train_inner | epoch 014:     10 / 16 loss=14.091, nll_loss=13.163, ppl=9171, wps=1450.1, ups=0.34, wpb=4229, bsz=192, num_updates=218, lr=2.616e-06, gnorm=2.047, train_wall=6, gb_free=10.1, wall=1684
2024-09-04 07:37:00 | INFO | train_inner | epoch 014:     12 / 16 loss=14.431, nll_loss=13.598, ppl=12402.7, wps=1561.7, ups=0.34, wpb=4544.5, bsz=116, num_updates=220, lr=2.64e-06, gnorm=1.514, train_wall=6, gb_free=10.3, wall=1690
2024-09-04 07:37:05 | INFO | train_inner | epoch 014:     14 / 16 loss=14.671, nll_loss=13.9, ppl=15290.4, wps=1526.1, ups=0.43, wpb=3509.5, bsz=56, num_updates=222, lr=2.664e-06, gnorm=1.725, train_wall=5, gb_free=10, wall=1694
2024-09-04 07:37:07 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 12.622 | nll_loss 11.242 | ppl 2422.77 | wps 4681.7 | wpb 2350.9 | bsz 94.7 | num_updates 336 | best_loss 12.622
2024-09-04 07:37:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 336 updates
2024-09-04 07:37:07 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:37:11 | INFO | train_inner | epoch 014:     16 / 16 loss=14.129, nll_loss=13.208, ppl=9459.98, wps=1530.6, ups=0.34, wpb=4527, bsz=196, num_updates=224, lr=2.688e-06, gnorm=1.85, train_wall=6, gb_free=9.9, wall=1700
2024-09-04 07:37:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30024.265625Mb; avail=225005.53125Mb
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000747
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30024.26171875Mb; avail=225005.53125Mb
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012632
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30024.26171875Mb; avail=225005.53125Mb
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011151
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024959
2024-09-04 07:37:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30024.26171875Mb; avail=225005.53125Mb
2024-09-04 07:37:25 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 14.185 | nll_loss 13.219 | ppl 9537.41 | wps 3852.6 | wpb 2070.5 | bsz 122.7 | num_updates 224 | best_loss 14.185
2024-09-04 07:37:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 224 updates
2024-09-04 07:37:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:37:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:38:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 16 @ 336 updates, score 12.622) (writing took 62.66653869487345 seconds)
2024-09-04 07:38:09 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-09-04 07:38:09 | INFO | train | epoch 016 | loss 12.709 | nll_loss 11.434 | ppl 2767.66 | wps 717.8 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 336 | lr 4.032e-06 | gnorm 2.078 | train_wall 53 | gb_free 13.8 | wall 2246
2024-09-04 07:38:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:38:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:38:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:38:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000757
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40372.78515625Mb; avail=214656.59765625Mb
2024-09-04 07:38:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000624
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40376.72265625Mb; avail=214652.66015625Mb
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40377.21484375Mb; avail=214656.59765625Mb
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000190
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001123
2024-09-04 07:38:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40369.83203125Mb; avail=214659.55078125Mb
2024-09-04 07:38:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:38:09 | INFO | fairseq.trainer | begin training epoch 17
2024-09-04 07:38:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:38:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:38:14 | INFO | train_inner | epoch 017:      2 / 21 loss=12.38, nll_loss=11.019, ppl=2075, wps=110, ups=0.02, wpb=4662.5, bsz=148, num_updates=338, lr=4.056e-06, gnorm=1.998, train_wall=5, gb_free=12.7, wall=2251
2024-09-04 07:38:19 | INFO | train_inner | epoch 017:      4 / 21 loss=12.476, nll_loss=11.131, ppl=2243.18, wps=1900.9, ups=0.4, wpb=4798.5, bsz=128, num_updates=340, lr=4.08e-06, gnorm=2.091, train_wall=5, gb_free=14, wall=2256
2024-09-04 07:38:24 | INFO | train_inner | epoch 017:      6 / 21 loss=12.561, nll_loss=11.247, ppl=2431.2, wps=1672.2, ups=0.36, wpb=4620, bsz=84, num_updates=342, lr=4.104e-06, gnorm=2.067, train_wall=6, gb_free=13.6, wall=2261
2024-09-04 07:38:30 | INFO | train_inner | epoch 017:      8 / 21 loss=12.542, nll_loss=11.225, ppl=2394.37, wps=1490.6, ups=0.37, wpb=4083, bsz=65, num_updates=344, lr=4.128e-06, gnorm=2.272, train_wall=5, gb_free=10.1, wall=2267
2024-09-04 07:38:35 | INFO | train_inner | epoch 017:     10 / 21 loss=12.534, nll_loss=11.209, ppl=2367.41, wps=1974.3, ups=0.41, wpb=4787, bsz=92, num_updates=346, lr=4.152e-06, gnorm=2.128, train_wall=5, gb_free=13.2, wall=2272
2024-09-04 07:38:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 14 @ 224 updates, score 14.185) (writing took 70.27580225095153 seconds)
2024-09-04 07:38:35 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-09-04 07:38:35 | INFO | train | epoch 014 | loss 14.417 | nll_loss 13.577 | ppl 12222.1 | wps 523.3 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 224 | lr 2.688e-06 | gnorm 1.667 | train_wall 45 | gb_free 9.9 | wall 1785
2024-09-04 07:38:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:38:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:38:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:38:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000615
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28398.0234375Mb; avail=226631.859375Mb
2024-09-04 07:38:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000597
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28398.0234375Mb; avail=226631.859375Mb
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28398.0234375Mb; avail=226631.859375Mb
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000155
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001069
2024-09-04 07:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28398.0234375Mb; avail=226631.859375Mb
2024-09-04 07:38:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:38:35 | INFO | fairseq.trainer | begin training epoch 15
2024-09-04 07:38:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:38:41 | INFO | train_inner | epoch 017:     12 / 21 loss=12.579, nll_loss=11.267, ppl=2464.8, wps=1645.6, ups=0.35, wpb=4758, bsz=64, num_updates=348, lr=4.176e-06, gnorm=2.047, train_wall=6, gb_free=12.3, wall=2278
2024-09-04 07:38:41 | INFO | train_inner | epoch 015:      2 / 16 loss=14.539, nll_loss=13.736, ppl=13642.5, wps=96.6, ups=0.02, wpb=4364.5, bsz=96, num_updates=226, lr=2.712e-06, gnorm=1.493, train_wall=6, gb_free=8.3, wall=1790
2024-09-04 07:38:45 | INFO | train_inner | epoch 017:     14 / 21 loss=12.225, nll_loss=10.819, ppl=1806.05, wps=1883.5, ups=0.42, wpb=4482.5, bsz=120, num_updates=350, lr=4.2e-06, gnorm=2.204, train_wall=5, gb_free=13, wall=2282
2024-09-04 07:38:46 | INFO | train_inner | epoch 015:      4 / 16 loss=14.318, nll_loss=13.453, ppl=11215.9, wps=1480.1, ups=0.36, wpb=4106.5, bsz=124, num_updates=228, lr=2.736e-06, gnorm=1.636, train_wall=6, gb_free=14.6, wall=1796
2024-09-04 07:38:51 | INFO | train_inner | epoch 017:     16 / 21 loss=12.57, nll_loss=11.259, ppl=2450.92, wps=1716.4, ups=0.35, wpb=4895.5, bsz=80, num_updates=352, lr=4.224e-06, gnorm=2.35, train_wall=6, gb_free=12.3, wall=2288
2024-09-04 07:38:52 | INFO | train_inner | epoch 015:      6 / 16 loss=14.021, nll_loss=13.075, ppl=8627.62, wps=1471.9, ups=0.33, wpb=4428, bsz=196, num_updates=230, lr=2.76e-06, gnorm=1.892, train_wall=6, gb_free=8.5, wall=1802
2024-09-04 07:38:58 | INFO | train_inner | epoch 015:      8 / 16 loss=14.336, nll_loss=13.473, ppl=11368.2, wps=1515.1, ups=0.34, wpb=4498, bsz=124, num_updates=232, lr=2.784e-06, gnorm=1.537, train_wall=6, gb_free=10.9, wall=1808
2024-09-04 07:39:04 | INFO | train_inner | epoch 015:     10 / 16 loss=14.452, nll_loss=13.626, ppl=12638.2, wps=1518.6, ups=0.36, wpb=4238.5, bsz=92, num_updates=234, lr=2.808e-06, gnorm=1.479, train_wall=6, gb_free=13.5, wall=1814
2024-09-04 07:39:06 | INFO | train_inner | epoch 017:     18 / 21 loss=12.425, nll_loss=11.069, ppl=2147.9, wps=597.7, ups=0.13, wpb=4483, bsz=116, num_updates=354, lr=4.248e-06, gnorm=2.168, train_wall=15, gb_free=13.5, wall=2303
2024-09-04 07:39:09 | INFO | train_inner | epoch 015:     12 / 16 loss=14.306, nll_loss=13.437, ppl=11093.2, wps=1490.8, ups=0.38, wpb=3942, bsz=104, num_updates=236, lr=2.832e-06, gnorm=1.552, train_wall=5, gb_free=10.7, wall=1819
2024-09-04 07:39:11 | INFO | train_inner | epoch 017:     20 / 21 loss=12.463, nll_loss=11.112, ppl=2213.04, wps=1896.8, ups=0.45, wpb=4256.5, bsz=88, num_updates=356, lr=4.272e-06, gnorm=2.268, train_wall=4, gb_free=12.3, wall=2307
2024-09-04 07:39:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22808.37890625Mb; avail=232221.54296875Mb
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000652
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22808.37890625Mb; avail=232221.54296875Mb
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012720
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22808.37890625Mb; avail=232221.54296875Mb
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011183
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024992
2024-09-04 07:39:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22808.37890625Mb; avail=232221.54296875Mb
2024-09-04 07:39:15 | INFO | train_inner | epoch 015:     14 / 16 loss=14.281, nll_loss=13.404, ppl=10842.8, wps=1575.1, ups=0.35, wpb=4545.5, bsz=124, num_updates=238, lr=2.856e-06, gnorm=1.485, train_wall=6, gb_free=11.4, wall=1825
2024-09-04 07:39:20 | INFO | train_inner | epoch 015:     16 / 16 loss=14.039, nll_loss=13.089, ppl=8711.57, wps=1437.7, ups=0.38, wpb=3768.5, bsz=157, num_updates=240, lr=2.88e-06, gnorm=1.679, train_wall=5, gb_free=13.7, wall=1830
2024-09-04 07:39:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22842.78515625Mb; avail=232187.09765625Mb
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000737
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22842.78515625Mb; avail=232187.09765625Mb
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012683
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22842.78515625Mb; avail=232187.09765625Mb
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011116
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024902
2024-09-04 07:39:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22842.78515625Mb; avail=232187.09765625Mb
2024-09-04 07:39:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 12.455 | nll_loss 11.013 | ppl 2067.15 | wps 4869.4 | wpb 2350.9 | bsz 94.7 | num_updates 357 | best_loss 12.455
2024-09-04 07:39:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 357 updates
2024-09-04 07:39:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:39:35 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 14.103 | nll_loss 13.092 | ppl 8734.25 | wps 3854.9 | wpb 2070.5 | bsz 122.7 | num_updates 240 | best_loss 14.103
2024-09-04 07:39:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 240 updates
2024-09-04 07:39:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:40:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:40:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:40:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 17 @ 357 updates, score 12.455) (writing took 63.54493291024119 seconds)
2024-09-04 07:40:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-09-04 07:40:33 | INFO | train | epoch 017 | loss 12.485 | nll_loss 11.148 | ppl 2269.23 | wps 665.6 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 357 | lr 4.284e-06 | gnorm 2.146 | train_wall 63 | gb_free 17.2 | wall 2390
2024-09-04 07:40:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:40:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:40:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:40:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000702
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28491.57421875Mb; avail=226538.3359375Mb
2024-09-04 07:40:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000069
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000624
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28492.06640625Mb; avail=226537.84375Mb
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28492.06640625Mb; avail=226537.84375Mb
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000181
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001152
2024-09-04 07:40:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28492.06640625Mb; avail=226537.84375Mb
2024-09-04 07:40:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:40:33 | INFO | fairseq.trainer | begin training epoch 18
2024-09-04 07:40:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:40:35 | INFO | train_inner | epoch 018:      1 / 21 loss=12.596, nll_loss=11.3, ppl=2521.25, wps=101.6, ups=0.02, wpb=4310, bsz=56, num_updates=358, lr=4.296e-06, gnorm=1.952, train_wall=4, gb_free=10.2, wall=2392
2024-09-04 07:40:39 | INFO | train_inner | epoch 018:      3 / 21 loss=12.328, nll_loss=10.956, ppl=1986.6, wps=1919.5, ups=0.51, wpb=3735.5, bsz=61, num_updates=360, lr=4.32e-06, gnorm=2.383, train_wall=4, gb_free=18.3, wall=2396
2024-09-04 07:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 15 @ 240 updates, score 14.103) (writing took 64.60303280502558 seconds)
2024-09-04 07:40:39 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-09-04 07:40:39 | INFO | train | epoch 015 | loss 14.289 | nll_loss 13.415 | ppl 10925.3 | wps 546.2 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 240 | lr 2.88e-06 | gnorm 1.594 | train_wall 45 | gb_free 13.7 | wall 1909
2024-09-04 07:40:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:40:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:40:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:40:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000757
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28527.01953125Mb; avail=226502.8671875Mb
2024-09-04 07:40:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000079
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000656
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28527.01953125Mb; avail=226502.8671875Mb
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28527.01953125Mb; avail=226502.8671875Mb
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001144
2024-09-04 07:40:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28527.01953125Mb; avail=226502.8671875Mb
2024-09-04 07:40:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:40:39 | INFO | fairseq.trainer | begin training epoch 16
2024-09-04 07:40:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:40:45 | INFO | train_inner | epoch 018:      5 / 21 loss=12.452, nll_loss=11.109, ppl=2208.82, wps=1692.2, ups=0.36, wpb=4685, bsz=60, num_updates=362, lr=4.344e-06, gnorm=1.939, train_wall=6, gb_free=13.3, wall=2402
2024-09-04 07:40:45 | INFO | train_inner | epoch 016:      2 / 16 loss=14.092, nll_loss=13.162, ppl=9164.9, wps=97.5, ups=0.02, wpb=4131, bsz=148, num_updates=242, lr=2.904e-06, gnorm=1.963, train_wall=6, gb_free=9.7, wall=1915
2024-09-04 07:40:50 | INFO | train_inner | epoch 018:      7 / 21 loss=12.313, nll_loss=10.925, ppl=1943.72, wps=1878.9, ups=0.39, wpb=4799.5, bsz=84, num_updates=364, lr=4.368e-06, gnorm=1.951, train_wall=5, gb_free=12.4, wall=2407
2024-09-04 07:40:51 | INFO | train_inner | epoch 016:      4 / 16 loss=14.253, nll_loss=13.375, ppl=10620.8, wps=1572.9, ups=0.36, wpb=4314, bsz=124, num_updates=244, lr=2.928e-06, gnorm=1.518, train_wall=5, gb_free=14.2, wall=1920
2024-09-04 07:40:56 | INFO | train_inner | epoch 018:      9 / 21 loss=11.988, nll_loss=10.501, ppl=1449.43, wps=1609.9, ups=0.34, wpb=4695, bsz=156, num_updates=366, lr=4.392e-06, gnorm=2.024, train_wall=6, gb_free=11.7, wall=2413
2024-09-04 07:40:56 | INFO | train_inner | epoch 016:      6 / 16 loss=14.191, nll_loss=13.289, ppl=10010.4, wps=1581.2, ups=0.35, wpb=4535.5, bsz=140, num_updates=246, lr=2.952e-06, gnorm=1.432, train_wall=6, gb_free=10.6, wall=1926
2024-09-04 07:41:00 | INFO | train_inner | epoch 018:     11 / 21 loss=11.977, nll_loss=10.488, ppl=1436.08, wps=1892.5, ups=0.42, wpb=4468, bsz=136, num_updates=368, lr=4.416e-06, gnorm=1.96, train_wall=5, gb_free=14.5, wall=2417
2024-09-04 07:41:02 | INFO | train_inner | epoch 016:      8 / 16 loss=14.447, nll_loss=13.62, ppl=12588.3, wps=1544.6, ups=0.38, wpb=4091.5, bsz=80, num_updates=248, lr=2.976e-06, gnorm=1.498, train_wall=5, gb_free=9.9, wall=1931
2024-09-04 07:41:06 | INFO | train_inner | epoch 018:     13 / 21 loss=12.183, nll_loss=10.747, ppl=1718.22, wps=1606.2, ups=0.34, wpb=4696.5, bsz=116, num_updates=370, lr=4.44e-06, gnorm=1.962, train_wall=6, gb_free=12.3, wall=2423
2024-09-04 07:41:07 | INFO | train_inner | epoch 016:     10 / 16 loss=14.158, nll_loss=13.249, ppl=9735.33, wps=1412.9, ups=0.4, wpb=3491.5, bsz=89, num_updates=250, lr=3e-06, gnorm=1.626, train_wall=5, gb_free=10.5, wall=1936
2024-09-04 07:41:11 | INFO | train_inner | epoch 018:     15 / 21 loss=12.257, nll_loss=10.857, ppl=1854.44, wps=1911.1, ups=0.42, wpb=4498.5, bsz=112, num_updates=372, lr=4.464e-06, gnorm=1.871, train_wall=5, gb_free=13.3, wall=2428
2024-09-04 07:41:13 | INFO | train_inner | epoch 016:     12 / 16 loss=14.095, nll_loss=13.166, ppl=9190.36, wps=1513.8, ups=0.33, wpb=4545, bsz=156, num_updates=252, lr=3.024e-06, gnorm=1.509, train_wall=6, gb_free=8.7, wall=1942
2024-09-04 07:41:16 | INFO | train_inner | epoch 018:     17 / 21 loss=12.302, nll_loss=10.918, ppl=1934.8, wps=1669.8, ups=0.4, wpb=4136.5, bsz=72, num_updates=374, lr=4.488e-06, gnorm=2.067, train_wall=5, gb_free=14.5, wall=2433
2024-09-04 07:41:18 | INFO | train_inner | epoch 016:     14 / 16 loss=14.117, nll_loss=13.193, ppl=9366.06, wps=1506.2, ups=0.34, wpb=4472.5, bsz=148, num_updates=254, lr=3.048e-06, gnorm=1.446, train_wall=6, gb_free=9.8, wall=1948
2024-09-04 07:41:21 | INFO | train_inner | epoch 018:     19 / 21 loss=12.148, nll_loss=10.716, ppl=1682.43, wps=1780.9, ups=0.37, wpb=4770, bsz=112, num_updates=376, lr=4.512e-06, gnorm=1.785, train_wall=5, gb_free=12.5, wall=2438
2024-09-04 07:41:24 | INFO | train_inner | epoch 016:     16 / 16 loss=14.106, nll_loss=13.183, ppl=9298.11, wps=1447.3, ups=0.34, wpb=4310.5, bsz=132, num_updates=256, lr=3.072e-06, gnorm=1.472, train_wall=6, gb_free=9.4, wall=1954
2024-09-04 07:41:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28736.8828125Mb; avail=226292.9140625Mb
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000750
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28737.375Mb; avail=226292.421875Mb
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012590
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28737.375Mb; avail=226292.421875Mb
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011061
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024759
2024-09-04 07:41:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28737.375Mb; avail=226292.421875Mb
2024-09-04 07:41:27 | INFO | train_inner | epoch 018:     21 / 21 loss=12.202, nll_loss=10.792, ppl=1772.68, wps=1819.8, ups=0.36, wpb=5061, bsz=84, num_updates=378, lr=4.536e-06, gnorm=1.775, train_wall=6, gb_free=12.3, wall=2444
2024-09-04 07:41:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28775.953125Mb; avail=226253.83203125Mb
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000719
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28775.953125Mb; avail=226253.83203125Mb
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012880
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28776.1640625Mb; avail=226253.5859375Mb
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011182
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025163
2024-09-04 07:41:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28776.15625Mb; avail=226253.5859375Mb
2024-09-04 07:41:39 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 14.024 | nll_loss 12.972 | ppl 8032.8 | wps 3852.4 | wpb 2070.5 | bsz 122.7 | num_updates 256 | best_loss 14.024
2024-09-04 07:41:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 256 updates
2024-09-04 07:41:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:41:45 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 12.294 | nll_loss 10.789 | ppl 1768.74 | wps 4608.2 | wpb 2350.9 | bsz 94.7 | num_updates 378 | best_loss 12.294
2024-09-04 07:41:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 378 updates
2024-09-04 07:41:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:42:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:42:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 18 @ 378 updates, score 12.294) (writing took 67.49292165692896 seconds)
2024-09-04 07:42:52 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-09-04 07:42:52 | INFO | train | epoch 018 | loss 12.228 | nll_loss 10.818 | ppl 1804.98 | wps 687.5 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 378 | lr 4.536e-06 | gnorm 1.974 | train_wall 54 | gb_free 12.3 | wall 2529
2024-09-04 07:42:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:42:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:42:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:42:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000728
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25875.30859375Mb; avail=229154.62109375Mb
2024-09-04 07:42:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000642
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25875.30859375Mb; avail=229154.62109375Mb
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25875.30859375Mb; avail=229154.62109375Mb
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000194
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001148
2024-09-04 07:42:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25875.30859375Mb; avail=229154.62109375Mb
2024-09-04 07:42:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:42:52 | INFO | fairseq.trainer | begin training epoch 19
2024-09-04 07:42:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:42:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 16 @ 256 updates, score 14.024) (writing took 75.75409139692783 seconds)
2024-09-04 07:42:55 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-09-04 07:42:55 | INFO | train | epoch 016 | loss 14.181 | nll_loss 13.278 | ppl 9930.05 | wps 501.4 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 256 | lr 3.072e-06 | gnorm 1.558 | train_wall 45 | gb_free 9.4 | wall 2044
2024-09-04 07:42:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:42:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:42:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:42:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000644
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25985.234375Mb; avail=229045.14453125Mb
2024-09-04 07:42:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000585
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25985.234375Mb; avail=229045.14453125Mb
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25985.234375Mb; avail=229045.14453125Mb
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000162
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001057
2024-09-04 07:42:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25985.234375Mb; avail=229045.14453125Mb
2024-09-04 07:42:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:42:55 | INFO | fairseq.trainer | begin training epoch 17
2024-09-04 07:42:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:42:58 | INFO | train_inner | epoch 019:      2 / 21 loss=12.163, nll_loss=10.732, ppl=1700.95, wps=95.8, ups=0.02, wpb=4341.5, bsz=72, num_updates=380, lr=4.56e-06, gnorm=1.961, train_wall=5, gb_free=17, wall=2534
2024-09-04 07:43:00 | INFO | train_inner | epoch 017:      2 / 16 loss=14.268, nll_loss=13.394, ppl=10762.2, wps=92, ups=0.02, wpb=4417, bsz=100, num_updates=258, lr=3.096e-06, gnorm=1.422, train_wall=6, gb_free=10.2, wall=2050
2024-09-04 07:43:02 | INFO | train_inner | epoch 019:      4 / 21 loss=12.256, nll_loss=10.864, ppl=1863.25, wps=1898.1, ups=0.42, wpb=4556, bsz=72, num_updates=382, lr=4.584e-06, gnorm=1.782, train_wall=5, gb_free=12.9, wall=2539
2024-09-04 07:43:06 | INFO | train_inner | epoch 017:      4 / 16 loss=14.178, nll_loss=13.278, ppl=9929.9, wps=1592.5, ups=0.36, wpb=4466, bsz=120, num_updates=260, lr=3.12e-06, gnorm=1.451, train_wall=6, gb_free=10.5, wall=2055
2024-09-04 07:43:08 | INFO | train_inner | epoch 019:      6 / 21 loss=11.828, nll_loss=10.301, ppl=1261.68, wps=1631.2, ups=0.33, wpb=4902.5, bsz=140, num_updates=384, lr=4.608e-06, gnorm=1.946, train_wall=6, gb_free=12.3, wall=2545
2024-09-04 07:43:11 | INFO | train_inner | epoch 017:      6 / 16 loss=14.34, nll_loss=13.482, ppl=11445, wps=1480.1, ups=0.41, wpb=3652.5, bsz=64, num_updates=262, lr=3.144e-06, gnorm=1.487, train_wall=5, gb_free=10, wall=2060
2024-09-04 07:43:14 | INFO | train_inner | epoch 019:      8 / 21 loss=12.046, nll_loss=10.583, ppl=1533.69, wps=1716.2, ups=0.33, wpb=5257.5, bsz=96, num_updates=386, lr=4.632e-06, gnorm=1.764, train_wall=6, gb_free=12, wall=2551
2024-09-04 07:43:17 | INFO | train_inner | epoch 017:      8 / 16 loss=14.067, nll_loss=13.127, ppl=8947.6, wps=1555.5, ups=0.33, wpb=4685, bsz=136, num_updates=264, lr=3.168e-06, gnorm=1.543, train_wall=6, gb_free=8.4, wall=2066
2024-09-04 07:43:23 | INFO | train_inner | epoch 017:     10 / 16 loss=13.914, nll_loss=12.938, ppl=7847.02, wps=1481.9, ups=0.36, wpb=4164, bsz=156, num_updates=266, lr=3.192e-06, gnorm=2.037, train_wall=6, gb_free=11.4, wall=2072
2024-09-04 07:43:29 | INFO | train_inner | epoch 017:     12 / 16 loss=13.964, nll_loss=12.995, ppl=8162.62, wps=1463.5, ups=0.33, wpb=4435, bsz=152, num_updates=268, lr=3.216e-06, gnorm=1.559, train_wall=6, gb_free=9.2, wall=2078
2024-09-04 07:43:29 | INFO | train_inner | epoch 019:     10 / 21 loss=11.916, nll_loss=10.406, ppl=1356.52, wps=509.6, ups=0.14, wpb=3614.5, bsz=89, num_updates=388, lr=4.656e-06, gnorm=1.923, train_wall=14, gb_free=14.4, wall=2566
2024-09-04 07:43:34 | INFO | train_inner | epoch 019:     12 / 21 loss=12.232, nll_loss=10.83, ppl=1820.63, wps=1664.6, ups=0.37, wpb=4453.5, bsz=76, num_updates=390, lr=4.68e-06, gnorm=1.765, train_wall=5, gb_free=14.2, wall=2571
2024-09-04 07:43:35 | INFO | train_inner | epoch 017:     14 / 16 loss=13.771, nll_loss=12.751, ppl=6894.91, wps=1524.7, ups=0.33, wpb=4605, bsz=204, num_updates=270, lr=3.24e-06, gnorm=1.536, train_wall=6, gb_free=9.2, wall=2084
2024-09-04 07:43:39 | INFO | train_inner | epoch 019:     14 / 21 loss=11.682, nll_loss=10.113, ppl=1107.67, wps=1762.2, ups=0.41, wpb=4291, bsz=132, num_updates=392, lr=4.704e-06, gnorm=2.095, train_wall=5, gb_free=12.5, wall=2576
2024-09-04 07:43:40 | INFO | train_inner | epoch 017:     16 / 16 loss=14.167, nll_loss=13.264, ppl=9838.91, wps=1427.3, ups=0.41, wpb=3467, bsz=85, num_updates=272, lr=3.264e-06, gnorm=1.578, train_wall=5, gb_free=10.3, wall=2089
2024-09-04 07:43:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20429.8828125Mb; avail=234600.046875Mb
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000753
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20430.375Mb; avail=234599.5546875Mb
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012604
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20430.375Mb; avail=234599.5546875Mb
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011124
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024842
2024-09-04 07:43:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20430.375Mb; avail=234599.5546875Mb
2024-09-04 07:43:45 | INFO | train_inner | epoch 019:     16 / 21 loss=11.906, nll_loss=10.399, ppl=1350.27, wps=1630.3, ups=0.35, wpb=4714.5, bsz=124, num_updates=394, lr=4.728e-06, gnorm=1.899, train_wall=6, gb_free=14, wall=2582
2024-09-04 07:43:49 | INFO | train_inner | epoch 019:     18 / 21 loss=11.99, nll_loss=10.51, ppl=1458.48, wps=1950.8, ups=0.43, wpb=4517.5, bsz=84, num_updates=396, lr=4.752e-06, gnorm=1.792, train_wall=5, gb_free=12.7, wall=2586
2024-09-04 07:43:54 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 13.917 | nll_loss 12.842 | ppl 7342.32 | wps 3857.1 | wpb 2070.5 | bsz 122.7 | num_updates 272 | best_loss 13.917
2024-09-04 07:43:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 272 updates
2024-09-04 07:43:54 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:43:55 | INFO | train_inner | epoch 019:     20 / 21 loss=11.829, nll_loss=10.309, ppl=1268.15, wps=1663, ups=0.34, wpb=4880.5, bsz=100, num_updates=398, lr=4.776e-06, gnorm=1.768, train_wall=6, gb_free=13.4, wall=2592
2024-09-04 07:43:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20472.48046875Mb; avail=234557.40234375Mb
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000644
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20472.48046875Mb; avail=234557.40234375Mb
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012767
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20472.48046875Mb; avail=234557.40234375Mb
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011211
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024987
2024-09-04 07:43:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20472.48046875Mb; avail=234557.40234375Mb
2024-09-04 07:44:14 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 12.127 | nll_loss 10.562 | ppl 1511.58 | wps 5013 | wpb 2350.9 | bsz 94.7 | num_updates 399 | best_loss 12.127
2024-09-04 07:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 399 updates
2024-09-04 07:44:14 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:44:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 17 @ 272 updates, score 13.917) (writing took 62.550574528984725 seconds)
2024-09-04 07:44:56 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-09-04 07:44:56 | INFO | train | epoch 017 | loss 14.075 | nll_loss 13.142 | ppl 9041.92 | wps 556 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 272 | lr 3.264e-06 | gnorm 1.576 | train_wall 45 | gb_free 10.3 | wall 2166
2024-09-04 07:44:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:44:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:44:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:44:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000745
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40483.90234375Mb; avail=214545.53515625Mb
2024-09-04 07:44:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000640
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40488.82421875Mb; avail=214540.61328125Mb
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000042
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40490.30078125Mb; avail=214539.13671875Mb
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000191
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001201
2024-09-04 07:44:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40482.42578125Mb; avail=214547.50390625Mb
2024-09-04 07:44:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:44:57 | INFO | fairseq.trainer | begin training epoch 18
2024-09-04 07:44:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:44:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:45:02 | INFO | train_inner | epoch 018:      2 / 16 loss=13.808, nll_loss=12.807, ppl=7165.01, wps=111.3, ups=0.02, wpb=4616, bsz=184, num_updates=274, lr=3.288e-06, gnorm=1.542, train_wall=6, gb_free=10.8, wall=2172
2024-09-04 07:45:08 | INFO | train_inner | epoch 018:      4 / 16 loss=13.783, nll_loss=12.778, ppl=7023.05, wps=1375.1, ups=0.39, wpb=3569, bsz=136, num_updates=276, lr=3.312e-06, gnorm=1.896, train_wall=5, gb_free=11.8, wall=2177
2024-09-04 07:45:13 | INFO | train_inner | epoch 018:      6 / 16 loss=14.116, nll_loss=13.193, ppl=9363.27, wps=1559, ups=0.35, wpb=4490.5, bsz=112, num_updates=278, lr=3.336e-06, gnorm=1.476, train_wall=6, gb_free=8.7, wall=2183
2024-09-04 07:45:19 | INFO | train_inner | epoch 018:      8 / 16 loss=13.892, nll_loss=12.9, ppl=7643.41, wps=1553.1, ups=0.36, wpb=4309.5, bsz=144, num_updates=280, lr=3.36e-06, gnorm=1.693, train_wall=6, gb_free=10.3, wall=2188
2024-09-04 07:45:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 19 @ 399 updates, score 12.127) (writing took 68.5163193102926 seconds)
2024-09-04 07:45:22 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-09-04 07:45:22 | INFO | train | epoch 019 | loss 11.986 | nll_loss 10.507 | ppl 1454.96 | wps 636.9 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 399 | lr 4.788e-06 | gnorm 1.867 | train_wall 65 | gb_free 12.2 | wall 2679
2024-09-04 07:45:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:45:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:45:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:45:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000687
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27889.28125Mb; avail=227140.60546875Mb
2024-09-04 07:45:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000632
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27889.28125Mb; avail=227140.60546875Mb
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27889.28125Mb; avail=227140.60546875Mb
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001133
2024-09-04 07:45:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27889.28125Mb; avail=227140.60546875Mb
2024-09-04 07:45:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:45:22 | INFO | fairseq.trainer | begin training epoch 20
2024-09-04 07:45:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:45:24 | INFO | train_inner | epoch 018:     10 / 16 loss=14.109, nll_loss=13.185, ppl=9314.75, wps=1491.7, ups=0.43, wpb=3484.5, bsz=85, num_updates=282, lr=3.384e-06, gnorm=1.69, train_wall=5, gb_free=15, wall=2193
2024-09-04 07:45:25 | INFO | train_inner | epoch 020:      1 / 21 loss=11.623, nll_loss=10.032, ppl=1047.32, wps=95.9, ups=0.02, wpb=4314.5, bsz=128, num_updates=400, lr=4.8e-06, gnorm=2.125, train_wall=5, gb_free=12.5, wall=2682
2024-09-04 07:45:30 | INFO | train_inner | epoch 018:     12 / 16 loss=14.142, nll_loss=13.234, ppl=9632.69, wps=1501.5, ups=0.34, wpb=4431, bsz=92, num_updates=284, lr=3.408e-06, gnorm=1.473, train_wall=6, gb_free=8.8, wall=2199
2024-09-04 07:45:31 | INFO | train_inner | epoch 020:      3 / 21 loss=12.043, nll_loss=10.581, ppl=1531.7, wps=1793.3, ups=0.36, wpb=4929.5, bsz=84, num_updates=402, lr=4.824e-06, gnorm=2.297, train_wall=5, gb_free=12.1, wall=2688
2024-09-04 07:45:35 | INFO | train_inner | epoch 020:      5 / 21 loss=11.957, nll_loss=10.476, ppl=1424.64, wps=1825.2, ups=0.42, wpb=4319.5, bsz=60, num_updates=404, lr=4.848e-06, gnorm=2.278, train_wall=5, gb_free=13.5, wall=2692
2024-09-04 07:45:36 | INFO | train_inner | epoch 018:     14 / 16 loss=13.943, nll_loss=12.985, ppl=8105.86, wps=1489.9, ups=0.32, wpb=4586.5, bsz=136, num_updates=286, lr=3.432e-06, gnorm=1.518, train_wall=6, gb_free=8.9, wall=2205
2024-09-04 07:45:40 | INFO | train_inner | epoch 020:      7 / 21 loss=11.745, nll_loss=10.197, ppl=1174.21, wps=1841, ups=0.4, wpb=4561, bsz=116, num_updates=406, lr=4.872e-06, gnorm=2.217, train_wall=5, gb_free=11.9, wall=2697
2024-09-04 07:45:42 | INFO | train_inner | epoch 018:     16 / 16 loss=13.855, nll_loss=12.872, ppl=7497.9, wps=1499.9, ups=0.34, wpb=4404.5, bsz=128, num_updates=288, lr=3.456e-06, gnorm=1.51, train_wall=6, gb_free=10.5, wall=2211
2024-09-04 07:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27929.953125Mb; avail=227099.890625Mb
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000735
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27929.953125Mb; avail=227099.890625Mb
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012721
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27929.953125Mb; avail=227099.890625Mb
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011078
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024934
2024-09-04 07:45:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27929.953125Mb; avail=227099.890625Mb
2024-09-04 07:45:46 | INFO | train_inner | epoch 020:      9 / 21 loss=11.707, nll_loss=10.144, ppl=1131.64, wps=1747, ups=0.36, wpb=4820.5, bsz=120, num_updates=408, lr=4.896e-06, gnorm=2.272, train_wall=6, gb_free=14.2, wall=2703
2024-09-04 07:45:52 | INFO | train_inner | epoch 020:     11 / 21 loss=11.8, nll_loss=10.26, ppl=1226.6, wps=1651.3, ups=0.33, wpb=4931.5, bsz=100, num_updates=410, lr=4.92e-06, gnorm=1.886, train_wall=6, gb_free=12.6, wall=2709
2024-09-04 07:45:56 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 13.84 | nll_loss 12.731 | ppl 6800.74 | wps 3846.7 | wpb 2070.5 | bsz 122.7 | num_updates 288 | best_loss 13.84
2024-09-04 07:45:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 288 updates
2024-09-04 07:45:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:45:57 | INFO | train_inner | epoch 020:     13 / 21 loss=11.863, nll_loss=10.362, ppl=1315.87, wps=1532.3, ups=0.38, wpb=4034.5, bsz=61, num_updates=412, lr=4.944e-06, gnorm=2.218, train_wall=5, gb_free=12.8, wall=2714
2024-09-04 07:46:01 | INFO | train_inner | epoch 020:     15 / 21 loss=12.018, nll_loss=10.551, ppl=1500.66, wps=1831.2, ups=0.46, wpb=4010, bsz=40, num_updates=414, lr=4.968e-06, gnorm=2.012, train_wall=4, gb_free=14.5, wall=2718
2024-09-04 07:46:07 | INFO | train_inner | epoch 020:     17 / 21 loss=11.708, nll_loss=10.154, ppl=1139.66, wps=1708.9, ups=0.35, wpb=4873, bsz=68, num_updates=416, lr=4.992e-06, gnorm=2.085, train_wall=6, gb_free=12.2, wall=2724
2024-09-04 07:46:12 | INFO | train_inner | epoch 020:     19 / 21 loss=11.34, nll_loss=9.673, ppl=816.59, wps=1796.8, ups=0.39, wpb=4597.5, bsz=164, num_updates=418, lr=5.016e-06, gnorm=1.752, train_wall=5, gb_free=12.2, wall=2729
2024-09-04 07:46:18 | INFO | train_inner | epoch 020:     21 / 21 loss=11.645, nll_loss=10.069, ppl=1073.8, wps=1680.2, ups=0.35, wpb=4761.5, bsz=108, num_updates=420, lr=5.04e-06, gnorm=2, train_wall=6, gb_free=13, wall=2735
2024-09-04 07:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42045.50390625Mb; avail=212984.42578125Mb
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000744
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42045.50390625Mb; avail=212984.42578125Mb
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012647
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42045.50390625Mb; avail=212984.42578125Mb
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011264
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025054
2024-09-04 07:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42045.50390625Mb; avail=212984.42578125Mb
2024-09-04 07:46:34 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 11.936 | nll_loss 10.319 | ppl 1277.59 | wps 4979.7 | wpb 2350.9 | bsz 94.7 | num_updates 420 | best_loss 11.936
2024-09-04 07:46:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 420 updates
2024-09-04 07:46:34 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 18 @ 288 updates, score 13.84) (writing took 72.97115786653012 seconds)
2024-09-04 07:47:09 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-09-04 07:47:09 | INFO | train | epoch 018 | loss 13.956 | nll_loss 12.994 | ppl 8158.36 | wps 511.5 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 288 | lr 3.456e-06 | gnorm 1.6 | train_wall 45 | gb_free 10.5 | wall 2298
2024-09-04 07:47:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:47:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:47:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:47:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000613
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38664.12109375Mb; avail=216365.8125Mb
2024-09-04 07:47:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000561
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38664.12109375Mb; avail=216365.8125Mb
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38664.12109375Mb; avail=216365.8125Mb
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000155
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001046
2024-09-04 07:47:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38663.62890625Mb; avail=216366.3046875Mb
2024-09-04 07:47:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:47:09 | INFO | fairseq.trainer | begin training epoch 19
2024-09-04 07:47:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:47:14 | INFO | train_inner | epoch 019:      2 / 16 loss=13.905, nll_loss=12.93, ppl=7801.96, wps=84.8, ups=0.02, wpb=3938.5, bsz=112, num_updates=290, lr=3.48e-06, gnorm=1.506, train_wall=5, gb_free=13.8, wall=2304
2024-09-04 07:47:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:47:20 | INFO | train_inner | epoch 019:      4 / 16 loss=13.853, nll_loss=12.862, ppl=7443.56, wps=1542.2, ups=0.35, wpb=4467, bsz=140, num_updates=292, lr=3.504e-06, gnorm=1.575, train_wall=6, gb_free=10, wall=2310
2024-09-04 07:47:26 | INFO | train_inner | epoch 019:      6 / 16 loss=13.865, nll_loss=12.873, ppl=7501.58, wps=1590.8, ups=0.34, wpb=4722.5, bsz=144, num_updates=294, lr=3.528e-06, gnorm=1.485, train_wall=6, gb_free=11, wall=2316
2024-09-04 07:47:32 | INFO | train_inner | epoch 019:      8 / 16 loss=13.533, nll_loss=12.45, ppl=5596.22, wps=1415.4, ups=0.35, wpb=4084, bsz=176, num_updates=296, lr=3.552e-06, gnorm=1.86, train_wall=6, gb_free=9.8, wall=2321
2024-09-04 07:47:38 | INFO | train_inner | epoch 019:     10 / 16 loss=13.717, nll_loss=12.692, ppl=6618.88, wps=1488.3, ups=0.34, wpb=4314.5, bsz=148, num_updates=298, lr=3.576e-06, gnorm=1.467, train_wall=6, gb_free=10.2, wall=2327
2024-09-04 07:47:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 20 @ 420 updates, score 11.936) (writing took 68.51051712688059 seconds)
2024-09-04 07:47:43 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-09-04 07:47:43 | INFO | train | epoch 020 | loss 11.753 | nll_loss 10.208 | ppl 1182.64 | wps 681.1 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 420 | lr 5.04e-06 | gnorm 2.117 | train_wall 55 | gb_free 13 | wall 2820
2024-09-04 07:47:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:47:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:47:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:47:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000601
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27414.4296875Mb; avail=227615.54296875Mb
2024-09-04 07:47:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000069
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000583
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27414.4296875Mb; avail=227615.54296875Mb
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27414.4296875Mb; avail=227615.54296875Mb
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000163
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001072
2024-09-04 07:47:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27414.4296875Mb; avail=227615.54296875Mb
2024-09-04 07:47:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:47:43 | INFO | fairseq.trainer | begin training epoch 21
2024-09-04 07:47:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:47:43 | INFO | train_inner | epoch 019:     12 / 16 loss=13.707, nll_loss=12.678, ppl=6553.49, wps=1435.5, ups=0.39, wpb=3714, bsz=113, num_updates=300, lr=3.6e-06, gnorm=1.687, train_wall=5, gb_free=10, wall=2332
2024-09-04 07:47:48 | INFO | train_inner | epoch 021:      2 / 21 loss=11.821, nll_loss=10.297, ppl=1258.28, wps=101.2, ups=0.02, wpb=4572, bsz=76, num_updates=422, lr=5.064e-06, gnorm=1.847, train_wall=5, gb_free=13.2, wall=2825
2024-09-04 07:47:49 | INFO | train_inner | epoch 019:     14 / 16 loss=13.939, nll_loss=12.978, ppl=8070.6, wps=1555.1, ups=0.34, wpb=4612.5, bsz=108, num_updates=302, lr=3.624e-06, gnorm=1.417, train_wall=6, gb_free=8.8, wall=2338
2024-09-04 07:47:53 | INFO | train_inner | epoch 021:      4 / 21 loss=11.465, nll_loss=9.819, ppl=903.42, wps=1839.1, ups=0.44, wpb=4151, bsz=120, num_updates=424, lr=5.088e-06, gnorm=2.041, train_wall=5, gb_free=13.6, wall=2830
2024-09-04 07:47:54 | INFO | train_inner | epoch 019:     16 / 16 loss=14.128, nll_loss=13.222, ppl=9553.55, wps=1517.3, ups=0.38, wpb=4038.5, bsz=76, num_updates=304, lr=3.648e-06, gnorm=1.515, train_wall=5, gb_free=8.5, wall=2344
2024-09-04 07:47:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27459.16015625Mb; avail=227570.765625Mb
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27459.16015625Mb; avail=227570.765625Mb
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012653
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27459.16015625Mb; avail=227570.765625Mb
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011187
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024953
2024-09-04 07:47:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27458.89453125Mb; avail=227571.03125Mb
2024-09-04 07:47:58 | INFO | train_inner | epoch 021:      6 / 21 loss=11.623, nll_loss=10.05, ppl=1060.26, wps=1583.3, ups=0.36, wpb=4407.5, bsz=100, num_updates=426, lr=5.112e-06, gnorm=1.952, train_wall=6, gb_free=17.5, wall=2835
2024-09-04 07:48:04 | INFO | train_inner | epoch 021:      8 / 21 loss=11.733, nll_loss=10.194, ppl=1171.46, wps=1611.2, ups=0.36, wpb=4488, bsz=60, num_updates=428, lr=5.136e-06, gnorm=1.891, train_wall=6, gb_free=13.6, wall=2841
2024-09-04 07:48:09 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 13.756 | nll_loss 12.62 | ppl 6295.37 | wps 3852.8 | wpb 2070.5 | bsz 122.7 | num_updates 304 | best_loss 13.756
2024-09-04 07:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 304 updates
2024-09-04 07:48:09 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:48:09 | INFO | train_inner | epoch 021:     10 / 21 loss=11.418, nll_loss=9.778, ppl=877.79, wps=2010.8, ups=0.43, wpb=4643.5, bsz=112, num_updates=430, lr=5.16e-06, gnorm=1.785, train_wall=5, gb_free=14.1, wall=2846
2024-09-04 07:48:13 | INFO | train_inner | epoch 021:     12 / 21 loss=11.428, nll_loss=9.783, ppl=880.9, wps=1983.3, ups=0.42, wpb=4771, bsz=132, num_updates=432, lr=5.184e-06, gnorm=1.949, train_wall=5, gb_free=14.7, wall=2850
2024-09-04 07:48:20 | INFO | train_inner | epoch 021:     14 / 21 loss=11.381, nll_loss=9.701, ppl=832.19, wps=1609.3, ups=0.31, wpb=5111, bsz=112, num_updates=434, lr=5.208e-06, gnorm=2.006, train_wall=6, gb_free=12.2, wall=2857
2024-09-04 07:48:26 | INFO | train_inner | epoch 021:     16 / 21 loss=11.376, nll_loss=9.715, ppl=840.71, wps=1504.8, ups=0.31, wpb=4795, bsz=92, num_updates=436, lr=5.232e-06, gnorm=1.903, train_wall=6, gb_free=11.8, wall=2863
2024-09-04 07:48:31 | INFO | train_inner | epoch 021:     18 / 21 loss=11.485, nll_loss=9.875, ppl=939.03, wps=1423.2, ups=0.39, wpb=3677, bsz=69, num_updates=438, lr=5.256e-06, gnorm=1.839, train_wall=5, gb_free=18, wall=2868
2024-09-04 07:48:37 | INFO | train_inner | epoch 021:     20 / 21 loss=11.382, nll_loss=9.726, ppl=847.16, wps=1567.2, ups=0.33, wpb=4804.5, bsz=116, num_updates=440, lr=5.28e-06, gnorm=1.912, train_wall=6, gb_free=13, wall=2874
2024-09-04 07:48:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44604.1328125Mb; avail=210425.796875Mb
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000798
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44604.1328125Mb; avail=210425.796875Mb
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012846
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44603.640625Mb; avail=210426.2890625Mb
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011376
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025439
2024-09-04 07:48:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44603.640625Mb; avail=210426.2890625Mb
2024-09-04 07:48:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:48:58 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.82 | nll_loss 10.147 | ppl 1133.91 | wps 4708.4 | wpb 2350.9 | bsz 94.7 | num_updates 441 | best_loss 11.82
2024-09-04 07:48:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 441 updates
2024-09-04 07:48:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:49:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 19 @ 304 updates, score 13.756) (writing took 61.318151507526636 seconds)
2024-09-04 07:49:10 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-09-04 07:49:10 | INFO | train | epoch 019 | loss 13.833 | nll_loss 12.839 | ppl 7325.51 | wps 560.5 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 304 | lr 3.648e-06 | gnorm 1.564 | train_wall 45 | gb_free 8.5 | wall 2419
2024-09-04 07:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000725
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=49372.1953125Mb; avail=205657.734375Mb
2024-09-04 07:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000593
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49373.671875Mb; avail=205656.2578125Mb
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49373.671875Mb; avail=205655.765625Mb
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000183
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001083
2024-09-04 07:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49374.65625Mb; avail=205655.2734375Mb
2024-09-04 07:49:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:49:10 | INFO | fairseq.trainer | begin training epoch 20
2024-09-04 07:49:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:49:16 | INFO | train_inner | epoch 020:      2 / 16 loss=13.269, nll_loss=12.108, ppl=4414.24, wps=107.5, ups=0.02, wpb=4394.5, bsz=212, num_updates=306, lr=3.672e-06, gnorm=1.829, train_wall=6, gb_free=9, wall=2425
2024-09-04 07:49:21 | INFO | train_inner | epoch 020:      4 / 16 loss=13.915, nll_loss=12.951, ppl=7917.58, wps=1451.1, ups=0.41, wpb=3510.5, bsz=65, num_updates=308, lr=3.696e-06, gnorm=1.874, train_wall=5, gb_free=10.2, wall=2430
2024-09-04 07:49:26 | INFO | train_inner | epoch 020:      6 / 16 loss=13.926, nll_loss=12.958, ppl=7957.98, wps=1531.5, ups=0.4, wpb=3865, bsz=92, num_updates=310, lr=3.72e-06, gnorm=1.592, train_wall=5, gb_free=9, wall=2435
2024-09-04 07:49:32 | INFO | train_inner | epoch 020:      8 / 16 loss=13.816, nll_loss=12.823, ppl=7248.65, wps=1475.3, ups=0.34, wpb=4403, bsz=120, num_updates=312, lr=3.744e-06, gnorm=1.544, train_wall=6, gb_free=9, wall=2441
2024-09-04 07:49:37 | INFO | train_inner | epoch 020:     10 / 16 loss=14.022, nll_loss=13.091, ppl=8725.25, wps=1573.4, ups=0.35, wpb=4487, bsz=88, num_updates=314, lr=3.768e-06, gnorm=1.468, train_wall=6, gb_free=8.9, wall=2447
2024-09-04 07:49:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:49:43 | INFO | train_inner | epoch 020:     12 / 16 loss=13.835, nll_loss=12.851, ppl=7388.81, wps=1499.7, ups=0.36, wpb=4178, bsz=104, num_updates=316, lr=3.792e-06, gnorm=1.448, train_wall=6, gb_free=10.4, wall=2453
2024-09-04 07:49:49 | INFO | train_inner | epoch 020:     14 / 16 loss=13.453, nll_loss=12.343, ppl=5193.95, wps=1485.8, ups=0.33, wpb=4480.5, bsz=176, num_updates=318, lr=3.816e-06, gnorm=1.65, train_wall=6, gb_free=11.7, wall=2459
2024-09-04 07:49:55 | INFO | train_inner | epoch 020:     16 / 16 loss=13.571, nll_loss=12.5, ppl=5793.41, wps=1513, ups=0.33, wpb=4573, bsz=160, num_updates=320, lr=3.84e-06, gnorm=1.674, train_wall=6, gb_free=10, wall=2465
2024-09-04 07:49:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38752.3359375Mb; avail=216277.49609375Mb
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38752.828125Mb; avail=216277.00390625Mb
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012657
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38752.828125Mb; avail=216277.00390625Mb
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011108
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024878
2024-09-04 07:49:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38752.828125Mb; avail=216277.00390625Mb
2024-09-04 07:50:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 21 @ 441 updates, score 11.82) (writing took 66.2750222850591 seconds)
2024-09-04 07:50:04 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-09-04 07:50:04 | INFO | train | epoch 021 | loss 11.515 | nll_loss 9.899 | ppl 954.47 | wps 678.6 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 441 | lr 5.292e-06 | gnorm 1.907 | train_wall 57 | gb_free 12.4 | wall 2961
2024-09-04 07:50:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:50:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:50:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:50:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000745
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38793.33203125Mb; avail=216236.328125Mb
2024-09-04 07:50:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000613
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38793.33203125Mb; avail=216236.328125Mb
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38793.33203125Mb; avail=216236.328125Mb
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000194
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001134
2024-09-04 07:50:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38793.33203125Mb; avail=216236.328125Mb
2024-09-04 07:50:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:50:04 | INFO | fairseq.trainer | begin training epoch 22
2024-09-04 07:50:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:50:06 | INFO | train_inner | epoch 022:      1 / 21 loss=11.704, nll_loss=10.142, ppl=1129.74, wps=103, ups=0.02, wpb=4569, bsz=52, num_updates=442, lr=5.304e-06, gnorm=1.978, train_wall=5, gb_free=14.6, wall=2963
2024-09-04 07:50:10 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 13.648 | nll_loss 12.5 | ppl 5791.95 | wps 3845.7 | wpb 2070.5 | bsz 122.7 | num_updates 320 | best_loss 13.648
2024-09-04 07:50:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 320 updates
2024-09-04 07:50:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:50:11 | INFO | train_inner | epoch 022:      3 / 21 loss=11.365, nll_loss=9.694, ppl=828.45, wps=1927.7, ups=0.38, wpb=5086.5, bsz=108, num_updates=444, lr=5.328e-06, gnorm=2.009, train_wall=5, gb_free=11.8, wall=2968
2024-09-04 07:50:17 | INFO | train_inner | epoch 022:      5 / 21 loss=11.094, nll_loss=9.359, ppl=656.46, wps=1646.4, ups=0.37, wpb=4408, bsz=128, num_updates=446, lr=5.352e-06, gnorm=2.305, train_wall=5, gb_free=12.7, wall=2974
2024-09-04 07:50:21 | INFO | train_inner | epoch 022:      7 / 21 loss=11.332, nll_loss=9.667, ppl=813.12, wps=1885.2, ups=0.42, wpb=4436, bsz=100, num_updates=448, lr=5.376e-06, gnorm=1.803, train_wall=5, gb_free=11.9, wall=2978
2024-09-04 07:50:27 | INFO | train_inner | epoch 022:      9 / 21 loss=11.318, nll_loss=9.643, ppl=799.67, wps=1603.3, ups=0.33, wpb=4808.5, bsz=92, num_updates=450, lr=5.4e-06, gnorm=1.77, train_wall=6, gb_free=13.8, wall=2984
2024-09-04 07:50:32 | INFO | train_inner | epoch 022:     11 / 21 loss=11.172, nll_loss=9.418, ppl=684, wps=1999.8, ups=0.41, wpb=4826.5, bsz=140, num_updates=452, lr=5.424e-06, gnorm=2.367, train_wall=5, gb_free=12.8, wall=2989
2024-09-04 07:50:38 | INFO | train_inner | epoch 022:     13 / 21 loss=11.142, nll_loss=9.425, ppl=687.17, wps=1689.5, ups=0.33, wpb=5046, bsz=132, num_updates=454, lr=5.448e-06, gnorm=2.307, train_wall=6, gb_free=13, wall=2995
2024-09-04 07:50:43 | INFO | train_inner | epoch 022:     15 / 21 loss=11.477, nll_loss=9.867, ppl=933.73, wps=2079.3, ups=0.43, wpb=4807, bsz=60, num_updates=456, lr=5.472e-06, gnorm=2.247, train_wall=5, gb_free=13.6, wall=3000
2024-09-04 07:50:48 | INFO | train_inner | epoch 022:     17 / 21 loss=11.456, nll_loss=9.824, ppl=906.2, wps=1815, ups=0.42, wpb=4272, bsz=76, num_updates=458, lr=5.496e-06, gnorm=2.468, train_wall=5, gb_free=13.3, wall=3005
2024-09-04 07:50:52 | INFO | train_inner | epoch 022:     19 / 21 loss=11.271, nll_loss=9.584, ppl=767.25, wps=1565, ups=0.42, wpb=3761.5, bsz=73, num_updates=460, lr=5.52e-06, gnorm=2.112, train_wall=5, gb_free=14.2, wall=3009
2024-09-04 07:50:58 | INFO | train_inner | epoch 022:     21 / 21 loss=11.22, nll_loss=9.527, ppl=737.57, wps=1559.7, ups=0.37, wpb=4240.5, bsz=84, num_updates=462, lr=5.544e-06, gnorm=2.32, train_wall=5, gb_free=12.6, wall=3015
2024-09-04 07:50:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=48630.53515625Mb; avail=206398.90234375Mb
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000747
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48625.12109375Mb; avail=206404.31640625Mb
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012660
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48630.52734375Mb; avail=206398.91015625Mb
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011139
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024937
2024-09-04 07:50:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48633.97265625Mb; avail=206395.46484375Mb
2024-09-04 07:50:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:51:14 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 11.583 | nll_loss 9.876 | ppl 939.86 | wps 5004.6 | wpb 2350.9 | bsz 94.7 | num_updates 462 | best_loss 11.583
2024-09-04 07:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 462 updates
2024-09-04 07:51:14 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:51:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 20 @ 320 updates, score 13.648) (writing took 73.07692575640976 seconds)
2024-09-04 07:51:23 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-09-04 07:51:23 | INFO | train | epoch 020 | loss 13.716 | nll_loss 12.691 | ppl 6612.09 | wps 510.6 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 320 | lr 3.84e-06 | gnorm 1.635 | train_wall 45 | gb_free 10 | wall 2552
2024-09-04 07:51:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:51:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:51:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:51:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000661
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47029.3671875Mb; avail=208000.0703125Mb
2024-09-04 07:51:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000059
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000619
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47030.84375Mb; avail=207998.59375Mb
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000038
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47031.3359375Mb; avail=207998.59375Mb
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000179
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001143
2024-09-04 07:51:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47032.3203125Mb; avail=207997.609375Mb
2024-09-04 07:51:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:51:23 | INFO | fairseq.trainer | begin training epoch 21
2024-09-04 07:51:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:51:28 | INFO | train_inner | epoch 021:      2 / 16 loss=13.308, nll_loss=12.16, ppl=4575.92, wps=84.5, ups=0.02, wpb=3932.5, bsz=172, num_updates=322, lr=3.864e-06, gnorm=2.571, train_wall=5, gb_free=9.6, wall=2558
2024-09-04 07:51:34 | INFO | train_inner | epoch 021:      4 / 16 loss=13.628, nll_loss=12.578, ppl=6112.61, wps=1558.3, ups=0.35, wpb=4408, bsz=128, num_updates=324, lr=3.888e-06, gnorm=1.558, train_wall=6, gb_free=11.4, wall=2563
2024-09-04 07:51:39 | INFO | train_inner | epoch 021:      6 / 16 loss=13.451, nll_loss=12.353, ppl=5229.66, wps=1419, ups=0.41, wpb=3478, bsz=125, num_updates=326, lr=3.912e-06, gnorm=1.678, train_wall=5, gb_free=14.8, wall=2568
2024-09-04 07:51:44 | INFO | train_inner | epoch 021:      8 / 16 loss=13.52, nll_loss=12.446, ppl=5579.51, wps=1497.9, ups=0.35, wpb=4263.5, bsz=144, num_updates=328, lr=3.936e-06, gnorm=1.633, train_wall=6, gb_free=9.4, wall=2574
2024-09-04 07:51:50 | INFO | train_inner | epoch 021:     10 / 16 loss=13.796, nll_loss=12.807, ppl=7163.92, wps=1455.2, ups=0.35, wpb=4191.5, bsz=84, num_updates=330, lr=3.96e-06, gnorm=1.527, train_wall=6, gb_free=14, wall=2580
2024-09-04 07:51:56 | INFO | train_inner | epoch 021:     12 / 16 loss=13.652, nll_loss=12.614, ppl=6267.82, wps=1515.1, ups=0.32, wpb=4725.5, bsz=132, num_updates=332, lr=3.984e-06, gnorm=1.687, train_wall=6, gb_free=9.6, wall=2586
2024-09-04 07:52:02 | INFO | train_inner | epoch 021:     14 / 16 loss=13.578, nll_loss=12.518, ppl=5865.77, wps=1542, ups=0.34, wpb=4560, bsz=128, num_updates=334, lr=4.008e-06, gnorm=1.656, train_wall=6, gb_free=11.3, wall=2592
2024-09-04 07:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:52:08 | INFO | train_inner | epoch 021:     16 / 16 loss=13.73, nll_loss=12.718, ppl=6736.35, wps=1574.8, ups=0.36, wpb=4332.5, bsz=104, num_updates=336, lr=4.032e-06, gnorm=1.785, train_wall=5, gb_free=11.9, wall=2597
2024-09-04 07:52:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30774.6484375Mb; avail=224254.2734375Mb
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000748
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30779.5703125Mb; avail=224250.3359375Mb
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012949
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30778.5859375Mb; avail=224251.3203125Mb
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011107
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025191
2024-09-04 07:52:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30775.140625Mb; avail=224254.2734375Mb
2024-09-04 07:52:22 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 13.568 | nll_loss 12.396 | ppl 5388.94 | wps 3850.2 | wpb 2070.5 | bsz 122.7 | num_updates 336 | best_loss 13.568
2024-09-04 07:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 336 updates
2024-09-04 07:52:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 22 @ 462 updates, score 11.583) (writing took 76.85523036494851 seconds)
2024-09-04 07:52:31 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-09-04 07:52:31 | INFO | train | epoch 022 | loss 11.307 | nll_loss 9.628 | ppl 791.36 | wps 650.3 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 462 | lr 5.544e-06 | gnorm 2.17 | train_wall 54 | gb_free 12.6 | wall 3108
2024-09-04 07:52:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:52:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:52:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:52:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000646
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37486.30078125Mb; avail=217543.62890625Mb
2024-09-04 07:52:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000595
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37487.28515625Mb; avail=217542.15234375Mb
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37487.77734375Mb; avail=217542.15234375Mb
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000174
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001102
2024-09-04 07:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37488.76171875Mb; avail=217541.16796875Mb
2024-09-04 07:52:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:52:31 | INFO | fairseq.trainer | begin training epoch 23
2024-09-04 07:52:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:52:37 | INFO | train_inner | epoch 023:      2 / 21 loss=11.302, nll_loss=9.632, ppl=793.2, wps=98.9, ups=0.02, wpb=4887.5, bsz=64, num_updates=464, lr=5.568e-06, gnorm=1.972, train_wall=6, gb_free=12.7, wall=3114
2024-09-04 07:52:41 | INFO | train_inner | epoch 023:      4 / 21 loss=11, nll_loss=9.235, ppl=602.63, wps=1926.1, ups=0.47, wpb=4123.5, bsz=121, num_updates=466, lr=5.592e-06, gnorm=2.169, train_wall=4, gb_free=17.3, wall=3118
2024-09-04 07:52:45 | INFO | train_inner | epoch 023:      6 / 21 loss=11.199, nll_loss=9.484, ppl=715.88, wps=1972.5, ups=0.45, wpb=4416.5, bsz=64, num_updates=468, lr=5.616e-06, gnorm=1.895, train_wall=4, gb_free=13.1, wall=3122
2024-09-04 07:52:51 | INFO | train_inner | epoch 023:      8 / 21 loss=10.991, nll_loss=9.221, ppl=596.95, wps=1831.9, ups=0.39, wpb=4748.5, bsz=136, num_updates=470, lr=5.64e-06, gnorm=2.058, train_wall=5, gb_free=11.9, wall=3128
2024-09-04 07:52:56 | INFO | train_inner | epoch 023:     10 / 21 loss=11.144, nll_loss=9.42, ppl=685.01, wps=1761.7, ups=0.38, wpb=4630.5, bsz=84, num_updates=472, lr=5.664e-06, gnorm=2.071, train_wall=5, gb_free=12.5, wall=3133
2024-09-04 07:53:02 | INFO | train_inner | epoch 023:     12 / 21 loss=11.101, nll_loss=9.355, ppl=654.97, wps=1628.4, ups=0.33, wpb=4914.5, bsz=100, num_updates=474, lr=5.688e-06, gnorm=1.758, train_wall=6, gb_free=11.7, wall=3139
2024-09-04 07:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:53:08 | INFO | train_inner | epoch 023:     14 / 21 loss=11.225, nll_loss=9.512, ppl=730.33, wps=1701.4, ups=0.33, wpb=5119, bsz=80, num_updates=476, lr=5.712e-06, gnorm=1.967, train_wall=6, gb_free=12.9, wall=3145
2024-09-04 07:53:13 | INFO | train_inner | epoch 023:     16 / 21 loss=10.73, nll_loss=8.908, ppl=480.26, wps=1785.1, ups=0.44, wpb=4086.5, bsz=140, num_updates=478, lr=5.736e-06, gnorm=2.147, train_wall=5, gb_free=14.9, wall=3149
2024-09-04 07:53:18 | INFO | train_inner | epoch 023:     18 / 21 loss=11.141, nll_loss=9.424, ppl=686.72, wps=1643.5, ups=0.39, wpb=4179.5, bsz=88, num_updates=480, lr=5.76e-06, gnorm=1.885, train_wall=5, gb_free=12.8, wall=3155
2024-09-04 07:53:23 | INFO | train_inner | epoch 023:     20 / 21 loss=10.914, nll_loss=9.108, ppl=551.91, wps=1767.8, ups=0.38, wpb=4667, bsz=104, num_updates=482, lr=5.784e-06, gnorm=1.837, train_wall=5, gb_free=12.1, wall=3160
2024-09-04 07:53:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33763.8828125Mb; avail=221266.00390625Mb
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000737
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33763.8828125Mb; avail=221266.00390625Mb
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012711
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33764.37109375Mb; avail=221265.51171875Mb
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011270
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025099
2024-09-04 07:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33764.37109375Mb; avail=221265.51171875Mb
2024-09-04 07:53:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 21 @ 336 updates, score 13.568) (writing took 65.91021458432078 seconds)
2024-09-04 07:53:28 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-09-04 07:53:28 | INFO | train | epoch 021 | loss 13.59 | nll_loss 12.533 | ppl 5925.5 | wps 539.8 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 336 | lr 4.032e-06 | gnorm 1.762 | train_wall 45 | gb_free 11.9 | wall 2678
2024-09-04 07:53:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:53:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:53:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:53:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000783
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33789.7890625Mb; avail=221240.0546875Mb
2024-09-04 07:53:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000070
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000638
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33789.7890625Mb; avail=221240.0546875Mb
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33789.7890625Mb; avail=221240.0546875Mb
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000186
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001154
2024-09-04 07:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33789.7890625Mb; avail=221240.0546875Mb
2024-09-04 07:53:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:53:28 | INFO | fairseq.trainer | begin training epoch 22
2024-09-04 07:53:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:53:34 | INFO | train_inner | epoch 022:      2 / 16 loss=13.767, nll_loss=12.767, ppl=6972.65, wps=100.1, ups=0.02, wpb=4290.5, bsz=92, num_updates=338, lr=4.056e-06, gnorm=1.419, train_wall=5, gb_free=13.3, wall=2683
2024-09-04 07:53:39 | INFO | train_inner | epoch 022:      4 / 16 loss=13.165, nll_loss=11.984, ppl=4050.61, wps=1492.3, ups=0.36, wpb=4145, bsz=168, num_updates=340, lr=4.08e-06, gnorm=1.97, train_wall=6, gb_free=9.9, wall=2689
2024-09-04 07:53:42 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 11.456 | nll_loss 9.684 | ppl 822.75 | wps 4745.7 | wpb 2350.9 | bsz 94.7 | num_updates 483 | best_loss 11.456
2024-09-04 07:53:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 483 updates
2024-09-04 07:53:42 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:53:45 | INFO | train_inner | epoch 022:      6 / 16 loss=13.249, nll_loss=12.084, ppl=4342.14, wps=1536.6, ups=0.33, wpb=4641.5, bsz=192, num_updates=342, lr=4.104e-06, gnorm=1.69, train_wall=6, gb_free=10.3, wall=2695
2024-09-04 07:53:51 | INFO | train_inner | epoch 022:      8 / 16 loss=13.611, nll_loss=12.565, ppl=6060.86, wps=1537.6, ups=0.33, wpb=4730, bsz=116, num_updates=344, lr=4.128e-06, gnorm=1.724, train_wall=6, gb_free=7.5, wall=2701
2024-09-04 07:53:57 | INFO | train_inner | epoch 022:     10 / 16 loss=13.382, nll_loss=12.27, ppl=4938.71, wps=1491.7, ups=0.34, wpb=4430, bsz=136, num_updates=346, lr=4.152e-06, gnorm=1.555, train_wall=6, gb_free=10.5, wall=2707
2024-09-04 07:54:03 | INFO | train_inner | epoch 022:     12 / 16 loss=13.451, nll_loss=12.358, ppl=5251.02, wps=1419.7, ups=0.32, wpb=4384.5, bsz=136, num_updates=348, lr=4.176e-06, gnorm=1.581, train_wall=6, gb_free=8.6, wall=2713
2024-09-04 07:54:08 | INFO | train_inner | epoch 022:     14 / 16 loss=13.581, nll_loss=12.524, ppl=5889.12, wps=1445.4, ups=0.45, wpb=3221, bsz=81, num_updates=350, lr=4.2e-06, gnorm=1.783, train_wall=4, gb_free=10.7, wall=2717
2024-09-04 07:54:13 | INFO | train_inner | epoch 022:     16 / 16 loss=13.59, nll_loss=12.536, ppl=5939.91, wps=1514.9, ups=0.37, wpb=4049, bsz=96, num_updates=352, lr=4.224e-06, gnorm=1.793, train_wall=5, gb_free=11.2, wall=2723
2024-09-04 07:54:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50967.6796875Mb; avail=204062.05078125Mb
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50968.171875Mb; avail=204061.55859375Mb
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012699
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50968.171875Mb; avail=204061.55859375Mb
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011124
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024970
2024-09-04 07:54:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50967.6796875Mb; avail=204062.05078125Mb
2024-09-04 07:54:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:54:28 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 13.506 | nll_loss 12.302 | ppl 5051.42 | wps 3847.3 | wpb 2070.5 | bsz 122.7 | num_updates 352 | best_loss 13.506
2024-09-04 07:54:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 352 updates
2024-09-04 07:54:28 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:54:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 23 @ 483 updates, score 11.456) (writing took 62.625154519453645 seconds)
2024-09-04 07:54:45 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-09-04 07:54:45 | INFO | train | epoch 023 | loss 11.087 | nll_loss 9.344 | ppl 649.8 | wps 715 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 483 | lr 5.796e-06 | gnorm 2.014 | train_wall 54 | gb_free 16.6 | wall 3242
2024-09-04 07:54:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:54:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:54:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:54:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000762
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=51032.03125Mb; avail=203997.80859375Mb
2024-09-04 07:54:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000068
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000636
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51032.03125Mb; avail=203997.80859375Mb
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51032.03125Mb; avail=203997.80859375Mb
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000197
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001154
2024-09-04 07:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51032.03125Mb; avail=203997.80859375Mb
2024-09-04 07:54:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:54:45 | INFO | fairseq.trainer | begin training epoch 24
2024-09-04 07:54:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:54:48 | INFO | train_inner | epoch 024:      1 / 21 loss=11.096, nll_loss=9.326, ppl=641.76, wps=100.5, ups=0.02, wpb=4268, bsz=88, num_updates=484, lr=5.808e-06, gnorm=2.262, train_wall=5, gb_free=12.2, wall=3245
2024-09-04 07:54:53 | INFO | train_inner | epoch 024:      3 / 21 loss=11.219, nll_loss=9.526, ppl=737.42, wps=1806, ups=0.41, wpb=4421, bsz=72, num_updates=486, lr=5.832e-06, gnorm=1.828, train_wall=5, gb_free=14.4, wall=3250
2024-09-04 07:54:59 | INFO | train_inner | epoch 024:      5 / 21 loss=10.838, nll_loss=9.032, ppl=523.53, wps=1523.3, ups=0.34, wpb=4451, bsz=116, num_updates=488, lr=5.856e-06, gnorm=2.011, train_wall=6, gb_free=13.5, wall=3255
2024-09-04 07:55:04 | INFO | train_inner | epoch 024:      7 / 21 loss=10.801, nll_loss=8.999, ppl=511.52, wps=1592.9, ups=0.4, wpb=4027, bsz=121, num_updates=490, lr=5.88e-06, gnorm=1.915, train_wall=5, gb_free=10.7, wall=3261
2024-09-04 07:55:09 | INFO | train_inner | epoch 024:      9 / 21 loss=10.949, nll_loss=9.14, ppl=564.18, wps=1636.5, ups=0.34, wpb=4776.5, bsz=88, num_updates=492, lr=5.904e-06, gnorm=1.868, train_wall=6, gb_free=12.1, wall=3266
2024-09-04 07:55:15 | INFO | train_inner | epoch 024:     11 / 21 loss=10.881, nll_loss=9.068, ppl=536.78, wps=1700, ups=0.33, wpb=5130.5, bsz=96, num_updates=494, lr=5.928e-06, gnorm=1.805, train_wall=6, gb_free=10.8, wall=3272
2024-09-04 07:55:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:55:21 | INFO | train_inner | epoch 024:     13 / 21 loss=11.003, nll_loss=9.24, ppl=604.86, wps=1557.1, ups=0.36, wpb=4356.5, bsz=64, num_updates=496, lr=5.952e-06, gnorm=1.899, train_wall=6, gb_free=16.6, wall=3278
2024-09-04 07:55:26 | INFO | train_inner | epoch 024:     15 / 21 loss=11.002, nll_loss=9.242, ppl=605.55, wps=1924.5, ups=0.43, wpb=4511.5, bsz=64, num_updates=498, lr=5.976e-06, gnorm=1.955, train_wall=5, gb_free=13.9, wall=3283
2024-09-04 07:55:31 | INFO | train_inner | epoch 024:     17 / 21 loss=10.816, nll_loss=8.986, ppl=506.93, wps=1781.2, ups=0.38, wpb=4686.5, bsz=84, num_updates=500, lr=6e-06, gnorm=1.724, train_wall=5, gb_free=13.3, wall=3288
2024-09-04 07:55:38 | INFO | train_inner | epoch 024:     19 / 21 loss=10.617, nll_loss=8.727, ppl=423.74, wps=1486.6, ups=0.3, wpb=4895.5, bsz=128, num_updates=502, lr=6.024e-06, gnorm=1.595, train_wall=7, gb_free=13.6, wall=3295
2024-09-04 07:55:44 | INFO | train_inner | epoch 024:     21 / 21 loss=10.542, nll_loss=8.625, ppl=394.71, wps=1481.3, ups=0.34, wpb=4385, bsz=132, num_updates=504, lr=6.048e-06, gnorm=2.008, train_wall=6, gb_free=12.8, wall=3300
2024-09-04 07:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25764.09765625Mb; avail=229265.7890625Mb
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000733
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25764.09765625Mb; avail=229265.7890625Mb
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012758
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25764.09765625Mb; avail=229265.7890625Mb
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011164
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025034
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25764.09765625Mb; avail=229265.7890625Mb
2024-09-04 07:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 22 @ 352 updates, score 13.506) (writing took 76.73866795003414 seconds)
2024-09-04 07:55:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-09-04 07:55:44 | INFO | train | epoch 022 | loss 13.471 | nll_loss 12.381 | ppl 5334.41 | wps 497.7 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 352 | lr 4.224e-06 | gnorm 1.689 | train_wall 45 | gb_free 11.2 | wall 2814
2024-09-04 07:55:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:55:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:55:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:55:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000631
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25790.3671875Mb; avail=229239.4765625Mb
2024-09-04 07:55:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000580
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25790.3671875Mb; avail=229239.4765625Mb
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25790.3671875Mb; avail=229239.4765625Mb
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000153
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001048
2024-09-04 07:55:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25790.3671875Mb; avail=229239.4765625Mb
2024-09-04 07:55:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:55:45 | INFO | fairseq.trainer | begin training epoch 23
2024-09-04 07:55:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:55:50 | INFO | train_inner | epoch 023:      2 / 16 loss=12.954, nll_loss=11.708, ppl=3345.53, wps=94.1, ups=0.02, wpb=4568, bsz=220, num_updates=354, lr=4.248e-06, gnorm=1.623, train_wall=6, gb_free=9.7, wall=2820
2024-09-04 07:55:55 | INFO | train_inner | epoch 023:      4 / 16 loss=13.584, nll_loss=12.545, ppl=5975.57, wps=1449.3, ups=0.41, wpb=3568, bsz=65, num_updates=356, lr=4.272e-06, gnorm=1.831, train_wall=5, gb_free=14.4, wall=2825
2024-09-04 07:56:00 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 11.307 | nll_loss 9.485 | ppl 716.8 | wps 4965.4 | wpb 2350.9 | bsz 94.7 | num_updates 504 | best_loss 11.307
2024-09-04 07:56:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 504 updates
2024-09-04 07:56:00 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:56:00 | INFO | train_inner | epoch 023:      6 / 16 loss=13.61, nll_loss=12.573, ppl=6094.65, wps=1487.9, ups=0.41, wpb=3651.5, bsz=80, num_updates=358, lr=4.296e-06, gnorm=1.713, train_wall=5, gb_free=17.1, wall=2830
2024-09-04 07:56:06 | INFO | train_inner | epoch 023:      8 / 16 loss=13.059, nll_loss=11.85, ppl=3691.75, wps=1414.8, ups=0.34, wpb=4168, bsz=144, num_updates=360, lr=4.32e-06, gnorm=1.687, train_wall=6, gb_free=9.5, wall=2836
2024-09-04 07:56:12 | INFO | train_inner | epoch 023:     10 / 16 loss=13.402, nll_loss=12.295, ppl=5026.96, wps=1476.9, ups=0.35, wpb=4173.5, bsz=116, num_updates=362, lr=4.344e-06, gnorm=1.392, train_wall=6, gb_free=13.6, wall=2841
2024-09-04 07:56:18 | INFO | train_inner | epoch 023:     12 / 16 loss=13.356, nll_loss=12.233, ppl=4812.59, wps=1556, ups=0.34, wpb=4600.5, bsz=132, num_updates=364, lr=4.368e-06, gnorm=1.406, train_wall=6, gb_free=10.8, wall=2847
2024-09-04 07:56:23 | INFO | train_inner | epoch 023:     14 / 16 loss=13.255, nll_loss=12.101, ppl=4392.64, wps=1538.6, ups=0.35, wpb=4382.5, bsz=144, num_updates=366, lr=4.392e-06, gnorm=1.397, train_wall=6, gb_free=11, wall=2853
2024-09-04 07:56:30 | INFO | train_inner | epoch 023:     16 / 16 loss=13.501, nll_loss=12.421, ppl=5482.8, wps=1548.1, ups=0.32, wpb=4779.5, bsz=116, num_updates=368, lr=4.416e-06, gnorm=1.336, train_wall=6, gb_free=8.3, wall=2859
2024-09-04 07:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36194.88671875Mb; avail=218835.46484375Mb
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36194.39453125Mb; avail=218835.46484375Mb
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012686
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36194.88671875Mb; avail=218835.46484375Mb
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011332
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025130
2024-09-04 07:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36194.88671875Mb; avail=218834.97265625Mb
2024-09-04 07:56:44 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 13.403 | nll_loss 12.179 | ppl 4635.9 | wps 3853.8 | wpb 2070.5 | bsz 122.7 | num_updates 368 | best_loss 13.403
2024-09-04 07:56:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 368 updates
2024-09-04 07:56:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:57:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 24 @ 504 updates, score 11.307) (writing took 73.88515088520944 seconds)
2024-09-04 07:57:14 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-09-04 07:57:14 | INFO | train | epoch 024 | loss 10.871 | nll_loss 9.062 | ppl 534.51 | wps 642.1 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 504 | lr 6.048e-06 | gnorm 1.855 | train_wall 59 | gb_free 12.8 | wall 3391
2024-09-04 07:57:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:57:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:57:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:57:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000623
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36186.21875Mb; avail=218843.63671875Mb
2024-09-04 07:57:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000572
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36186.21875Mb; avail=218843.63671875Mb
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36186.21875Mb; avail=218843.63671875Mb
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000166
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001040
2024-09-04 07:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36186.21875Mb; avail=218844.12890625Mb
2024-09-04 07:57:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:57:14 | INFO | fairseq.trainer | begin training epoch 25
2024-09-04 07:57:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:57:20 | INFO | train_inner | epoch 025:      2 / 21 loss=10.482, nll_loss=8.562, ppl=377.93, wps=88.8, ups=0.02, wpb=4263.5, bsz=132, num_updates=506, lr=6.072e-06, gnorm=1.855, train_wall=6, gb_free=16.1, wall=3396
2024-09-04 07:57:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:57:25 | INFO | train_inner | epoch 025:      4 / 21 loss=10.982, nll_loss=9.229, ppl=600.28, wps=1627.6, ups=0.37, wpb=4417, bsz=76, num_updates=508, lr=6.096e-06, gnorm=1.818, train_wall=5, gb_free=13.1, wall=3402
2024-09-04 07:57:29 | INFO | train_inner | epoch 025:      6 / 21 loss=10.752, nll_loss=8.909, ppl=480.66, wps=1984.8, ups=0.49, wpb=4086.5, bsz=77, num_updates=510, lr=6.12e-06, gnorm=1.995, train_wall=4, gb_free=16.5, wall=3406
2024-09-04 07:57:35 | INFO | train_inner | epoch 025:      8 / 21 loss=10.826, nll_loss=9.003, ppl=512.94, wps=1588.6, ups=0.35, wpb=4576.5, bsz=76, num_updates=512, lr=6.144e-06, gnorm=1.609, train_wall=6, gb_free=12.7, wall=3412
2024-09-04 07:57:41 | INFO | train_inner | epoch 025:     10 / 21 loss=10.693, nll_loss=8.829, ppl=454.86, wps=1603.4, ups=0.34, wpb=4753.5, bsz=116, num_updates=514, lr=6.168e-06, gnorm=1.645, train_wall=6, gb_free=10.7, wall=3418
2024-09-04 07:57:46 | INFO | train_inner | epoch 025:     12 / 21 loss=10.772, nll_loss=8.925, ppl=486.23, wps=1966, ups=0.4, wpb=4951, bsz=76, num_updates=516, lr=6.192e-06, gnorm=1.768, train_wall=5, gb_free=14, wall=3423
2024-09-04 07:57:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 23 @ 368 updates, score 13.403) (writing took 63.02453429624438 seconds)
2024-09-04 07:57:47 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-09-04 07:57:47 | INFO | train | epoch 023 | loss 13.33 | nll_loss 12.202 | ppl 4710.53 | wps 553.2 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 368 | lr 4.416e-06 | gnorm 1.548 | train_wall 45 | gb_free 8.3 | wall 2936
2024-09-04 07:57:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:57:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:57:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:57:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000727
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23907.79296875Mb; avail=231122.09375Mb
2024-09-04 07:57:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000607
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23907.79296875Mb; avail=231122.09375Mb
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23907.79296875Mb; avail=231122.09375Mb
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000188
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001157
2024-09-04 07:57:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23907.79296875Mb; avail=231122.09375Mb
2024-09-04 07:57:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:57:47 | INFO | fairseq.trainer | begin training epoch 24
2024-09-04 07:57:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:57:51 | INFO | train_inner | epoch 025:     14 / 21 loss=10.781, nll_loss=8.956, ppl=496.47, wps=1644.7, ups=0.35, wpb=4684.5, bsz=96, num_updates=518, lr=6.216e-06, gnorm=1.911, train_wall=6, gb_free=11.5, wall=3428
2024-09-04 07:57:53 | INFO | train_inner | epoch 024:      2 / 16 loss=13.506, nll_loss=12.433, ppl=5528.86, wps=102.7, ups=0.02, wpb=4259.5, bsz=96, num_updates=370, lr=4.44e-06, gnorm=1.508, train_wall=5, gb_free=10.8, wall=2942
2024-09-04 07:57:56 | INFO | train_inner | epoch 025:     16 / 21 loss=10.186, nll_loss=8.184, ppl=290.82, wps=1775.9, ups=0.4, wpb=4395, bsz=144, num_updates=520, lr=6.24e-06, gnorm=1.725, train_wall=5, gb_free=12, wall=3433
2024-09-04 07:57:58 | INFO | train_inner | epoch 024:      4 / 16 loss=13.294, nll_loss=12.167, ppl=4598.04, wps=1572.9, ups=0.34, wpb=4656.5, bsz=148, num_updates=372, lr=4.464e-06, gnorm=1.466, train_wall=6, gb_free=11.4, wall=2948
2024-09-04 07:58:01 | INFO | train_inner | epoch 025:     18 / 21 loss=10.584, nll_loss=8.673, ppl=408.17, wps=1906.4, ups=0.4, wpb=4769.5, bsz=104, num_updates=522, lr=6.264e-06, gnorm=1.969, train_wall=5, gb_free=14.7, wall=3438
2024-09-04 07:58:04 | INFO | train_inner | epoch 024:      6 / 16 loss=13.367, nll_loss=12.26, ppl=4905.29, wps=1557.5, ups=0.35, wpb=4444, bsz=104, num_updates=374, lr=4.488e-06, gnorm=1.686, train_wall=6, gb_free=9.4, wall=2954
2024-09-04 07:58:07 | INFO | train_inner | epoch 025:     20 / 21 loss=10.794, nll_loss=8.972, ppl=502.03, wps=1634.4, ups=0.37, wpb=4384.5, bsz=72, num_updates=524, lr=6.288e-06, gnorm=1.745, train_wall=5, gb_free=13.8, wall=3444
2024-09-04 07:58:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23954.68359375Mb; avail=231075.16015625Mb
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000730
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23955.17578125Mb; avail=231074.66796875Mb
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012698
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23955.17578125Mb; avail=231074.66796875Mb
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011232
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025044
2024-09-04 07:58:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23955.17578125Mb; avail=231074.66796875Mb
2024-09-04 07:58:10 | INFO | train_inner | epoch 024:      8 / 16 loss=13.398, nll_loss=12.3, ppl=5043.19, wps=1490.7, ups=0.35, wpb=4291, bsz=100, num_updates=376, lr=4.512e-06, gnorm=1.395, train_wall=6, gb_free=8.5, wall=2959
2024-09-04 07:58:16 | INFO | train_inner | epoch 024:     10 / 16 loss=13.389, nll_loss=12.293, ppl=5019.15, wps=1547.9, ups=0.35, wpb=4413.5, bsz=96, num_updates=378, lr=4.536e-06, gnorm=1.74, train_wall=6, gb_free=11.7, wall=2965
2024-09-04 07:58:22 | INFO | train_inner | epoch 024:     12 / 16 loss=12.643, nll_loss=11.307, ppl=2534.07, wps=1478.8, ups=0.32, wpb=4590.5, bsz=236, num_updates=380, lr=4.56e-06, gnorm=1.377, train_wall=6, gb_free=10.1, wall=2971
2024-09-04 07:58:27 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 11.158 | nll_loss 9.304 | ppl 632.24 | wps 4724.9 | wpb 2350.9 | bsz 94.7 | num_updates 525 | best_loss 11.158
2024-09-04 07:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 525 updates
2024-09-04 07:58:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:58:28 | INFO | train_inner | epoch 024:     14 / 16 loss=12.941, nll_loss=11.717, ppl=3365.95, wps=1425.4, ups=0.35, wpb=4072.5, bsz=152, num_updates=382, lr=4.584e-06, gnorm=2.053, train_wall=6, gb_free=10.4, wall=2977
2024-09-04 07:58:32 | INFO | train_inner | epoch 024:     16 / 16 loss=13.22, nll_loss=12.061, ppl=4272.62, wps=1382.6, ups=0.44, wpb=3164, bsz=85, num_updates=384, lr=4.608e-06, gnorm=1.478, train_wall=5, gb_free=9.5, wall=2982
2024-09-04 07:58:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36576.51953125Mb; avail=218452.78515625Mb
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36578.48828125Mb; avail=218451.30859375Mb
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012661
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36608.51171875Mb; avail=218421.28515625Mb
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011077
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024867
2024-09-04 07:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36634.59765625Mb; avail=218395.19921875Mb
2024-09-04 07:58:47 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 13.359 | nll_loss 12.094 | ppl 4371.51 | wps 3843.7 | wpb 2070.5 | bsz 122.7 | num_updates 384 | best_loss 13.359
2024-09-04 07:58:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 384 updates
2024-09-04 07:58:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:59:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 07:59:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 25 @ 525 updates, score 11.158) (writing took 62.46542721055448 seconds)
2024-09-04 07:59:29 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-09-04 07:59:29 | INFO | train | epoch 025 | loss 10.682 | nll_loss 8.82 | ppl 451.94 | wps 707.8 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 525 | lr 6.3e-06 | gnorm 1.815 | train_wall 55 | gb_free 13.1 | wall 3526
2024-09-04 07:59:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 07:59:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 07:59:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 07:59:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000843
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44893.99609375Mb; avail=210135.35546875Mb
2024-09-04 07:59:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000081
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000697
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44898.91796875Mb; avail=210130.92578125Mb
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44899.90234375Mb; avail=210133.87890625Mb
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001169
2024-09-04 07:59:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44891.53515625Mb; avail=210137.32421875Mb
2024-09-04 07:59:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 07:59:29 | INFO | fairseq.trainer | begin training epoch 26
2024-09-04 07:59:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:59:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 07:59:31 | INFO | train_inner | epoch 026:      1 / 21 loss=10.21, nll_loss=8.203, ppl=294.69, wps=108.8, ups=0.02, wpb=4605.5, bsz=136, num_updates=526, lr=6.312e-06, gnorm=2.106, train_wall=5, gb_free=13.3, wall=3528
2024-09-04 07:59:36 | INFO | train_inner | epoch 026:      3 / 21 loss=10.483, nll_loss=8.57, ppl=380.17, wps=1786.9, ups=0.4, wpb=4450, bsz=120, num_updates=528, lr=6.336e-06, gnorm=1.682, train_wall=5, gb_free=12.8, wall=3533
2024-09-04 07:59:42 | INFO | train_inner | epoch 026:      5 / 21 loss=10.707, nll_loss=8.859, ppl=464.27, wps=1611.9, ups=0.34, wpb=4743, bsz=76, num_updates=530, lr=6.36e-06, gnorm=2.136, train_wall=6, gb_free=12.1, wall=3539
2024-09-04 07:59:47 | INFO | train_inner | epoch 026:      7 / 21 loss=10.758, nll_loss=8.927, ppl=486.64, wps=1790, ups=0.4, wpb=4430.5, bsz=72, num_updates=532, lr=6.384e-06, gnorm=1.811, train_wall=5, gb_free=13.7, wall=3544
2024-09-04 07:59:53 | INFO | train_inner | epoch 026:      9 / 21 loss=10.237, nll_loss=8.242, ppl=302.73, wps=1627.5, ups=0.34, wpb=4734.5, bsz=172, num_updates=534, lr=6.408e-06, gnorm=1.878, train_wall=6, gb_free=12.3, wall=3550
2024-09-04 07:59:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 24 @ 384 updates, score 13.359) (writing took 67.963394748047 seconds)
2024-09-04 07:59:54 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-09-04 07:59:54 | INFO | train | epoch 024 | loss 13.218 | nll_loss 12.065 | ppl 4286.03 | wps 531.5 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 384 | lr 4.608e-06 | gnorm 1.588 | train_wall 45 | gb_free 9.5 | wall 3064
2024-09-04 07:59:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 07:59:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 07:59:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 07:59:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000751
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29679.125Mb; avail=225350.67578125Mb
2024-09-04 07:59:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000596
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29679.125Mb; avail=225350.67578125Mb
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29679.125Mb; avail=225350.67578125Mb
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000172
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001092
2024-09-04 07:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29679.125Mb; avail=225350.67578125Mb
2024-09-04 07:59:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 07:59:55 | INFO | fairseq.trainer | begin training epoch 25
2024-09-04 07:59:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 07:59:58 | INFO | train_inner | epoch 026:     11 / 21 loss=10.711, nll_loss=8.86, ppl=464.64, wps=1548.1, ups=0.39, wpb=3947.5, bsz=45, num_updates=536, lr=6.432e-06, gnorm=2.114, train_wall=5, gb_free=13.1, wall=3555
2024-09-04 08:00:00 | INFO | train_inner | epoch 025:      2 / 16 loss=13.134, nll_loss=11.942, ppl=3935.19, wps=99.6, ups=0.02, wpb=4385.5, bsz=136, num_updates=386, lr=4.632e-06, gnorm=1.895, train_wall=6, gb_free=9.2, wall=3070
2024-09-04 08:00:04 | INFO | train_inner | epoch 026:     13 / 21 loss=10.593, nll_loss=8.707, ppl=417.78, wps=1767.3, ups=0.33, wpb=5299, bsz=108, num_updates=538, lr=6.456e-06, gnorm=2.419, train_wall=6, gb_free=11.5, wall=3561
2024-09-04 08:00:06 | INFO | train_inner | epoch 025:      4 / 16 loss=13.295, nll_loss=12.165, ppl=4593.2, wps=1503.7, ups=0.33, wpb=4499, bsz=112, num_updates=388, lr=4.656e-06, gnorm=1.518, train_wall=6, gb_free=8.3, wall=3076
2024-09-04 08:00:09 | INFO | train_inner | epoch 026:     15 / 21 loss=10.576, nll_loss=8.695, ppl=414.45, wps=1715.8, ups=0.41, wpb=4159, bsz=92, num_updates=540, lr=6.48e-06, gnorm=1.697, train_wall=5, gb_free=12.7, wall=3566
2024-09-04 08:00:11 | INFO | train_inner | epoch 025:      6 / 16 loss=13.293, nll_loss=12.166, ppl=4594.82, wps=1463.8, ups=0.38, wpb=3815.5, bsz=96, num_updates=390, lr=4.68e-06, gnorm=1.91, train_wall=5, gb_free=9.2, wall=3081
2024-09-04 08:00:15 | INFO | train_inner | epoch 026:     17 / 21 loss=10.553, nll_loss=8.646, ppl=400.65, wps=1648.7, ups=0.35, wpb=4770.5, bsz=68, num_updates=542, lr=6.504e-06, gnorm=1.759, train_wall=6, gb_free=13.9, wall=3572
2024-09-04 08:00:17 | INFO | train_inner | epoch 025:      8 / 16 loss=13.114, nll_loss=11.926, ppl=3892.45, wps=1506.2, ups=0.35, wpb=4316, bsz=120, num_updates=392, lr=4.704e-06, gnorm=1.813, train_wall=6, gb_free=10.7, wall=3087
2024-09-04 08:00:21 | INFO | train_inner | epoch 026:     19 / 21 loss=10.407, nll_loss=8.47, ppl=354.55, wps=1613, ups=0.35, wpb=4608.5, bsz=108, num_updates=544, lr=6.528e-06, gnorm=1.78, train_wall=6, gb_free=13.4, wall=3577
2024-09-04 08:00:23 | INFO | train_inner | epoch 025:     10 / 16 loss=12.991, nll_loss=11.778, ppl=3512.07, wps=1577.8, ups=0.35, wpb=4546.5, bsz=148, num_updates=394, lr=4.728e-06, gnorm=2.031, train_wall=6, gb_free=9.9, wall=3092
2024-09-04 08:00:25 | INFO | train_inner | epoch 026:     21 / 21 loss=10.483, nll_loss=8.568, ppl=379.47, wps=1965.8, ups=0.42, wpb=4652.5, bsz=68, num_updates=546, lr=6.552e-06, gnorm=2.127, train_wall=5, gb_free=13.8, wall=3582
2024-09-04 08:00:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29723.125Mb; avail=225306.6328125Mb
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000728
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29723.125Mb; avail=225306.6328125Mb
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012866
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29723.125Mb; avail=225306.6328125Mb
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011299
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025251
2024-09-04 08:00:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29723.125Mb; avail=225306.6328125Mb
2024-09-04 08:00:43 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 11.027 | nll_loss 9.112 | ppl 553.24 | wps 4704.9 | wpb 2350.9 | bsz 94.7 | num_updates 546 | best_loss 11.027
2024-09-04 08:00:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 546 updates
2024-09-04 08:00:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:00:44 | INFO | train_inner | epoch 025:     12 / 16 loss=12.674, nll_loss=11.362, ppl=2632.07, wps=400.1, ups=0.1, wpb=4196, bsz=168, num_updates=396, lr=4.752e-06, gnorm=2.646, train_wall=21, gb_free=10.1, wall=3113
2024-09-04 08:00:50 | INFO | train_inner | epoch 025:     14 / 16 loss=12.907, nll_loss=11.652, ppl=3219.03, wps=1483.2, ups=0.35, wpb=4276, bsz=152, num_updates=398, lr=4.776e-06, gnorm=2.541, train_wall=6, gb_free=9.3, wall=3119
2024-09-04 08:00:55 | INFO | train_inner | epoch 025:     16 / 16 loss=13.306, nll_loss=12.187, ppl=4662.1, wps=1503.2, ups=0.39, wpb=3857, bsz=85, num_updates=400, lr=4.8e-06, gnorm=1.496, train_wall=5, gb_free=13.3, wall=3124
2024-09-04 08:00:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31650.19921875Mb; avail=223379.1484375Mb
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31651.67578125Mb; avail=223378.1640625Mb
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012826
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31680.22265625Mb; avail=223349.6171875Mb
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011171
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024981
2024-09-04 08:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31705.32421875Mb; avail=223324.515625Mb
2024-09-04 08:01:09 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 13.196 | nll_loss 11.952 | ppl 3963.19 | wps 3864.9 | wpb 2070.5 | bsz 122.7 | num_updates 400 | best_loss 13.196
2024-09-04 08:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 400 updates
2024-09-04 08:01:09 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:01:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:01:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:01:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 26 @ 546 updates, score 11.027) (writing took 71.20953316707164 seconds)
2024-09-04 08:01:54 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-09-04 08:01:54 | INFO | train | epoch 026 | loss 10.513 | nll_loss 8.604 | ppl 389.11 | wps 660.9 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 546 | lr 6.552e-06 | gnorm 1.951 | train_wall 56 | gb_free 13.8 | wall 3671
2024-09-04 08:01:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:01:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:01:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:01:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000660
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22669.6953125Mb; avail=232359.7421875Mb
2024-09-04 08:01:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000590
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22673.6328125Mb; avail=232355.8046875Mb
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22674.6171875Mb; avail=232354.8203125Mb
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001070
2024-09-04 08:01:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22666.25Mb; avail=232363.6796875Mb
2024-09-04 08:01:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:01:54 | INFO | fairseq.trainer | begin training epoch 27
2024-09-04 08:01:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:02:00 | INFO | train_inner | epoch 027:      2 / 21 loss=10.479, nll_loss=8.555, ppl=376.15, wps=100, ups=0.02, wpb=4720, bsz=104, num_updates=548, lr=6.576e-06, gnorm=1.818, train_wall=6, gb_free=12.1, wall=3677
2024-09-04 08:02:04 | INFO | train_inner | epoch 027:      4 / 21 loss=10.436, nll_loss=8.496, ppl=361.05, wps=1714.5, ups=0.42, wpb=4034.5, bsz=72, num_updates=550, lr=6.6e-06, gnorm=1.796, train_wall=5, gb_free=13.8, wall=3681
2024-09-04 08:02:10 | INFO | train_inner | epoch 027:      6 / 21 loss=10.458, nll_loss=8.531, ppl=369.82, wps=1681.3, ups=0.34, wpb=4988, bsz=92, num_updates=552, lr=6.624e-06, gnorm=2.163, train_wall=6, gb_free=10.8, wall=3687
2024-09-04 08:02:15 | INFO | train_inner | epoch 027:      8 / 21 loss=10.474, nll_loss=8.563, ppl=378.21, wps=1648.3, ups=0.42, wpb=3911.5, bsz=45, num_updates=554, lr=6.648e-06, gnorm=2.11, train_wall=5, gb_free=11.5, wall=3692
2024-09-04 08:02:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 25 @ 400 updates, score 13.196) (writing took 66.76726955734193 seconds)
2024-09-04 08:02:16 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-09-04 08:02:16 | INFO | train | epoch 025 | loss 13.086 | nll_loss 11.892 | ppl 3801.64 | wps 479.5 | ups 0.11 | wpb 4236.4 | bsz 127.1 | num_updates 400 | lr 4.8e-06 | gnorm 1.981 | train_wall 60 | gb_free 13.3 | wall 3205
2024-09-04 08:02:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:02:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:02:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:02:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000668
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22713.328125Mb; avail=232316.5546875Mb
2024-09-04 08:02:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000590
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22713.328125Mb; avail=232316.5546875Mb
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22713.328125Mb; avail=232316.5546875Mb
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000164
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001074
2024-09-04 08:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22713.328125Mb; avail=232316.5546875Mb
2024-09-04 08:02:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:02:16 | INFO | fairseq.trainer | begin training epoch 26
2024-09-04 08:02:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:02:21 | INFO | train_inner | epoch 027:     10 / 21 loss=10.054, nll_loss=8.005, ppl=256.88, wps=1521.2, ups=0.36, wpb=4261.5, bsz=128, num_updates=556, lr=6.672e-06, gnorm=1.869, train_wall=6, gb_free=12.3, wall=3698
2024-09-04 08:02:22 | INFO | train_inner | epoch 026:      2 / 16 loss=12.488, nll_loss=11.128, ppl=2237.59, wps=100.1, ups=0.02, wpb=4358.5, bsz=212, num_updates=402, lr=4.824e-06, gnorm=3.14, train_wall=6, gb_free=10.5, wall=3211
2024-09-04 08:02:25 | INFO | train_inner | epoch 027:     12 / 21 loss=10.365, nll_loss=8.424, ppl=343.56, wps=1950.9, ups=0.42, wpb=4600.5, bsz=100, num_updates=558, lr=6.696e-06, gnorm=1.681, train_wall=5, gb_free=11.5, wall=3702
2024-09-04 08:02:28 | INFO | train_inner | epoch 026:      4 / 16 loss=12.986, nll_loss=11.766, ppl=3482.09, wps=1538.3, ups=0.35, wpb=4382, bsz=136, num_updates=404, lr=4.848e-06, gnorm=1.781, train_wall=6, gb_free=12.1, wall=3217
2024-09-04 08:02:31 | INFO | train_inner | epoch 027:     14 / 21 loss=10.498, nll_loss=8.559, ppl=377.07, wps=1651.7, ups=0.35, wpb=4676, bsz=84, num_updates=560, lr=6.72e-06, gnorm=2.122, train_wall=6, gb_free=12.4, wall=3708
2024-09-04 08:02:33 | INFO | train_inner | epoch 026:      6 / 16 loss=13.079, nll_loss=11.886, ppl=3784.66, wps=1541.7, ups=0.34, wpb=4469.5, bsz=120, num_updates=406, lr=4.872e-06, gnorm=2.095, train_wall=6, gb_free=8.9, wall=3223
2024-09-04 08:02:36 | INFO | train_inner | epoch 027:     16 / 21 loss=10.251, nll_loss=8.267, ppl=308, wps=2080.7, ups=0.41, wpb=5039, bsz=120, num_updates=562, lr=6.744e-06, gnorm=1.978, train_wall=5, gb_free=13.2, wall=3713
2024-09-04 08:02:39 | INFO | train_inner | epoch 026:      8 / 16 loss=13.086, nll_loss=11.913, ppl=3856.93, wps=1527.5, ups=0.38, wpb=4072, bsz=96, num_updates=408, lr=4.896e-06, gnorm=1.736, train_wall=5, gb_free=10.5, wall=3228
2024-09-04 08:02:42 | INFO | train_inner | epoch 027:     18 / 21 loss=10.137, nll_loss=8.13, ppl=280.16, wps=1559.5, ups=0.34, wpb=4529.5, bsz=144, num_updates=564, lr=6.768e-06, gnorm=1.752, train_wall=6, gb_free=12.1, wall=3719
2024-09-04 08:02:45 | INFO | train_inner | epoch 026:     10 / 16 loss=12.563, nll_loss=11.206, ppl=2362.51, wps=1471.8, ups=0.33, wpb=4488, bsz=196, num_updates=410, lr=4.92e-06, gnorm=2.226, train_wall=6, gb_free=9, wall=3234
2024-09-04 08:02:47 | INFO | train_inner | epoch 027:     20 / 21 loss=10.401, nll_loss=8.443, ppl=348.03, wps=1749.2, ups=0.38, wpb=4637, bsz=88, num_updates=566, lr=6.792e-06, gnorm=2.363, train_wall=5, gb_free=14.1, wall=3724
2024-09-04 08:02:50 | INFO | train_inner | epoch 026:     12 / 16 loss=13.231, nll_loss=12.09, ppl=4361.15, wps=1475.6, ups=0.41, wpb=3619.5, bsz=81, num_updates=412, lr=4.944e-06, gnorm=1.65, train_wall=5, gb_free=9.4, wall=3239
2024-09-04 08:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21379.54296875Mb; avail=233650.12109375Mb
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000733
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.54296875Mb; avail=233650.12109375Mb
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012706
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.54296875Mb; avail=233650.12109375Mb
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011289
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025150
2024-09-04 08:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21379.54296875Mb; avail=233650.12109375Mb
2024-09-04 08:02:56 | INFO | train_inner | epoch 026:     14 / 16 loss=13.085, nll_loss=11.894, ppl=3807, wps=1548.9, ups=0.32, wpb=4827.5, bsz=124, num_updates=414, lr=4.968e-06, gnorm=1.381, train_wall=6, gb_free=8.1, wall=3245
2024-09-04 08:03:01 | INFO | train_inner | epoch 026:     16 / 16 loss=13.336, nll_loss=12.236, ppl=4822.3, wps=1442.8, ups=0.39, wpb=3674.5, bsz=52, num_updates=416, lr=4.992e-06, gnorm=1.744, train_wall=5, gb_free=17.2, wall=3251
2024-09-04 08:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21408.21875Mb; avail=233621.578125Mb
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21408.21875Mb; avail=233621.578125Mb
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012723
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21408.21875Mb; avail=233621.578125Mb
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011156
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024986
2024-09-04 08:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21408.21875Mb; avail=233621.578125Mb
2024-09-04 08:03:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.87 | nll_loss 8.927 | ppl 486.81 | wps 5043.3 | wpb 2350.9 | bsz 94.7 | num_updates 567 | best_loss 10.87
2024-09-04 08:03:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 567 updates
2024-09-04 08:03:06 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:03:15 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 13.092 | nll_loss 11.815 | ppl 3603.66 | wps 3850.4 | wpb 2070.5 | bsz 122.7 | num_updates 416 | best_loss 13.092
2024-09-04 08:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 416 updates
2024-09-04 08:03:15 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:03:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:03:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:04:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 27 @ 567 updates, score 10.87) (writing took 63.127399975433946 seconds)
2024-09-04 08:04:09 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-09-04 08:04:09 | INFO | train | epoch 027 | loss 10.346 | nll_loss 8.386 | ppl 334.49 | wps 706.3 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 567 | lr 6.804e-06 | gnorm 1.952 | train_wall 56 | gb_free 13.5 | wall 3806
2024-09-04 08:04:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:04:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:04:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:04:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000748
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24409.2734375Mb; avail=230620.56640625Mb
2024-09-04 08:04:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000616
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24409.2734375Mb; avail=230620.56640625Mb
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24409.765625Mb; avail=230620.07421875Mb
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000193
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001115
2024-09-04 08:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24409.2734375Mb; avail=230620.07421875Mb
2024-09-04 08:04:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:04:09 | INFO | fairseq.trainer | begin training epoch 28
2024-09-04 08:04:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:04:13 | INFO | train_inner | epoch 028:      1 / 21 loss=9.963, nll_loss=7.887, ppl=236.64, wps=114.6, ups=0.02, wpb=4919.5, bsz=124, num_updates=568, lr=6.816e-06, gnorm=1.817, train_wall=6, gb_free=12.5, wall=3810
2024-09-04 08:04:17 | INFO | train_inner | epoch 028:      3 / 21 loss=10.142, nll_loss=8.117, ppl=277.63, wps=1836.8, ups=0.48, wpb=3860.5, bsz=93, num_updates=570, lr=6.84e-06, gnorm=2.119, train_wall=4, gb_free=12.8, wall=3814
2024-09-04 08:04:22 | INFO | train_inner | epoch 028:      5 / 21 loss=10.343, nll_loss=8.375, ppl=331.92, wps=2037.5, ups=0.41, wpb=4972, bsz=80, num_updates=572, lr=6.864e-06, gnorm=2.393, train_wall=5, gb_free=12.3, wall=3819
2024-09-04 08:04:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 26 @ 416 updates, score 13.092) (writing took 67.71081234887242 seconds)
2024-09-04 08:04:23 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-09-04 08:04:23 | INFO | train | epoch 026 | loss 12.969 | nll_loss 11.747 | ppl 3437.4 | wps 532.7 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 416 | lr 4.992e-06 | gnorm 1.969 | train_wall 45 | gb_free 17.2 | wall 3333
2024-09-04 08:04:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:04:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:04:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:04:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000726
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24450.38671875Mb; avail=230579.41796875Mb
2024-09-04 08:04:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000609
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24450.38671875Mb; avail=230579.41796875Mb
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24450.38671875Mb; avail=230579.41796875Mb
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001086
2024-09-04 08:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24450.38671875Mb; avail=230579.41796875Mb
2024-09-04 08:04:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:04:23 | INFO | fairseq.trainer | begin training epoch 27
2024-09-04 08:04:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:04:28 | INFO | train_inner | epoch 028:      7 / 21 loss=10.187, nll_loss=8.193, ppl=292.69, wps=1668.8, ups=0.34, wpb=4901, bsz=112, num_updates=574, lr=6.888e-06, gnorm=2.058, train_wall=6, gb_free=12.4, wall=3825
2024-09-04 08:04:28 | INFO | train_inner | epoch 027:      2 / 16 loss=13.21, nll_loss=12.069, ppl=4295.87, wps=88.4, ups=0.02, wpb=3853, bsz=76, num_updates=418, lr=5.016e-06, gnorm=1.45, train_wall=5, gb_free=9.6, wall=3338
2024-09-04 08:04:33 | INFO | train_inner | epoch 027:      4 / 16 loss=12.327, nll_loss=10.917, ppl=1934.08, wps=1370.6, ups=0.42, wpb=3233, bsz=137, num_updates=420, lr=5.04e-06, gnorm=1.861, train_wall=5, gb_free=14, wall=3342
2024-09-04 08:04:34 | INFO | train_inner | epoch 028:      9 / 21 loss=9.741, nll_loss=7.604, ppl=194.5, wps=1460.4, ups=0.34, wpb=4239.5, bsz=144, num_updates=576, lr=6.912e-06, gnorm=2.16, train_wall=6, gb_free=12.8, wall=3831
2024-09-04 08:04:39 | INFO | train_inner | epoch 028:     11 / 21 loss=10.159, nll_loss=8.153, ppl=284.57, wps=1758.2, ups=0.38, wpb=4580, bsz=124, num_updates=578, lr=6.936e-06, gnorm=1.732, train_wall=5, gb_free=16.2, wall=3836
2024-09-04 08:04:44 | INFO | train_inner | epoch 028:     13 / 21 loss=10.378, nll_loss=8.403, ppl=338.38, wps=1859.6, ups=0.39, wpb=4780.5, bsz=80, num_updates=580, lr=6.96e-06, gnorm=2.354, train_wall=5, gb_free=12.6, wall=3841
2024-09-04 08:04:49 | INFO | train_inner | epoch 028:     15 / 21 loss=10.118, nll_loss=8.093, ppl=273.03, wps=1985.6, ups=0.42, wpb=4728, bsz=96, num_updates=582, lr=6.984e-06, gnorm=2.232, train_wall=5, gb_free=12.1, wall=3846
2024-09-04 08:04:49 | INFO | train_inner | epoch 027:      6 / 16 loss=13.076, nll_loss=11.899, ppl=3817.9, wps=576.9, ups=0.12, wpb=4625.5, bsz=112, num_updates=422, lr=5.064e-06, gnorm=1.595, train_wall=16, gb_free=9.1, wall=3358
2024-09-04 08:04:55 | INFO | train_inner | epoch 027:      8 / 16 loss=12.923, nll_loss=11.688, ppl=3298.69, wps=1528.7, ups=0.36, wpb=4295.5, bsz=124, num_updates=424, lr=5.088e-06, gnorm=1.427, train_wall=6, gb_free=12.4, wall=3364
2024-09-04 08:04:55 | INFO | train_inner | epoch 028:     17 / 21 loss=10.281, nll_loss=8.31, ppl=317.45, wps=1494.8, ups=0.32, wpb=4725, bsz=84, num_updates=584, lr=7.008e-06, gnorm=2.358, train_wall=6, gb_free=10.3, wall=3852
2024-09-04 08:05:00 | INFO | train_inner | epoch 028:     19 / 21 loss=10.409, nll_loss=8.443, ppl=347.95, wps=1709, ups=0.4, wpb=4257, bsz=52, num_updates=586, lr=7.032e-06, gnorm=2.219, train_wall=5, gb_free=12.7, wall=3857
2024-09-04 08:05:00 | INFO | train_inner | epoch 027:     10 / 16 loss=12.777, nll_loss=11.5, ppl=2897.11, wps=1538.3, ups=0.34, wpb=4531, bsz=148, num_updates=426, lr=5.112e-06, gnorm=1.666, train_wall=6, gb_free=9.7, wall=3370
2024-09-04 08:05:05 | INFO | train_inner | epoch 028:     21 / 21 loss=10.302, nll_loss=8.311, ppl=317.66, wps=1786.6, ups=0.41, wpb=4321.5, bsz=68, num_updates=588, lr=7.056e-06, gnorm=2.415, train_wall=5, gb_free=12.3, wall=3862
2024-09-04 08:05:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20641.27734375Mb; avail=234388.61328125Mb
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20641.27734375Mb; avail=234388.61328125Mb
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.013034
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20641.76953125Mb; avail=234388.12109375Mb
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011198
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025347
2024-09-04 08:05:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20641.76953125Mb; avail=234388.12109375Mb
2024-09-04 08:05:06 | INFO | train_inner | epoch 027:     12 / 16 loss=12.829, nll_loss=11.572, ppl=3043.92, wps=1558.7, ups=0.35, wpb=4459.5, bsz=132, num_updates=428, lr=5.136e-06, gnorm=1.567, train_wall=6, gb_free=11.4, wall=3376
2024-09-04 08:05:13 | INFO | train_inner | epoch 027:     14 / 16 loss=12.491, nll_loss=11.13, ppl=2241.33, wps=1443.1, ups=0.31, wpb=4612.5, bsz=196, num_updates=430, lr=5.16e-06, gnorm=1.765, train_wall=6, gb_free=8.1, wall=3382
2024-09-04 08:05:18 | INFO | train_inner | epoch 027:     16 / 16 loss=12.909, nll_loss=11.685, ppl=3291.48, wps=1576.5, ups=0.37, wpb=4281.5, bsz=92, num_updates=432, lr=5.184e-06, gnorm=1.562, train_wall=5, gb_free=11.7, wall=3388
2024-09-04 08:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19655.62109375Mb; avail=235374.16015625Mb
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000624
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19655.62109375Mb; avail=235374.16015625Mb
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012852
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19655.62109375Mb; avail=235374.16015625Mb
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011392
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025237
2024-09-04 08:05:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19655.62109375Mb; avail=235374.16015625Mb
2024-09-04 08:05:22 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.707 | nll_loss 8.734 | ppl 425.7 | wps 4762.2 | wpb 2350.9 | bsz 94.7 | num_updates 588 | best_loss 10.707
2024-09-04 08:05:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 588 updates
2024-09-04 08:05:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:05:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 12.99 | nll_loss 11.669 | ppl 3256.32 | wps 3847.7 | wpb 2070.5 | bsz 122.7 | num_updates 432 | best_loss 12.99
2024-09-04 08:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 432 updates
2024-09-04 08:05:32 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:06:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 28 @ 588 updates, score 10.707) (writing took 80.37136703636497 seconds)
2024-09-04 08:06:42 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-09-04 08:06:42 | INFO | train | epoch 028 | loss 10.187 | nll_loss 8.175 | ppl 289 | wps 624.6 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 588 | lr 7.056e-06 | gnorm 2.192 | train_wall 55 | gb_free 12.3 | wall 3959
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000602
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15929.390625Mb; avail=239100.58984375Mb
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000065
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000587
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15929.390625Mb; avail=239100.58984375Mb
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15929.390625Mb; avail=239100.58984375Mb
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000161
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001058
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15929.390625Mb; avail=239100.58984375Mb
2024-09-04 08:06:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 27 @ 432 updates, score 12.99) (writing took 70.06188543047756 seconds)
2024-09-04 08:06:42 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-09-04 08:06:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:06:42 | INFO | fairseq.trainer | begin training epoch 29
2024-09-04 08:06:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:06:42 | INFO | train | epoch 027 | loss 12.827 | nll_loss 11.57 | ppl 3039.73 | wps 486.3 | ups 0.11 | wpb 4236.4 | bsz 127.1 | num_updates 432 | lr 5.184e-06 | gnorm 1.612 | train_wall 55 | gb_free 11.7 | wall 3472
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000687
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15944.39453125Mb; avail=239085.58203125Mb
2024-09-04 08:06:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000615
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15944.88671875Mb; avail=239085.08984375Mb
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15944.88671875Mb; avail=239085.08984375Mb
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000166
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001081
2024-09-04 08:06:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15945.37890625Mb; avail=239084.59765625Mb
2024-09-04 08:06:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:06:43 | INFO | fairseq.trainer | begin training epoch 28
2024-09-04 08:06:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:06:48 | INFO | train_inner | epoch 029:      2 / 21 loss=9.953, nll_loss=7.891, ppl=237.37, wps=92.8, ups=0.02, wpb=4772, bsz=120, num_updates=590, lr=7.08e-06, gnorm=2.1, train_wall=5, gb_free=11.9, wall=3965
2024-09-04 08:06:48 | INFO | train_inner | epoch 028:      2 / 16 loss=12.628, nll_loss=11.294, ppl=2510.98, wps=91, ups=0.02, wpb=4089.5, bsz=144, num_updates=434, lr=5.208e-06, gnorm=1.493, train_wall=5, gb_free=13.6, wall=3477
2024-09-04 08:06:53 | INFO | train_inner | epoch 029:      4 / 21 loss=9.718, nll_loss=7.577, ppl=190.92, wps=1556.5, ups=0.36, wpb=4380, bsz=144, num_updates=592, lr=7.104e-06, gnorm=2.107, train_wall=6, gb_free=13.8, wall=3970
2024-09-04 08:06:54 | INFO | train_inner | epoch 028:      4 / 16 loss=12.923, nll_loss=11.695, ppl=3315.13, wps=1499.7, ups=0.35, wpb=4239, bsz=100, num_updates=436, lr=5.232e-06, gnorm=1.369, train_wall=6, gb_free=9.9, wall=3483
2024-09-04 08:06:58 | INFO | train_inner | epoch 029:      6 / 21 loss=10.162, nll_loss=8.144, ppl=282.84, wps=1715.6, ups=0.4, wpb=4284.5, bsz=84, num_updates=594, lr=7.128e-06, gnorm=2.191, train_wall=5, gb_free=11.9, wall=3975
2024-09-04 08:07:00 | INFO | train_inner | epoch 028:      6 / 16 loss=12.576, nll_loss=11.247, ppl=2430.7, wps=1519, ups=0.32, wpb=4677, bsz=180, num_updates=438, lr=5.256e-06, gnorm=1.384, train_wall=6, gb_free=9.6, wall=3489
2024-09-04 08:07:04 | INFO | train_inner | epoch 029:      8 / 21 loss=10.142, nll_loss=8.104, ppl=275.12, wps=1747, ups=0.39, wpb=4499.5, bsz=72, num_updates=596, lr=7.152e-06, gnorm=2.092, train_wall=5, gb_free=13.7, wall=3980
2024-09-04 08:07:05 | INFO | train_inner | epoch 028:      8 / 16 loss=12.657, nll_loss=11.351, ppl=2612.38, wps=1592, ups=0.36, wpb=4447, bsz=144, num_updates=440, lr=5.28e-06, gnorm=1.344, train_wall=6, gb_free=10.9, wall=3495
2024-09-04 08:07:08 | INFO | train_inner | epoch 029:     10 / 21 loss=9.911, nll_loss=7.818, ppl=225.71, wps=1942.9, ups=0.4, wpb=4837, bsz=148, num_updates=598, lr=7.176e-06, gnorm=2.054, train_wall=5, gb_free=11.8, wall=3985
2024-09-04 08:07:11 | INFO | train_inner | epoch 028:     10 / 16 loss=12.834, nll_loss=11.59, ppl=3082.15, wps=1530, ups=0.36, wpb=4297, bsz=96, num_updates=442, lr=5.304e-06, gnorm=1.411, train_wall=6, gb_free=10.3, wall=3500
2024-09-04 08:07:14 | INFO | train_inner | epoch 029:     12 / 21 loss=10.219, nll_loss=8.221, ppl=298.32, wps=1487, ups=0.39, wpb=3787, bsz=41, num_updates=600, lr=7.2e-06, gnorm=2.661, train_wall=5, gb_free=19.9, wall=3991
2024-09-04 08:07:16 | INFO | train_inner | epoch 028:     12 / 16 loss=12.817, nll_loss=11.57, ppl=3039.66, wps=1462.1, ups=0.41, wpb=3599.5, bsz=81, num_updates=444, lr=5.328e-06, gnorm=1.692, train_wall=5, gb_free=15.7, wall=3505
2024-09-04 08:07:19 | INFO | train_inner | epoch 029:     14 / 21 loss=10.028, nll_loss=7.983, ppl=252.98, wps=1982.1, ups=0.38, wpb=5156, bsz=92, num_updates=602, lr=7.224e-06, gnorm=1.927, train_wall=5, gb_free=12, wall=3996
2024-09-04 08:07:21 | INFO | train_inner | epoch 028:     14 / 16 loss=12.836, nll_loss=11.588, ppl=3077.68, wps=1507.8, ups=0.36, wpb=4212, bsz=104, num_updates=446, lr=5.352e-06, gnorm=1.374, train_wall=6, gb_free=11.7, wall=3511
2024-09-04 08:07:25 | INFO | train_inner | epoch 029:     16 / 21 loss=10.02, nll_loss=7.942, ppl=245.95, wps=1639.7, ups=0.34, wpb=4769.5, bsz=104, num_updates=604, lr=7.248e-06, gnorm=2.041, train_wall=6, gb_free=13.5, wall=4002
2024-09-04 08:07:27 | INFO | train_inner | epoch 028:     16 / 16 loss=12.271, nll_loss=10.852, ppl=1847.86, wps=1456.1, ups=0.34, wpb=4330.5, bsz=168, num_updates=448, lr=5.376e-06, gnorm=1.778, train_wall=6, gb_free=10.2, wall=3517
2024-09-04 08:07:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16003.92578125Mb; avail=239025.9765625Mb
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000754
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16003.92578125Mb; avail=239025.9765625Mb
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012780
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16003.92578125Mb; avail=239025.9765625Mb
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011171
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025081
2024-09-04 08:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16003.92578125Mb; avail=239025.9765625Mb
2024-09-04 08:07:29 | INFO | train_inner | epoch 029:     18 / 21 loss=10.154, nll_loss=8.123, ppl=278.81, wps=2010.1, ups=0.44, wpb=4543.5, bsz=60, num_updates=606, lr=7.272e-06, gnorm=1.862, train_wall=5, gb_free=17, wall=4006
2024-09-04 08:07:35 | INFO | train_inner | epoch 029:     20 / 21 loss=9.891, nll_loss=7.802, ppl=223.15, wps=1550.1, ups=0.34, wpb=4538, bsz=120, num_updates=608, lr=7.296e-06, gnorm=2.267, train_wall=6, gb_free=12.8, wall=4012
2024-09-04 08:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16034.390625Mb; avail=238994.9765625Mb
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16034.8828125Mb; avail=238994.9765625Mb
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012747
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16034.8828125Mb; avail=238994.9765625Mb
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011362
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025220
2024-09-04 08:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16034.8828125Mb; avail=238994.9765625Mb
2024-09-04 08:07:42 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 12.894 | nll_loss 11.55 | ppl 2999.18 | wps 3848.5 | wpb 2070.5 | bsz 122.7 | num_updates 448 | best_loss 12.894
2024-09-04 08:07:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 448 updates
2024-09-04 08:07:42 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:07:54 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.577 | nll_loss 8.555 | ppl 376.11 | wps 4994.1 | wpb 2350.9 | bsz 94.7 | num_updates 609 | best_loss 10.577
2024-09-04 08:07:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 609 updates
2024-09-04 08:07:54 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:08:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:08:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:08:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 28 @ 448 updates, score 12.894) (writing took 58.47013513930142 seconds)
2024-09-04 08:08:40 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-09-04 08:08:40 | INFO | train | epoch 028 | loss 12.688 | nll_loss 11.392 | ppl 2687.25 | wps 575.4 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 448 | lr 5.376e-06 | gnorm 1.481 | train_wall 45 | gb_free 10.2 | wall 3590
2024-09-04 08:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000754
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20983.98828125Mb; avail=234049.3359375Mb
2024-09-04 08:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000621
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20975.7421875Mb; avail=234052.66015625Mb
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20975.7421875Mb; avail=234052.66015625Mb
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001199
2024-09-04 08:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20977.37109375Mb; avail=234051.03125Mb
2024-09-04 08:08:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:08:40 | INFO | fairseq.trainer | begin training epoch 29
2024-09-04 08:08:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:08:46 | INFO | train_inner | epoch 029:      2 / 16 loss=12.631, nll_loss=11.318, ppl=2553.07, wps=118.8, ups=0.03, wpb=4694, bsz=148, num_updates=450, lr=5.4e-06, gnorm=1.332, train_wall=6, gb_free=10, wall=3596
2024-09-04 08:08:52 | INFO | train_inner | epoch 029:      4 / 16 loss=12.728, nll_loss=11.435, ppl=2768.36, wps=1550.9, ups=0.37, wpb=4206, bsz=112, num_updates=452, lr=5.424e-06, gnorm=2.072, train_wall=5, gb_free=10.8, wall=3601
2024-09-04 08:08:58 | INFO | train_inner | epoch 029:      6 / 16 loss=12.758, nll_loss=11.495, ppl=2887.05, wps=1500.8, ups=0.33, wpb=4528.5, bsz=112, num_updates=454, lr=5.448e-06, gnorm=1.714, train_wall=6, gb_free=9.3, wall=3607
2024-09-04 08:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 29 @ 609 updates, score 10.577) (writing took 67.53785064909607 seconds)
2024-09-04 08:09:01 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-09-04 08:09:01 | INFO | train | epoch 029 | loss 10.025 | nll_loss 7.967 | ppl 250.29 | wps 689.5 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 609 | lr 7.308e-06 | gnorm 2.136 | train_wall 55 | gb_free 12.3 | wall 4098
2024-09-04 08:09:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:09:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:09:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:09:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000726
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21015.69921875Mb; avail=234014.15625Mb
2024-09-04 08:09:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000623
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21015.69921875Mb; avail=234014.15625Mb
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21015.69921875Mb; avail=234014.15625Mb
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000178
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001122
2024-09-04 08:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21015.69921875Mb; avail=234014.15625Mb
2024-09-04 08:09:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:09:01 | INFO | fairseq.trainer | begin training epoch 30
2024-09-04 08:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:09:04 | INFO | train_inner | epoch 030:      1 / 21 loss=10.106, nll_loss=8.086, ppl=271.73, wps=110.4, ups=0.02, wpb=4933, bsz=84, num_updates=610, lr=7.32e-06, gnorm=2.095, train_wall=5, gb_free=12.3, wall=4101
2024-09-04 08:09:09 | INFO | train_inner | epoch 030:      3 / 21 loss=9.648, nll_loss=7.451, ppl=174.99, wps=1853.7, ups=0.41, wpb=4486, bsz=132, num_updates=612, lr=7.344e-06, gnorm=1.985, train_wall=5, gb_free=13.1, wall=4106
2024-09-04 08:09:14 | INFO | train_inner | epoch 029:      8 / 16 loss=12.126, nll_loss=10.664, ppl=1622.28, wps=520.3, ups=0.13, wpb=4098, bsz=156, num_updates=456, lr=5.472e-06, gnorm=1.857, train_wall=16, gb_free=11.4, wall=3623
2024-09-04 08:09:15 | INFO | train_inner | epoch 030:      5 / 21 loss=9.929, nll_loss=7.837, ppl=228.68, wps=1537.2, ups=0.34, wpb=4458.5, bsz=104, num_updates=614, lr=7.368e-06, gnorm=1.766, train_wall=6, gb_free=12.5, wall=4112
2024-09-04 08:09:19 | INFO | train_inner | epoch 029:     10 / 16 loss=12.461, nll_loss=11.108, ppl=2207.21, wps=1591.7, ups=0.35, wpb=4523.5, bsz=148, num_updates=458, lr=5.496e-06, gnorm=1.825, train_wall=6, gb_free=10.4, wall=3629
2024-09-04 08:09:21 | INFO | train_inner | epoch 030:      7 / 21 loss=9.97, nll_loss=7.88, ppl=235.55, wps=1740.5, ups=0.34, wpb=5163.5, bsz=116, num_updates=616, lr=7.392e-06, gnorm=1.793, train_wall=6, gb_free=12.6, wall=4118
2024-09-04 08:09:24 | INFO | train_inner | epoch 029:     12 / 16 loss=12.707, nll_loss=11.427, ppl=2753.98, wps=1473.4, ups=0.42, wpb=3543.5, bsz=85, num_updates=460, lr=5.52e-06, gnorm=2.006, train_wall=5, gb_free=11.8, wall=3634
2024-09-04 08:09:26 | INFO | train_inner | epoch 030:      9 / 21 loss=10.003, nll_loss=7.945, ppl=246.35, wps=1666.5, ups=0.36, wpb=4634, bsz=68, num_updates=618, lr=7.416e-06, gnorm=1.67, train_wall=6, gb_free=12.8, wall=4123
2024-09-04 08:09:30 | INFO | train_inner | epoch 029:     14 / 16 loss=12.603, nll_loss=11.29, ppl=2503.26, wps=1517, ups=0.35, wpb=4375, bsz=128, num_updates=462, lr=5.544e-06, gnorm=1.891, train_wall=6, gb_free=10.2, wall=3639
2024-09-04 08:09:32 | INFO | train_inner | epoch 030:     11 / 21 loss=9.401, nll_loss=7.174, ppl=144.4, wps=1531.9, ups=0.35, wpb=4391.5, bsz=148, num_updates=620, lr=7.44e-06, gnorm=2.279, train_wall=6, gb_free=12.9, wall=4129
2024-09-04 08:09:35 | INFO | train_inner | epoch 029:     16 / 16 loss=12.525, nll_loss=11.178, ppl=2317.63, wps=1511, ups=0.39, wpb=3923, bsz=128, num_updates=464, lr=5.568e-06, gnorm=1.64, train_wall=5, gb_free=12.8, wall=3645
2024-09-04 08:09:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17198.328125Mb; avail=237831.45703125Mb
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000629
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17198.328125Mb; avail=237831.45703125Mb
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012694
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17198.328125Mb; avail=237831.45703125Mb
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011128
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024810
2024-09-04 08:09:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17198.328125Mb; avail=237831.45703125Mb
2024-09-04 08:09:38 | INFO | train_inner | epoch 030:     13 / 21 loss=10.047, nll_loss=7.98, ppl=252.47, wps=1713.2, ups=0.36, wpb=4742, bsz=88, num_updates=622, lr=7.464e-06, gnorm=2.14, train_wall=6, gb_free=13.7, wall=4135
2024-09-04 08:09:42 | INFO | train_inner | epoch 030:     15 / 21 loss=9.79, nll_loss=7.661, ppl=202.39, wps=1807.1, ups=0.45, wpb=3986, bsz=85, num_updates=624, lr=7.488e-06, gnorm=2.353, train_wall=4, gb_free=11.8, wall=4139
2024-09-04 08:09:47 | INFO | train_inner | epoch 030:     17 / 21 loss=9.915, nll_loss=7.818, ppl=225.65, wps=1920, ups=0.42, wpb=4561.5, bsz=108, num_updates=626, lr=7.512e-06, gnorm=1.815, train_wall=5, gb_free=12.2, wall=4144
2024-09-04 08:09:49 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 12.786 | nll_loss 11.432 | ppl 2762.83 | wps 3850.6 | wpb 2070.5 | bsz 122.7 | num_updates 464 | best_loss 12.786
2024-09-04 08:09:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 464 updates
2024-09-04 08:09:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:09:53 | INFO | train_inner | epoch 030:     19 / 21 loss=9.928, nll_loss=7.837, ppl=228.61, wps=1635.5, ups=0.35, wpb=4739.5, bsz=60, num_updates=628, lr=7.536e-06, gnorm=1.822, train_wall=6, gb_free=11.9, wall=4150
2024-09-04 08:09:57 | INFO | train_inner | epoch 030:     21 / 21 loss=10.062, nll_loss=8.008, ppl=257.49, wps=1807, ups=0.45, wpb=4019.5, bsz=56, num_updates=630, lr=7.56e-06, gnorm=1.734, train_wall=4, gb_free=13.3, wall=4154
2024-09-04 08:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22515.94140625Mb; avail=232513.9609375Mb
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000749
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22517.41796875Mb; avail=232512.484375Mb
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012823
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22547.44140625Mb; avail=232482.4609375Mb
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011438
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025405
2024-09-04 08:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22574.51171875Mb; avail=232454.8984375Mb
2024-09-04 08:10:14 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.511 | nll_loss 8.431 | ppl 345.09 | wps 4713.3 | wpb 2350.9 | bsz 94.7 | num_updates 630 | best_loss 10.511
2024-09-04 08:10:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 630 updates
2024-09-04 08:10:14 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:10:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 29 @ 464 updates, score 12.786) (writing took 64.27415481489152 seconds)
2024-09-04 08:10:54 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-09-04 08:10:54 | INFO | train | epoch 029 | loss 12.568 | nll_loss 11.241 | ppl 2419.81 | wps 507.9 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 464 | lr 5.568e-06 | gnorm 1.792 | train_wall 55 | gb_free 12.8 | wall 3723
2024-09-04 08:10:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:10:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:10:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:10:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000647
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32931.07421875Mb; avail=222098.890625Mb
2024-09-04 08:10:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000581
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32931.07421875Mb; avail=222098.890625Mb
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32931.07421875Mb; avail=222098.890625Mb
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000159
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001044
2024-09-04 08:10:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32931.07421875Mb; avail=222098.890625Mb
2024-09-04 08:10:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:10:54 | INFO | fairseq.trainer | begin training epoch 30
2024-09-04 08:10:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:11:00 | INFO | train_inner | epoch 030:      2 / 16 loss=12.293, nll_loss=10.88, ppl=1884.94, wps=105.1, ups=0.02, wpb=4438, bsz=168, num_updates=466, lr=5.592e-06, gnorm=1.863, train_wall=6, gb_free=10, wall=3729
2024-09-04 08:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:11:06 | INFO | train_inner | epoch 030:      4 / 16 loss=11.771, nll_loss=10.203, ppl=1178.56, wps=1469.2, ups=0.33, wpb=4422.5, bsz=224, num_updates=468, lr=5.616e-06, gnorm=2.235, train_wall=6, gb_free=8.9, wall=3735
2024-09-04 08:11:11 | INFO | train_inner | epoch 030:      6 / 16 loss=12.66, nll_loss=11.374, ppl=2654.5, wps=1511.5, ups=0.34, wpb=4469, bsz=88, num_updates=470, lr=5.64e-06, gnorm=1.814, train_wall=6, gb_free=9.8, wall=3741
2024-09-04 08:11:16 | INFO | train_inner | epoch 030:      8 / 16 loss=12.782, nll_loss=11.524, ppl=2945.07, wps=1533.7, ups=0.41, wpb=3714, bsz=72, num_updates=472, lr=5.664e-06, gnorm=2.407, train_wall=5, gb_free=17.5, wall=3746
2024-09-04 08:11:22 | INFO | train_inner | epoch 030:     10 / 16 loss=12.531, nll_loss=11.195, ppl=2343.9, wps=1526.7, ups=0.34, wpb=4469, bsz=120, num_updates=474, lr=5.688e-06, gnorm=1.963, train_wall=6, gb_free=9.5, wall=3752
2024-09-04 08:11:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 30 @ 630 updates, score 10.511) (writing took 72.4555843938142 seconds)
2024-09-04 08:11:27 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-09-04 08:11:27 | INFO | train | epoch 030 | loss 9.88 | nll_loss 7.775 | ppl 219.03 | wps 656.8 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 630 | lr 7.56e-06 | gnorm 1.936 | train_wall 56 | gb_free 13.3 | wall 4244
2024-09-04 08:11:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:11:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:11:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:11:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000649
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16778.66796875Mb; avail=238251.27734375Mb
2024-09-04 08:11:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000070
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000636
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16778.66796875Mb; avail=238251.27734375Mb
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000039
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16778.66796875Mb; avail=238251.27734375Mb
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000188
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001237
2024-09-04 08:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16778.66796875Mb; avail=238251.27734375Mb
2024-09-04 08:11:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:11:27 | INFO | fairseq.trainer | begin training epoch 31
2024-09-04 08:11:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:11:28 | INFO | train_inner | epoch 030:     12 / 16 loss=12.518, nll_loss=11.178, ppl=2316.27, wps=1474.9, ups=0.32, wpb=4544, bsz=132, num_updates=476, lr=5.712e-06, gnorm=1.971, train_wall=6, gb_free=10.3, wall=3758
2024-09-04 08:11:32 | INFO | train_inner | epoch 031:      2 / 21 loss=9.77, nll_loss=7.622, ppl=197.02, wps=97.3, ups=0.02, wpb=4627.5, bsz=92, num_updates=632, lr=7.584e-06, gnorm=1.532, train_wall=5, gb_free=14.1, wall=4249
2024-09-04 08:11:33 | INFO | train_inner | epoch 030:     14 / 16 loss=12.563, nll_loss=11.25, ppl=2435.48, wps=1459.4, ups=0.42, wpb=3449.5, bsz=77, num_updates=478, lr=5.736e-06, gnorm=2.307, train_wall=5, gb_free=9.5, wall=3763
2024-09-04 08:11:38 | INFO | train_inner | epoch 031:      4 / 21 loss=9.869, nll_loss=7.74, ppl=213.71, wps=1579.2, ups=0.35, wpb=4564, bsz=76, num_updates=634, lr=7.608e-06, gnorm=1.882, train_wall=6, gb_free=13.6, wall=4255
2024-09-04 08:11:39 | INFO | train_inner | epoch 030:     16 / 16 loss=12.382, nll_loss=11.018, ppl=2074.3, wps=1505.5, ups=0.34, wpb=4385.5, bsz=136, num_updates=480, lr=5.76e-06, gnorm=1.717, train_wall=6, gb_free=13.8, wall=3768
2024-09-04 08:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16811.44921875Mb; avail=238218.3125Mb
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16811.44921875Mb; avail=238218.3125Mb
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012682
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16811.44921875Mb; avail=238218.3125Mb
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011231
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025018
2024-09-04 08:11:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16811.44921875Mb; avail=238218.3125Mb
2024-09-04 08:11:44 | INFO | train_inner | epoch 031:      6 / 21 loss=9.386, nll_loss=7.14, ppl=141.01, wps=1606.2, ups=0.36, wpb=4456.5, bsz=148, num_updates=636, lr=7.632e-06, gnorm=1.92, train_wall=6, gb_free=13.9, wall=4261
2024-09-04 08:11:48 | INFO | train_inner | epoch 031:      8 / 21 loss=9.782, nll_loss=7.624, ppl=197.26, wps=1775.4, ups=0.49, wpb=3646.5, bsz=85, num_updates=638, lr=7.656e-06, gnorm=1.86, train_wall=4, gb_free=17.7, wall=4265
2024-09-04 08:11:53 | INFO | train_inner | epoch 031:     10 / 21 loss=9.711, nll_loss=7.562, ppl=188.93, wps=1797, ups=0.37, wpb=4852, bsz=124, num_updates=640, lr=7.68e-06, gnorm=1.917, train_wall=5, gb_free=13.7, wall=4270
2024-09-04 08:11:53 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 12.754 | nll_loss 11.371 | ppl 2648.54 | wps 3846.2 | wpb 2070.5 | bsz 122.7 | num_updates 480 | best_loss 12.754
2024-09-04 08:11:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 480 updates
2024-09-04 08:11:53 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:11:58 | INFO | train_inner | epoch 031:     12 / 21 loss=9.947, nll_loss=7.891, ppl=237.33, wps=1886.9, ups=0.38, wpb=4943, bsz=76, num_updates=642, lr=7.704e-06, gnorm=1.988, train_wall=5, gb_free=12.4, wall=4275
2024-09-04 08:12:03 | INFO | train_inner | epoch 031:     14 / 21 loss=9.771, nll_loss=7.617, ppl=196.3, wps=1915, ups=0.46, wpb=4163.5, bsz=92, num_updates=644, lr=7.728e-06, gnorm=2.283, train_wall=4, gb_free=16.4, wall=4280
2024-09-04 08:12:09 | INFO | train_inner | epoch 031:     16 / 21 loss=9.791, nll_loss=7.661, ppl=202.4, wps=1630.6, ups=0.34, wpb=4767.5, bsz=108, num_updates=646, lr=7.752e-06, gnorm=1.651, train_wall=6, gb_free=13.1, wall=4285
2024-09-04 08:12:13 | INFO | train_inner | epoch 031:     18 / 21 loss=9.825, nll_loss=7.675, ppl=204.32, wps=1985.3, ups=0.41, wpb=4810, bsz=84, num_updates=648, lr=7.776e-06, gnorm=1.831, train_wall=5, gb_free=13.1, wall=4290
2024-09-04 08:12:19 | INFO | train_inner | epoch 031:     20 / 21 loss=9.763, nll_loss=7.598, ppl=193.71, wps=1657.4, ups=0.35, wpb=4685.5, bsz=84, num_updates=650, lr=7.8e-06, gnorm=1.857, train_wall=6, gb_free=13, wall=4296
2024-09-04 08:12:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33946.32421875Mb; avail=221084.01171875Mb
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33946.32421875Mb; avail=221083.51953125Mb
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012736
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33946.32421875Mb; avail=221083.51953125Mb
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011386
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025222
2024-09-04 08:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33945.83203125Mb; avail=221084.01171875Mb
2024-09-04 08:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:12:39 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.313 | nll_loss 8.223 | ppl 298.86 | wps 4587.1 | wpb 2350.9 | bsz 94.7 | num_updates 651 | best_loss 10.313
2024-09-04 08:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 651 updates
2024-09-04 08:12:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:12:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 30 @ 480 updates, score 12.754) (writing took 61.88459835574031 seconds)
2024-09-04 08:12:55 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-09-04 08:12:55 | INFO | train | epoch 030 | loss 12.428 | nll_loss 11.064 | ppl 2141.47 | wps 558.3 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 480 | lr 5.76e-06 | gnorm 2.035 | train_wall 45 | gb_free 13.8 | wall 3845
2024-09-04 08:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000726
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39397.0625Mb; avail=215632.828125Mb
2024-09-04 08:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000062
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000620
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39397.0625Mb; avail=215632.828125Mb
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39397.0625Mb; avail=215632.828125Mb
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000184
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001108
2024-09-04 08:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39397.0625Mb; avail=215632.828125Mb
2024-09-04 08:12:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:12:55 | INFO | fairseq.trainer | begin training epoch 31
2024-09-04 08:12:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:13:01 | INFO | train_inner | epoch 031:      2 / 16 loss=12.432, nll_loss=11.066, ppl=2144.01, wps=114.4, ups=0.02, wpb=4694.5, bsz=140, num_updates=482, lr=5.784e-06, gnorm=1.618, train_wall=6, gb_free=10.4, wall=3850
2024-09-04 08:13:07 | INFO | train_inner | epoch 031:      4 / 16 loss=11.879, nll_loss=10.323, ppl=1281.29, wps=1406.3, ups=0.34, wpb=4155, bsz=180, num_updates=484, lr=5.808e-06, gnorm=2.779, train_wall=6, gb_free=11.5, wall=3856
2024-09-04 08:13:13 | INFO | train_inner | epoch 031:      6 / 16 loss=12.554, nll_loss=11.245, ppl=2426.62, wps=1556.3, ups=0.34, wpb=4608.5, bsz=96, num_updates=486, lr=5.832e-06, gnorm=1.836, train_wall=6, gb_free=10.1, wall=3862
2024-09-04 08:13:18 | INFO | train_inner | epoch 031:      8 / 16 loss=12.222, nll_loss=10.813, ppl=1798.98, wps=1521.2, ups=0.36, wpb=4247, bsz=148, num_updates=488, lr=5.856e-06, gnorm=1.703, train_wall=6, gb_free=10.1, wall=3868
2024-09-04 08:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:13:24 | INFO | train_inner | epoch 031:     10 / 16 loss=12.174, nll_loss=10.748, ppl=1719.69, wps=1442.5, ups=0.34, wpb=4266.5, bsz=140, num_updates=490, lr=5.88e-06, gnorm=1.992, train_wall=6, gb_free=8.2, wall=3874
2024-09-04 08:13:39 | INFO | train_inner | epoch 031:     12 / 16 loss=12.296, nll_loss=10.873, ppl=1875.84, wps=496.9, ups=0.13, wpb=3716.5, bsz=105, num_updates=492, lr=5.904e-06, gnorm=2.346, train_wall=15, gb_free=10.9, wall=3889
2024-09-04 08:13:45 | INFO | train_inner | epoch 031:     14 / 16 loss=12.43, nll_loss=11.073, ppl=2154.92, wps=1491.2, ups=0.37, wpb=4033.5, bsz=96, num_updates=494, lr=5.928e-06, gnorm=1.726, train_wall=5, gb_free=13.8, wall=3894
2024-09-04 08:13:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 31 @ 651 updates, score 10.313) (writing took 66.1411754200235 seconds)
2024-09-04 08:13:45 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-09-04 08:13:45 | INFO | train | epoch 031 | loss 9.76 | nll_loss 7.611 | ppl 195.45 | wps 691.3 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 651 | lr 7.812e-06 | gnorm 1.855 | train_wall 54 | gb_free 13.4 | wall 4382
2024-09-04 08:13:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:13:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:13:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:13:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000742
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19657.35546875Mb; avail=235372.58984375Mb
2024-09-04 08:13:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000598
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19657.35546875Mb; avail=235372.58984375Mb
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19657.35546875Mb; avail=235372.58984375Mb
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000186
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001098
2024-09-04 08:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19657.35546875Mb; avail=235372.58984375Mb
2024-09-04 08:13:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:13:45 | INFO | fairseq.trainer | begin training epoch 32
2024-09-04 08:13:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:13:48 | INFO | train_inner | epoch 032:      1 / 21 loss=9.766, nll_loss=7.608, ppl=195.11, wps=98.2, ups=0.02, wpb=4385.5, bsz=80, num_updates=652, lr=7.824e-06, gnorm=1.61, train_wall=5, gb_free=12.7, wall=4385
2024-09-04 08:13:50 | INFO | train_inner | epoch 031:     16 / 16 loss=12.362, nll_loss=10.986, ppl=2028.01, wps=1462.6, ups=0.35, wpb=4170, bsz=112, num_updates=496, lr=5.952e-06, gnorm=1.751, train_wall=6, gb_free=8.2, wall=3900
2024-09-04 08:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19689.546875Mb; avail=235340.35546875Mb
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000627
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19689.546875Mb; avail=235340.35546875Mb
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012499
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19689.546875Mb; avail=235340.35546875Mb
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011209
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024696
2024-09-04 08:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19689.546875Mb; avail=235340.35546875Mb
2024-09-04 08:13:54 | INFO | train_inner | epoch 032:      3 / 21 loss=9.603, nll_loss=7.408, ppl=169.84, wps=1785.1, ups=0.38, wpb=4683, bsz=128, num_updates=654, lr=7.848e-06, gnorm=1.719, train_wall=5, gb_free=13, wall=4391
2024-09-04 08:13:59 | INFO | train_inner | epoch 032:      5 / 21 loss=9.666, nll_loss=7.491, ppl=179.84, wps=1842.4, ups=0.38, wpb=4811, bsz=84, num_updates=656, lr=7.872e-06, gnorm=1.612, train_wall=5, gb_free=11.4, wall=4396
2024-09-04 08:14:05 | INFO | train_inner | epoch 032:      7 / 21 loss=9.147, nll_loss=6.778, ppl=109.76, wps=1540.3, ups=0.34, wpb=4466.5, bsz=172, num_updates=658, lr=7.896e-06, gnorm=2.377, train_wall=6, gb_free=11.9, wall=4402
2024-09-04 08:14:05 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 12.594 | nll_loss 11.202 | ppl 2355.24 | wps 3855.9 | wpb 2070.5 | bsz 122.7 | num_updates 496 | best_loss 12.594
2024-09-04 08:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 496 updates
2024-09-04 08:14:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:14:10 | INFO | train_inner | epoch 032:      9 / 21 loss=9.812, nll_loss=7.669, ppl=203.52, wps=1574.4, ups=0.34, wpb=4611.5, bsz=76, num_updates=660, lr=7.92e-06, gnorm=1.809, train_wall=6, gb_free=12.4, wall=4407
2024-09-04 08:14:15 | INFO | train_inner | epoch 032:     11 / 21 loss=9.554, nll_loss=7.338, ppl=161.74, wps=1963.1, ups=0.41, wpb=4818, bsz=116, num_updates=662, lr=7.944e-06, gnorm=1.765, train_wall=5, gb_free=13, wall=4412
2024-09-04 08:14:20 | INFO | train_inner | epoch 032:     13 / 21 loss=9.627, nll_loss=7.442, ppl=173.84, wps=2046.4, ups=0.42, wpb=4866.5, bsz=92, num_updates=664, lr=7.968e-06, gnorm=1.91, train_wall=5, gb_free=12.1, wall=4417
2024-09-04 08:14:26 | INFO | train_inner | epoch 032:     15 / 21 loss=9.768, nll_loss=7.611, ppl=195.52, wps=1629.9, ups=0.34, wpb=4785.5, bsz=72, num_updates=666, lr=7.992e-06, gnorm=1.723, train_wall=6, gb_free=12.3, wall=4423
2024-09-04 08:14:32 | INFO | train_inner | epoch 032:     17 / 21 loss=9.646, nll_loss=7.457, ppl=175.74, wps=1561.9, ups=0.34, wpb=4556, bsz=104, num_updates=668, lr=8.016e-06, gnorm=1.897, train_wall=6, gb_free=12.3, wall=4429
2024-09-04 08:14:37 | INFO | train_inner | epoch 032:     19 / 21 loss=9.763, nll_loss=7.615, ppl=196.06, wps=1888.6, ups=0.43, wpb=4401.5, bsz=60, num_updates=670, lr=8.04e-06, gnorm=1.913, train_wall=5, gb_free=13.3, wall=4433
2024-09-04 08:14:42 | INFO | train_inner | epoch 032:     21 / 21 loss=9.42, nll_loss=7.159, ppl=142.88, wps=1482.7, ups=0.39, wpb=3781, bsz=81, num_updates=672, lr=8.064e-06, gnorm=2.055, train_wall=5, gb_free=12.3, wall=4439
2024-09-04 08:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36821.01953125Mb; avail=218207.8984375Mb
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000744
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36827.41796875Mb; avail=218202.484375Mb
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012834
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36823.48046875Mb; avail=218205.9296875Mb
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011319
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025325
2024-09-04 08:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36819.54296875Mb; avail=218210.359375Mb
2024-09-04 08:14:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:14:59 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.266 | nll_loss 8.127 | ppl 279.58 | wps 4692.4 | wpb 2350.9 | bsz 94.7 | num_updates 672 | best_loss 10.266
2024-09-04 08:14:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 672 updates
2024-09-04 08:14:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 31 @ 496 updates, score 12.594) (writing took 61.99855816271156 seconds)
2024-09-04 08:15:07 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-09-04 08:15:07 | INFO | train | epoch 031 | loss 12.298 | nll_loss 10.897 | ppl 1907.43 | wps 515.1 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 496 | lr 5.952e-06 | gnorm 1.969 | train_wall 55 | gb_free 8.2 | wall 3976
2024-09-04 08:15:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:15:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:15:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:15:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000695
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27945.1796875Mb; avail=227084.23046875Mb
2024-09-04 08:15:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000062
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000604
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27946.65625Mb; avail=227083.24609375Mb
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27947.1484375Mb; avail=227082.75390625Mb
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000165
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001122
2024-09-04 08:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27947.640625Mb; avail=227082.26171875Mb
2024-09-04 08:15:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:15:07 | INFO | fairseq.trainer | begin training epoch 32
2024-09-04 08:15:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:15:12 | INFO | train_inner | epoch 032:      2 / 16 loss=11.977, nll_loss=10.491, ppl=1439.35, wps=100.7, ups=0.02, wpb=4130.5, bsz=140, num_updates=498, lr=5.976e-06, gnorm=2.404, train_wall=6, gb_free=9.4, wall=3982
2024-09-04 08:15:18 | INFO | train_inner | epoch 032:      4 / 16 loss=12.353, nll_loss=10.972, ppl=2009.15, wps=1565.6, ups=0.34, wpb=4612, bsz=104, num_updates=500, lr=6e-06, gnorm=1.73, train_wall=6, gb_free=10.4, wall=3988
2024-09-04 08:15:24 | INFO | train_inner | epoch 032:      6 / 16 loss=11.998, nll_loss=10.49, ppl=1438, wps=1495.2, ups=0.37, wpb=4030, bsz=144, num_updates=502, lr=6.024e-06, gnorm=1.777, train_wall=5, gb_free=9.7, wall=3993
2024-09-04 08:15:30 | INFO | train_inner | epoch 032:      8 / 16 loss=12.17, nll_loss=10.717, ppl=1683.44, wps=1523.2, ups=0.33, wpb=4575.5, bsz=148, num_updates=504, lr=6.048e-06, gnorm=1.574, train_wall=6, gb_free=10, wall=3999
2024-09-04 08:15:36 | INFO | train_inner | epoch 032:     10 / 16 loss=12.216, nll_loss=10.802, ppl=1784.97, wps=1423.5, ups=0.33, wpb=4265, bsz=120, num_updates=506, lr=6.072e-06, gnorm=1.926, train_wall=6, gb_free=9.3, wall=4005
2024-09-04 08:15:41 | INFO | train_inner | epoch 032:     12 / 16 loss=12.485, nll_loss=11.161, ppl=2289.62, wps=1502.7, ups=0.37, wpb=4045.5, bsz=76, num_updates=508, lr=6.096e-06, gnorm=1.73, train_wall=5, gb_free=14.3, wall=4011
2024-09-04 08:15:47 | INFO | train_inner | epoch 032:     14 / 16 loss=11.967, nll_loss=10.464, ppl=1412.74, wps=1553.8, ups=0.33, wpb=4657, bsz=168, num_updates=510, lr=6.12e-06, gnorm=1.891, train_wall=6, gb_free=11, wall=4017
2024-09-04 08:15:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:15:52 | INFO | train_inner | epoch 032:     16 / 16 loss=12.024, nll_loss=10.545, ppl=1494.03, wps=1415.2, ups=0.4, wpb=3576, bsz=117, num_updates=512, lr=6.144e-06, gnorm=1.745, train_wall=5, gb_free=10.1, wall=4022
2024-09-04 08:15:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21351.4609375Mb; avail=233677.49609375Mb
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000769
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21346.046875Mb; avail=233683.40234375Mb
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012630
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21352.4453125Mb; avail=233677.49609375Mb
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011128
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024910
2024-09-04 08:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21348.015625Mb; avail=233681.43359375Mb
2024-09-04 08:16:07 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 12.499 | nll_loss 11.06 | ppl 2135.43 | wps 3836.3 | wpb 2070.5 | bsz 122.7 | num_updates 512 | best_loss 12.499
2024-09-04 08:16:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 512 updates
2024-09-04 08:16:07 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:16:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 32 @ 672 updates, score 10.266) (writing took 75.78636085335165 seconds)
2024-09-04 08:16:15 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-09-04 08:16:15 | INFO | train | epoch 032 | loss 9.616 | nll_loss 7.417 | ppl 170.92 | wps 640.2 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 672 | lr 8.064e-06 | gnorm 1.87 | train_wall 56 | gb_free 12.3 | wall 4532
2024-09-04 08:16:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:16:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:16:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:16:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000641
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27785.42578125Mb; avail=227244.51953125Mb
2024-09-04 08:16:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000602
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27786.90234375Mb; avail=227243.04296875Mb
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27787.39453125Mb; avail=227242.55078125Mb
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000179
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001096
2024-09-04 08:16:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27787.88671875Mb; avail=227242.05859375Mb
2024-09-04 08:16:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:16:15 | INFO | fairseq.trainer | begin training epoch 33
2024-09-04 08:16:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:16:21 | INFO | train_inner | epoch 033:      2 / 21 loss=9.559, nll_loss=7.341, ppl=162.16, wps=98.3, ups=0.02, wpb=4871, bsz=96, num_updates=674, lr=8.088e-06, gnorm=2.228, train_wall=6, gb_free=12.8, wall=4538
2024-09-04 08:16:25 | INFO | train_inner | epoch 033:      4 / 21 loss=9.601, nll_loss=7.373, ppl=165.78, wps=1917.5, ups=0.45, wpb=4254, bsz=84, num_updates=676, lr=8.112e-06, gnorm=2.327, train_wall=4, gb_free=17, wall=4542
2024-09-04 08:16:30 | INFO | train_inner | epoch 033:      6 / 21 loss=9.868, nll_loss=7.746, ppl=214.69, wps=1869.4, ups=0.45, wpb=4156.5, bsz=36, num_updates=678, lr=8.136e-06, gnorm=1.917, train_wall=4, gb_free=12, wall=4547
2024-09-04 08:16:35 | INFO | train_inner | epoch 033:      8 / 21 loss=9.403, nll_loss=7.155, ppl=142.49, wps=1626.4, ups=0.36, wpb=4528, bsz=136, num_updates=680, lr=8.16e-06, gnorm=1.819, train_wall=6, gb_free=13.4, wall=4552
2024-09-04 08:16:41 | INFO | train_inner | epoch 033:     10 / 21 loss=9.355, nll_loss=7.076, ppl=134.91, wps=1617.7, ups=0.35, wpb=4681, bsz=132, num_updates=682, lr=8.184e-06, gnorm=2.205, train_wall=6, gb_free=12.9, wall=4558
2024-09-04 08:16:46 | INFO | train_inner | epoch 033:     12 / 21 loss=9.497, nll_loss=7.25, ppl=152.18, wps=2012.6, ups=0.41, wpb=4965.5, bsz=112, num_updates=684, lr=8.208e-06, gnorm=2.093, train_wall=5, gb_free=13, wall=4563
2024-09-04 08:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:16:52 | INFO | train_inner | epoch 033:     14 / 21 loss=9.731, nll_loss=7.551, ppl=187.59, wps=1627.7, ups=0.35, wpb=4669.5, bsz=72, num_updates=686, lr=8.232e-06, gnorm=2.31, train_wall=6, gb_free=14.8, wall=4569
2024-09-04 08:16:57 | INFO | train_inner | epoch 033:     16 / 21 loss=9.502, nll_loss=7.27, ppl=154.32, wps=2021.6, ups=0.41, wpb=4899.5, bsz=92, num_updates=688, lr=8.256e-06, gnorm=2.092, train_wall=5, gb_free=11.2, wall=4573
2024-09-04 08:17:02 | INFO | train_inner | epoch 033:     18 / 21 loss=9.056, nll_loss=6.687, ppl=103.02, wps=1486.5, ups=0.34, wpb=4355.5, bsz=136, num_updates=690, lr=8.28e-06, gnorm=1.923, train_wall=6, gb_free=12.3, wall=4579
2024-09-04 08:17:07 | INFO | train_inner | epoch 033:     20 / 21 loss=9.518, nll_loss=7.255, ppl=152.76, wps=2002.7, ups=0.42, wpb=4779.5, bsz=104, num_updates=692, lr=8.304e-06, gnorm=2.414, train_wall=5, gb_free=13.1, wall=4584
2024-09-04 08:17:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23298.4375Mb; avail=231731.46484375Mb
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23298.4375Mb; avail=231731.46484375Mb
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012939
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23298.4375Mb; avail=231731.46484375Mb
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011213
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025250
2024-09-04 08:17:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23298.4375Mb; avail=231731.46484375Mb
2024-09-04 08:17:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 32 @ 512 updates, score 12.499) (writing took 66.31332699768245 seconds)
2024-09-04 08:17:13 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-09-04 08:17:13 | INFO | train | epoch 032 | loss 12.151 | nll_loss 10.708 | ppl 1672.76 | wps 537.3 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 512 | lr 6.144e-06 | gnorm 1.847 | train_wall 45 | gb_free 10.1 | wall 4102
2024-09-04 08:17:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:17:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:17:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:17:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000748
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23325.28125Mb; avail=231704.578125Mb
2024-09-04 08:17:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000609
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23325.28125Mb; avail=231704.578125Mb
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23325.28125Mb; avail=231704.578125Mb
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000181
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001092
2024-09-04 08:17:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23325.28125Mb; avail=231704.578125Mb
2024-09-04 08:17:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:17:13 | INFO | fairseq.trainer | begin training epoch 33
2024-09-04 08:17:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:17:19 | INFO | train_inner | epoch 033:      2 / 16 loss=12.047, nll_loss=10.577, ppl=1527.37, wps=101.8, ups=0.02, wpb=4413, bsz=136, num_updates=514, lr=6.168e-06, gnorm=1.565, train_wall=6, gb_free=8.1, wall=4108
2024-09-04 08:17:25 | INFO | train_inner | epoch 033:      4 / 16 loss=11.653, nll_loss=10.052, ppl=1061.95, wps=1546.3, ups=0.34, wpb=4559, bsz=204, num_updates=516, lr=6.192e-06, gnorm=1.778, train_wall=6, gb_free=10.9, wall=4114
2024-09-04 08:17:27 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.138 | nll_loss 7.963 | ppl 249.45 | wps 4509.8 | wpb 2350.9 | bsz 94.7 | num_updates 693 | best_loss 10.138
2024-09-04 08:17:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 693 updates
2024-09-04 08:17:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:17:41 | INFO | train_inner | epoch 033:      6 / 16 loss=12.191, nll_loss=10.773, ppl=1750.32, wps=576.7, ups=0.13, wpb=4544, bsz=120, num_updates=518, lr=6.216e-06, gnorm=1.449, train_wall=16, gb_free=11, wall=4130
2024-09-04 08:17:46 | INFO | train_inner | epoch 033:      8 / 16 loss=12.241, nll_loss=10.833, ppl=1823.66, wps=1419.7, ups=0.35, wpb=4001.5, bsz=84, num_updates=520, lr=6.24e-06, gnorm=1.805, train_wall=6, gb_free=13.4, wall=4136
2024-09-04 08:17:51 | INFO | train_inner | epoch 033:     10 / 16 loss=12.098, nll_loss=10.643, ppl=1598.99, wps=1447.9, ups=0.39, wpb=3678.5, bsz=93, num_updates=522, lr=6.264e-06, gnorm=2.056, train_wall=5, gb_free=10.1, wall=4141
2024-09-04 08:17:56 | INFO | train_inner | epoch 033:     12 / 16 loss=11.609, nll_loss=10.021, ppl=1038.86, wps=1452.7, ups=0.39, wpb=3771, bsz=140, num_updates=524, lr=6.288e-06, gnorm=1.762, train_wall=5, gb_free=16.5, wall=4146
2024-09-04 08:18:03 | INFO | train_inner | epoch 033:     14 / 16 loss=12.125, nll_loss=10.674, ppl=1633.72, wps=1518, ups=0.32, wpb=4672.5, bsz=120, num_updates=526, lr=6.312e-06, gnorm=1.832, train_wall=6, gb_free=8.8, wall=4152
2024-09-04 08:18:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:18:08 | INFO | train_inner | epoch 033:     16 / 16 loss=12.051, nll_loss=10.582, ppl=1533.2, wps=1482.2, ups=0.35, wpb=4252, bsz=120, num_updates=528, lr=6.336e-06, gnorm=1.464, train_wall=6, gb_free=10.2, wall=4158
2024-09-04 08:18:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16017.67578125Mb; avail=239011.77734375Mb
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000638
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16010.78515625Mb; avail=239019.16015625Mb
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012514
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16017.18359375Mb; avail=239012.26953125Mb
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011051
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024566
2024-09-04 08:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16014.72265625Mb; avail=239014.73046875Mb
2024-09-04 08:18:23 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 12.388 | nll_loss 10.922 | ppl 1940.07 | wps 3845.4 | wpb 2070.5 | bsz 122.7 | num_updates 528 | best_loss 12.388
2024-09-04 08:18:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 528 updates
2024-09-04 08:18:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 33 @ 693 updates, score 10.138) (writing took 63.757908564060926 seconds)
2024-09-04 08:18:31 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-09-04 08:18:31 | INFO | train | epoch 033 | loss 9.5 | nll_loss 7.259 | ppl 153.19 | wps 701.6 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 693 | lr 8.316e-06 | gnorm 2.123 | train_wall 54 | gb_free 17.3 | wall 4668
2024-09-04 08:18:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:18:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:18:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:18:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000771
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33181.390625Mb; avail=221848.51171875Mb
2024-09-04 08:18:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000062
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000615
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33181.390625Mb; avail=221848.51171875Mb
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33181.390625Mb; avail=221848.51171875Mb
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000178
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001114
2024-09-04 08:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33181.390625Mb; avail=221848.51171875Mb
2024-09-04 08:18:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:18:31 | INFO | fairseq.trainer | begin training epoch 34
2024-09-04 08:18:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:18:34 | INFO | train_inner | epoch 034:      1 / 21 loss=9.427, nll_loss=7.168, ppl=143.82, wps=89.8, ups=0.02, wpb=3904.5, bsz=65, num_updates=694, lr=8.328e-06, gnorm=1.997, train_wall=5, gb_free=11.6, wall=4671
2024-09-04 08:18:38 | INFO | train_inner | epoch 034:      3 / 21 loss=9.53, nll_loss=7.277, ppl=155.06, wps=1978.6, ups=0.47, wpb=4230.5, bsz=69, num_updates=696, lr=8.352e-06, gnorm=2.863, train_wall=4, gb_free=13, wall=4675
2024-09-04 08:18:43 | INFO | train_inner | epoch 034:      5 / 21 loss=9.398, nll_loss=7.109, ppl=138.06, wps=1871.5, ups=0.43, wpb=4314, bsz=88, num_updates=698, lr=8.376e-06, gnorm=2.505, train_wall=5, gb_free=13.2, wall=4680
2024-09-04 08:18:48 | INFO | train_inner | epoch 034:      7 / 21 loss=9.393, nll_loss=7.13, ppl=140.09, wps=1807.7, ups=0.38, wpb=4746.5, bsz=104, num_updates=700, lr=8.4e-06, gnorm=1.941, train_wall=5, gb_free=13.6, wall=4685
2024-09-04 08:18:53 | INFO | train_inner | epoch 034:      9 / 21 loss=9.657, nll_loss=7.474, ppl=177.82, wps=1711.4, ups=0.4, wpb=4236, bsz=76, num_updates=702, lr=8.424e-06, gnorm=2.182, train_wall=5, gb_free=12.5, wall=4690
2024-09-04 08:18:59 | INFO | train_inner | epoch 034:     11 / 21 loss=9.381, nll_loss=7.095, ppl=136.74, wps=1583.3, ups=0.36, wpb=4454, bsz=104, num_updates=704, lr=8.448e-06, gnorm=2.035, train_wall=6, gb_free=11.1, wall=4696
2024-09-04 08:19:04 | INFO | train_inner | epoch 034:     13 / 21 loss=9.508, nll_loss=7.269, ppl=154.21, wps=1981.4, ups=0.41, wpb=4830, bsz=76, num_updates=706, lr=8.472e-06, gnorm=1.544, train_wall=5, gb_free=13.2, wall=4701
2024-09-04 08:19:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:19:09 | INFO | train_inner | epoch 034:     15 / 21 loss=9.397, nll_loss=7.107, ppl=137.82, wps=1848.9, ups=0.37, wpb=4991.5, bsz=80, num_updates=708, lr=8.496e-06, gnorm=1.905, train_wall=5, gb_free=12.7, wall=4706
2024-09-04 08:19:14 | INFO | train_inner | epoch 034:     17 / 21 loss=9.019, nll_loss=6.634, ppl=99.32, wps=1683.7, ups=0.38, wpb=4379.5, bsz=144, num_updates=710, lr=8.52e-06, gnorm=1.741, train_wall=5, gb_free=13, wall=4711
2024-09-04 08:19:20 | INFO | train_inner | epoch 034:     19 / 21 loss=9.294, nll_loss=6.958, ppl=124.36, wps=1673.4, ups=0.35, wpb=4771.5, bsz=124, num_updates=712, lr=8.544e-06, gnorm=2.068, train_wall=6, gb_free=12.7, wall=4717
2024-09-04 08:19:25 | INFO | train_inner | epoch 034:     21 / 21 loss=9.325, nll_loss=7.022, ppl=129.97, wps=1852, ups=0.4, wpb=4664, bsz=104, num_updates=714, lr=8.568e-06, gnorm=1.917, train_wall=5, gb_free=14.7, wall=4722
2024-09-04 08:19:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21399.4296875Mb; avail=233630.46875Mb
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.921875Mb; avail=233629.9765625Mb
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012686
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.921875Mb; avail=233629.9765625Mb
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011239
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025021
2024-09-04 08:19:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21399.921875Mb; avail=233629.9765625Mb
2024-09-04 08:19:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 33 @ 528 updates, score 12.388) (writing took 65.9047040110454 seconds)
2024-09-04 08:19:29 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-09-04 08:19:29 | INFO | train | epoch 033 | loss 12.004 | nll_loss 10.522 | ppl 1470.65 | wps 499.5 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 528 | lr 6.336e-06 | gnorm 1.714 | train_wall 55 | gb_free 10.2 | wall 4238
2024-09-04 08:19:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:19:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:19:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:19:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000676
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21432.1328125Mb; avail=233597.8046875Mb
2024-09-04 08:19:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000620
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21432.1328125Mb; avail=233597.8046875Mb
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21432.1328125Mb; avail=233597.8046875Mb
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000177
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001102
2024-09-04 08:19:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21432.1328125Mb; avail=233597.8046875Mb
2024-09-04 08:19:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:19:29 | INFO | fairseq.trainer | begin training epoch 34
2024-09-04 08:19:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:19:34 | INFO | train_inner | epoch 034:      2 / 16 loss=11.913, nll_loss=10.402, ppl=1352.87, wps=102, ups=0.02, wpb=4388, bsz=144, num_updates=530, lr=6.36e-06, gnorm=1.768, train_wall=6, gb_free=9.2, wall=4244
2024-09-04 08:19:40 | INFO | train_inner | epoch 034:      4 / 16 loss=12.051, nll_loss=10.58, ppl=1531.22, wps=1491.5, ups=0.33, wpb=4464, bsz=116, num_updates=532, lr=6.384e-06, gnorm=1.674, train_wall=6, gb_free=8.3, wall=4250
2024-09-04 08:19:44 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.039 | nll_loss 7.848 | ppl 230.33 | wps 4379.8 | wpb 2350.9 | bsz 94.7 | num_updates 714 | best_loss 10.039
2024-09-04 08:19:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 714 updates
2024-09-04 08:19:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:19:46 | INFO | train_inner | epoch 034:      6 / 16 loss=11.981, nll_loss=10.499, ppl=1446.78, wps=1526.4, ups=0.39, wpb=3939, bsz=108, num_updates=534, lr=6.408e-06, gnorm=1.637, train_wall=5, gb_free=11.1, wall=4255
2024-09-04 08:19:50 | INFO | train_inner | epoch 034:      8 / 16 loss=12.006, nll_loss=10.53, ppl=1478.13, wps=1482.3, ups=0.41, wpb=3641.5, bsz=77, num_updates=536, lr=6.432e-06, gnorm=1.772, train_wall=5, gb_free=10.4, wall=4260
2024-09-04 08:19:56 | INFO | train_inner | epoch 034:     10 / 16 loss=12.085, nll_loss=10.63, ppl=1585.24, wps=1565.2, ups=0.37, wpb=4222.5, bsz=100, num_updates=538, lr=6.456e-06, gnorm=2.066, train_wall=5, gb_free=14.8, wall=4265
2024-09-04 08:20:02 | INFO | train_inner | epoch 034:     12 / 16 loss=11.882, nll_loss=10.372, ppl=1325.55, wps=1482.5, ups=0.33, wpb=4451.5, bsz=140, num_updates=540, lr=6.48e-06, gnorm=1.369, train_wall=6, gb_free=11.5, wall=4271
2024-09-04 08:20:08 | INFO | train_inner | epoch 034:     14 / 16 loss=11.952, nll_loss=10.449, ppl=1397.8, wps=1478.4, ups=0.33, wpb=4464, bsz=112, num_updates=542, lr=6.504e-06, gnorm=2.229, train_wall=6, gb_free=10.6, wall=4277
2024-09-04 08:20:14 | INFO | train_inner | epoch 034:     16 / 16 loss=11.136, nll_loss=9.383, ppl=667.7, wps=1439.7, ups=0.33, wpb=4321, bsz=220, num_updates=544, lr=6.528e-06, gnorm=2.1, train_wall=6, gb_free=10.2, wall=4283
2024-09-04 08:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33429.43359375Mb; avail=221600.5Mb
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000747
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33429.43359375Mb; avail=221600.5Mb
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012816
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33429.43359375Mb; avail=221600.5Mb
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011244
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025166
2024-09-04 08:20:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33429.43359375Mb; avail=221600.5Mb
2024-09-04 08:20:28 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 12.274 | nll_loss 10.789 | ppl 1769.29 | wps 3839.7 | wpb 2070.5 | bsz 122.7 | num_updates 544 | best_loss 12.274
2024-09-04 08:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 544 updates
2024-09-04 08:20:28 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:20:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:20:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 34 @ 714 updates, score 10.039) (writing took 74.11895085405558 seconds)
2024-09-04 08:20:58 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-09-04 08:20:58 | INFO | train | epoch 034 | loss 9.395 | nll_loss 7.115 | ppl 138.57 | wps 652.8 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 714 | lr 8.568e-06 | gnorm 2.07 | train_wall 54 | gb_free 14.7 | wall 4815
2024-09-04 08:20:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:20:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:20:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:20:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000771
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37278.48046875Mb; avail=217751.41796875Mb
2024-09-04 08:20:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000065
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000605
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37278.48046875Mb; avail=217751.41796875Mb
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37278.97265625Mb; avail=217750.92578125Mb
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000174
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001119
2024-09-04 08:20:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37278.97265625Mb; avail=217750.92578125Mb
2024-09-04 08:20:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:20:58 | INFO | fairseq.trainer | begin training epoch 35
2024-09-04 08:20:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:21:04 | INFO | train_inner | epoch 035:      2 / 21 loss=9.484, nll_loss=7.235, ppl=150.65, wps=94.7, ups=0.02, wpb=4663.5, bsz=96, num_updates=716, lr=8.592e-06, gnorm=1.982, train_wall=6, gb_free=15.1, wall=4820
2024-09-04 08:21:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:21:09 | INFO | train_inner | epoch 035:      4 / 21 loss=9.502, nll_loss=7.246, ppl=151.75, wps=1637.6, ups=0.35, wpb=4648.5, bsz=80, num_updates=718, lr=8.616e-06, gnorm=1.719, train_wall=6, gb_free=13.5, wall=4826
2024-09-04 08:21:15 | INFO | train_inner | epoch 035:      6 / 21 loss=8.86, nll_loss=6.407, ppl=84.86, wps=1527.3, ups=0.35, wpb=4324, bsz=160, num_updates=720, lr=8.64e-06, gnorm=1.914, train_wall=6, gb_free=12, wall=4832
2024-09-04 08:21:21 | INFO | train_inner | epoch 035:      8 / 21 loss=9.366, nll_loss=7.044, ppl=131.93, wps=1694.1, ups=0.34, wpb=5043.5, bsz=88, num_updates=722, lr=8.664e-06, gnorm=2.085, train_wall=6, gb_free=13.5, wall=4838
2024-09-04 08:21:26 | INFO | train_inner | epoch 035:     10 / 21 loss=9.427, nll_loss=7.158, ppl=142.86, wps=1977, ups=0.42, wpb=4745.5, bsz=68, num_updates=724, lr=8.688e-06, gnorm=2.057, train_wall=5, gb_free=13.6, wall=4843
2024-09-04 08:21:31 | INFO | train_inner | epoch 035:     12 / 21 loss=9.418, nll_loss=7.135, ppl=140.54, wps=1613.2, ups=0.35, wpb=4572.5, bsz=84, num_updates=726, lr=8.712e-06, gnorm=1.682, train_wall=6, gb_free=16.9, wall=4848
2024-09-04 08:21:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 34 @ 544 updates, score 12.274) (writing took 63.21211246214807 seconds)
2024-09-04 08:21:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-09-04 08:21:32 | INFO | train | epoch 034 | loss 11.873 | nll_loss 10.351 | ppl 1306.3 | wps 551.4 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 544 | lr 6.528e-06 | gnorm 1.827 | train_wall 45 | gb_free 10.2 | wall 4361
2024-09-04 08:21:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:21:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:21:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:21:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000716
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25492.4921875Mb; avail=229537.2578125Mb
2024-09-04 08:21:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000606
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25492.4921875Mb; avail=229537.2578125Mb
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25492.4921875Mb; avail=229537.2578125Mb
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000187
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001093
2024-09-04 08:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25492.4921875Mb; avail=229537.2578125Mb
2024-09-04 08:21:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:21:32 | INFO | fairseq.trainer | begin training epoch 35
2024-09-04 08:21:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:21:36 | INFO | train_inner | epoch 035:     14 / 21 loss=9.243, nll_loss=6.904, ppl=119.75, wps=1906.3, ups=0.43, wpb=4458.5, bsz=108, num_updates=728, lr=8.736e-06, gnorm=2.094, train_wall=5, gb_free=11.5, wall=4853
2024-09-04 08:21:37 | INFO | train_inner | epoch 035:      2 / 16 loss=11.811, nll_loss=10.278, ppl=1242, wps=107.8, ups=0.02, wpb=4504.5, bsz=148, num_updates=546, lr=6.552e-06, gnorm=1.753, train_wall=6, gb_free=8.9, wall=4367
2024-09-04 08:21:40 | INFO | train_inner | epoch 035:     16 / 21 loss=9.151, nll_loss=6.779, ppl=109.81, wps=1839.4, ups=0.48, wpb=3848, bsz=77, num_updates=730, lr=8.76e-06, gnorm=1.747, train_wall=4, gb_free=13.2, wall=4857
2024-09-04 08:21:43 | INFO | train_inner | epoch 035:      4 / 16 loss=11.899, nll_loss=10.383, ppl=1335.08, wps=1567.9, ups=0.35, wpb=4506.5, bsz=128, num_updates=548, lr=6.576e-06, gnorm=2.38, train_wall=6, gb_free=9.9, wall=4373
2024-09-04 08:21:46 | INFO | train_inner | epoch 035:     18 / 21 loss=9.135, nll_loss=6.777, ppl=109.67, wps=1589, ups=0.34, wpb=4629.5, bsz=124, num_updates=732, lr=8.784e-06, gnorm=1.893, train_wall=6, gb_free=11.4, wall=4863
2024-09-04 08:21:49 | INFO | train_inner | epoch 035:      6 / 16 loss=11.623, nll_loss=10.028, ppl=1044.36, wps=1513.2, ups=0.34, wpb=4514.5, bsz=164, num_updates=550, lr=6.6e-06, gnorm=1.754, train_wall=6, gb_free=10.1, wall=4379
2024-09-04 08:21:51 | INFO | train_inner | epoch 035:     20 / 21 loss=9.339, nll_loss=7.022, ppl=130, wps=1724.1, ups=0.38, wpb=4522.5, bsz=76, num_updates=734, lr=8.808e-06, gnorm=2.031, train_wall=5, gb_free=13.3, wall=4868
2024-09-04 08:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25541.0546875Mb; avail=229488.30859375Mb
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000748
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25541.546875Mb; avail=229488.30859375Mb
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012686
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25541.546875Mb; avail=229488.30859375Mb
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011210
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025017
2024-09-04 08:21:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25541.546875Mb; avail=229488.30859375Mb
2024-09-04 08:22:04 | INFO | train_inner | epoch 035:      8 / 16 loss=11.951, nll_loss=10.463, ppl=1411.65, wps=475, ups=0.13, wpb=3567, bsz=69, num_updates=552, lr=6.624e-06, gnorm=2.182, train_wall=15, gb_free=9.3, wall=4394
2024-09-04 08:22:10 | INFO | train_inner | epoch 035:     10 / 16 loss=11.073, nll_loss=9.291, ppl=626.63, wps=1409.8, ups=0.34, wpb=4204.5, bsz=208, num_updates=554, lr=6.648e-06, gnorm=3.011, train_wall=6, gb_free=8, wall=4400
2024-09-04 08:22:10 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.933 | nll_loss 7.701 | ppl 208.07 | wps 4955.6 | wpb 2350.9 | bsz 94.7 | num_updates 735 | best_loss 9.933
2024-09-04 08:22:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 735 updates
2024-09-04 08:22:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:22:16 | INFO | train_inner | epoch 035:     12 / 16 loss=11.87, nll_loss=10.361, ppl=1314.86, wps=1438.5, ups=0.37, wpb=3920.5, bsz=84, num_updates=556, lr=6.672e-06, gnorm=2.135, train_wall=5, gb_free=14.2, wall=4405
2024-09-04 08:22:21 | INFO | train_inner | epoch 035:     14 / 16 loss=11.865, nll_loss=10.354, ppl=1308.58, wps=1538, ups=0.37, wpb=4113.5, bsz=108, num_updates=558, lr=6.696e-06, gnorm=2.26, train_wall=5, gb_free=9.4, wall=4410
2024-09-04 08:22:27 | INFO | train_inner | epoch 035:     16 / 16 loss=11.927, nll_loss=10.41, ppl=1360.81, wps=1538.1, ups=0.34, wpb=4560.5, bsz=108, num_updates=560, lr=6.72e-06, gnorm=2.563, train_wall=6, gb_free=9.8, wall=4416
2024-09-04 08:22:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35318.63671875Mb; avail=219711.2734375Mb
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000615
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35318.14453125Mb; avail=219711.765625Mb
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012743
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35318.14453125Mb; avail=219711.765625Mb
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011164
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024897
2024-09-04 08:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35318.14453125Mb; avail=219711.765625Mb
2024-09-04 08:22:41 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 12.196 | nll_loss 10.659 | ppl 1616.91 | wps 3846.8 | wpb 2070.5 | bsz 122.7 | num_updates 560 | best_loss 12.196
2024-09-04 08:22:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 560 updates
2024-09-04 08:22:41 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:22:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 35 @ 735 updates, score 9.933) (writing took 61.63726754579693 seconds)
2024-09-04 08:23:12 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-09-04 08:23:12 | INFO | train | epoch 035 | loss 9.299 | nll_loss 6.977 | ppl 126.01 | wps 713.4 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 735 | lr 8.82e-06 | gnorm 1.928 | train_wall 56 | gb_free 12.3 | wall 4949
2024-09-04 08:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000720
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35332.109375Mb; avail=219698.2890625Mb
2024-09-04 08:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000614
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35332.109375Mb; avail=219698.2890625Mb
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35331.6171875Mb; avail=219698.2890625Mb
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001096
2024-09-04 08:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35332.109375Mb; avail=219697.796875Mb
2024-09-04 08:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:23:12 | INFO | fairseq.trainer | begin training epoch 36
2024-09-04 08:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:23:15 | INFO | train_inner | epoch 036:      1 / 21 loss=9.364, nll_loss=7.064, ppl=133.82, wps=117.3, ups=0.02, wpb=4891.5, bsz=100, num_updates=736, lr=8.832e-06, gnorm=2.16, train_wall=5, gb_free=11.8, wall=4952
2024-09-04 08:23:20 | INFO | train_inner | epoch 036:      3 / 21 loss=9.353, nll_loss=7.043, ppl=131.89, wps=1735, ups=0.38, wpb=4591.5, bsz=80, num_updates=738, lr=8.856e-06, gnorm=2.082, train_wall=5, gb_free=12.2, wall=4957
2024-09-04 08:23:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:23:25 | INFO | train_inner | epoch 036:      5 / 21 loss=9.357, nll_loss=7.049, ppl=132.42, wps=1915.5, ups=0.4, wpb=4849, bsz=92, num_updates=740, lr=8.88e-06, gnorm=2.248, train_wall=5, gb_free=13.4, wall=4962
2024-09-04 08:23:29 | INFO | train_inner | epoch 036:      7 / 21 loss=9.24, nll_loss=6.895, ppl=119.04, wps=1743.2, ups=0.49, wpb=3531, bsz=61, num_updates=742, lr=8.904e-06, gnorm=2.259, train_wall=4, gb_free=13.3, wall=4966
2024-09-04 08:23:35 | INFO | train_inner | epoch 036:      9 / 21 loss=9.039, nll_loss=6.638, ppl=99.62, wps=1690, ups=0.34, wpb=4985, bsz=128, num_updates=744, lr=8.928e-06, gnorm=2.042, train_wall=6, gb_free=11.4, wall=4972
2024-09-04 08:23:40 | INFO | train_inner | epoch 036:     11 / 21 loss=9.317, nll_loss=6.997, ppl=127.75, wps=1769.6, ups=0.41, wpb=4361, bsz=92, num_updates=746, lr=8.952e-06, gnorm=1.892, train_wall=5, gb_free=11.9, wall=4977
2024-09-04 08:23:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 35 @ 560 updates, score 12.196) (writing took 63.998404840007424 seconds)
2024-09-04 08:23:45 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-09-04 08:23:45 | INFO | train | epoch 035 | loss 11.75 | nll_loss 10.192 | ppl 1170.09 | wps 506.9 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 560 | lr 6.72e-06 | gnorm 2.255 | train_wall 55 | gb_free 9.8 | wall 4495
2024-09-04 08:23:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:23:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:23:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:23:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000692
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21300.41015625Mb; avail=233729.484375Mb
2024-09-04 08:23:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000634
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21300.41015625Mb; avail=233729.484375Mb
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21300.41015625Mb; avail=233729.484375Mb
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000165
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001102
2024-09-04 08:23:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21300.41015625Mb; avail=233729.484375Mb
2024-09-04 08:23:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:23:45 | INFO | fairseq.trainer | begin training epoch 36
2024-09-04 08:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:23:46 | INFO | train_inner | epoch 036:     13 / 21 loss=8.851, nll_loss=6.382, ppl=83.42, wps=1640.2, ups=0.35, wpb=4626, bsz=120, num_updates=748, lr=8.976e-06, gnorm=1.855, train_wall=6, gb_free=11.3, wall=4982
2024-09-04 08:23:50 | INFO | train_inner | epoch 036:      2 / 16 loss=11.74, nll_loss=10.176, ppl=1156.92, wps=82.5, ups=0.02, wpb=3429, bsz=89, num_updates=562, lr=6.744e-06, gnorm=2.531, train_wall=5, gb_free=12, wall=4500
2024-09-04 08:23:50 | INFO | train_inner | epoch 036:     15 / 21 loss=9.275, nll_loss=6.934, ppl=122.28, wps=1962.7, ups=0.43, wpb=4555, bsz=84, num_updates=750, lr=9e-06, gnorm=1.56, train_wall=5, gb_free=13.8, wall=4987
2024-09-04 08:23:55 | INFO | train_inner | epoch 036:      4 / 16 loss=11.879, nll_loss=10.38, ppl=1332.29, wps=1515.5, ups=0.37, wpb=4150.5, bsz=88, num_updates=564, lr=6.768e-06, gnorm=1.629, train_wall=5, gb_free=10.9, wall=4505
2024-09-04 08:23:56 | INFO | train_inner | epoch 036:     17 / 21 loss=8.967, nll_loss=6.517, ppl=91.58, wps=1525.2, ups=0.32, wpb=4733, bsz=152, num_updates=752, lr=9.024e-06, gnorm=1.57, train_wall=6, gb_free=12.4, wall=4993
2024-09-04 08:24:01 | INFO | train_inner | epoch 036:      6 / 16 loss=11.3, nll_loss=9.616, ppl=784.47, wps=1509, ups=0.34, wpb=4427.5, bsz=168, num_updates=566, lr=6.792e-06, gnorm=2.35, train_wall=6, gb_free=11.2, wall=4511
2024-09-04 08:24:02 | INFO | train_inner | epoch 036:     19 / 21 loss=9.331, nll_loss=7.028, ppl=130.49, wps=1767.7, ups=0.36, wpb=4926.5, bsz=96, num_updates=754, lr=9.048e-06, gnorm=1.754, train_wall=6, gb_free=13.1, wall=4999
2024-09-04 08:24:07 | INFO | train_inner | epoch 036:     21 / 21 loss=9.213, nll_loss=6.846, ppl=115.05, wps=1539.6, ups=0.37, wpb=4176.5, bsz=68, num_updates=756, lr=9.072e-06, gnorm=1.949, train_wall=5, gb_free=12.3, wall=5004
2024-09-04 08:24:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21339.95703125Mb; avail=233689.40625Mb
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000739
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21340.44921875Mb; avail=233689.40625Mb
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012803
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21340.4453125Mb; avail=233689.40625Mb
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011204
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025132
2024-09-04 08:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21340.4453125Mb; avail=233689.40625Mb
2024-09-04 08:24:07 | INFO | train_inner | epoch 036:      8 / 16 loss=11.751, nll_loss=10.181, ppl=1160.62, wps=1518.8, ups=0.33, wpb=4631, bsz=124, num_updates=568, lr=6.816e-06, gnorm=1.953, train_wall=6, gb_free=9.5, wall=4517
2024-09-04 08:24:12 | INFO | train_inner | epoch 036:     10 / 16 loss=11.918, nll_loss=10.409, ppl=1359.19, wps=1576, ups=0.41, wpb=3802.5, bsz=64, num_updates=570, lr=6.84e-06, gnorm=2.636, train_wall=5, gb_free=11.6, wall=4522
2024-09-04 08:24:18 | INFO | train_inner | epoch 036:     12 / 16 loss=11.401, nll_loss=9.741, ppl=855.45, wps=1543.3, ups=0.33, wpb=4633.5, bsz=192, num_updates=572, lr=6.864e-06, gnorm=2.116, train_wall=6, gb_free=9.9, wall=4528
2024-09-04 08:24:24 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.997 | nll_loss 7.713 | ppl 209.8 | wps 4985.3 | wpb 2350.9 | bsz 94.7 | num_updates 756 | best_loss 9.933
2024-09-04 08:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 756 updates
2024-09-04 08:24:24 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:24:24 | INFO | train_inner | epoch 036:     14 / 16 loss=11.768, nll_loss=10.218, ppl=1191.17, wps=1480.2, ups=0.34, wpb=4296.5, bsz=112, num_updates=574, lr=6.888e-06, gnorm=1.923, train_wall=6, gb_free=12.6, wall=4534
2024-09-04 08:24:30 | INFO | train_inner | epoch 036:     16 / 16 loss=11.387, nll_loss=9.709, ppl=836.71, wps=1525.7, ups=0.34, wpb=4521, bsz=180, num_updates=576, lr=6.912e-06, gnorm=2.232, train_wall=6, gb_free=10.6, wall=4540
2024-09-04 08:24:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24297.16015625Mb; avail=230732.203125Mb
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24299.12890625Mb; avail=230730.7265625Mb
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012721
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24328.66015625Mb; avail=230701.1953125Mb
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011100
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024926
2024-09-04 08:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24353.76171875Mb; avail=230676.09375Mb
2024-09-04 08:24:44 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 12.15 | nll_loss 10.592 | ppl 1543.66 | wps 3839.4 | wpb 2070.5 | bsz 122.7 | num_updates 576 | best_loss 12.15
2024-09-04 08:24:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 576 updates
2024-09-04 08:24:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:25:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:25:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 36 @ 756 updates, score 9.997) (writing took 48.21486274432391 seconds)
2024-09-04 08:25:12 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-09-04 08:25:12 | INFO | train | epoch 036 | loss 9.205 | nll_loss 6.848 | ppl 115.24 | wps 796.8 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 756 | lr 9.072e-06 | gnorm 1.936 | train_wall 55 | gb_free 12.3 | wall 5069
2024-09-04 08:25:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:25:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:25:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:25:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000569
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31126.0234375Mb; avail=223904.00390625Mb
2024-09-04 08:25:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000556
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31125.53125Mb; avail=223904.49609375Mb
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31125.53125Mb; avail=223904.49609375Mb
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000156
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001007
2024-09-04 08:25:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31125.53125Mb; avail=223904.49609375Mb
2024-09-04 08:25:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:25:12 | INFO | fairseq.trainer | begin training epoch 37
2024-09-04 08:25:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:25:17 | INFO | train_inner | epoch 037:      2 / 21 loss=9.104, nll_loss=6.711, ppl=104.73, wps=126.2, ups=0.03, wpb=4420.5, bsz=88, num_updates=758, lr=9.096e-06, gnorm=1.838, train_wall=5, gb_free=13.5, wall=5074
2024-09-04 08:25:22 | INFO | train_inner | epoch 037:      4 / 21 loss=9.051, nll_loss=6.633, ppl=99.26, wps=2022.5, ups=0.41, wpb=4920, bsz=136, num_updates=760, lr=9.12e-06, gnorm=1.541, train_wall=5, gb_free=13, wall=5079
2024-09-04 08:25:27 | INFO | train_inner | epoch 037:      6 / 21 loss=9.15, nll_loss=6.776, ppl=109.58, wps=2016.6, ups=0.41, wpb=4968, bsz=92, num_updates=762, lr=9.144e-06, gnorm=2.014, train_wall=5, gb_free=12, wall=5084
2024-09-04 08:25:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:25:33 | INFO | train_inner | epoch 037:      8 / 21 loss=9.181, nll_loss=6.81, ppl=112.18, wps=1565.3, ups=0.35, wpb=4531, bsz=80, num_updates=764, lr=9.168e-06, gnorm=1.613, train_wall=6, gb_free=12.9, wall=5090
2024-09-04 08:25:38 | INFO | train_inner | epoch 037:     10 / 21 loss=8.753, nll_loss=6.259, ppl=76.59, wps=1432.1, ups=0.39, wpb=3639.5, bsz=101, num_updates=766, lr=9.192e-06, gnorm=1.956, train_wall=5, gb_free=12.5, wall=5095
2024-09-04 08:25:43 | INFO | train_inner | epoch 037:     12 / 21 loss=9.003, nll_loss=6.553, ppl=93.87, wps=1978, ups=0.41, wpb=4781, bsz=112, num_updates=768, lr=9.216e-06, gnorm=2.233, train_wall=5, gb_free=12.2, wall=5100
2024-09-04 08:25:47 | INFO | train_inner | epoch 037:     14 / 21 loss=9.464, nll_loss=7.154, ppl=142.39, wps=2004.1, ups=0.44, wpb=4568.5, bsz=56, num_updates=770, lr=9.24e-06, gnorm=2.471, train_wall=5, gb_free=16.8, wall=5104
2024-09-04 08:25:53 | INFO | train_inner | epoch 037:     16 / 21 loss=9.133, nll_loss=6.737, ppl=106.63, wps=1578.5, ups=0.34, wpb=4607.5, bsz=104, num_updates=772, lr=9.264e-06, gnorm=2.733, train_wall=6, gb_free=12.9, wall=5110
2024-09-04 08:25:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 36 @ 576 updates, score 12.15) (writing took 72.33376194536686 seconds)
2024-09-04 08:25:57 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-09-04 08:25:57 | INFO | train | epoch 036 | loss 11.631 | nll_loss 10.038 | ppl 1051.28 | wps 515.3 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 576 | lr 6.912e-06 | gnorm 2.171 | train_wall 45 | gb_free 10.6 | wall 4626
2024-09-04 08:25:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:25:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:25:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:25:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000600
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19160.828125Mb; avail=235869.16015625Mb
2024-09-04 08:25:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000576
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19160.828125Mb; avail=235869.16015625Mb
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000041
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19160.828125Mb; avail=235869.16015625Mb
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000156
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001058
2024-09-04 08:25:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19160.828125Mb; avail=235869.16015625Mb
2024-09-04 08:25:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:25:57 | INFO | fairseq.trainer | begin training epoch 37
2024-09-04 08:25:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:25:58 | INFO | train_inner | epoch 037:     18 / 21 loss=9.052, nll_loss=6.65, ppl=100.4, wps=1924.4, ups=0.41, wpb=4661.5, bsz=120, num_updates=774, lr=9.288e-06, gnorm=1.948, train_wall=5, gb_free=14, wall=5115
2024-09-04 08:26:02 | INFO | train_inner | epoch 037:      2 / 16 loss=11.59, nll_loss=10.004, ppl=1027.17, wps=86.6, ups=0.02, wpb=3987.5, bsz=132, num_updates=578, lr=6.936e-06, gnorm=2.548, train_wall=5, gb_free=14.1, wall=4632
2024-09-04 08:26:04 | INFO | train_inner | epoch 037:     20 / 21 loss=9.22, nll_loss=6.847, ppl=115.13, wps=1601.2, ups=0.35, wpb=4598, bsz=100, num_updates=776, lr=9.312e-06, gnorm=2.317, train_wall=6, gb_free=13.8, wall=5121
2024-09-04 08:26:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19200.62109375Mb; avail=235829.32421875Mb
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19200.62109375Mb; avail=235829.32421875Mb
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012785
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19200.62109375Mb; avail=235829.32421875Mb
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011158
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025050
2024-09-04 08:26:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19201.11328125Mb; avail=235828.83203125Mb
2024-09-04 08:26:08 | INFO | train_inner | epoch 037:      4 / 16 loss=11.171, nll_loss=9.417, ppl=683.63, wps=1493.4, ups=0.34, wpb=4425.5, bsz=180, num_updates=580, lr=6.96e-06, gnorm=2.257, train_wall=6, gb_free=9.4, wall=4638
2024-09-04 08:26:13 | INFO | train_inner | epoch 037:      6 / 16 loss=11.681, nll_loss=10.101, ppl=1098.3, wps=1545.6, ups=0.37, wpb=4134.5, bsz=100, num_updates=582, lr=6.984e-06, gnorm=1.994, train_wall=5, gb_free=13.9, wall=4643
2024-09-04 08:26:18 | INFO | train_inner | epoch 037:      8 / 16 loss=11.693, nll_loss=10.114, ppl=1108.42, wps=1440.5, ups=0.39, wpb=3686.5, bsz=73, num_updates=584, lr=7.008e-06, gnorm=2.342, train_wall=5, gb_free=8.2, wall=4648
2024-09-04 08:26:23 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.831 | nll_loss 7.521 | ppl 183.63 | wps 4885.6 | wpb 2350.9 | bsz 94.7 | num_updates 777 | best_loss 9.831
2024-09-04 08:26:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 777 updates
2024-09-04 08:26:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:26:24 | INFO | train_inner | epoch 037:     10 / 16 loss=11.493, nll_loss=9.853, ppl=925.06, wps=1525.4, ups=0.33, wpb=4564.5, bsz=156, num_updates=586, lr=7.032e-06, gnorm=2.467, train_wall=6, gb_free=10, wall=4654
2024-09-04 08:26:30 | INFO | train_inner | epoch 037:     12 / 16 loss=11.284, nll_loss=9.583, ppl=767.2, wps=1500, ups=0.34, wpb=4411.5, bsz=160, num_updates=588, lr=7.056e-06, gnorm=2.229, train_wall=6, gb_free=9.1, wall=4660
2024-09-04 08:26:37 | INFO | train_inner | epoch 037:     14 / 16 loss=11.539, nll_loss=9.915, ppl=965.28, wps=1521.9, ups=0.32, wpb=4685.5, bsz=128, num_updates=590, lr=7.08e-06, gnorm=2.265, train_wall=6, gb_free=10.4, wall=4666
2024-09-04 08:26:42 | INFO | train_inner | epoch 037:     16 / 16 loss=11.689, nll_loss=10.121, ppl=1113.25, wps=1532, ups=0.38, wpb=3996, bsz=88, num_updates=592, lr=7.104e-06, gnorm=2.146, train_wall=5, gb_free=11.4, wall=4671
2024-09-04 08:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36338.578125Mb; avail=218691.28515625Mb
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000748
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36339.0703125Mb; avail=218690.79296875Mb
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012757
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36339.3046875Mb; avail=218690.55859375Mb
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011333
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025316
2024-09-04 08:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36339.3046875Mb; avail=218690.55859375Mb
2024-09-04 08:26:56 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 12.045 | nll_loss 10.48 | ppl 1427.82 | wps 3840.3 | wpb 2070.5 | bsz 122.7 | num_updates 592 | best_loss 12.045
2024-09-04 08:26:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 592 updates
2024-09-04 08:26:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:27:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:27:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 37 @ 777 updates, score 9.831) (writing took 63.504393422976136 seconds)
2024-09-04 08:27:26 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-09-04 08:27:26 | INFO | train | epoch 037 | loss 9.121 | nll_loss 6.724 | ppl 105.71 | wps 712.3 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 777 | lr 9.324e-06 | gnorm 2.071 | train_wall 54 | gb_free 16.3 | wall 5203
2024-09-04 08:27:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:27:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:27:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:27:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000713
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39482.29296875Mb; avail=215547.65234375Mb
2024-09-04 08:27:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000584
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39482.29296875Mb; avail=215547.65234375Mb
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39482.29296875Mb; avail=215547.65234375Mb
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000178
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001085
2024-09-04 08:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39482.78515625Mb; avail=215547.16015625Mb
2024-09-04 08:27:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:27:26 | INFO | fairseq.trainer | begin training epoch 38
2024-09-04 08:27:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:27:29 | INFO | train_inner | epoch 038:      1 / 21 loss=9.038, nll_loss=6.612, ppl=97.79, wps=108.2, ups=0.02, wpb=4624, bsz=104, num_updates=778, lr=9.336e-06, gnorm=1.941, train_wall=5, gb_free=11.7, wall=5206
2024-09-04 08:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:27:35 | INFO | train_inner | epoch 038:      3 / 21 loss=8.941, nll_loss=6.475, ppl=88.96, wps=1639.1, ups=0.35, wpb=4626, bsz=116, num_updates=780, lr=9.36e-06, gnorm=1.916, train_wall=6, gb_free=13.4, wall=5212
2024-09-04 08:27:40 | INFO | train_inner | epoch 038:      5 / 21 loss=9.032, nll_loss=6.63, ppl=99.04, wps=1918.3, ups=0.43, wpb=4424.5, bsz=112, num_updates=782, lr=9.384e-06, gnorm=2.019, train_wall=5, gb_free=12.4, wall=5217
2024-09-04 08:27:45 | INFO | train_inner | epoch 038:      7 / 21 loss=9.26, nll_loss=6.892, ppl=118.76, wps=1536.6, ups=0.36, wpb=4241, bsz=60, num_updates=784, lr=9.408e-06, gnorm=2.037, train_wall=6, gb_free=13.7, wall=5222
2024-09-04 08:27:51 | INFO | train_inner | epoch 038:      9 / 21 loss=9.243, nll_loss=6.871, ppl=117.05, wps=1568, ups=0.36, wpb=4345, bsz=56, num_updates=786, lr=9.432e-06, gnorm=1.654, train_wall=6, gb_free=12.4, wall=5228
2024-09-04 08:27:55 | INFO | train_inner | epoch 038:     11 / 21 loss=8.946, nll_loss=6.492, ppl=90.01, wps=1719.9, ups=0.44, wpb=3912, bsz=85, num_updates=788, lr=9.456e-06, gnorm=1.748, train_wall=5, gb_free=16.6, wall=5232
2024-09-04 08:27:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 37 @ 592 updates, score 12.045) (writing took 63.14722420182079 seconds)
2024-09-04 08:27:59 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-09-04 08:27:59 | INFO | train | epoch 037 | loss 11.509 | nll_loss 9.878 | ppl 940.73 | wps 553.3 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 592 | lr 7.104e-06 | gnorm 2.281 | train_wall 45 | gb_free 11.4 | wall 4749
2024-09-04 08:27:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:27:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:27:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:27:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000762
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24462.0078125Mb; avail=230567.890625Mb
2024-09-04 08:27:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000610
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24462.0078125Mb; avail=230567.890625Mb
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24462.0078125Mb; avail=230567.890625Mb
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000189
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001166
2024-09-04 08:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24462.0078125Mb; avail=230567.890625Mb
2024-09-04 08:27:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:27:59 | INFO | fairseq.trainer | begin training epoch 38
2024-09-04 08:27:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:28:00 | INFO | train_inner | epoch 038:     13 / 21 loss=9.062, nll_loss=6.636, ppl=99.48, wps=1829.7, ups=0.39, wpb=4749.5, bsz=84, num_updates=790, lr=9.48e-06, gnorm=1.748, train_wall=5, gb_free=12.6, wall=5237
2024-09-04 08:28:05 | INFO | train_inner | epoch 038:      2 / 16 loss=11.563, nll_loss=9.955, ppl=992.55, wps=108.9, ups=0.02, wpb=4538.5, bsz=104, num_updates=594, lr=7.128e-06, gnorm=2.042, train_wall=6, gb_free=8.1, wall=4755
2024-09-04 08:28:06 | INFO | train_inner | epoch 038:     15 / 21 loss=9.121, nll_loss=6.698, ppl=103.85, wps=1600.2, ups=0.36, wpb=4491.5, bsz=88, num_updates=792, lr=9.504e-06, gnorm=1.899, train_wall=6, gb_free=14.5, wall=5243
2024-09-04 08:28:11 | INFO | train_inner | epoch 038:      4 / 16 loss=11.362, nll_loss=9.69, ppl=825.79, wps=1499.9, ups=0.34, wpb=4405, bsz=128, num_updates=596, lr=7.152e-06, gnorm=1.969, train_wall=6, gb_free=10.7, wall=4760
2024-09-04 08:28:12 | INFO | train_inner | epoch 038:     17 / 21 loss=9.046, nll_loss=6.616, ppl=98.09, wps=1677.7, ups=0.33, wpb=5060.5, bsz=100, num_updates=794, lr=9.528e-06, gnorm=1.821, train_wall=6, gb_free=12.1, wall=5249
2024-09-04 08:28:17 | INFO | train_inner | epoch 038:      6 / 16 loss=11.553, nll_loss=9.938, ppl=980.87, wps=1507.9, ups=0.36, wpb=4227, bsz=108, num_updates=598, lr=7.176e-06, gnorm=2.049, train_wall=6, gb_free=8.2, wall=4766
2024-09-04 08:28:18 | INFO | train_inner | epoch 038:     19 / 21 loss=8.723, nll_loss=6.186, ppl=72.78, wps=1498, ups=0.35, wpb=4307.5, bsz=148, num_updates=796, lr=9.552e-06, gnorm=2.001, train_wall=6, gb_free=12.2, wall=5255
2024-09-04 08:28:22 | INFO | train_inner | epoch 038:      8 / 16 loss=11.392, nll_loss=9.71, ppl=837.56, wps=1539.8, ups=0.35, wpb=4427, bsz=148, num_updates=600, lr=7.2e-06, gnorm=2.01, train_wall=6, gb_free=12.5, wall=4772
2024-09-04 08:28:24 | INFO | train_inner | epoch 038:     21 / 21 loss=9.001, nll_loss=6.553, ppl=93.87, wps=1704.3, ups=0.33, wpb=5205, bsz=92, num_updates=798, lr=9.576e-06, gnorm=2.054, train_wall=6, gb_free=13.9, wall=5261
2024-09-04 08:28:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24495.65234375Mb; avail=230534.20703125Mb
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000730
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24495.65234375Mb; avail=230534.20703125Mb
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012692
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24495.65234375Mb; avail=230534.20703125Mb
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011335
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025115
2024-09-04 08:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24495.65234375Mb; avail=230534.20703125Mb
2024-09-04 08:28:28 | INFO | train_inner | epoch 038:     10 / 16 loss=10.818, nll_loss=8.974, ppl=502.86, wps=1384.5, ups=0.33, wpb=4252.5, bsz=192, num_updates=602, lr=7.224e-06, gnorm=1.612, train_wall=6, gb_free=9.2, wall=4778
2024-09-04 08:28:34 | INFO | train_inner | epoch 038:     12 / 16 loss=11.255, nll_loss=9.547, ppl=748.11, wps=1534.9, ups=0.36, wpb=4302, bsz=160, num_updates=604, lr=7.248e-06, gnorm=2.016, train_wall=6, gb_free=14.7, wall=4784
2024-09-04 08:28:39 | INFO | train_inner | epoch 038:     14 / 16 loss=11.551, nll_loss=9.929, ppl=975.15, wps=1436.6, ups=0.42, wpb=3413, bsz=61, num_updates=606, lr=7.272e-06, gnorm=1.941, train_wall=5, gb_free=10.2, wall=4788
2024-09-04 08:28:41 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.796 | nll_loss 7.458 | ppl 175.84 | wps 4710.9 | wpb 2350.9 | bsz 94.7 | num_updates 798 | best_loss 9.796
2024-09-04 08:28:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 798 updates
2024-09-04 08:28:41 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:28:45 | INFO | train_inner | epoch 038:     16 / 16 loss=11.422, nll_loss=9.763, ppl=868.77, wps=1469.4, ups=0.34, wpb=4326.5, bsz=116, num_updates=608, lr=7.296e-06, gnorm=1.605, train_wall=6, gb_free=10.1, wall=4794
2024-09-04 08:28:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24525.87109375Mb; avail=230503.9453125Mb
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000743
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24525.87109375Mb; avail=230503.9453125Mb
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012626
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24525.87109375Mb; avail=230503.9453125Mb
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011180
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024908
2024-09-04 08:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24525.8671875Mb; avail=230503.9453125Mb
2024-09-04 08:28:59 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 11.937 | nll_loss 10.343 | ppl 1298.66 | wps 3844.6 | wpb 2070.5 | bsz 122.7 | num_updates 608 | best_loss 11.937
2024-09-04 08:28:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 608 updates
2024-09-04 08:28:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:29:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 38 @ 798 updates, score 9.796) (writing took 73.80319040175527 seconds)
2024-09-04 08:29:55 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-09-04 08:29:55 | INFO | train | epoch 038 | loss 9.031 | nll_loss 6.597 | ppl 96.81 | wps 643.1 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 798 | lr 9.576e-06 | gnorm 1.881 | train_wall 58 | gb_free 13.9 | wall 5352
2024-09-04 08:29:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:29:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:29:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:29:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000621
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30297.83984375Mb; avail=224731.6015625Mb
2024-09-04 08:29:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000598
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30297.83984375Mb; avail=224732.09375Mb
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30297.83984375Mb; avail=224732.09375Mb
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000166
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001085
2024-09-04 08:29:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30297.83984375Mb; avail=224732.09375Mb
2024-09-04 08:29:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:29:55 | INFO | fairseq.trainer | begin training epoch 39
2024-09-04 08:29:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:30:00 | INFO | train_inner | epoch 039:      2 / 21 loss=9.108, nll_loss=6.687, ppl=103, wps=81.5, ups=0.02, wpb=3926, bsz=61, num_updates=800, lr=9.6e-06, gnorm=2.199, train_wall=5, gb_free=12.2, wall=5357
2024-09-04 08:30:06 | INFO | train_inner | epoch 039:      4 / 21 loss=9.082, nll_loss=6.661, ppl=101.21, wps=1557, ups=0.34, wpb=4558, bsz=80, num_updates=802, lr=9.624e-06, gnorm=1.845, train_wall=6, gb_free=11.6, wall=5363
2024-09-04 08:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 38 @ 608 updates, score 11.937) (writing took 68.18289402872324 seconds)
2024-09-04 08:30:07 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-09-04 08:30:07 | INFO | train | epoch 038 | loss 11.361 | nll_loss 9.684 | ppl 822.81 | wps 529.6 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 608 | lr 7.296e-06 | gnorm 1.905 | train_wall 45 | gb_free 10.1 | wall 4877
2024-09-04 08:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000757
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30348.2421875Mb; avail=224681.65625Mb
2024-09-04 08:30:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000600
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30348.2421875Mb; avail=224681.65625Mb
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30348.2421875Mb; avail=224681.65625Mb
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000189
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001141
2024-09-04 08:30:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30348.2421875Mb; avail=224681.65625Mb
2024-09-04 08:30:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:30:07 | INFO | fairseq.trainer | begin training epoch 39
2024-09-04 08:30:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:30:12 | INFO | train_inner | epoch 039:      6 / 21 loss=8.419, nll_loss=5.804, ppl=55.85, wps=1688.4, ups=0.36, wpb=4656.5, bsz=172, num_updates=804, lr=9.648e-06, gnorm=1.95, train_wall=6, gb_free=12.7, wall=5369
2024-09-04 08:30:13 | INFO | train_inner | epoch 039:      2 / 16 loss=11.351, nll_loss=9.689, ppl=825.34, wps=101.3, ups=0.02, wpb=4472, bsz=120, num_updates=610, lr=7.32e-06, gnorm=1.957, train_wall=6, gb_free=10.9, wall=4883
2024-09-04 08:30:16 | INFO | train_inner | epoch 039:      8 / 21 loss=9.06, nll_loss=6.635, ppl=99.41, wps=1817.3, ups=0.43, wpb=4275, bsz=76, num_updates=806, lr=9.672e-06, gnorm=2.127, train_wall=5, gb_free=16.8, wall=5373
2024-09-04 08:30:22 | INFO | train_inner | epoch 039:     10 / 21 loss=8.978, nll_loss=6.506, ppl=90.88, wps=1861.4, ups=0.37, wpb=5000, bsz=112, num_updates=808, lr=9.696e-06, gnorm=1.972, train_wall=5, gb_free=12.6, wall=5379
2024-09-04 08:30:27 | INFO | train_inner | epoch 039:     12 / 21 loss=8.9, nll_loss=6.428, ppl=86.11, wps=1705.8, ups=0.36, wpb=4756.5, bsz=120, num_updates=810, lr=9.72e-06, gnorm=1.574, train_wall=6, gb_free=11.2, wall=5384
2024-09-04 08:30:33 | INFO | train_inner | epoch 039:     14 / 21 loss=9.064, nll_loss=6.628, ppl=98.9, wps=1757.7, ups=0.36, wpb=4902.5, bsz=96, num_updates=812, lr=9.744e-06, gnorm=1.608, train_wall=6, gb_free=14, wall=5390
2024-09-04 08:30:33 | INFO | train_inner | epoch 039:      4 / 16 loss=11.422, nll_loss=9.762, ppl=868.3, wps=371.8, ups=0.1, wpb=3755.5, bsz=81, num_updates=612, lr=7.344e-06, gnorm=1.913, train_wall=20, gb_free=14.2, wall=4903
2024-09-04 08:30:39 | INFO | train_inner | epoch 039:     16 / 21 loss=8.935, nll_loss=6.462, ppl=88.17, wps=1489.9, ups=0.33, wpb=4536, bsz=120, num_updates=814, lr=9.768e-06, gnorm=1.807, train_wall=6, gb_free=13.8, wall=5396
2024-09-04 08:30:39 | INFO | train_inner | epoch 039:      6 / 16 loss=11.361, nll_loss=9.672, ppl=815.56, wps=1559, ups=0.33, wpb=4709, bsz=140, num_updates=614, lr=7.368e-06, gnorm=2.527, train_wall=6, gb_free=10, wall=4909
2024-09-04 08:30:44 | INFO | train_inner | epoch 039:     18 / 21 loss=9.122, nll_loss=6.69, ppl=103.27, wps=1714.9, ups=0.4, wpb=4293, bsz=48, num_updates=816, lr=9.792e-06, gnorm=1.92, train_wall=5, gb_free=14.7, wall=5401
2024-09-04 08:30:45 | INFO | train_inner | epoch 039:      8 / 16 loss=11.26, nll_loss=9.554, ppl=751.88, wps=1549, ups=0.38, wpb=4124.5, bsz=136, num_updates=616, lr=7.392e-06, gnorm=2.781, train_wall=5, gb_free=13.4, wall=4914
2024-09-04 08:30:49 | INFO | train_inner | epoch 039:     20 / 21 loss=8.956, nll_loss=6.475, ppl=88.98, wps=1931.1, ups=0.4, wpb=4802, bsz=104, num_updates=818, lr=9.816e-06, gnorm=1.596, train_wall=5, gb_free=13, wall=5406
2024-09-04 08:30:50 | INFO | train_inner | epoch 039:     10 / 16 loss=11.653, nll_loss=10.069, ppl=1074.3, wps=1499.2, ups=0.36, wpb=4120.5, bsz=60, num_updates=618, lr=7.416e-06, gnorm=1.909, train_wall=5, gb_free=11.9, wall=4920
2024-09-04 08:30:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19991.55078125Mb; avail=235038.33984375Mb
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000735
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19991.55078125Mb; avail=235038.33984375Mb
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012766
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19991.55078125Mb; avail=235038.33984375Mb
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011136
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024987
2024-09-04 08:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19991.55078125Mb; avail=235038.33984375Mb
2024-09-04 08:30:56 | INFO | train_inner | epoch 039:     12 / 16 loss=11.024, nll_loss=9.241, ppl=605.1, wps=1453.8, ups=0.34, wpb=4310, bsz=176, num_updates=620, lr=7.44e-06, gnorm=2.498, train_wall=6, gb_free=8.9, wall=4926
2024-09-04 08:31:02 | INFO | train_inner | epoch 039:     14 / 16 loss=11.238, nll_loss=9.521, ppl=734.68, wps=1527.2, ups=0.35, wpb=4400, bsz=136, num_updates=622, lr=7.464e-06, gnorm=2.437, train_wall=6, gb_free=11.5, wall=4931
2024-09-04 08:31:08 | INFO | train_inner | epoch 039:     16 / 16 loss=10.756, nll_loss=8.9, ppl=477.83, wps=1388.6, ups=0.35, wpb=4000, bsz=168, num_updates=624, lr=7.488e-06, gnorm=2.459, train_wall=6, gb_free=9.7, wall=4937
2024-09-04 08:31:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20031.21484375Mb; avail=234998.55078125Mb
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000640
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20031.21484375Mb; avail=234998.55078125Mb
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012652
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20031.21484375Mb; avail=234998.55078125Mb
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011182
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024842
2024-09-04 08:31:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20031.21484375Mb; avail=234998.55078125Mb
2024-09-04 08:31:08 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.68 | nll_loss 7.306 | ppl 158.25 | wps 4999.9 | wpb 2350.9 | bsz 94.7 | num_updates 819 | best_loss 9.68
2024-09-04 08:31:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 819 updates
2024-09-04 08:31:08 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:31:22 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 11.839 | nll_loss 10.198 | ppl 1174.77 | wps 3847.3 | wpb 2070.5 | bsz 122.7 | num_updates 624 | best_loss 11.839
2024-09-04 08:31:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 624 updates
2024-09-04 08:31:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:32:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:32:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 39 @ 819 updates, score 9.68) (writing took 59.5965830264613 seconds)
2024-09-04 08:32:08 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-09-04 08:32:08 | INFO | train | epoch 039 | loss 8.961 | nll_loss 6.495 | ppl 90.23 | wps 719.6 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 819 | lr 9.828e-06 | gnorm 1.865 | train_wall 57 | gb_free 15 | wall 5485
2024-09-04 08:32:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:32:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:32:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:32:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000762
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28667.49609375Mb; avail=226361.49609375Mb
2024-09-04 08:32:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000067
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000618
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28673.45703125Mb; avail=226355.04296875Mb
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28674.44140625Mb; avail=226354.05859375Mb
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000193
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001139
2024-09-04 08:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28678.4296875Mb; avail=226350.0703125Mb
2024-09-04 08:32:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:32:08 | INFO | fairseq.trainer | begin training epoch 40
2024-09-04 08:32:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:32:11 | INFO | train_inner | epoch 040:      1 / 21 loss=9.018, nll_loss=6.567, ppl=94.82, wps=100.8, ups=0.02, wpb=4130, bsz=64, num_updates=820, lr=9.84e-06, gnorm=1.784, train_wall=6, gb_free=16.5, wall=5488
2024-09-04 08:32:17 | INFO | train_inner | epoch 040:      3 / 21 loss=8.887, nll_loss=6.393, ppl=84.04, wps=1633.1, ups=0.34, wpb=4863.5, bsz=104, num_updates=822, lr=9.864e-06, gnorm=1.665, train_wall=6, gb_free=12.3, wall=5494
2024-09-04 08:32:22 | INFO | train_inner | epoch 040:      5 / 21 loss=9.005, nll_loss=6.531, ppl=92.48, wps=2078.9, ups=0.41, wpb=5028, bsz=100, num_updates=824, lr=9.888e-06, gnorm=2.22, train_wall=5, gb_free=13.5, wall=5499
2024-09-04 08:32:27 | INFO | train_inner | epoch 040:      7 / 21 loss=8.776, nll_loss=6.254, ppl=76.32, wps=1637.9, ups=0.35, wpb=4661, bsz=124, num_updates=826, lr=9.912e-06, gnorm=1.881, train_wall=6, gb_free=12.9, wall=5504
2024-09-04 08:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 39 @ 624 updates, score 11.839) (writing took 69.12301748525351 seconds)
2024-09-04 08:32:31 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-09-04 08:32:31 | INFO | train | epoch 039 | loss 11.259 | nll_loss 9.553 | ppl 750.98 | wps 471.4 | ups 0.11 | wpb 4236.4 | bsz 127.1 | num_updates 624 | lr 7.488e-06 | gnorm 2.31 | train_wall 60 | gb_free 9.7 | wall 5021
2024-09-04 08:32:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:32:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:32:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:32:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000654
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28714.2890625Mb; avail=226315.55859375Mb
2024-09-04 08:32:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000056
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000620
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28714.2890625Mb; avail=226315.55859375Mb
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28714.2890625Mb; avail=226315.55859375Mb
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000160
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001164
2024-09-04 08:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28714.2890625Mb; avail=226315.55859375Mb
2024-09-04 08:32:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:32:31 | INFO | fairseq.trainer | begin training epoch 40
2024-09-04 08:32:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:32:32 | INFO | train_inner | epoch 040:      9 / 21 loss=8.965, nll_loss=6.495, ppl=90.22, wps=1488.1, ups=0.4, wpb=3744.5, bsz=69, num_updates=828, lr=9.936e-06, gnorm=2.27, train_wall=5, gb_free=13.3, wall=5509
2024-09-04 08:32:37 | INFO | train_inner | epoch 040:      2 / 16 loss=11.409, nll_loss=9.735, ppl=852, wps=94.8, ups=0.02, wpb=4219, bsz=104, num_updates=626, lr=7.512e-06, gnorm=2.294, train_wall=5, gb_free=10.7, wall=5026
2024-09-04 08:32:38 | INFO | train_inner | epoch 040:     11 / 21 loss=8.725, nll_loss=6.189, ppl=72.94, wps=1544, ups=0.33, wpb=4683, bsz=128, num_updates=830, lr=9.96e-06, gnorm=2.414, train_wall=6, gb_free=12, wall=5515
2024-09-04 08:32:42 | INFO | train_inner | epoch 040:      4 / 16 loss=10.962, nll_loss=9.152, ppl=569, wps=1484.6, ups=0.35, wpb=4242, bsz=156, num_updates=628, lr=7.536e-06, gnorm=1.795, train_wall=6, gb_free=8.4, wall=5032
2024-09-04 08:32:44 | INFO | train_inner | epoch 040:     13 / 21 loss=8.935, nll_loss=6.449, ppl=87.38, wps=1613.5, ups=0.33, wpb=4857, bsz=104, num_updates=832, lr=9.984e-06, gnorm=2.592, train_wall=6, gb_free=11.8, wall=5521
2024-09-04 08:32:47 | INFO | train_inner | epoch 040:      6 / 16 loss=11.266, nll_loss=9.57, ppl=760.05, wps=1434.5, ups=0.46, wpb=3129.5, bsz=73, num_updates=630, lr=7.56e-06, gnorm=2.168, train_wall=4, gb_free=10.7, wall=5036
2024-09-04 08:32:50 | INFO | train_inner | epoch 040:     15 / 21 loss=8.697, nll_loss=6.139, ppl=70.45, wps=1446.6, ups=0.38, wpb=3768.5, bsz=88, num_updates=834, lr=1.0008e-05, gnorm=2.353, train_wall=5, gb_free=15, wall=5527
2024-09-04 08:32:53 | INFO | train_inner | epoch 040:      8 / 16 loss=10.967, nll_loss=9.163, ppl=573.38, wps=1538.4, ups=0.34, wpb=4481, bsz=188, num_updates=632, lr=7.584e-06, gnorm=2.392, train_wall=6, gb_free=9.5, wall=5042
2024-09-04 08:32:55 | INFO | train_inner | epoch 040:     17 / 21 loss=8.931, nll_loss=6.44, ppl=86.85, wps=1723.1, ups=0.37, wpb=4601, bsz=96, num_updates=836, lr=1.0032e-05, gnorm=1.756, train_wall=5, gb_free=13.3, wall=5532
2024-09-04 08:32:58 | INFO | train_inner | epoch 040:     10 / 16 loss=10.903, nll_loss=9.068, ppl=536.78, wps=1496, ups=0.35, wpb=4307, bsz=144, num_updates=634, lr=7.608e-06, gnorm=1.945, train_wall=6, gb_free=8.8, wall=5048
2024-09-04 08:33:00 | INFO | train_inner | epoch 040:     19 / 21 loss=9.17, nll_loss=6.734, ppl=106.48, wps=1822.9, ups=0.39, wpb=4700, bsz=68, num_updates=838, lr=1.0056e-05, gnorm=2.179, train_wall=5, gb_free=15, wall=5537
2024-09-04 08:33:04 | INFO | train_inner | epoch 040:     12 / 16 loss=11.276, nll_loss=9.568, ppl=758.99, wps=1536.3, ups=0.34, wpb=4562, bsz=116, num_updates=636, lr=7.632e-06, gnorm=2.163, train_wall=6, gb_free=10.3, wall=5054
2024-09-04 08:33:05 | INFO | train_inner | epoch 040:     21 / 21 loss=8.809, nll_loss=6.283, ppl=77.89, wps=2012.1, ups=0.41, wpb=4940, bsz=100, num_updates=840, lr=1.008e-05, gnorm=1.806, train_wall=5, gb_free=12.7, wall=5542
2024-09-04 08:33:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28756.76171875Mb; avail=226273.04296875Mb
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000745
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28757.25390625Mb; avail=226272.55078125Mb
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.013072
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28757.25390625Mb; avail=226272.55078125Mb
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011302
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025504
2024-09-04 08:33:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28757.25390625Mb; avail=226272.55078125Mb
2024-09-04 08:33:10 | INFO | train_inner | epoch 040:     14 / 16 loss=11.205, nll_loss=9.474, ppl=711.31, wps=1532.7, ups=0.34, wpb=4489, bsz=124, num_updates=638, lr=7.656e-06, gnorm=2.084, train_wall=6, gb_free=10.7, wall=5060
2024-09-04 08:33:16 | INFO | train_inner | epoch 040:     16 / 16 loss=11.193, nll_loss=9.463, ppl=705.57, wps=1501.4, ups=0.34, wpb=4462, bsz=112, num_updates=640, lr=7.68e-06, gnorm=1.926, train_wall=6, gb_free=8.8, wall=5066
2024-09-04 08:33:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28794.59765625Mb; avail=226235.1640625Mb
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000756
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28795.08984375Mb; avail=226234.671875Mb
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012706
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28795.08984375Mb; avail=226234.671875Mb
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011115
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024962
2024-09-04 08:33:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28795.08984375Mb; avail=226234.671875Mb
2024-09-04 08:33:22 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.691 | nll_loss 7.292 | ppl 156.73 | wps 4692.3 | wpb 2350.9 | bsz 94.7 | num_updates 840 | best_loss 9.68
2024-09-04 08:33:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 840 updates
2024-09-04 08:33:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:33:30 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 11.782 | nll_loss 10.116 | ppl 1109.95 | wps 3843.5 | wpb 2070.5 | bsz 122.7 | num_updates 640 | best_loss 11.782
2024-09-04 08:33:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 640 updates
2024-09-04 08:33:30 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:34:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:34:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 40 @ 840 updates, score 9.691) (writing took 51.73910160921514 seconds)
2024-09-04 08:34:14 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-09-04 08:34:14 | INFO | train | epoch 040 | loss 8.898 | nll_loss 6.402 | ppl 84.55 | wps 758 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 840 | lr 1.008e-05 | gnorm 2.089 | train_wall 57 | gb_free 12.7 | wall 5611
2024-09-04 08:34:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:34:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:34:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:34:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000593
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29457.6640625Mb; avail=225572.2265625Mb
2024-09-04 08:34:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000585
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29457.6640625Mb; avail=225572.2265625Mb
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29457.171875Mb; avail=225572.71875Mb
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000162
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001063
2024-09-04 08:34:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29457.6640625Mb; avail=225572.2265625Mb
2024-09-04 08:34:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:34:14 | INFO | fairseq.trainer | begin training epoch 41
2024-09-04 08:34:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:34:20 | INFO | train_inner | epoch 041:      2 / 21 loss=8.688, nll_loss=6.133, ppl=70.19, wps=120.9, ups=0.03, wpb=4529, bsz=124, num_updates=842, lr=1.0104e-05, gnorm=1.647, train_wall=6, gb_free=11.7, wall=5617
2024-09-04 08:34:26 | INFO | train_inner | epoch 041:      4 / 21 loss=8.657, nll_loss=6.074, ppl=67.37, wps=1590.2, ups=0.34, wpb=4665, bsz=124, num_updates=844, lr=1.0128e-05, gnorm=2.141, train_wall=6, gb_free=12.6, wall=5623
2024-09-04 08:34:30 | INFO | train_inner | epoch 041:      6 / 21 loss=9.026, nll_loss=6.563, ppl=94.52, wps=2101.7, ups=0.43, wpb=4837.5, bsz=88, num_updates=846, lr=1.0152e-05, gnorm=1.856, train_wall=5, gb_free=14.5, wall=5627
2024-09-04 08:34:35 | INFO | train_inner | epoch 041:      8 / 21 loss=8.772, nll_loss=6.228, ppl=74.96, wps=1851.6, ups=0.4, wpb=4623, bsz=112, num_updates=848, lr=1.0176e-05, gnorm=1.735, train_wall=5, gb_free=13.5, wall=5632
2024-09-04 08:34:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 40 @ 640 updates, score 11.782) (writing took 65.17329353280365 seconds)
2024-09-04 08:34:36 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-09-04 08:34:36 | INFO | train | epoch 040 | loss 11.144 | nll_loss 9.394 | ppl 672.6 | wps 544.4 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 640 | lr 7.68e-06 | gnorm 2.096 | train_wall 45 | gb_free 8.8 | wall 5145
2024-09-04 08:34:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:34:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:34:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:34:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000723
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29504.671875Mb; avail=225525.328125Mb
2024-09-04 08:34:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000682
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29504.671875Mb; avail=225525.328125Mb
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29504.671875Mb; avail=225525.328125Mb
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001189
2024-09-04 08:34:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29504.671875Mb; avail=225525.328125Mb
2024-09-04 08:34:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:34:36 | INFO | fairseq.trainer | begin training epoch 41
2024-09-04 08:34:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:34:41 | INFO | train_inner | epoch 041:     10 / 21 loss=8.687, nll_loss=6.12, ppl=69.54, wps=1555.8, ups=0.36, wpb=4320.5, bsz=100, num_updates=850, lr=1.02e-05, gnorm=2.149, train_wall=6, gb_free=12.1, wall=5638
2024-09-04 08:34:47 | INFO | train_inner | epoch 041:     12 / 21 loss=8.829, nll_loss=6.292, ppl=78.34, wps=1545, ups=0.35, wpb=4390.5, bsz=96, num_updates=852, lr=1.0224e-05, gnorm=2.122, train_wall=6, gb_free=13, wall=5644
2024-09-04 08:34:51 | INFO | train_inner | epoch 041:     14 / 21 loss=8.843, nll_loss=6.33, ppl=80.42, wps=1873.8, ups=0.49, wpb=3821.5, bsz=53, num_updates=854, lr=1.0248e-05, gnorm=2.588, train_wall=4, gb_free=12, wall=5648
2024-09-04 08:34:51 | INFO | train_inner | epoch 041:      2 / 16 loss=11.031, nll_loss=9.265, ppl=615.41, wps=97.1, ups=0.02, wpb=4627, bsz=160, num_updates=642, lr=7.704e-06, gnorm=1.738, train_wall=16, gb_free=11.2, wall=5161
2024-09-04 08:34:56 | INFO | train_inner | epoch 041:      4 / 16 loss=11.128, nll_loss=9.374, ppl=663.35, wps=1432.8, ups=0.47, wpb=3046.5, bsz=81, num_updates=644, lr=7.728e-06, gnorm=2.515, train_wall=4, gb_free=10.3, wall=5165
2024-09-04 08:34:57 | INFO | train_inner | epoch 041:     16 / 21 loss=8.909, nll_loss=6.41, ppl=85.02, wps=1730.3, ups=0.34, wpb=5105.5, bsz=80, num_updates=856, lr=1.0272e-05, gnorm=2.41, train_wall=6, gb_free=11.1, wall=5654
2024-09-04 08:35:01 | INFO | train_inner | epoch 041:      6 / 16 loss=10.582, nll_loss=8.671, ppl=407.62, wps=1539.2, ups=0.35, wpb=4407, bsz=188, num_updates=646, lr=7.752e-06, gnorm=2.177, train_wall=6, gb_free=10.4, wall=5171
2024-09-04 08:35:01 | INFO | train_inner | epoch 041:     18 / 21 loss=8.899, nll_loss=6.382, ppl=83.38, wps=1921.3, ups=0.42, wpb=4605, bsz=108, num_updates=858, lr=1.0296e-05, gnorm=2.296, train_wall=5, gb_free=12.8, wall=5658
2024-09-04 08:35:07 | INFO | train_inner | epoch 041:     20 / 21 loss=8.837, nll_loss=6.312, ppl=79.46, wps=1956.9, ups=0.39, wpb=4961.5, bsz=112, num_updates=860, lr=1.032e-05, gnorm=1.774, train_wall=5, gb_free=12.7, wall=5663
2024-09-04 08:35:07 | INFO | train_inner | epoch 041:      8 / 16 loss=11.046, nll_loss=9.243, ppl=605.76, wps=1417.7, ups=0.34, wpb=4136, bsz=120, num_updates=648, lr=7.776e-06, gnorm=2.642, train_wall=6, gb_free=9.2, wall=5177
2024-09-04 08:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24902.4140625Mb; avail=230127.51953125Mb
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000730
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24902.4140625Mb; avail=230127.51953125Mb
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012936
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24902.4140625Mb; avail=230127.51953125Mb
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011325
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025390
2024-09-04 08:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24902.4140625Mb; avail=230127.51953125Mb
2024-09-04 08:35:13 | INFO | train_inner | epoch 041:     10 / 16 loss=11.14, nll_loss=9.371, ppl=662.17, wps=1547, ups=0.34, wpb=4603, bsz=128, num_updates=650, lr=7.8e-06, gnorm=2.03, train_wall=6, gb_free=11.8, wall=5183
2024-09-04 08:35:19 | INFO | train_inner | epoch 041:     12 / 16 loss=11.298, nll_loss=9.588, ppl=769.57, wps=1559.5, ups=0.34, wpb=4639, bsz=100, num_updates=652, lr=7.824e-06, gnorm=1.628, train_wall=6, gb_free=8.3, wall=5189
2024-09-04 08:35:24 | INFO | train_inner | epoch 041:     14 / 16 loss=11.21, nll_loss=9.479, ppl=713.41, wps=1540, ups=0.38, wpb=4048.5, bsz=92, num_updates=654, lr=7.848e-06, gnorm=1.556, train_wall=5, gb_free=11.3, wall=5194
2024-09-04 08:35:26 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.649 | nll_loss 7.246 | ppl 151.85 | wps 4696.4 | wpb 2350.9 | bsz 94.7 | num_updates 861 | best_loss 9.649
2024-09-04 08:35:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 861 updates
2024-09-04 08:35:26 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:35:30 | INFO | train_inner | epoch 041:     16 / 16 loss=10.974, nll_loss=9.172, ppl=576.93, wps=1495.6, ups=0.34, wpb=4384.5, bsz=148, num_updates=656, lr=7.872e-06, gnorm=1.581, train_wall=6, gb_free=8.6, wall=5200
2024-09-04 08:35:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24921.3046875Mb; avail=230108.5859375Mb
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24921.3046875Mb; avail=230108.5859375Mb
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012535
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24921.3046875Mb; avail=230108.5859375Mb
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011060
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024604
2024-09-04 08:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24921.3046875Mb; avail=230108.5859375Mb
2024-09-04 08:35:45 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 11.661 | nll_loss 9.955 | ppl 992.89 | wps 3841.5 | wpb 2070.5 | bsz 122.7 | num_updates 656 | best_loss 11.661
2024-09-04 08:35:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 656 updates
2024-09-04 08:35:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:36:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:36:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 41 @ 861 updates, score 9.649) (writing took 61.415614522993565 seconds)
2024-09-04 08:36:27 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-09-04 08:36:27 | INFO | train | epoch 041 | loss 8.835 | nll_loss 6.309 | ppl 79.29 | wps 718 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 861 | lr 1.0332e-05 | gnorm 2.09 | train_wall 54 | gb_free 16.6 | wall 5744
2024-09-04 08:36:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:36:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:36:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:36:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000723
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42919.578125Mb; avail=212109.37890625Mb
2024-09-04 08:36:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000572
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42913.671875Mb; avail=212115.77734375Mb
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42914.65625Mb; avail=212115.28515625Mb
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000171
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001041
2024-09-04 08:36:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42916.62109375Mb; avail=212112.82421875Mb
2024-09-04 08:36:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:36:28 | INFO | fairseq.trainer | begin training epoch 42
2024-09-04 08:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:36:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:36:30 | INFO | train_inner | epoch 042:      1 / 21 loss=9.123, nll_loss=6.667, ppl=101.61, wps=102.6, ups=0.02, wpb=4283.5, bsz=44, num_updates=862, lr=1.0344e-05, gnorm=1.994, train_wall=5, gb_free=12.4, wall=5747
2024-09-04 08:36:35 | INFO | train_inner | epoch 042:      3 / 21 loss=8.785, nll_loss=6.239, ppl=75.54, wps=1557.7, ups=0.4, wpb=3897.5, bsz=85, num_updates=864, lr=1.0368e-05, gnorm=2.82, train_wall=5, gb_free=12.4, wall=5752
2024-09-04 08:36:40 | INFO | train_inner | epoch 042:      5 / 21 loss=8.864, nll_loss=6.333, ppl=80.59, wps=1600.4, ups=0.37, wpb=4343.5, bsz=72, num_updates=866, lr=1.0392e-05, gnorm=2.218, train_wall=5, gb_free=11.7, wall=5757
2024-09-04 08:36:45 | INFO | train_inner | epoch 042:      7 / 21 loss=8.872, nll_loss=6.351, ppl=81.61, wps=2110.9, ups=0.42, wpb=5045, bsz=104, num_updates=868, lr=1.0416e-05, gnorm=2.007, train_wall=5, gb_free=12.5, wall=5762
2024-09-04 08:36:50 | INFO | train_inner | epoch 042:      9 / 21 loss=8.77, nll_loss=6.218, ppl=74.46, wps=1901.2, ups=0.41, wpb=4650, bsz=108, num_updates=870, lr=1.044e-05, gnorm=1.888, train_wall=5, gb_free=12.9, wall=5767
2024-09-04 08:36:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 41 @ 656 updates, score 11.661) (writing took 68.74781440850347 seconds)
2024-09-04 08:36:53 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-09-04 08:36:53 | INFO | train | epoch 041 | loss 11.048 | nll_loss 9.267 | ppl 616.08 | wps 492 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 656 | lr 7.872e-06 | gnorm 1.983 | train_wall 54 | gb_free 8.6 | wall 5283
2024-09-04 08:36:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:36:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:36:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:36:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000680
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28868.0859375Mb; avail=226161.6484375Mb
2024-09-04 08:36:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000575
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28868.0859375Mb; avail=226161.6484375Mb
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28868.0859375Mb; avail=226161.6484375Mb
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000162
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001060
2024-09-04 08:36:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28868.0859375Mb; avail=226161.6484375Mb
2024-09-04 08:36:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:36:53 | INFO | fairseq.trainer | begin training epoch 42
2024-09-04 08:36:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:36:56 | INFO | train_inner | epoch 042:     11 / 21 loss=8.517, nll_loss=5.889, ppl=59.26, wps=1538, ups=0.34, wpb=4531.5, bsz=132, num_updates=872, lr=1.0464e-05, gnorm=2.055, train_wall=6, gb_free=12.5, wall=5773
2024-09-04 08:36:59 | INFO | train_inner | epoch 042:      2 / 16 loss=11.187, nll_loss=9.439, ppl=694.12, wps=102.1, ups=0.02, wpb=4530.5, bsz=92, num_updates=658, lr=7.896e-06, gnorm=1.706, train_wall=6, gb_free=12.2, wall=5288
2024-09-04 08:37:01 | INFO | train_inner | epoch 042:     13 / 21 loss=8.645, nll_loss=6.054, ppl=66.44, wps=1916.9, ups=0.41, wpb=4725.5, bsz=120, num_updates=874, lr=1.0488e-05, gnorm=1.94, train_wall=5, gb_free=13.2, wall=5778
2024-09-04 08:37:05 | INFO | train_inner | epoch 042:      4 / 16 loss=11.196, nll_loss=9.465, ppl=706.58, wps=1532.8, ups=0.36, wpb=4291, bsz=92, num_updates=660, lr=7.92e-06, gnorm=1.749, train_wall=6, gb_free=13.8, wall=5294
2024-09-04 08:37:07 | INFO | train_inner | epoch 042:     15 / 21 loss=8.846, nll_loss=6.307, ppl=79.18, wps=1496.2, ups=0.33, wpb=4541, bsz=96, num_updates=876, lr=1.0512e-05, gnorm=1.888, train_wall=6, gb_free=13.7, wall=5784
2024-09-04 08:37:11 | INFO | train_inner | epoch 042:      6 / 16 loss=10.774, nll_loss=8.896, ppl=476.45, wps=1535.6, ups=0.34, wpb=4563.5, bsz=160, num_updates=662, lr=7.944e-06, gnorm=1.918, train_wall=6, gb_free=10.2, wall=5300
2024-09-04 08:37:12 | INFO | train_inner | epoch 042:     17 / 21 loss=8.957, nll_loss=6.441, ppl=86.87, wps=2006.8, ups=0.42, wpb=4726.5, bsz=60, num_updates=878, lr=1.0536e-05, gnorm=1.9, train_wall=5, gb_free=14.6, wall=5789
2024-09-04 08:37:16 | INFO | train_inner | epoch 042:      8 / 16 loss=11.051, nll_loss=9.27, ppl=617.35, wps=1439.9, ups=0.37, wpb=3912.5, bsz=92, num_updates=664, lr=7.968e-06, gnorm=1.771, train_wall=5, gb_free=8.8, wall=5305
2024-09-04 08:37:17 | INFO | train_inner | epoch 042:     19 / 21 loss=8.622, nll_loss=6.013, ppl=64.58, wps=1912.3, ups=0.41, wpb=4631, bsz=124, num_updates=880, lr=1.056e-05, gnorm=1.682, train_wall=5, gb_free=13, wall=5794
2024-09-04 08:37:21 | INFO | train_inner | epoch 042:     10 / 16 loss=11.205, nll_loss=9.475, ppl=711.84, wps=1409.4, ups=0.4, wpb=3504, bsz=57, num_updates=666, lr=7.992e-06, gnorm=1.889, train_wall=5, gb_free=16.6, wall=5310
2024-09-04 08:37:22 | INFO | train_inner | epoch 042:     21 / 21 loss=8.759, nll_loss=6.192, ppl=73.09, wps=1524.3, ups=0.34, wpb=4448, bsz=92, num_updates=882, lr=1.0584e-05, gnorm=1.8, train_wall=6, gb_free=11.4, wall=5799
2024-09-04 08:37:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28925.03515625Mb; avail=226104.8046875Mb
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000750
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28925.03515625Mb; avail=226104.8046875Mb
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012814
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28925.03515625Mb; avail=226104.8046875Mb
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011258
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025205
2024-09-04 08:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28925.03515625Mb; avail=226104.8046875Mb
2024-09-04 08:37:27 | INFO | train_inner | epoch 042:     12 / 16 loss=10.045, nll_loss=7.939, ppl=245.44, wps=1383.9, ups=0.33, wpb=4191, bsz=240, num_updates=668, lr=8.016e-06, gnorm=2.295, train_wall=6, gb_free=10.4, wall=5316
2024-09-04 08:37:33 | INFO | train_inner | epoch 042:     14 / 16 loss=10.973, nll_loss=9.157, ppl=570.82, wps=1530.8, ups=0.34, wpb=4468.5, bsz=144, num_updates=670, lr=8.04e-06, gnorm=2.166, train_wall=6, gb_free=10, wall=5322
2024-09-04 08:37:39 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.631 | nll_loss 7.198 | ppl 146.84 | wps 4981.1 | wpb 2350.9 | bsz 94.7 | num_updates 882 | best_loss 9.631
2024-09-04 08:37:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 882 updates
2024-09-04 08:37:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:37:39 | INFO | train_inner | epoch 042:     16 / 16 loss=10.859, nll_loss=9.009, ppl=515.14, wps=1458.4, ups=0.33, wpb=4430.5, bsz=140, num_updates=672, lr=8.064e-06, gnorm=2.144, train_wall=6, gb_free=10.1, wall=5328
2024-09-04 08:37:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28954.68359375Mb; avail=226075.13671875Mb
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000735
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28954.68359375Mb; avail=226075.13671875Mb
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012624
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28954.68359375Mb; avail=226075.13671875Mb
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011185
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024924
2024-09-04 08:37:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28954.68359375Mb; avail=226075.13671875Mb
2024-09-04 08:37:53 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 11.607 | nll_loss 9.906 | ppl 959.2 | wps 3834 | wpb 2070.5 | bsz 122.7 | num_updates 672 | best_loss 11.607
2024-09-04 08:37:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 672 updates
2024-09-04 08:37:53 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:38:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:38:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 42 @ 882 updates, score 9.631) (writing took 77.93658603914082 seconds)
2024-09-04 08:38:57 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-09-04 08:38:57 | INFO | train | epoch 042 | loss 8.777 | nll_loss 6.221 | ppl 74.58 | wps 641.1 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 882 | lr 1.0584e-05 | gnorm 1.997 | train_wall 55 | gb_free 11.4 | wall 5894
2024-09-04 08:38:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:38:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:38:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:38:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000593
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18975.15625Mb; avail=236054.77734375Mb
2024-09-04 08:38:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000066
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000599
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18975.15625Mb; avail=236054.77734375Mb
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18975.15625Mb; avail=236054.77734375Mb
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000186
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001172
2024-09-04 08:38:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18975.15625Mb; avail=236054.77734375Mb
2024-09-04 08:38:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:38:57 | INFO | fairseq.trainer | begin training epoch 43
2024-09-04 08:38:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:39:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 42 @ 672 updates, score 11.607) (writing took 66.50099704694003 seconds)
2024-09-04 08:39:00 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-09-04 08:39:00 | INFO | train | epoch 042 | loss 10.906 | nll_loss 9.075 | ppl 539.14 | wps 535.9 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 672 | lr 8.064e-06 | gnorm 1.955 | train_wall 45 | gb_free 10.1 | wall 5409
2024-09-04 08:39:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:39:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:39:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:39:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000753
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19000.83984375Mb; avail=236028.9140625Mb
2024-09-04 08:39:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000585
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19000.83984375Mb; avail=236028.9140625Mb
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19000.83984375Mb; avail=236028.9140625Mb
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000167
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001071
2024-09-04 08:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19000.83984375Mb; avail=236028.9140625Mb
2024-09-04 08:39:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:39:00 | INFO | fairseq.trainer | begin training epoch 43
2024-09-04 08:39:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:39:02 | INFO | train_inner | epoch 043:      2 / 21 loss=8.637, nll_loss=6.026, ppl=65.17, wps=86.6, ups=0.02, wpb=4329.5, bsz=92, num_updates=884, lr=1.0608e-05, gnorm=2.186, train_wall=6, gb_free=10.8, wall=5899
2024-09-04 08:39:06 | INFO | train_inner | epoch 043:      2 / 16 loss=10.876, nll_loss=9.043, ppl=527.67, wps=106.1, ups=0.02, wpb=4601.5, bsz=136, num_updates=674, lr=8.088e-06, gnorm=2.346, train_wall=6, gb_free=11.9, wall=5415
2024-09-04 08:39:08 | INFO | train_inner | epoch 043:      4 / 21 loss=8.825, nll_loss=6.265, ppl=76.92, wps=1639, ups=0.33, wpb=4929, bsz=100, num_updates=886, lr=1.0632e-05, gnorm=2.337, train_wall=6, gb_free=10.8, wall=5905
2024-09-04 08:39:11 | INFO | train_inner | epoch 043:      4 / 16 loss=11.052, nll_loss=9.251, ppl=609.12, wps=1505.5, ups=0.41, wpb=3681, bsz=69, num_updates=676, lr=8.112e-06, gnorm=2.617, train_wall=5, gb_free=11.4, wall=5420
2024-09-04 08:39:13 | INFO | train_inner | epoch 043:      6 / 21 loss=8.696, nll_loss=6.123, ppl=69.68, wps=1944.9, ups=0.42, wpb=4603, bsz=112, num_updates=888, lr=1.0656e-05, gnorm=2.088, train_wall=5, gb_free=13.6, wall=5910
2024-09-04 08:39:16 | INFO | train_inner | epoch 043:      6 / 16 loss=10.399, nll_loss=8.418, ppl=341.94, wps=1454.5, ups=0.37, wpb=3980, bsz=164, num_updates=678, lr=8.136e-06, gnorm=1.779, train_wall=5, gb_free=9.9, wall=5426
2024-09-04 08:39:19 | INFO | train_inner | epoch 043:      8 / 21 loss=8.805, nll_loss=6.249, ppl=76.08, wps=1684.2, ups=0.35, wpb=4779, bsz=88, num_updates=890, lr=1.068e-05, gnorm=2.238, train_wall=6, gb_free=15.3, wall=5916
2024-09-04 08:39:21 | INFO | train_inner | epoch 043:      8 / 16 loss=11.055, nll_loss=9.28, ppl=621.84, wps=1501.4, ups=0.37, wpb=4025, bsz=88, num_updates=680, lr=8.16e-06, gnorm=1.986, train_wall=5, gb_free=10, wall=5431
2024-09-04 08:39:23 | INFO | train_inner | epoch 043:     10 / 21 loss=8.854, nll_loss=6.309, ppl=79.3, wps=1956.6, ups=0.44, wpb=4494, bsz=80, num_updates=892, lr=1.0704e-05, gnorm=2.07, train_wall=5, gb_free=13.2, wall=5920
2024-09-04 08:39:27 | INFO | train_inner | epoch 043:     10 / 16 loss=10.954, nll_loss=9.128, ppl=559.48, wps=1406.5, ups=0.37, wpb=3780, bsz=96, num_updates=682, lr=8.184e-06, gnorm=2.017, train_wall=5, gb_free=9.3, wall=5436
2024-09-04 08:39:27 | INFO | train_inner | epoch 043:     12 / 21 loss=8.636, nll_loss=6.043, ppl=65.92, wps=1898.6, ups=0.5, wpb=3835.5, bsz=89, num_updates=894, lr=1.0728e-05, gnorm=1.809, train_wall=4, gb_free=13.4, wall=5924
2024-09-04 08:39:33 | INFO | train_inner | epoch 043:     12 / 16 loss=10.659, nll_loss=8.735, ppl=426.12, wps=1501.8, ups=0.32, wpb=4691, bsz=176, num_updates=684, lr=8.208e-06, gnorm=1.862, train_wall=6, gb_free=8.7, wall=5443
2024-09-04 08:39:33 | INFO | train_inner | epoch 043:     14 / 21 loss=8.712, nll_loss=6.127, ppl=69.89, wps=1653.3, ups=0.34, wpb=4815, bsz=92, num_updates=896, lr=1.0752e-05, gnorm=2.026, train_wall=6, gb_free=12.3, wall=5930
2024-09-04 08:39:39 | INFO | train_inner | epoch 043:     16 / 21 loss=8.706, nll_loss=6.125, ppl=69.81, wps=1577.3, ups=0.37, wpb=4300.5, bsz=116, num_updates=898, lr=1.0776e-05, gnorm=2.331, train_wall=5, gb_free=13.3, wall=5936
2024-09-04 08:39:39 | INFO | train_inner | epoch 043:     14 / 16 loss=10.78, nll_loss=8.909, ppl=480.85, wps=1530.3, ups=0.34, wpb=4454, bsz=136, num_updates=686, lr=8.232e-06, gnorm=1.918, train_wall=6, gb_free=10.6, wall=5448
2024-09-04 08:39:44 | INFO | train_inner | epoch 043:     18 / 21 loss=8.495, nll_loss=5.843, ppl=57.39, wps=1705.8, ups=0.38, wpb=4527, bsz=128, num_updates=900, lr=1.08e-05, gnorm=2.099, train_wall=5, gb_free=13.3, wall=5941
2024-09-04 08:39:45 | INFO | train_inner | epoch 043:     16 / 16 loss=10.829, nll_loss=8.977, ppl=503.92, wps=1543.9, ups=0.33, wpb=4679, bsz=152, num_updates=688, lr=8.256e-06, gnorm=1.765, train_wall=6, gb_free=8.7, wall=5454
2024-09-04 08:39:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14775.65625Mb; avail=240254.33984375Mb
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000631
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14775.65625Mb; avail=240254.33984375Mb
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012512
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14775.65625Mb; avail=240254.33984375Mb
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011145
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024659
2024-09-04 08:39:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14775.65625Mb; avail=240254.33984375Mb
2024-09-04 08:39:50 | INFO | train_inner | epoch 043:     20 / 21 loss=8.884, nll_loss=6.339, ppl=80.97, wps=1702, ups=0.35, wpb=4828.5, bsz=72, num_updates=902, lr=1.0824e-05, gnorm=1.756, train_wall=6, gb_free=12.9, wall=5947
2024-09-04 08:39:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14818.046875Mb; avail=240211.40625Mb
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000781
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14818.5390625Mb; avail=240211.40625Mb
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012666
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14818.703125Mb; avail=240211.16015625Mb
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011196
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025064
2024-09-04 08:39:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14818.703125Mb; avail=240211.16015625Mb
2024-09-04 08:39:59 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 11.547 | nll_loss 9.787 | ppl 883.31 | wps 3840.5 | wpb 2070.5 | bsz 122.7 | num_updates 688 | best_loss 11.547
2024-09-04 08:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 688 updates
2024-09-04 08:39:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:40:11 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.601 | nll_loss 7.143 | ppl 141.34 | wps 4414.8 | wpb 2350.9 | bsz 94.7 | num_updates 903 | best_loss 9.601
2024-09-04 08:40:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 903 updates
2024-09-04 08:40:11 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:40:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:40:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:41:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 43 @ 688 updates, score 11.547) (writing took 62.109098485670984 seconds)
2024-09-04 08:41:01 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-09-04 08:41:01 | INFO | train | epoch 043 | loss 10.82 | nll_loss 8.961 | ppl 498.2 | wps 557.5 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 688 | lr 8.256e-06 | gnorm 2.036 | train_wall 45 | gb_free 8.7 | wall 5531
2024-09-04 08:41:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:41:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:41:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:41:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000701
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18154.6328125Mb; avail=236875.2734375Mb
2024-09-04 08:41:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000613
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18154.6328125Mb; avail=236874.78125Mb
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18154.6328125Mb; avail=236875.2734375Mb
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000181
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001140
2024-09-04 08:41:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18154.6328125Mb; avail=236875.2734375Mb
2024-09-04 08:41:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:41:01 | INFO | fairseq.trainer | begin training epoch 44
2024-09-04 08:41:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:41:07 | INFO | train_inner | epoch 044:      2 / 16 loss=10.676, nll_loss=8.763, ppl=434.38, wps=104.2, ups=0.02, wpb=4289, bsz=132, num_updates=690, lr=8.28e-06, gnorm=1.796, train_wall=6, gb_free=11.8, wall=5537
2024-09-04 08:41:13 | INFO | train_inner | epoch 044:      4 / 16 loss=10.927, nll_loss=9.089, ppl=544.69, wps=1569.6, ups=0.33, wpb=4745, bsz=116, num_updates=692, lr=8.304e-06, gnorm=1.884, train_wall=6, gb_free=10, wall=5543
2024-09-04 08:41:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 43 @ 903 updates, score 9.601) (writing took 67.29608035460114 seconds)
2024-09-04 08:41:18 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-09-04 08:41:18 | INFO | train | epoch 043 | loss 8.725 | nll_loss 6.144 | ppl 70.71 | wps 677.6 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 903 | lr 1.0836e-05 | gnorm 2.079 | train_wall 55 | gb_free 12.3 | wall 6035
2024-09-04 08:41:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:41:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:41:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:41:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000775
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18199.203125Mb; avail=236830.7421875Mb
2024-09-04 08:41:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000062
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000674
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18199.203125Mb; avail=236830.7421875Mb
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18199.203125Mb; avail=236830.7421875Mb
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000207
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001297
2024-09-04 08:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18199.203125Mb; avail=236830.7421875Mb
2024-09-04 08:41:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:41:18 | INFO | fairseq.trainer | begin training epoch 44
2024-09-04 08:41:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:41:19 | INFO | train_inner | epoch 044:      6 / 16 loss=10.116, nll_loss=8.028, ppl=261.07, wps=1397.3, ups=0.33, wpb=4189.5, bsz=204, num_updates=694, lr=8.328e-06, gnorm=1.731, train_wall=6, gb_free=9.4, wall=5549
2024-09-04 08:41:21 | INFO | train_inner | epoch 044:      1 / 21 loss=8.665, nll_loss=6.072, ppl=67.26, wps=97.3, ups=0.02, wpb=4437.5, bsz=100, num_updates=904, lr=1.0848e-05, gnorm=1.683, train_wall=5, gb_free=15.3, wall=6038
2024-09-04 08:41:25 | INFO | train_inner | epoch 044:      8 / 16 loss=10.655, nll_loss=8.736, ppl=426.32, wps=1471.7, ups=0.34, wpb=4298.5, bsz=144, num_updates=696, lr=8.352e-06, gnorm=1.896, train_wall=6, gb_free=10.6, wall=5555
2024-09-04 08:41:26 | INFO | train_inner | epoch 044:      3 / 21 loss=8.934, nll_loss=6.392, ppl=84.01, wps=2014.7, ups=0.39, wpb=5145.5, bsz=72, num_updates=906, lr=1.0872e-05, gnorm=2.102, train_wall=5, gb_free=15.2, wall=6043
2024-09-04 08:41:30 | INFO | train_inner | epoch 044:     10 / 16 loss=10.706, nll_loss=8.813, ppl=449.67, wps=1469.8, ups=0.39, wpb=3815.5, bsz=113, num_updates=698, lr=8.376e-06, gnorm=1.977, train_wall=5, gb_free=10, wall=5560
2024-09-04 08:41:31 | INFO | train_inner | epoch 044:      5 / 21 loss=8.568, nll_loss=5.939, ppl=61.36, wps=1937.1, ups=0.4, wpb=4863.5, bsz=128, num_updates=908, lr=1.0896e-05, gnorm=1.783, train_wall=5, gb_free=11.5, wall=6048
2024-09-04 08:41:36 | INFO | train_inner | epoch 044:     12 / 16 loss=10.73, nll_loss=8.832, ppl=455.64, wps=1550.4, ups=0.35, wpb=4395, bsz=132, num_updates=700, lr=8.4e-06, gnorm=2.044, train_wall=6, gb_free=11.2, wall=5565
2024-09-04 08:41:36 | INFO | train_inner | epoch 044:      7 / 21 loss=8.619, nll_loss=5.99, ppl=63.54, wps=1783.2, ups=0.38, wpb=4721.5, bsz=112, num_updates=910, lr=1.092e-05, gnorm=1.829, train_wall=5, gb_free=12.7, wall=6053
2024-09-04 08:41:42 | INFO | train_inner | epoch 044:      9 / 21 loss=8.668, nll_loss=6.066, ppl=67, wps=1809.1, ups=0.38, wpb=4754.5, bsz=104, num_updates=912, lr=1.0944e-05, gnorm=2.11, train_wall=5, gb_free=12.4, wall=6059
2024-09-04 08:41:42 | INFO | train_inner | epoch 044:     14 / 16 loss=10.81, nll_loss=8.938, ppl=490.56, wps=1459, ups=0.32, wpb=4530, bsz=120, num_updates=702, lr=8.424e-06, gnorm=2.005, train_wall=6, gb_free=9.6, wall=5572
2024-09-04 08:41:46 | INFO | train_inner | epoch 044:     11 / 21 loss=8.929, nll_loss=6.402, ppl=84.56, wps=1854.8, ups=0.45, wpb=4088, bsz=56, num_updates=914, lr=1.0968e-05, gnorm=2.445, train_wall=4, gb_free=12.9, wall=6063
2024-09-04 08:41:47 | INFO | train_inner | epoch 044:     16 / 16 loss=11.056, nll_loss=9.257, ppl=611.95, wps=1483, ups=0.41, wpb=3629, bsz=56, num_updates=704, lr=8.448e-06, gnorm=2.109, train_wall=5, gb_free=11.5, wall=5577
2024-09-04 08:41:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18236.26953125Mb; avail=236793.62890625Mb
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18236.76171875Mb; avail=236793.13671875Mb
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012726
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18236.76171875Mb; avail=236793.13671875Mb
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011169
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025006
2024-09-04 08:41:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18236.76171875Mb; avail=236793.13671875Mb
2024-09-04 08:42:01 | INFO | train_inner | epoch 044:     13 / 21 loss=8.234, nll_loss=5.506, ppl=45.44, wps=582.5, ups=0.13, wpb=4441, bsz=164, num_updates=916, lr=1.0992e-05, gnorm=2.216, train_wall=15, gb_free=13.1, wall=6078
2024-09-04 08:42:01 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 11.464 | nll_loss 9.707 | ppl 836.01 | wps 3841.6 | wpb 2070.5 | bsz 122.7 | num_updates 704 | best_loss 11.464
2024-09-04 08:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 704 updates
2024-09-04 08:42:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:42:06 | INFO | train_inner | epoch 044:     15 / 21 loss=8.864, nll_loss=6.298, ppl=78.69, wps=2033.2, ups=0.41, wpb=4973.5, bsz=88, num_updates=918, lr=1.1016e-05, gnorm=1.669, train_wall=5, gb_free=13.4, wall=6083
2024-09-04 08:42:12 | INFO | train_inner | epoch 044:     17 / 21 loss=8.527, nll_loss=5.884, ppl=59.06, wps=1656.8, ups=0.37, wpb=4495, bsz=96, num_updates=920, lr=1.104e-05, gnorm=1.866, train_wall=5, gb_free=13.2, wall=6089
2024-09-04 08:42:17 | INFO | train_inner | epoch 044:     19 / 21 loss=8.682, nll_loss=6.067, ppl=67.06, wps=1790.7, ups=0.39, wpb=4601.5, bsz=96, num_updates=922, lr=1.1064e-05, gnorm=1.82, train_wall=5, gb_free=12.1, wall=6094
2024-09-04 08:42:21 | INFO | train_inner | epoch 044:     21 / 21 loss=8.839, nll_loss=6.266, ppl=76.95, wps=1864.8, ups=0.5, wpb=3719.5, bsz=49, num_updates=924, lr=1.1088e-05, gnorm=2.355, train_wall=4, gb_free=13.3, wall=6098
2024-09-04 08:42:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30147.08203125Mb; avail=224882.94140625Mb
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000647
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30147.08203125Mb; avail=224882.94140625Mb
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012741
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30147.08203125Mb; avail=224882.94140625Mb
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011416
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025168
2024-09-04 08:42:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30147.08203125Mb; avail=224882.94140625Mb
2024-09-04 08:42:38 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.584 | nll_loss 7.14 | ppl 141 | wps 4692.9 | wpb 2350.9 | bsz 94.7 | num_updates 924 | best_loss 9.584
2024-09-04 08:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 924 updates
2024-09-04 08:42:38 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:43:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 44 @ 704 updates, score 11.464) (writing took 66.50490480381995 seconds)
2024-09-04 08:43:08 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-09-04 08:43:08 | INFO | train | epoch 044 | loss 10.708 | nll_loss 8.805 | ppl 447.35 | wps 535.4 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 704 | lr 8.448e-06 | gnorm 1.93 | train_wall 46 | gb_free 11.5 | wall 5658
2024-09-04 08:43:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:43:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:43:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:43:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000593
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35291.28515625Mb; avail=219738.6953125Mb
2024-09-04 08:43:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000046
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000565
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35290.79296875Mb; avail=219738.6953125Mb
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35291.28515625Mb; avail=219738.6953125Mb
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000153
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001032
2024-09-04 08:43:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35290.79296875Mb; avail=219739.1875Mb
2024-09-04 08:43:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:43:08 | INFO | fairseq.trainer | begin training epoch 45
2024-09-04 08:43:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:43:13 | INFO | train_inner | epoch 045:      2 / 16 loss=10.579, nll_loss=8.655, ppl=403.14, wps=86.3, ups=0.02, wpb=3720.5, bsz=129, num_updates=706, lr=8.472e-06, gnorm=2.156, train_wall=5, gb_free=9.4, wall=5663
2024-09-04 08:43:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:43:19 | INFO | train_inner | epoch 045:      4 / 16 loss=10.233, nll_loss=8.168, ppl=287.64, wps=1432.4, ups=0.36, wpb=4025, bsz=164, num_updates=708, lr=8.496e-06, gnorm=2.027, train_wall=6, gb_free=9.7, wall=5668
2024-09-04 08:43:25 | INFO | train_inner | epoch 045:      6 / 16 loss=10.69, nll_loss=8.778, ppl=438.83, wps=1560.9, ups=0.34, wpb=4531, bsz=140, num_updates=710, lr=8.52e-06, gnorm=1.64, train_wall=6, gb_free=10.5, wall=5674
2024-09-04 08:43:30 | INFO | train_inner | epoch 045:      8 / 16 loss=10.805, nll_loss=8.93, ppl=487.59, wps=1571, ups=0.37, wpb=4295.5, bsz=92, num_updates=712, lr=8.544e-06, gnorm=1.756, train_wall=5, gb_free=11.6, wall=5680
2024-09-04 08:43:36 | INFO | train_inner | epoch 045:     10 / 16 loss=10.576, nll_loss=8.63, ppl=396.28, wps=1517.8, ups=0.33, wpb=4631, bsz=156, num_updates=714, lr=8.568e-06, gnorm=1.821, train_wall=6, gb_free=9.2, wall=5686
2024-09-04 08:43:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 44 @ 924 updates, score 9.584) (writing took 64.09691843297333 seconds)
2024-09-04 08:43:42 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-09-04 08:43:42 | INFO | train | epoch 044 | loss 8.686 | nll_loss 6.082 | ppl 67.76 | wps 663.6 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 924 | lr 1.1088e-05 | gnorm 1.999 | train_wall 63 | gb_free 13.3 | wall 6179
2024-09-04 08:43:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:43:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:43:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:43:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000700
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19452.98046875Mb; avail=235577.0Mb
2024-09-04 08:43:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000681
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19452.98046875Mb; avail=235577.0Mb
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000050
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19452.98046875Mb; avail=235577.0Mb
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000182
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001243
2024-09-04 08:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19452.98046875Mb; avail=235577.0Mb
2024-09-04 08:43:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:43:42 | INFO | fairseq.trainer | begin training epoch 45
2024-09-04 08:43:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:43:42 | INFO | train_inner | epoch 045:     12 / 16 loss=10.745, nll_loss=8.847, ppl=460.56, wps=1506, ups=0.33, wpb=4499, bsz=108, num_updates=716, lr=8.592e-06, gnorm=1.73, train_wall=6, gb_free=12, wall=5692
2024-09-04 08:43:48 | INFO | train_inner | epoch 045:      2 / 21 loss=8.787, nll_loss=6.215, ppl=74.3, wps=105.3, ups=0.02, wpb=4584, bsz=92, num_updates=926, lr=1.1112e-05, gnorm=2.401, train_wall=6, gb_free=14.4, wall=6185
2024-09-04 08:43:48 | INFO | train_inner | epoch 045:     14 / 16 loss=10.584, nll_loss=8.638, ppl=398.26, wps=1527.3, ups=0.36, wpb=4287.5, bsz=140, num_updates=718, lr=8.616e-06, gnorm=1.727, train_wall=6, gb_free=13.2, wall=5697
2024-09-04 08:43:53 | INFO | train_inner | epoch 045:      4 / 21 loss=8.801, nll_loss=6.216, ppl=74.33, wps=1473.4, ups=0.39, wpb=3788, bsz=57, num_updates=928, lr=1.1136e-05, gnorm=2.148, train_wall=5, gb_free=18.5, wall=6190
2024-09-04 08:43:53 | INFO | train_inner | epoch 045:     16 / 16 loss=10.725, nll_loss=8.827, ppl=454.24, wps=1438.8, ups=0.37, wpb=3902, bsz=88, num_updates=720, lr=8.64e-06, gnorm=1.948, train_wall=5, gb_free=9.3, wall=5703
2024-09-04 08:43:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19493.26171875Mb; avail=235536.6796875Mb
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19493.26171875Mb; avail=235536.6796875Mb
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012706
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19493.75390625Mb; avail=235536.1875Mb
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011132
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024952
2024-09-04 08:43:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19493.75Mb; avail=235536.1875Mb
2024-09-04 08:43:57 | INFO | train_inner | epoch 045:      6 / 21 loss=8.91, nll_loss=6.362, ppl=82.25, wps=1956.4, ups=0.45, wpb=4307.5, bsz=60, num_updates=930, lr=1.116e-05, gnorm=2.31, train_wall=4, gb_free=13, wall=6194
2024-09-04 08:44:02 | INFO | train_inner | epoch 045:      8 / 21 loss=8.653, nll_loss=6.058, ppl=66.62, wps=1745.7, ups=0.39, wpb=4477.5, bsz=128, num_updates=932, lr=1.1184e-05, gnorm=3.085, train_wall=5, gb_free=13.7, wall=6199
2024-09-04 08:44:08 | INFO | train_inner | epoch 045:     10 / 21 loss=8.821, nll_loss=6.238, ppl=75.47, wps=1733.6, ups=0.38, wpb=4567, bsz=64, num_updates=934, lr=1.1208e-05, gnorm=2.336, train_wall=5, gb_free=13.3, wall=6205
2024-09-04 08:44:08 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 11.42 | nll_loss 9.62 | ppl 786.97 | wps 3839.6 | wpb 2070.5 | bsz 122.7 | num_updates 720 | best_loss 11.42
2024-09-04 08:44:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 720 updates
2024-09-04 08:44:08 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:44:13 | INFO | train_inner | epoch 045:     12 / 21 loss=8.06, nll_loss=5.262, ppl=38.37, wps=1584, ups=0.35, wpb=4472, bsz=176, num_updates=936, lr=1.1232e-05, gnorm=2.873, train_wall=6, gb_free=12.7, wall=6210
2024-09-04 08:44:19 | INFO | train_inner | epoch 045:     14 / 21 loss=8.812, nll_loss=6.254, ppl=76.32, wps=1741.4, ups=0.33, wpb=5259, bsz=84, num_updates=938, lr=1.1256e-05, gnorm=2.57, train_wall=6, gb_free=10.3, wall=6216
2024-09-04 08:44:24 | INFO | train_inner | epoch 045:     16 / 21 loss=8.628, nll_loss=5.99, ppl=63.56, wps=1812.9, ups=0.43, wpb=4231.5, bsz=92, num_updates=940, lr=1.128e-05, gnorm=2.656, train_wall=5, gb_free=12.7, wall=6221
2024-09-04 08:44:29 | INFO | train_inner | epoch 045:     18 / 21 loss=8.63, nll_loss=5.99, ppl=63.54, wps=1879.7, ups=0.4, wpb=4691, bsz=84, num_updates=942, lr=1.1304e-05, gnorm=2.064, train_wall=5, gb_free=13.1, wall=6226
2024-09-04 08:44:35 | INFO | train_inner | epoch 045:     20 / 21 loss=8.74, nll_loss=6.135, ppl=70.29, wps=1729.2, ups=0.34, wpb=5061.5, bsz=96, num_updates=944, lr=1.1328e-05, gnorm=2.079, train_wall=6, gb_free=12, wall=6232
2024-09-04 08:44:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36633.30078125Mb; avail=218396.625Mb
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000749
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36633.79296875Mb; avail=218396.1328125Mb
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012754
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36633.29296875Mb; avail=218396.6328125Mb
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011252
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025112
2024-09-04 08:44:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36632.80078125Mb; avail=218397.125Mb
2024-09-04 08:44:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:44:56 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.544 | nll_loss 7.063 | ppl 133.75 | wps 4434 | wpb 2350.9 | bsz 94.7 | num_updates 945 | best_loss 9.544
2024-09-04 08:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 945 updates
2024-09-04 08:44:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:45:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 45 @ 720 updates, score 11.42) (writing took 62.24390719644725 seconds)
2024-09-04 08:45:10 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-09-04 08:45:10 | INFO | train | epoch 045 | loss 10.62 | nll_loss 8.688 | ppl 412.47 | wps 555.6 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 720 | lr 8.64e-06 | gnorm 1.851 | train_wall 45 | gb_free 9.3 | wall 5780
2024-09-04 08:45:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:45:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:45:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:45:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000763
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44755.76953125Mb; avail=210274.13671875Mb
2024-09-04 08:45:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000065
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000614
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44755.76953125Mb; avail=210274.13671875Mb
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44755.76953125Mb; avail=210274.13671875Mb
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001101
2024-09-04 08:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44755.76953125Mb; avail=210274.13671875Mb
2024-09-04 08:45:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:45:10 | INFO | fairseq.trainer | begin training epoch 46
2024-09-04 08:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:45:16 | INFO | train_inner | epoch 046:      2 / 16 loss=10.613, nll_loss=8.675, ppl=408.68, wps=101.6, ups=0.02, wpb=4189.5, bsz=116, num_updates=722, lr=8.664e-06, gnorm=1.949, train_wall=6, gb_free=9.6, wall=5785
2024-09-04 08:45:21 | INFO | train_inner | epoch 046:      4 / 16 loss=10.683, nll_loss=8.759, ppl=433.23, wps=1541.1, ups=0.37, wpb=4135, bsz=116, num_updates=724, lr=8.688e-06, gnorm=2.47, train_wall=5, gb_free=15.7, wall=5791
2024-09-04 08:45:27 | INFO | train_inner | epoch 046:      6 / 16 loss=10.746, nll_loss=8.837, ppl=457.35, wps=1502.4, ups=0.34, wpb=4416, bsz=104, num_updates=726, lr=8.712e-06, gnorm=2.046, train_wall=6, gb_free=8.8, wall=5797
2024-09-04 08:45:33 | INFO | train_inner | epoch 046:      8 / 16 loss=10.133, nll_loss=8.041, ppl=263.34, wps=1451.8, ups=0.36, wpb=4067, bsz=156, num_updates=728, lr=8.736e-06, gnorm=2.428, train_wall=6, gb_free=9.8, wall=5802
2024-09-04 08:45:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:45:38 | INFO | train_inner | epoch 046:     10 / 16 loss=10.336, nll_loss=8.307, ppl=316.69, wps=1430.3, ups=0.41, wpb=3491, bsz=113, num_updates=730, lr=8.76e-06, gnorm=2.263, train_wall=5, gb_free=9, wall=5807
2024-09-04 08:45:43 | INFO | train_inner | epoch 046:     12 / 16 loss=10.418, nll_loss=8.424, ppl=343.41, wps=1523.6, ups=0.34, wpb=4456.5, bsz=168, num_updates=732, lr=8.784e-06, gnorm=1.626, train_wall=6, gb_free=10.2, wall=5813
2024-09-04 08:45:49 | INFO | train_inner | epoch 046:     14 / 16 loss=10.525, nll_loss=8.545, ppl=373.62, wps=1531.3, ups=0.34, wpb=4515, bsz=136, num_updates=734, lr=8.808e-06, gnorm=2.077, train_wall=6, gb_free=11.1, wall=5819
2024-09-04 08:45:55 | INFO | train_inner | epoch 046:     16 / 16 loss=10.728, nll_loss=8.819, ppl=451.5, wps=1566.5, ups=0.34, wpb=4621.5, bsz=108, num_updates=736, lr=8.832e-06, gnorm=2.058, train_wall=6, gb_free=11.1, wall=5825
2024-09-04 08:45:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28804.2890625Mb; avail=226225.59765625Mb
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28804.2890625Mb; avail=226225.59765625Mb
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012826
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28804.2890625Mb; avail=226225.59765625Mb
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011154
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025081
2024-09-04 08:45:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28804.2890625Mb; avail=226225.59765625Mb
2024-09-04 08:46:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 45 @ 945 updates, score 9.544) (writing took 66.78596548829228 seconds)
2024-09-04 08:46:02 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-09-04 08:46:02 | INFO | train | epoch 045 | loss 8.671 | nll_loss 6.055 | ppl 66.5 | wps 681.7 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 945 | lr 1.134e-05 | gnorm 2.452 | train_wall 55 | gb_free 14 | wall 6319
2024-09-04 08:46:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:46:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:46:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:46:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000749
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28849.7265625Mb; avail=226180.04296875Mb
2024-09-04 08:46:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000649
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28849.7265625Mb; avail=226180.04296875Mb
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28849.7265625Mb; avail=226180.04296875Mb
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000192
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001141
2024-09-04 08:46:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28849.7265625Mb; avail=226180.04296875Mb
2024-09-04 08:46:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:46:03 | INFO | fairseq.trainer | begin training epoch 46
2024-09-04 08:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:46:05 | INFO | train_inner | epoch 046:      1 / 21 loss=8.506, nll_loss=5.848, ppl=57.6, wps=85.7, ups=0.02, wpb=3845.5, bsz=97, num_updates=946, lr=1.1352e-05, gnorm=2.838, train_wall=4, gb_free=14.1, wall=6322
2024-09-04 08:46:10 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 11.346 | nll_loss 9.514 | ppl 731.14 | wps 3840.5 | wpb 2070.5 | bsz 122.7 | num_updates 736 | best_loss 11.346
2024-09-04 08:46:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 736 updates
2024-09-04 08:46:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:46:19 | INFO | train_inner | epoch 046:      3 / 21 loss=8.586, nll_loss=5.945, ppl=61.6, wps=626.1, ups=0.14, wpb=4606.5, bsz=88, num_updates=948, lr=1.1376e-05, gnorm=2.179, train_wall=15, gb_free=13.4, wall=6336
2024-09-04 08:46:25 | INFO | train_inner | epoch 046:      5 / 21 loss=8.522, nll_loss=5.875, ppl=58.71, wps=1675.4, ups=0.35, wpb=4806.5, bsz=112, num_updates=950, lr=1.14e-05, gnorm=2.624, train_wall=6, gb_free=12.4, wall=6342
2024-09-04 08:46:30 | INFO | train_inner | epoch 046:      7 / 21 loss=8.801, nll_loss=6.219, ppl=74.48, wps=2031.2, ups=0.44, wpb=4601.5, bsz=80, num_updates=952, lr=1.1424e-05, gnorm=2.582, train_wall=5, gb_free=14.3, wall=6347
2024-09-04 08:46:35 | INFO | train_inner | epoch 046:      9 / 21 loss=8.539, nll_loss=5.877, ppl=58.76, wps=1550.7, ups=0.36, wpb=4272.5, bsz=116, num_updates=954, lr=1.1448e-05, gnorm=2.029, train_wall=6, gb_free=11.9, wall=6352
2024-09-04 08:46:40 | INFO | train_inner | epoch 046:     11 / 21 loss=8.87, nll_loss=6.3, ppl=78.81, wps=2063.9, ups=0.45, wpb=4570, bsz=48, num_updates=956, lr=1.1472e-05, gnorm=2.389, train_wall=4, gb_free=12.7, wall=6356
2024-09-04 08:46:44 | INFO | train_inner | epoch 046:     13 / 21 loss=8.42, nll_loss=5.712, ppl=52.42, wps=1797.1, ups=0.41, wpb=4394.5, bsz=116, num_updates=958, lr=1.1496e-05, gnorm=2.118, train_wall=5, gb_free=12.8, wall=6361
2024-09-04 08:46:50 | INFO | train_inner | epoch 046:     15 / 21 loss=8.626, nll_loss=5.97, ppl=62.67, wps=1639.4, ups=0.34, wpb=4763, bsz=92, num_updates=960, lr=1.152e-05, gnorm=2.071, train_wall=6, gb_free=11.1, wall=6367
2024-09-04 08:46:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:46:55 | INFO | train_inner | epoch 046:     17 / 21 loss=8.425, nll_loss=5.74, ppl=53.44, wps=1895.7, ups=0.39, wpb=4833.5, bsz=152, num_updates=962, lr=1.1544e-05, gnorm=1.979, train_wall=5, gb_free=12, wall=6372
2024-09-04 08:47:00 | INFO | train_inner | epoch 046:     19 / 21 loss=8.672, nll_loss=6.061, ppl=66.75, wps=1993.6, ups=0.41, wpb=4854, bsz=96, num_updates=964, lr=1.1568e-05, gnorm=1.67, train_wall=5, gb_free=14.1, wall=6377
2024-09-04 08:47:06 | INFO | train_inner | epoch 046:     21 / 21 loss=8.676, nll_loss=6.056, ppl=66.53, wps=1679.3, ups=0.36, wpb=4695.5, bsz=104, num_updates=966, lr=1.1592e-05, gnorm=2.711, train_wall=6, gb_free=14.5, wall=6383
2024-09-04 08:47:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34324.17578125Mb; avail=220705.19140625Mb
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000643
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34324.17578125Mb; avail=220705.68359375Mb
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012837
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34324.66796875Mb; avail=220705.19140625Mb
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011195
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025029
2024-09-04 08:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34324.171875Mb; avail=220705.68359375Mb
2024-09-04 08:47:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 46 @ 736 updates, score 11.346) (writing took 66.61014625709504 seconds)
2024-09-04 08:47:17 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-09-04 08:47:17 | INFO | train | epoch 046 | loss 10.531 | nll_loss 8.562 | ppl 377.84 | wps 535.8 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 736 | lr 8.832e-06 | gnorm 2.115 | train_wall 45 | gb_free 11.1 | wall 5906
2024-09-04 08:47:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:47:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:47:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:47:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000700
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34356.86328125Mb; avail=220673.03515625Mb
2024-09-04 08:47:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000058
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000613
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34356.86328125Mb; avail=220673.03515625Mb
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34356.86328125Mb; avail=220673.03515625Mb
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000172
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001091
2024-09-04 08:47:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34356.86328125Mb; avail=220673.03515625Mb
2024-09-04 08:47:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:47:17 | INFO | fairseq.trainer | begin training epoch 47
2024-09-04 08:47:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:47:23 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.523 | nll_loss 7.024 | ppl 130.14 | wps 4688.2 | wpb 2350.9 | bsz 94.7 | num_updates 966 | best_loss 9.523
2024-09-04 08:47:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 966 updates
2024-09-04 08:47:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:47:37 | INFO | train_inner | epoch 047:      2 / 16 loss=10.339, nll_loss=8.303, ppl=315.85, wps=86.6, ups=0.02, wpb=4424, bsz=152, num_updates=738, lr=8.856e-06, gnorm=2.047, train_wall=21, gb_free=9.4, wall=5927
2024-09-04 08:47:43 | INFO | train_inner | epoch 047:      4 / 16 loss=10.53, nll_loss=8.562, ppl=377.99, wps=1588.4, ups=0.36, wpb=4408.5, bsz=116, num_updates=740, lr=8.88e-06, gnorm=1.842, train_wall=6, gb_free=9.6, wall=5932
2024-09-04 08:47:49 | INFO | train_inner | epoch 047:      6 / 16 loss=10.134, nll_loss=8.035, ppl=262.32, wps=1464.6, ups=0.35, wpb=4138.5, bsz=148, num_updates=742, lr=8.904e-06, gnorm=2.153, train_wall=6, gb_free=14.5, wall=5938
2024-09-04 08:47:54 | INFO | train_inner | epoch 047:      8 / 16 loss=10.346, nll_loss=8.318, ppl=319.22, wps=1532.1, ups=0.34, wpb=4533.5, bsz=172, num_updates=744, lr=8.928e-06, gnorm=1.609, train_wall=6, gb_free=9.2, wall=5944
2024-09-04 08:48:00 | INFO | train_inner | epoch 047:     10 / 16 loss=10.695, nll_loss=8.772, ppl=437.23, wps=1575.3, ups=0.36, wpb=4338, bsz=100, num_updates=746, lr=8.952e-06, gnorm=1.546, train_wall=5, gb_free=12.1, wall=5949
2024-09-04 08:48:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:48:05 | INFO | train_inner | epoch 047:     12 / 16 loss=10.413, nll_loss=8.396, ppl=336.8, wps=1390.6, ups=0.39, wpb=3610.5, bsz=113, num_updates=748, lr=8.976e-06, gnorm=1.742, train_wall=5, gb_free=8.1, wall=5955
2024-09-04 08:48:11 | INFO | train_inner | epoch 047:     14 / 16 loss=10.587, nll_loss=8.616, ppl=392.37, wps=1513, ups=0.37, wpb=4046.5, bsz=100, num_updates=750, lr=9e-06, gnorm=2.099, train_wall=5, gb_free=9.3, wall=5960
2024-09-04 08:48:17 | INFO | train_inner | epoch 047:     16 / 16 loss=10.521, nll_loss=8.545, ppl=373.41, wps=1431.8, ups=0.33, wpb=4392, bsz=116, num_updates=752, lr=9.024e-06, gnorm=2.217, train_wall=6, gb_free=9.1, wall=5966
2024-09-04 08:48:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20416.109375Mb; avail=234613.8046875Mb
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000640
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20416.6015625Mb; avail=234613.8046875Mb
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012486
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20416.6015625Mb; avail=234613.8046875Mb
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011140
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024669
2024-09-04 08:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20416.109375Mb; avail=234613.3125Mb
2024-09-04 08:48:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 46 @ 966 updates, score 9.523) (writing took 66.4136896552518 seconds)
2024-09-04 08:48:30 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-09-04 08:48:30 | INFO | train | epoch 046 | loss 8.616 | nll_loss 5.979 | ppl 63.08 | wps 649.4 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 966 | lr 1.1592e-05 | gnorm 2.283 | train_wall 63 | gb_free 14.5 | wall 6467
2024-09-04 08:48:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:48:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:48:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:48:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000723
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20469.01953125Mb; avail=234561.0390625Mb
2024-09-04 08:48:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000066
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000629
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20469.01953125Mb; avail=234561.0390625Mb
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20469.01953125Mb; avail=234561.0390625Mb
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000196
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001189
2024-09-04 08:48:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20469.01953125Mb; avail=234561.0390625Mb
2024-09-04 08:48:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:48:30 | INFO | fairseq.trainer | begin training epoch 47
2024-09-04 08:48:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:48:31 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 11.332 | nll_loss 9.49 | ppl 718.87 | wps 3837.8 | wpb 2070.5 | bsz 122.7 | num_updates 752 | best_loss 11.332
2024-09-04 08:48:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 752 updates
2024-09-04 08:48:31 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:48:36 | INFO | train_inner | epoch 047:      2 / 21 loss=8.134, nll_loss=5.35, ppl=40.78, wps=101.9, ups=0.02, wpb=4581.5, bsz=148, num_updates=968, lr=1.1616e-05, gnorm=2.024, train_wall=6, gb_free=13.9, wall=6473
2024-09-04 08:48:41 | INFO | train_inner | epoch 047:      4 / 21 loss=8.674, nll_loss=6.043, ppl=65.95, wps=2000, ups=0.41, wpb=4877.5, bsz=92, num_updates=970, lr=1.164e-05, gnorm=2.213, train_wall=5, gb_free=12.9, wall=6478
2024-09-04 08:48:46 | INFO | train_inner | epoch 047:      6 / 21 loss=8.626, nll_loss=5.988, ppl=63.49, wps=1665.9, ups=0.35, wpb=4738, bsz=92, num_updates=972, lr=1.1664e-05, gnorm=2.191, train_wall=6, gb_free=14.3, wall=6483
2024-09-04 08:48:51 | INFO | train_inner | epoch 047:      8 / 21 loss=8.309, nll_loss=5.589, ppl=48.13, wps=1845.6, ups=0.4, wpb=4557.5, bsz=160, num_updates=974, lr=1.1688e-05, gnorm=2.28, train_wall=5, gb_free=13.1, wall=6488
2024-09-04 08:48:57 | INFO | train_inner | epoch 047:     10 / 21 loss=8.763, nll_loss=6.136, ppl=70.32, wps=1656, ups=0.35, wpb=4668.5, bsz=80, num_updates=976, lr=1.1712e-05, gnorm=2.099, train_wall=6, gb_free=12.7, wall=6494
2024-09-04 08:49:01 | INFO | train_inner | epoch 047:     12 / 21 loss=8.53, nll_loss=5.853, ppl=57.79, wps=1772, ups=0.47, wpb=3772.5, bsz=73, num_updates=978, lr=1.1736e-05, gnorm=2.225, train_wall=4, gb_free=11.9, wall=6498
2024-09-04 08:49:07 | INFO | train_inner | epoch 047:     14 / 21 loss=8.597, nll_loss=5.957, ppl=62.12, wps=1578, ups=0.34, wpb=4709.5, bsz=104, num_updates=980, lr=1.176e-05, gnorm=2.346, train_wall=6, gb_free=11.3, wall=6504
2024-09-04 08:49:13 | INFO | train_inner | epoch 047:     16 / 21 loss=8.538, nll_loss=5.876, ppl=58.74, wps=1630.3, ups=0.35, wpb=4662, bsz=96, num_updates=982, lr=1.1784e-05, gnorm=2.415, train_wall=6, gb_free=13.9, wall=6510
2024-09-04 08:49:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:49:18 | INFO | train_inner | epoch 047:     18 / 21 loss=8.645, nll_loss=5.999, ppl=63.94, wps=1963.7, ups=0.41, wpb=4774.5, bsz=92, num_updates=984, lr=1.1808e-05, gnorm=2.197, train_wall=5, gb_free=11.9, wall=6515
2024-09-04 08:49:23 | INFO | train_inner | epoch 047:     20 / 21 loss=8.825, nll_loss=6.221, ppl=74.59, wps=1643.6, ups=0.36, wpb=4517.5, bsz=60, num_updates=986, lr=1.1832e-05, gnorm=2.225, train_wall=5, gb_free=17.4, wall=6520
2024-09-04 08:49:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29645.3046875Mb; avail=225384.58203125Mb
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29645.79296875Mb; avail=225384.08984375Mb
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012660
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29645.30078125Mb; avail=225384.58203125Mb
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011246
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025008
2024-09-04 08:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29646.28515625Mb; avail=225383.59765625Mb
2024-09-04 08:49:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 47 @ 752 updates, score 11.332) (writing took 67.1288837203756 seconds)
2024-09-04 08:49:38 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-09-04 08:49:38 | INFO | train | epoch 047 | loss 10.446 | nll_loss 8.445 | ppl 348.43 | wps 478.2 | ups 0.11 | wpb 4236.4 | bsz 127.1 | num_updates 752 | lr 9.024e-06 | gnorm 1.907 | train_wall 60 | gb_free 9.1 | wall 6048
2024-09-04 08:49:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:49:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:49:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:49:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000665
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29680.0625Mb; avail=225349.7890625Mb
2024-09-04 08:49:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000651
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29680.0625Mb; avail=225349.7890625Mb
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29680.0625Mb; avail=225349.7890625Mb
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000167
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001123
2024-09-04 08:49:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29680.0625Mb; avail=225349.7890625Mb
2024-09-04 08:49:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:49:38 | INFO | fairseq.trainer | begin training epoch 48
2024-09-04 08:49:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:49:43 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.484 | nll_loss 6.978 | ppl 126.05 | wps 4694.1 | wpb 2350.9 | bsz 94.7 | num_updates 987 | best_loss 9.484
2024-09-04 08:49:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 987 updates
2024-09-04 08:49:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:49:44 | INFO | train_inner | epoch 048:      2 / 16 loss=10.576, nll_loss=8.623, ppl=394.32, wps=94.8, ups=0.02, wpb=4119.5, bsz=104, num_updates=754, lr=9.048e-06, gnorm=2.173, train_wall=5, gb_free=14.3, wall=6053
2024-09-04 08:49:49 | INFO | train_inner | epoch 048:      4 / 16 loss=10.558, nll_loss=8.579, ppl=382.38, wps=1543.2, ups=0.4, wpb=3889, bsz=96, num_updates=756, lr=9.072e-06, gnorm=2.589, train_wall=5, gb_free=12, wall=6058
2024-09-04 08:49:54 | INFO | train_inner | epoch 048:      6 / 16 loss=10.422, nll_loss=8.411, ppl=340.48, wps=1523, ups=0.34, wpb=4486.5, bsz=112, num_updates=758, lr=9.096e-06, gnorm=1.958, train_wall=6, gb_free=11, wall=6064
2024-09-04 08:50:00 | INFO | train_inner | epoch 048:      8 / 16 loss=10.3, nll_loss=8.241, ppl=302.48, wps=1496.6, ups=0.33, wpb=4478.5, bsz=168, num_updates=760, lr=9.12e-06, gnorm=1.866, train_wall=6, gb_free=8.9, wall=6070
2024-09-04 08:50:06 | INFO | train_inner | epoch 048:     10 / 16 loss=10.282, nll_loss=8.23, ppl=300.22, wps=1347.8, ups=0.39, wpb=3442.5, bsz=101, num_updates=762, lr=9.144e-06, gnorm=2.262, train_wall=5, gb_free=9.4, wall=6075
2024-09-04 08:50:11 | INFO | train_inner | epoch 048:     12 / 16 loss=10.421, nll_loss=8.395, ppl=336.62, wps=1503.1, ups=0.34, wpb=4426.5, bsz=132, num_updates=764, lr=9.168e-06, gnorm=2.369, train_wall=6, gb_free=10.7, wall=6081
2024-09-04 08:50:18 | INFO | train_inner | epoch 048:     14 / 16 loss=10.453, nll_loss=8.45, ppl=349.82, wps=1553.1, ups=0.33, wpb=4717.5, bsz=128, num_updates=766, lr=9.192e-06, gnorm=2.165, train_wall=6, gb_free=9.9, wall=6087
2024-09-04 08:50:23 | INFO | train_inner | epoch 048:     16 / 16 loss=9.931, nll_loss=7.744, ppl=214.33, wps=1493.2, ups=0.34, wpb=4331.5, bsz=176, num_updates=768, lr=9.216e-06, gnorm=1.915, train_wall=6, gb_free=10.2, wall=6093
2024-09-04 08:50:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45671.21875Mb; avail=209358.72265625Mb
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000756
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45671.7109375Mb; avail=209358.23046875Mb
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012523
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45671.7109375Mb; avail=209358.23046875Mb
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011114
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024774
2024-09-04 08:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45671.7109375Mb; avail=209358.23046875Mb
2024-09-04 08:50:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 08:50:38 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 11.26 | nll_loss 9.378 | ppl 665.35 | wps 3839.6 | wpb 2070.5 | bsz 122.7 | num_updates 768 | best_loss 11.26
2024-09-04 08:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 768 updates
2024-09-04 08:50:38 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 47 @ 987 updates, score 9.484) (writing took 70.90289276558906 seconds)
2024-09-04 08:50:54 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-09-04 08:50:54 | INFO | train | epoch 047 | loss 8.575 | nll_loss 5.915 | ppl 60.32 | wps 664 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 987 | lr 1.1844e-05 | gnorm 2.206 | train_wall 55 | gb_free 12.3 | wall 6611
2024-09-04 08:50:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:50:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:50:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:50:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000608
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50657.67578125Mb; avail=204372.265625Mb
2024-09-04 08:50:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000604
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50657.67578125Mb; avail=204372.265625Mb
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50657.67578125Mb; avail=204372.265625Mb
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001092
2024-09-04 08:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50657.67578125Mb; avail=204372.265625Mb
2024-09-04 08:50:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:50:54 | INFO | fairseq.trainer | begin training epoch 48
2024-09-04 08:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:50:57 | INFO | train_inner | epoch 048:      1 / 21 loss=8.725, nll_loss=6.099, ppl=68.56, wps=98.1, ups=0.02, wpb=4584, bsz=60, num_updates=988, lr=1.1856e-05, gnorm=2.048, train_wall=5, gb_free=10.7, wall=6614
2024-09-04 08:51:02 | INFO | train_inner | epoch 048:      3 / 21 loss=8.709, nll_loss=6.086, ppl=67.91, wps=1713, ups=0.36, wpb=4819.5, bsz=88, num_updates=990, lr=1.188e-05, gnorm=1.652, train_wall=6, gb_free=11.8, wall=6619
2024-09-04 08:51:08 | INFO | train_inner | epoch 048:      5 / 21 loss=8.687, nll_loss=6.059, ppl=66.66, wps=1504.7, ups=0.35, wpb=4257, bsz=72, num_updates=992, lr=1.1904e-05, gnorm=1.965, train_wall=6, gb_free=12.7, wall=6625
2024-09-04 08:51:13 | INFO | train_inner | epoch 048:      7 / 21 loss=8.634, nll_loss=5.991, ppl=63.61, wps=1720, ups=0.39, wpb=4370.5, bsz=72, num_updates=994, lr=1.1928e-05, gnorm=2.319, train_wall=5, gb_free=13.8, wall=6630
2024-09-04 08:51:18 | INFO | train_inner | epoch 048:      9 / 21 loss=8.385, nll_loss=5.662, ppl=50.62, wps=1883.7, ups=0.41, wpb=4637.5, bsz=140, num_updates=996, lr=1.1952e-05, gnorm=1.788, train_wall=5, gb_free=13.9, wall=6635
2024-09-04 08:51:24 | INFO | train_inner | epoch 048:     11 / 21 loss=8.588, nll_loss=5.923, ppl=60.66, wps=1635.4, ups=0.36, wpb=4576.5, bsz=100, num_updates=998, lr=1.1976e-05, gnorm=2.077, train_wall=6, gb_free=13.8, wall=6640
2024-09-04 08:51:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:51:29 | INFO | train_inner | epoch 048:     13 / 21 loss=8.52, nll_loss=5.835, ppl=57.1, wps=1821.8, ups=0.36, wpb=5025, bsz=92, num_updates=1000, lr=1.2e-05, gnorm=2.368, train_wall=6, gb_free=12.6, wall=6646
2024-09-04 08:51:34 | INFO | train_inner | epoch 048:     15 / 21 loss=8.598, nll_loss=5.966, ppl=62.5, wps=1731.2, ups=0.44, wpb=3914.5, bsz=65, num_updates=1002, lr=1.2024e-05, gnorm=2.444, train_wall=5, gb_free=15.2, wall=6651
2024-09-04 08:51:39 | INFO | train_inner | epoch 048:     17 / 21 loss=8.045, nll_loss=5.234, ppl=37.64, wps=1502.2, ups=0.34, wpb=4420, bsz=156, num_updates=1004, lr=1.2048e-05, gnorm=1.885, train_wall=6, gb_free=12.5, wall=6656
2024-09-04 08:51:45 | INFO | train_inner | epoch 048:     19 / 21 loss=8.548, nll_loss=5.856, ppl=57.92, wps=1575.9, ups=0.34, wpb=4690.5, bsz=92, num_updates=1006, lr=1.2072e-05, gnorm=1.971, train_wall=6, gb_free=13.4, wall=6662
2024-09-04 08:51:50 | INFO | train_inner | epoch 048:     21 / 21 loss=8.558, nll_loss=5.892, ppl=59.4, wps=1958.1, ups=0.43, wpb=4528, bsz=100, num_updates=1008, lr=1.2096e-05, gnorm=2.17, train_wall=5, gb_free=14.8, wall=6667
2024-09-04 08:51:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34757.86328125Mb; avail=220272.1640625Mb
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000720
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34757.86328125Mb; avail=220272.1640625Mb
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012790
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34757.86328125Mb; avail=220272.1640625Mb
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011317
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025203
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34758.35546875Mb; avail=220271.671875Mb
2024-09-04 08:51:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 48 @ 768 updates, score 11.26) (writing took 72.3226036047563 seconds)
2024-09-04 08:51:50 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-09-04 08:51:50 | INFO | train | epoch 048 | loss 10.367 | nll_loss 8.333 | ppl 322.54 | wps 514.1 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 768 | lr 9.216e-06 | gnorm 2.162 | train_wall 45 | gb_free 10.2 | wall 6180
2024-09-04 08:51:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:51:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:51:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:51:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000598
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34758.35546875Mb; avail=220271.671875Mb
2024-09-04 08:51:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000573
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34758.35546875Mb; avail=220271.671875Mb
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34758.35546875Mb; avail=220271.671875Mb
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000151
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001035
2024-09-04 08:51:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34758.35546875Mb; avail=220271.671875Mb
2024-09-04 08:51:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:51:50 | INFO | fairseq.trainer | begin training epoch 49
2024-09-04 08:51:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:51:56 | INFO | train_inner | epoch 049:      2 / 16 loss=10.488, nll_loss=8.486, ppl=358.47, wps=97.8, ups=0.02, wpb=4534.5, bsz=116, num_updates=770, lr=9.24e-06, gnorm=2.374, train_wall=6, gb_free=9.6, wall=6186
2024-09-04 08:52:01 | INFO | train_inner | epoch 049:      4 / 16 loss=10.387, nll_loss=8.345, ppl=325.2, wps=1553.5, ups=0.37, wpb=4154.5, bsz=116, num_updates=772, lr=9.264e-06, gnorm=2.469, train_wall=5, gb_free=11.6, wall=6191
2024-09-04 08:52:07 | INFO | train_inner | epoch 049:      6 / 16 loss=10.55, nll_loss=8.566, ppl=379.08, wps=1467.2, ups=0.33, wpb=4420, bsz=100, num_updates=774, lr=9.288e-06, gnorm=2.067, train_wall=6, gb_free=8.3, wall=6197
2024-09-04 08:52:07 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.586 | nll_loss 7.098 | ppl 137.03 | wps 4683.4 | wpb 2350.9 | bsz 94.7 | num_updates 1008 | best_loss 9.484
2024-09-04 08:52:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 1008 updates
2024-09-04 08:52:07 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:52:13 | INFO | train_inner | epoch 049:      8 / 16 loss=10.35, nll_loss=8.303, ppl=315.91, wps=1526.5, ups=0.35, wpb=4303.5, bsz=104, num_updates=776, lr=9.312e-06, gnorm=2.104, train_wall=6, gb_free=10, wall=6203
2024-09-04 08:52:19 | INFO | train_inner | epoch 049:     10 / 16 loss=10.39, nll_loss=8.35, ppl=326.21, wps=1561.2, ups=0.35, wpb=4442.5, bsz=104, num_updates=778, lr=9.336e-06, gnorm=1.972, train_wall=6, gb_free=11.5, wall=6208
2024-09-04 08:52:24 | INFO | train_inner | epoch 049:     12 / 16 loss=10.238, nll_loss=8.159, ppl=285.79, wps=1564, ups=0.38, wpb=4080.5, bsz=144, num_updates=780, lr=9.36e-06, gnorm=2.087, train_wall=5, gb_free=9.9, wall=6213
2024-09-04 08:52:30 | INFO | train_inner | epoch 049:     14 / 16 loss=10.315, nll_loss=8.263, ppl=307.27, wps=1476.6, ups=0.33, wpb=4441.5, bsz=132, num_updates=782, lr=9.384e-06, gnorm=2.144, train_wall=6, gb_free=8.1, wall=6219
2024-09-04 08:52:35 | INFO | train_inner | epoch 049:     16 / 16 loss=9.378, nll_loss=7.019, ppl=129.72, wps=1341.9, ups=0.38, wpb=3514.5, bsz=201, num_updates=784, lr=9.408e-06, gnorm=2.708, train_wall=5, gb_free=10, wall=6225
2024-09-04 08:52:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=51957.953125Mb; avail=203071.515625Mb
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51958.4453125Mb; avail=203071.515625Mb
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012610
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51958.6796875Mb; avail=203071.28125Mb
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011013
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024710
2024-09-04 08:52:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51959.171875Mb; avail=203070.7890625Mb
2024-09-04 08:52:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:52:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 48 @ 1008 updates, score 9.586) (writing took 38.519798994995654 seconds)
2024-09-04 08:52:46 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-09-04 08:52:46 | INFO | train | epoch 048 | loss 8.535 | nll_loss 5.86 | ppl 58.1 | wps 853.6 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1008 | lr 1.2096e-05 | gnorm 2.07 | train_wall 56 | gb_free 14.8 | wall 6723
2024-09-04 08:52:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:52:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:52:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:52:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000705
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37438.78515625Mb; avail=217591.1484375Mb
2024-09-04 08:52:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000588
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37438.78515625Mb; avail=217591.1484375Mb
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37438.78515625Mb; avail=217591.1484375Mb
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001052
2024-09-04 08:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37438.78515625Mb; avail=217591.1484375Mb
2024-09-04 08:52:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:52:46 | INFO | fairseq.trainer | begin training epoch 49
2024-09-04 08:52:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:52:50 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 11.245 | nll_loss 9.349 | ppl 652.08 | wps 3842.9 | wpb 2070.5 | bsz 122.7 | num_updates 784 | best_loss 11.245
2024-09-04 08:52:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 784 updates
2024-09-04 08:52:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:52:51 | INFO | train_inner | epoch 049:      2 / 21 loss=8.799, nll_loss=6.192, ppl=73.13, wps=163.5, ups=0.03, wpb=4991, bsz=60, num_updates=1010, lr=1.212e-05, gnorm=2.296, train_wall=5, gb_free=10.6, wall=6728
2024-09-04 08:52:56 | INFO | train_inner | epoch 049:      4 / 21 loss=8.198, nll_loss=5.416, ppl=42.69, wps=1909, ups=0.4, wpb=4797.5, bsz=144, num_updates=1012, lr=1.2144e-05, gnorm=2.626, train_wall=5, gb_free=12.9, wall=6733
2024-09-04 08:53:02 | INFO | train_inner | epoch 049:      6 / 21 loss=8.609, nll_loss=5.943, ppl=61.53, wps=1580.3, ups=0.35, wpb=4516.5, bsz=80, num_updates=1014, lr=1.2168e-05, gnorm=2.437, train_wall=6, gb_free=12.9, wall=6739
2024-09-04 08:53:07 | INFO | train_inner | epoch 049:      8 / 21 loss=8.467, nll_loss=5.773, ppl=54.67, wps=1952.8, ups=0.41, wpb=4786, bsz=112, num_updates=1016, lr=1.2192e-05, gnorm=1.995, train_wall=5, gb_free=12.1, wall=6744
2024-09-04 08:53:12 | INFO | train_inner | epoch 049:     10 / 21 loss=8.563, nll_loss=5.888, ppl=59.21, wps=1563.7, ups=0.35, wpb=4509.5, bsz=96, num_updates=1018, lr=1.2216e-05, gnorm=1.889, train_wall=6, gb_free=14.4, wall=6749
2024-09-04 08:53:17 | INFO | train_inner | epoch 049:     12 / 21 loss=8.627, nll_loss=5.957, ppl=62.1, wps=2096.2, ups=0.44, wpb=4716.5, bsz=68, num_updates=1020, lr=1.224e-05, gnorm=2.04, train_wall=4, gb_free=16.7, wall=6754
2024-09-04 08:53:23 | INFO | train_inner | epoch 049:     14 / 21 loss=8.595, nll_loss=5.928, ppl=60.87, wps=1613.2, ups=0.35, wpb=4654, bsz=84, num_updates=1022, lr=1.2264e-05, gnorm=1.761, train_wall=6, gb_free=13.9, wall=6760
2024-09-04 08:53:28 | INFO | train_inner | epoch 049:     16 / 21 loss=8.549, nll_loss=5.869, ppl=58.44, wps=1857.7, ups=0.4, wpb=4625, bsz=88, num_updates=1024, lr=1.2288e-05, gnorm=2.089, train_wall=5, gb_free=12.7, wall=6765
2024-09-04 08:53:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:53:33 | INFO | train_inner | epoch 049:     18 / 21 loss=8.547, nll_loss=5.868, ppl=58.39, wps=1683.1, ups=0.37, wpb=4489.5, bsz=104, num_updates=1026, lr=1.2312e-05, gnorm=1.797, train_wall=5, gb_free=12.8, wall=6770
2024-09-04 08:53:38 | INFO | train_inner | epoch 049:     20 / 21 loss=8.007, nll_loss=5.198, ppl=36.7, wps=1794.7, ups=0.42, wpb=4312, bsz=168, num_updates=1028, lr=1.2336e-05, gnorm=2.112, train_wall=5, gb_free=12.4, wall=6775
2024-09-04 08:53:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45548.6953125Mb; avail=209481.69140625Mb
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000721
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45548.203125Mb; avail=209481.19921875Mb
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012777
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45548.203125Mb; avail=209481.69140625Mb
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011378
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025236
2024-09-04 08:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45548.203125Mb; avail=209481.69140625Mb
2024-09-04 08:53:56 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.57 | nll_loss 7.087 | ppl 135.95 | wps 4865.1 | wpb 2350.9 | bsz 94.7 | num_updates 1029 | best_loss 9.484
2024-09-04 08:53:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 1029 updates
2024-09-04 08:53:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:53:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 49 @ 784 updates, score 11.245) (writing took 67.33145730104297 seconds)
2024-09-04 08:53:57 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-09-04 08:53:57 | INFO | train | epoch 049 | loss 10.285 | nll_loss 8.217 | ppl 297.65 | wps 534.2 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 784 | lr 9.408e-06 | gnorm 2.24 | train_wall 45 | gb_free 10 | wall 6307
2024-09-04 08:53:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:53:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:53:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:53:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000729
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45588.5234375Mb; avail=209441.375Mb
2024-09-04 08:53:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000598
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45588.5234375Mb; avail=209441.375Mb
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45588.5234375Mb; avail=209441.375Mb
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000171
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001119
2024-09-04 08:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45588.5234375Mb; avail=209441.375Mb
2024-09-04 08:53:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:53:57 | INFO | fairseq.trainer | begin training epoch 50
2024-09-04 08:53:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:54:03 | INFO | train_inner | epoch 050:      2 / 16 loss=10.342, nll_loss=8.285, ppl=311.83, wps=97.6, ups=0.02, wpb=4268, bsz=120, num_updates=786, lr=9.432e-06, gnorm=2.509, train_wall=6, gb_free=9.4, wall=6312
2024-09-04 08:54:08 | INFO | train_inner | epoch 050:      4 / 16 loss=9.993, nll_loss=7.834, ppl=228.23, wps=1405.9, ups=0.38, wpb=3653, bsz=120, num_updates=788, lr=9.456e-06, gnorm=2.574, train_wall=5, gb_free=11.2, wall=6317
2024-09-04 08:54:14 | INFO | train_inner | epoch 050:      6 / 16 loss=10.384, nll_loss=8.347, ppl=325.57, wps=1516.4, ups=0.36, wpb=4245, bsz=108, num_updates=790, lr=9.48e-06, gnorm=2.49, train_wall=6, gb_free=8.3, wall=6323
2024-09-04 08:54:19 | INFO | train_inner | epoch 050:      8 / 16 loss=9.996, nll_loss=7.835, ppl=228.36, wps=1456.1, ups=0.39, wpb=3725.5, bsz=141, num_updates=792, lr=9.504e-06, gnorm=2.618, train_wall=5, gb_free=15.3, wall=6328
2024-09-04 08:54:25 | INFO | train_inner | epoch 050:     10 / 16 loss=10.143, nll_loss=8.013, ppl=258.31, wps=1528.4, ups=0.34, wpb=4495.5, bsz=132, num_updates=794, lr=9.528e-06, gnorm=2.385, train_wall=6, gb_free=10.8, wall=6334
2024-09-04 08:54:30 | INFO | train_inner | epoch 050:     12 / 16 loss=10.23, nll_loss=8.139, ppl=281.89, wps=1612.8, ups=0.34, wpb=4695, bsz=132, num_updates=796, lr=9.552e-06, gnorm=2.082, train_wall=6, gb_free=10.4, wall=6340
2024-09-04 08:54:36 | INFO | train_inner | epoch 050:     14 / 16 loss=10.317, nll_loss=8.249, ppl=304.3, wps=1468.7, ups=0.33, wpb=4454, bsz=120, num_updates=798, lr=9.576e-06, gnorm=2.115, train_wall=6, gb_free=10, wall=6346
2024-09-04 08:54:42 | INFO | train_inner | epoch 050:     16 / 16 loss=10.175, nll_loss=8.061, ppl=266.97, wps=1473.9, ups=0.34, wpb=4355.5, bsz=144, num_updates=800, lr=9.6e-06, gnorm=2.082, train_wall=6, gb_free=12, wall=6352
2024-09-04 08:54:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=56398.19921875Mb; avail=198631.7421875Mb
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56398.19921875Mb; avail=198631.7421875Mb
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012720
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56398.19921875Mb; avail=198631.7421875Mb
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011165
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024987
2024-09-04 08:54:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56398.19921875Mb; avail=198631.7421875Mb
2024-09-04 08:54:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 49 @ 1029 updates, score 9.57) (writing took 51.65959743782878 seconds)
2024-09-04 08:54:48 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-09-04 08:54:48 | INFO | train | epoch 049 | loss 8.501 | nll_loss 5.809 | ppl 56.08 | wps 784.2 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1029 | lr 1.2348e-05 | gnorm 2.131 | train_wall 53 | gb_free 18.6 | wall 6845
2024-09-04 08:54:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:54:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:54:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:54:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000592
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44141.46484375Mb; avail=210888.33984375Mb
2024-09-04 08:54:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000567
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44141.46484375Mb; avail=210888.33984375Mb
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44141.46484375Mb; avail=210888.33984375Mb
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000160
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001019
2024-09-04 08:54:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44141.46484375Mb; avail=210888.33984375Mb
2024-09-04 08:54:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:54:48 | INFO | fairseq.trainer | begin training epoch 50
2024-09-04 08:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:54:51 | INFO | train_inner | epoch 050:      1 / 21 loss=8.469, nll_loss=5.776, ppl=54.8, wps=105.4, ups=0.03, wpb=3833, bsz=85, num_updates=1030, lr=1.236e-05, gnorm=2.238, train_wall=4, gb_free=13.3, wall=6848
2024-09-04 08:54:56 | INFO | train_inner | epoch 050:      3 / 21 loss=8.364, nll_loss=5.654, ppl=50.35, wps=1729.2, ups=0.35, wpb=4973, bsz=132, num_updates=1032, lr=1.2384e-05, gnorm=2.474, train_wall=6, gb_free=11.8, wall=6853
2024-09-04 08:54:57 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 11.155 | nll_loss 9.232 | ppl 601.42 | wps 3840.4 | wpb 2070.5 | bsz 122.7 | num_updates 800 | best_loss 11.155
2024-09-04 08:54:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 800 updates
2024-09-04 08:54:57 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:55:01 | INFO | train_inner | epoch 050:      5 / 21 loss=8.107, nll_loss=5.285, ppl=39, wps=1859.4, ups=0.43, wpb=4286, bsz=108, num_updates=1034, lr=1.2408e-05, gnorm=2.17, train_wall=5, gb_free=13.3, wall=6858
2024-09-04 08:55:06 | INFO | train_inner | epoch 050:      7 / 21 loss=8.687, nll_loss=6.034, ppl=65.51, wps=1543.2, ups=0.37, wpb=4127, bsz=52, num_updates=1036, lr=1.2432e-05, gnorm=2.222, train_wall=5, gb_free=13.6, wall=6863
2024-09-04 08:55:12 | INFO | train_inner | epoch 050:      9 / 21 loss=8.641, nll_loss=5.989, ppl=63.5, wps=1722.9, ups=0.35, wpb=4947, bsz=84, num_updates=1038, lr=1.2456e-05, gnorm=2.049, train_wall=6, gb_free=14.2, wall=6869
2024-09-04 08:55:17 | INFO | train_inner | epoch 050:     11 / 21 loss=8.668, nll_loss=6.019, ppl=64.87, wps=1950.5, ups=0.44, wpb=4478.5, bsz=80, num_updates=1040, lr=1.248e-05, gnorm=2.374, train_wall=5, gb_free=15, wall=6874
2024-09-04 08:55:22 | INFO | train_inner | epoch 050:     13 / 21 loss=8.593, nll_loss=5.913, ppl=60.25, wps=1511.8, ups=0.35, wpb=4271.5, bsz=84, num_updates=1042, lr=1.2504e-05, gnorm=2.098, train_wall=6, gb_free=17.1, wall=6879
2024-09-04 08:55:27 | INFO | train_inner | epoch 050:     15 / 21 loss=8.445, nll_loss=5.724, ppl=52.87, wps=1621.1, ups=0.39, wpb=4192.5, bsz=69, num_updates=1044, lr=1.2528e-05, gnorm=1.915, train_wall=5, gb_free=16.3, wall=6884
2024-09-04 08:55:33 | INFO | train_inner | epoch 050:     17 / 21 loss=8.375, nll_loss=5.66, ppl=50.55, wps=1857.9, ups=0.38, wpb=4880, bsz=120, num_updates=1046, lr=1.2552e-05, gnorm=2.428, train_wall=5, gb_free=11.7, wall=6890
2024-09-04 08:55:39 | INFO | train_inner | epoch 050:     19 / 21 loss=8.336, nll_loss=5.6, ppl=48.5, wps=1497.6, ups=0.34, wpb=4461.5, bsz=120, num_updates=1048, lr=1.2576e-05, gnorm=1.905, train_wall=6, gb_free=12.9, wall=6896
2024-09-04 08:55:43 | INFO | train_inner | epoch 050:     21 / 21 loss=8.496, nll_loss=5.796, ppl=55.55, wps=2047, ups=0.42, wpb=4834.5, bsz=96, num_updates=1050, lr=1.26e-05, gnorm=2.304, train_wall=5, gb_free=11.3, wall=6900
2024-09-04 08:55:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42385.046875Mb; avail=212644.40234375Mb
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000748
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42390.4609375Mb; avail=212638.98828125Mb
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012731
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42387.015625Mb; avail=212642.92578125Mb
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011207
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025051
2024-09-04 08:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42389.96875Mb; avail=212639.48046875Mb
2024-09-04 08:55:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:56:01 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.545 | nll_loss 7.01 | ppl 128.91 | wps 4687.1 | wpb 2350.9 | bsz 94.7 | num_updates 1050 | best_loss 9.484
2024-09-04 08:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 1050 updates
2024-09-04 08:56:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:56:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 50 @ 800 updates, score 11.155) (writing took 73.89222208224237 seconds)
2024-09-04 08:56:11 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-09-04 08:56:11 | INFO | train | epoch 050 | loss 10.205 | nll_loss 8.105 | ppl 275.29 | wps 507.1 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 800 | lr 9.6e-06 | gnorm 2.357 | train_wall 45 | gb_free 12 | wall 6440
2024-09-04 08:56:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:56:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:56:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:56:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000622
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37766.6640625Mb; avail=217263.2734375Mb
2024-09-04 08:56:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000596
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37767.6484375Mb; avail=217261.796875Mb
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37768.140625Mb; avail=217261.796875Mb
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000157
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001065
2024-09-04 08:56:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37769.125Mb; avail=217260.8125Mb
2024-09-04 08:56:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:56:11 | INFO | fairseq.trainer | begin training epoch 51
2024-09-04 08:56:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:56:16 | INFO | train_inner | epoch 051:      2 / 16 loss=10.371, nll_loss=8.319, ppl=319.34, wps=85.7, ups=0.02, wpb=4015, bsz=84, num_updates=802, lr=9.624e-06, gnorm=2.098, train_wall=5, gb_free=12.7, wall=6445
2024-09-04 08:56:22 | INFO | train_inner | epoch 051:      4 / 16 loss=9.863, nll_loss=7.66, ppl=202.2, wps=1511.3, ups=0.35, wpb=4299, bsz=188, num_updates=804, lr=9.648e-06, gnorm=2.024, train_wall=6, gb_free=13.9, wall=6451
2024-09-04 08:56:27 | INFO | train_inner | epoch 051:      6 / 16 loss=10.092, nll_loss=7.949, ppl=247.14, wps=1594.1, ups=0.34, wpb=4627.5, bsz=164, num_updates=806, lr=9.672e-06, gnorm=1.732, train_wall=6, gb_free=10.2, wall=6457
2024-09-04 08:56:33 | INFO | train_inner | epoch 051:      8 / 16 loss=10.11, nll_loss=7.986, ppl=253.48, wps=1555.3, ups=0.36, wpb=4306.5, bsz=140, num_updates=808, lr=9.696e-06, gnorm=2.22, train_wall=6, gb_free=10, wall=6463
2024-09-04 08:56:38 | INFO | train_inner | epoch 051:     10 / 16 loss=10.235, nll_loss=8.138, ppl=281.74, wps=1448.1, ups=0.39, wpb=3681, bsz=105, num_updates=810, lr=9.72e-06, gnorm=1.993, train_wall=5, gb_free=11.1, wall=6468
2024-09-04 08:56:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:56:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 50 @ 1050 updates, score 9.545) (writing took 41.98871177714318 seconds)
2024-09-04 08:56:43 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-09-04 08:56:43 | INFO | train | epoch 050 | loss 8.469 | nll_loss 5.766 | ppl 54.41 | wps 833.3 | ups 0.18 | wpb 4556.3 | bsz 96.9 | num_updates 1050 | lr 1.26e-05 | gnorm 2.176 | train_wall 55 | gb_free 11.3 | wall 6960
2024-09-04 08:56:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:56:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:56:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:56:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000719
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30465.56640625Mb; avail=224564.33203125Mb
2024-09-04 08:56:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000587
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30465.56640625Mb; avail=224564.33203125Mb
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30465.56640625Mb; avail=224564.33203125Mb
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001051
2024-09-04 08:56:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30465.56640625Mb; avail=224564.33203125Mb
2024-09-04 08:56:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:56:43 | INFO | fairseq.trainer | begin training epoch 51
2024-09-04 08:56:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:56:44 | INFO | train_inner | epoch 051:     12 / 16 loss=9.759, nll_loss=7.511, ppl=182.35, wps=1519.3, ups=0.34, wpb=4443, bsz=180, num_updates=812, lr=9.744e-06, gnorm=2.297, train_wall=6, gb_free=10.8, wall=6473
2024-09-04 08:56:48 | INFO | train_inner | epoch 051:      2 / 21 loss=8.552, nll_loss=5.853, ppl=57.79, wps=135.9, ups=0.03, wpb=4391, bsz=76, num_updates=1052, lr=1.2624e-05, gnorm=2.622, train_wall=5, gb_free=12.6, wall=6965
2024-09-04 08:56:50 | INFO | train_inner | epoch 051:     14 / 16 loss=10.364, nll_loss=8.303, ppl=315.86, wps=1468.7, ups=0.34, wpb=4352.5, bsz=72, num_updates=814, lr=9.768e-06, gnorm=2.009, train_wall=6, gb_free=9, wall=6479
2024-09-04 08:56:53 | INFO | train_inner | epoch 051:      4 / 21 loss=8.472, nll_loss=5.773, ppl=54.69, wps=1490.5, ups=0.41, wpb=3645, bsz=57, num_updates=1054, lr=1.2648e-05, gnorm=2.877, train_wall=5, gb_free=13.9, wall=6970
2024-09-04 08:56:56 | INFO | train_inner | epoch 051:     16 / 16 loss=10.332, nll_loss=8.252, ppl=304.84, wps=1457.4, ups=0.35, wpb=4167, bsz=84, num_updates=816, lr=9.792e-06, gnorm=2.34, train_wall=6, gb_free=11.5, wall=6485
2024-09-04 08:56:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30511.5Mb; avail=224518.35546875Mb
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000715
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30511.5Mb; avail=224518.35546875Mb
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012676
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30511.5Mb; avail=224518.35546875Mb
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011290
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025042
2024-09-04 08:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30511.5Mb; avail=224518.35546875Mb
2024-09-04 08:56:59 | INFO | train_inner | epoch 051:      6 / 21 loss=8.634, nll_loss=5.973, ppl=62.81, wps=1653.8, ups=0.34, wpb=4844.5, bsz=72, num_updates=1056, lr=1.2672e-05, gnorm=2.202, train_wall=6, gb_free=13.2, wall=6976
2024-09-04 08:57:04 | INFO | train_inner | epoch 051:      8 / 21 loss=8.321, nll_loss=5.575, ppl=47.67, wps=1776.6, ups=0.39, wpb=4512, bsz=120, num_updates=1058, lr=1.2696e-05, gnorm=2.036, train_wall=5, gb_free=13.8, wall=6981
2024-09-04 08:57:09 | INFO | train_inner | epoch 051:     10 / 21 loss=8.43, nll_loss=5.709, ppl=52.3, wps=1798.9, ups=0.37, wpb=4827.5, bsz=116, num_updates=1060, lr=1.272e-05, gnorm=2.088, train_wall=5, gb_free=13, wall=6986
2024-09-04 08:57:10 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 11.183 | nll_loss 9.251 | ppl 609.45 | wps 3836.9 | wpb 2070.5 | bsz 122.7 | num_updates 816 | best_loss 11.155
2024-09-04 08:57:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 816 updates
2024-09-04 08:57:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 08:57:15 | INFO | train_inner | epoch 051:     12 / 21 loss=8.461, nll_loss=5.747, ppl=53.7, wps=1599.5, ups=0.32, wpb=5002.5, bsz=120, num_updates=1062, lr=1.2744e-05, gnorm=2.21, train_wall=6, gb_free=10.9, wall=6992
2024-09-04 08:57:21 | INFO | train_inner | epoch 051:     14 / 21 loss=8.215, nll_loss=5.435, ppl=43.27, wps=1793.5, ups=0.38, wpb=4710, bsz=144, num_updates=1064, lr=1.2768e-05, gnorm=1.979, train_wall=5, gb_free=12.3, wall=6998
2024-09-04 08:57:25 | INFO | train_inner | epoch 051:     16 / 21 loss=8.484, nll_loss=5.782, ppl=55.04, wps=1844.4, ups=0.42, wpb=4386.5, bsz=84, num_updates=1066, lr=1.2792e-05, gnorm=2.163, train_wall=5, gb_free=12.8, wall=7002
2024-09-04 08:57:30 | INFO | train_inner | epoch 051:     18 / 21 loss=8.148, nll_loss=5.348, ppl=40.72, wps=1882, ups=0.41, wpb=4594.5, bsz=132, num_updates=1068, lr=1.2816e-05, gnorm=1.977, train_wall=5, gb_free=12.9, wall=7007
2024-09-04 08:57:36 | INFO | train_inner | epoch 051:     20 / 21 loss=8.623, nll_loss=5.95, ppl=61.81, wps=1588.6, ups=0.34, wpb=4619, bsz=60, num_updates=1070, lr=1.284e-05, gnorm=2.099, train_wall=6, gb_free=11.4, wall=7013
2024-09-04 08:57:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47668.328125Mb; avail=207361.984375Mb
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000743
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47668.328125Mb; avail=207361.4921875Mb
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012846
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47668.328125Mb; avail=207361.4921875Mb
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011296
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025260
2024-09-04 08:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47668.8203125Mb; avail=207361.4921875Mb
2024-09-04 08:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 08:57:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 51 @ 816 updates, score 11.183) (writing took 38.32829056866467 seconds)
2024-09-04 08:57:48 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-09-04 08:57:48 | INFO | train | epoch 051 | loss 10.135 | nll_loss 8.007 | ppl 257.18 | wps 693.9 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 816 | lr 9.792e-06 | gnorm 2.089 | train_wall 45 | gb_free 11.5 | wall 6538
2024-09-04 08:57:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 08:57:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 08:57:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 08:57:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000689
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32465.83984375Mb; avail=222563.97265625Mb
2024-09-04 08:57:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000581
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32465.8359375Mb; avail=222563.97265625Mb
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32465.8359375Mb; avail=222563.97265625Mb
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000162
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001036
2024-09-04 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32465.8359375Mb; avail=222563.97265625Mb
2024-09-04 08:57:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 08:57:48 | INFO | fairseq.trainer | begin training epoch 52
2024-09-04 08:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:57:54 | INFO | train_inner | epoch 052:      2 / 16 loss=9.913, nll_loss=7.712, ppl=209.65, wps=150.6, ups=0.03, wpb=4434.5, bsz=176, num_updates=818, lr=9.816e-06, gnorm=2.573, train_wall=6, gb_free=8.2, wall=6544
2024-09-04 08:57:56 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.508 | nll_loss 6.961 | ppl 124.59 | wps 4706.2 | wpb 2350.9 | bsz 94.7 | num_updates 1071 | best_loss 9.484
2024-09-04 08:57:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 1071 updates
2024-09-04 08:57:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:58:01 | INFO | train_inner | epoch 052:      4 / 16 loss=10.107, nll_loss=7.95, ppl=247.28, wps=1549.4, ups=0.33, wpb=4663, bsz=128, num_updates=820, lr=9.84e-06, gnorm=2.291, train_wall=6, gb_free=8.6, wall=6550
2024-09-04 08:58:06 | INFO | train_inner | epoch 052:      6 / 16 loss=10.113, nll_loss=7.978, ppl=252.16, wps=1430.9, ups=0.38, wpb=3785.5, bsz=105, num_updates=822, lr=9.864e-06, gnorm=2.188, train_wall=5, gb_free=9, wall=6555
2024-09-04 08:58:11 | INFO | train_inner | epoch 052:      8 / 16 loss=10.071, nll_loss=7.92, ppl=242.24, wps=1555.3, ups=0.36, wpb=4381, bsz=124, num_updates=824, lr=9.888e-06, gnorm=2.133, train_wall=6, gb_free=11.4, wall=6561
2024-09-04 08:58:17 | INFO | train_inner | epoch 052:     10 / 16 loss=9.761, nll_loss=7.51, ppl=182.31, wps=1443, ups=0.34, wpb=4285, bsz=164, num_updates=826, lr=9.912e-06, gnorm=2.468, train_wall=6, gb_free=9.8, wall=6567
2024-09-04 08:58:23 | INFO | train_inner | epoch 052:     12 / 16 loss=10.027, nll_loss=7.854, ppl=231.4, wps=1494.6, ups=0.34, wpb=4460, bsz=148, num_updates=828, lr=9.936e-06, gnorm=2.819, train_wall=6, gb_free=9.6, wall=6573
2024-09-04 08:58:28 | INFO | train_inner | epoch 052:     14 / 16 loss=10.418, nll_loss=8.367, ppl=330.08, wps=1439.7, ups=0.4, wpb=3559.5, bsz=48, num_updates=830, lr=9.96e-06, gnorm=2.968, train_wall=5, gb_free=11.9, wall=6578
2024-09-04 08:58:34 | INFO | train_inner | epoch 052:     16 / 16 loss=10.12, nll_loss=7.978, ppl=252.21, wps=1484, ups=0.34, wpb=4323, bsz=124, num_updates=832, lr=9.984e-06, gnorm=2.152, train_wall=6, gb_free=11.5, wall=6584
2024-09-04 08:58:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42106.5Mb; avail=212923.8515625Mb
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42106.0078125Mb; avail=212923.359375Mb
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012770
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42106.00390625Mb; avail=212923.8515625Mb
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011043
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024914
2024-09-04 08:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42106.98828125Mb; avail=212922.8671875Mb
2024-09-04 08:58:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 08:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 51 @ 1071 updates, score 9.508) (writing took 48.2386415945366 seconds)
2024-09-04 08:58:44 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-09-04 08:58:44 | INFO | train | epoch 051 | loss 8.431 | nll_loss 5.709 | ppl 52.31 | wps 789.1 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1071 | lr 1.2852e-05 | gnorm 2.212 | train_wall 56 | gb_free 12.8 | wall 7081
2024-09-04 08:58:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 08:58:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 08:58:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 08:58:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000571
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25424.5625Mb; avail=229605.2890625Mb
2024-09-04 08:58:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000065
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000558
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25424.5625Mb; avail=229605.2890625Mb
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25424.5625Mb; avail=229605.2890625Mb
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000156
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001006
2024-09-04 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25424.5625Mb; avail=229605.2890625Mb
2024-09-04 08:58:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 08:58:44 | INFO | fairseq.trainer | begin training epoch 52
2024-09-04 08:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 08:58:47 | INFO | train_inner | epoch 052:      1 / 21 loss=8.528, nll_loss=5.815, ppl=56.29, wps=134.1, ups=0.03, wpb=4726, bsz=76, num_updates=1072, lr=1.2864e-05, gnorm=1.907, train_wall=5, gb_free=11.9, wall=7084
2024-09-04 08:58:49 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 11.121 | nll_loss 9.143 | ppl 565.43 | wps 3828.8 | wpb 2070.5 | bsz 122.7 | num_updates 832 | best_loss 11.121
2024-09-04 08:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 832 updates
2024-09-04 08:58:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:58:52 | INFO | train_inner | epoch 052:      3 / 21 loss=8.408, nll_loss=5.681, ppl=51.29, wps=1623.4, ups=0.35, wpb=4631.5, bsz=76, num_updates=1074, lr=1.2888e-05, gnorm=2.07, train_wall=6, gb_free=13.1, wall=7089
2024-09-04 08:58:58 | INFO | train_inner | epoch 052:      5 / 21 loss=8.545, nll_loss=5.868, ppl=58.4, wps=1618.6, ups=0.36, wpb=4493.5, bsz=96, num_updates=1076, lr=1.2912e-05, gnorm=1.958, train_wall=6, gb_free=13.1, wall=7095
2024-09-04 08:59:03 | INFO | train_inner | epoch 052:      7 / 21 loss=8.28, nll_loss=5.515, ppl=45.73, wps=1789.2, ups=0.39, wpb=4606, bsz=112, num_updates=1078, lr=1.2936e-05, gnorm=2.071, train_wall=5, gb_free=13.4, wall=7100
2024-09-04 08:59:08 | INFO | train_inner | epoch 052:      9 / 21 loss=8.37, nll_loss=5.637, ppl=49.78, wps=1854.3, ups=0.4, wpb=4661, bsz=112, num_updates=1080, lr=1.296e-05, gnorm=2.365, train_wall=5, gb_free=13.7, wall=7105
2024-09-04 08:59:13 | INFO | train_inner | epoch 052:     11 / 21 loss=8.506, nll_loss=5.814, ppl=56.24, wps=1675.9, ups=0.38, wpb=4469, bsz=84, num_updates=1082, lr=1.2984e-05, gnorm=2.209, train_wall=5, gb_free=14, wall=7110
2024-09-04 08:59:18 | INFO | train_inner | epoch 052:     13 / 21 loss=8.315, nll_loss=5.558, ppl=47.12, wps=2079.9, ups=0.42, wpb=4910.5, bsz=100, num_updates=1084, lr=1.3008e-05, gnorm=2.246, train_wall=5, gb_free=14.9, wall=7115
2024-09-04 08:59:23 | INFO | train_inner | epoch 052:     15 / 21 loss=8.505, nll_loss=5.776, ppl=54.79, wps=1970.1, ups=0.4, wpb=4938.5, bsz=92, num_updates=1086, lr=1.3032e-05, gnorm=2.139, train_wall=5, gb_free=14.7, wall=7120
2024-09-04 08:59:29 | INFO | train_inner | epoch 052:     17 / 21 loss=8.208, nll_loss=5.425, ppl=42.97, wps=1451.8, ups=0.34, wpb=4232.5, bsz=108, num_updates=1088, lr=1.3056e-05, gnorm=2.214, train_wall=6, gb_free=12.7, wall=7126
2024-09-04 08:59:34 | INFO | train_inner | epoch 052:     19 / 21 loss=8.323, nll_loss=5.585, ppl=48, wps=1757.4, ups=0.41, wpb=4318, bsz=112, num_updates=1090, lr=1.308e-05, gnorm=2.396, train_wall=5, gb_free=14.1, wall=7131
2024-09-04 08:59:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 08:59:38 | INFO | train_inner | epoch 052:     21 / 21 loss=8.422, nll_loss=5.701, ppl=52.02, wps=1883.4, ups=0.45, wpb=4163, bsz=85, num_updates=1092, lr=1.3104e-05, gnorm=2.765, train_wall=4, gb_free=12.5, wall=7135
2024-09-04 08:59:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18815.48828125Mb; avail=236213.37109375Mb
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000727
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18810.07421875Mb; avail=236219.27734375Mb
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012911
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18813.50390625Mb; avail=236215.84765625Mb
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011285
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025282
2024-09-04 08:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18817.44140625Mb; avail=236211.91015625Mb
2024-09-04 08:59:55 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.473 | nll_loss 6.921 | ppl 121.19 | wps 4985.4 | wpb 2350.9 | bsz 94.7 | num_updates 1092 | best_loss 9.473
2024-09-04 08:59:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 1092 updates
2024-09-04 08:59:55 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:00:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 52 @ 832 updates, score 11.121) (writing took 72.98030528705567 seconds)
2024-09-04 09:00:02 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-09-04 09:00:02 | INFO | train | epoch 052 | loss 10.058 | nll_loss 7.897 | ppl 238.42 | wps 508.9 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 832 | lr 9.984e-06 | gnorm 2.449 | train_wall 46 | gb_free 11.5 | wall 6671
2024-09-04 09:00:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:00:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:00:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:00:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000622
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22997.25Mb; avail=232032.19921875Mb
2024-09-04 09:00:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000056
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000570
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22998.7265625Mb; avail=232031.21484375Mb
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22999.21875Mb; avail=232030.72265625Mb
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000182
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001055
2024-09-04 09:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22999.7109375Mb; avail=232030.23046875Mb
2024-09-04 09:00:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:00:02 | INFO | fairseq.trainer | begin training epoch 53
2024-09-04 09:00:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:00:07 | INFO | train_inner | epoch 053:      2 / 16 loss=9.905, nll_loss=7.69, ppl=206.47, wps=94.2, ups=0.02, wpb=4398.5, bsz=160, num_updates=834, lr=1.0008e-05, gnorm=2.623, train_wall=6, gb_free=10.3, wall=6677
2024-09-04 09:00:12 | INFO | train_inner | epoch 053:      4 / 16 loss=10.072, nll_loss=7.911, ppl=240.6, wps=1450.6, ups=0.44, wpb=3274.5, bsz=73, num_updates=836, lr=1.0032e-05, gnorm=2.663, train_wall=5, gb_free=15.2, wall=6681
2024-09-04 09:00:17 | INFO | train_inner | epoch 053:      6 / 16 loss=10.013, nll_loss=7.842, ppl=229.46, wps=1475.6, ups=0.38, wpb=3905, bsz=128, num_updates=838, lr=1.0056e-05, gnorm=2.377, train_wall=5, gb_free=14.9, wall=6687
2024-09-04 09:00:23 | INFO | train_inner | epoch 053:      8 / 16 loss=10.11, nll_loss=7.958, ppl=248.59, wps=1598.2, ups=0.34, wpb=4673.5, bsz=124, num_updates=840, lr=1.008e-05, gnorm=2.402, train_wall=6, gb_free=11.5, wall=6693
2024-09-04 09:00:29 | INFO | train_inner | epoch 053:     10 / 16 loss=10.109, nll_loss=7.939, ppl=245.33, wps=1467, ups=0.35, wpb=4209.5, bsz=120, num_updates=842, lr=1.0104e-05, gnorm=2.448, train_wall=6, gb_free=10.4, wall=6698
2024-09-04 09:00:35 | INFO | train_inner | epoch 053:     12 / 16 loss=10.179, nll_loss=8.043, ppl=263.8, wps=1535.6, ups=0.32, wpb=4804.5, bsz=116, num_updates=844, lr=1.0128e-05, gnorm=3.01, train_wall=6, gb_free=10.1, wall=6705
2024-09-04 09:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:00:41 | INFO | train_inner | epoch 053:     14 / 16 loss=9.433, nll_loss=7.076, ppl=134.96, wps=1456.7, ups=0.34, wpb=4288, bsz=212, num_updates=846, lr=1.0152e-05, gnorm=2.717, train_wall=6, gb_free=9.2, wall=6711
2024-09-04 09:00:47 | INFO | train_inner | epoch 053:     16 / 16 loss=10.279, nll_loss=8.167, ppl=287.38, wps=1551.3, ups=0.36, wpb=4338, bsz=84, num_updates=848, lr=1.0176e-05, gnorm=2.966, train_wall=6, gb_free=12.2, wall=6716
2024-09-04 09:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18894.140625Mb; avail=236135.7109375Mb
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000740
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18894.6328125Mb; avail=236135.21875Mb
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012586
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18894.6328125Mb; avail=236135.21875Mb
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011175
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024861
2024-09-04 09:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18894.6328125Mb; avail=236135.21875Mb
2024-09-04 09:01:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 52 @ 1092 updates, score 9.473) (writing took 65.4609282752499 seconds)
2024-09-04 09:01:00 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-09-04 09:01:00 | INFO | train | epoch 052 | loss 8.404 | nll_loss 5.675 | ppl 51.08 | wps 703.5 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 1092 | lr 1.3104e-05 | gnorm 2.225 | train_wall 54 | gb_free 12.5 | wall 7217
2024-09-04 09:01:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:01:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:01:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:01:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000716
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18926.50390625Mb; avail=236103.30859375Mb
2024-09-04 09:01:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000590
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18926.50390625Mb; avail=236103.30859375Mb
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18926.50390625Mb; avail=236103.30859375Mb
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000172
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001051
2024-09-04 09:01:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18926.50390625Mb; avail=236103.30859375Mb
2024-09-04 09:01:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:01:00 | INFO | fairseq.trainer | begin training epoch 53
2024-09-04 09:01:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:01:01 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 11.074 | nll_loss 9.086 | ppl 543.41 | wps 3838.4 | wpb 2070.5 | bsz 122.7 | num_updates 848 | best_loss 11.074
2024-09-04 09:01:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 848 updates
2024-09-04 09:01:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:01:06 | INFO | train_inner | epoch 053:      2 / 21 loss=8.435, nll_loss=5.703, ppl=52.1, wps=113.1, ups=0.02, wpb=4945.5, bsz=112, num_updates=1094, lr=1.3128e-05, gnorm=2.209, train_wall=6, gb_free=11.8, wall=7223
2024-09-04 09:01:12 | INFO | train_inner | epoch 053:      4 / 21 loss=8.64, nll_loss=5.961, ppl=62.31, wps=1646.4, ups=0.35, wpb=4747, bsz=80, num_updates=1096, lr=1.3152e-05, gnorm=2.351, train_wall=6, gb_free=13.4, wall=7228
2024-09-04 09:01:17 | INFO | train_inner | epoch 053:      6 / 21 loss=8.243, nll_loss=5.471, ppl=44.34, wps=1639.3, ups=0.34, wpb=4814.5, bsz=104, num_updates=1098, lr=1.3176e-05, gnorm=2.058, train_wall=6, gb_free=12.5, wall=7234
2024-09-04 09:01:22 | INFO | train_inner | epoch 053:      8 / 21 loss=8.352, nll_loss=5.611, ppl=48.88, wps=2041.1, ups=0.42, wpb=4834.5, bsz=100, num_updates=1100, lr=1.32e-05, gnorm=1.91, train_wall=5, gb_free=13.1, wall=7239
2024-09-04 09:01:26 | INFO | train_inner | epoch 053:     10 / 21 loss=8.096, nll_loss=5.301, ppl=39.41, wps=1889.2, ups=0.51, wpb=3689.5, bsz=101, num_updates=1102, lr=1.3224e-05, gnorm=2.773, train_wall=4, gb_free=16.3, wall=7243
2024-09-04 09:01:32 | INFO | train_inner | epoch 053:     12 / 21 loss=8, nll_loss=5.145, ppl=35.39, wps=1441.5, ups=0.35, wpb=4135.5, bsz=140, num_updates=1104, lr=1.3248e-05, gnorm=1.899, train_wall=6, gb_free=12.1, wall=7249
2024-09-04 09:01:37 | INFO | train_inner | epoch 053:     14 / 21 loss=8.329, nll_loss=5.562, ppl=47.24, wps=1847.3, ups=0.4, wpb=4583, bsz=108, num_updates=1106, lr=1.3272e-05, gnorm=2.091, train_wall=5, gb_free=13.2, wall=7254
2024-09-04 09:01:41 | INFO | train_inner | epoch 053:     16 / 21 loss=8.384, nll_loss=5.651, ppl=50.24, wps=1969.8, ups=0.43, wpb=4560, bsz=92, num_updates=1108, lr=1.3296e-05, gnorm=2.451, train_wall=5, gb_free=12, wall=7258
2024-09-04 09:01:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:01:47 | INFO | train_inner | epoch 053:     18 / 21 loss=8.562, nll_loss=5.872, ppl=58.56, wps=1588.1, ups=0.35, wpb=4596, bsz=76, num_updates=1110, lr=1.332e-05, gnorm=2.307, train_wall=6, gb_free=13.4, wall=7264
2024-09-04 09:01:53 | INFO | train_inner | epoch 053:     20 / 21 loss=8.506, nll_loss=5.799, ppl=55.67, wps=1750.3, ups=0.37, wpb=4719, bsz=72, num_updates=1112, lr=1.3344e-05, gnorm=2.078, train_wall=5, gb_free=15, wall=7270
2024-09-04 09:01:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24354.30859375Mb; avail=230675.01171875Mb
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000746
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24354.80078125Mb; avail=230675.01171875Mb
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012855
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24354.80078125Mb; avail=230675.01171875Mb
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011281
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025237
2024-09-04 09:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24354.80078125Mb; avail=230675.01171875Mb
2024-09-04 09:02:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 53 @ 848 updates, score 11.074) (writing took 66.18020849954337 seconds)
2024-09-04 09:02:07 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-09-04 09:02:07 | INFO | train | epoch 053 | loss 10.014 | nll_loss 7.83 | ppl 227.55 | wps 539.4 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 848 | lr 1.0176e-05 | gnorm 2.651 | train_wall 45 | gb_free 12.2 | wall 6797
2024-09-04 09:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000723
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24392.03515625Mb; avail=230637.703125Mb
2024-09-04 09:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000604
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24392.03515625Mb; avail=230637.703125Mb
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24392.03515625Mb; avail=230637.703125Mb
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000178
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001099
2024-09-04 09:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24392.03515625Mb; avail=230637.703125Mb
2024-09-04 09:02:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:02:07 | INFO | fairseq.trainer | begin training epoch 54
2024-09-04 09:02:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:02:12 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.507 | nll_loss 6.954 | ppl 123.97 | wps 4992 | wpb 2350.9 | bsz 94.7 | num_updates 1113 | best_loss 9.473
2024-09-04 09:02:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1113 updates
2024-09-04 09:02:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:02:13 | INFO | train_inner | epoch 054:      2 / 16 loss=9.862, nll_loss=7.635, ppl=198.72, wps=104.8, ups=0.02, wpb=4541.5, bsz=164, num_updates=850, lr=1.02e-05, gnorm=2.176, train_wall=6, gb_free=9.6, wall=6803
2024-09-04 09:02:19 | INFO | train_inner | epoch 054:      4 / 16 loss=10.117, nll_loss=7.961, ppl=249.14, wps=1634, ups=0.35, wpb=4696.5, bsz=112, num_updates=852, lr=1.0224e-05, gnorm=2.559, train_wall=6, gb_free=9.4, wall=6808
2024-09-04 09:02:25 | INFO | train_inner | epoch 054:      6 / 16 loss=9.604, nll_loss=7.283, ppl=155.76, wps=1423.5, ups=0.36, wpb=3950.5, bsz=132, num_updates=854, lr=1.0248e-05, gnorm=2.43, train_wall=6, gb_free=10.8, wall=6814
2024-09-04 09:02:29 | INFO | train_inner | epoch 054:      8 / 16 loss=10.134, nll_loss=7.99, ppl=254.29, wps=1489.6, ups=0.41, wpb=3643, bsz=81, num_updates=856, lr=1.0272e-05, gnorm=2.708, train_wall=5, gb_free=12, wall=6819
2024-09-04 09:02:35 | INFO | train_inner | epoch 054:     10 / 16 loss=9.951, nll_loss=7.754, ppl=215.86, wps=1487.6, ups=0.34, wpb=4374.5, bsz=136, num_updates=858, lr=1.0296e-05, gnorm=2.096, train_wall=6, gb_free=10.4, wall=6825
2024-09-04 09:02:41 | INFO | train_inner | epoch 054:     12 / 16 loss=10.02, nll_loss=7.826, ppl=226.9, wps=1509.9, ups=0.34, wpb=4445.5, bsz=116, num_updates=860, lr=1.032e-05, gnorm=2.198, train_wall=6, gb_free=8.6, wall=6831
2024-09-04 09:02:47 | INFO | train_inner | epoch 054:     14 / 16 loss=9.809, nll_loss=7.551, ppl=187.47, wps=1487.9, ups=0.35, wpb=4301, bsz=152, num_updates=862, lr=1.0344e-05, gnorm=2.66, train_wall=6, gb_free=13.6, wall=6836
2024-09-04 09:02:52 | INFO | train_inner | epoch 054:     16 / 16 loss=9.978, nll_loss=7.77, ppl=218.33, wps=1472.4, ups=0.37, wpb=3939, bsz=124, num_updates=864, lr=1.0368e-05, gnorm=2.604, train_wall=5, gb_free=10, wall=6842
2024-09-04 09:02:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37775.34375Mb; avail=217254.96875Mb
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000749
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37775.34375Mb; avail=217254.4765625Mb
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012890
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37774.8515625Mb; avail=217254.96875Mb
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011138
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025166
2024-09-04 09:02:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37774.8515625Mb; avail=217254.96875Mb
2024-09-04 09:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:03:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 53 @ 1113 updates, score 9.507) (writing took 49.22130819223821 seconds)
2024-09-04 09:03:01 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-09-04 09:03:01 | INFO | train | epoch 053 | loss 8.378 | nll_loss 5.635 | ppl 49.7 | wps 793 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1113 | lr 1.3356e-05 | gnorm 2.205 | train_wall 55 | gb_free 12.3 | wall 7338
2024-09-04 09:03:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:03:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:03:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:03:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000589
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22847.85546875Mb; avail=232181.91015625Mb
2024-09-04 09:03:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000067
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000602
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22847.85546875Mb; avail=232181.91015625Mb
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22847.85546875Mb; avail=232181.91015625Mb
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000181
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001162
2024-09-04 09:03:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22847.85546875Mb; avail=232181.91015625Mb
2024-09-04 09:03:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:03:01 | INFO | fairseq.trainer | begin training epoch 54
2024-09-04 09:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:03:03 | INFO | train_inner | epoch 054:      1 / 21 loss=8.581, nll_loss=5.878, ppl=58.8, wps=130.5, ups=0.03, wpb=4604.5, bsz=64, num_updates=1114, lr=1.3368e-05, gnorm=2.039, train_wall=5, gb_free=13.3, wall=7340
2024-09-04 09:03:07 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 11.124 | nll_loss 9.127 | ppl 559.12 | wps 3837.3 | wpb 2070.5 | bsz 122.7 | num_updates 864 | best_loss 11.074
2024-09-04 09:03:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 864 updates
2024-09-04 09:03:07 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:03:08 | INFO | train_inner | epoch 054:      3 / 21 loss=8.595, nll_loss=5.915, ppl=60.32, wps=1971.4, ups=0.46, wpb=4325, bsz=68, num_updates=1116, lr=1.3392e-05, gnorm=2.021, train_wall=4, gb_free=12.6, wall=7344
2024-09-04 09:03:12 | INFO | train_inner | epoch 054:      5 / 21 loss=8.291, nll_loss=5.525, ppl=46.05, wps=2027.3, ups=0.43, wpb=4698.5, bsz=100, num_updates=1118, lr=1.3416e-05, gnorm=1.607, train_wall=5, gb_free=11.7, wall=7349
2024-09-04 09:03:18 | INFO | train_inner | epoch 054:      7 / 21 loss=8.281, nll_loss=5.519, ppl=45.86, wps=1651.2, ups=0.35, wpb=4748.5, bsz=100, num_updates=1120, lr=1.344e-05, gnorm=1.833, train_wall=6, gb_free=12.7, wall=7355
2024-09-04 09:03:23 | INFO | train_inner | epoch 054:      9 / 21 loss=8.356, nll_loss=5.609, ppl=48.82, wps=1666.6, ups=0.37, wpb=4474, bsz=96, num_updates=1122, lr=1.3464e-05, gnorm=2.339, train_wall=5, gb_free=17.2, wall=7360
2024-09-04 09:03:28 | INFO | train_inner | epoch 054:     11 / 21 loss=8.479, nll_loss=5.751, ppl=53.86, wps=1667, ups=0.39, wpb=4261, bsz=92, num_updates=1124, lr=1.3488e-05, gnorm=2.204, train_wall=5, gb_free=13.7, wall=7365
2024-09-04 09:03:33 | INFO | train_inner | epoch 054:     13 / 21 loss=7.888, nll_loss=5.009, ppl=32.2, wps=1541.2, ups=0.42, wpb=3651.5, bsz=101, num_updates=1126, lr=1.3512e-05, gnorm=2.085, train_wall=5, gb_free=17.2, wall=7370
2024-09-04 09:03:38 | INFO | train_inner | epoch 054:     15 / 21 loss=8.364, nll_loss=5.623, ppl=49.28, wps=1914.1, ups=0.41, wpb=4671, bsz=100, num_updates=1128, lr=1.3536e-05, gnorm=2.431, train_wall=5, gb_free=12.4, wall=7375
2024-09-04 09:03:43 | INFO | train_inner | epoch 054:     17 / 21 loss=8.481, nll_loss=5.772, ppl=54.63, wps=1955.9, ups=0.41, wpb=4734, bsz=88, num_updates=1130, lr=1.356e-05, gnorm=2.129, train_wall=5, gb_free=14.2, wall=7380
2024-09-04 09:03:49 | INFO | train_inner | epoch 054:     19 / 21 loss=8.089, nll_loss=5.259, ppl=38.28, wps=1650, ups=0.34, wpb=4876.5, bsz=152, num_updates=1132, lr=1.3584e-05, gnorm=2.161, train_wall=6, gb_free=12.8, wall=7386
2024-09-04 09:03:55 | INFO | train_inner | epoch 054:     21 / 21 loss=8.412, nll_loss=5.664, ppl=50.72, wps=1707.1, ups=0.34, wpb=5013, bsz=88, num_updates=1134, lr=1.3608e-05, gnorm=2.024, train_wall=6, gb_free=10.7, wall=7392
2024-09-04 09:03:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34517.5703125Mb; avail=220511.35546875Mb
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000762
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34522.984375Mb; avail=220508.40234375Mb
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012769
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34521.5078125Mb; avail=220507.91015625Mb
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011178
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025075
2024-09-04 09:03:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34520.03125Mb; avail=220509.87890625Mb
2024-09-04 09:03:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:03:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 54 @ 864 updates, score 11.124) (writing took 49.500287441536784 seconds)
2024-09-04 09:03:56 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-09-04 09:03:56 | INFO | train | epoch 054 | loss 9.935 | nll_loss 7.723 | ppl 211.23 | wps 621.5 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 864 | lr 1.0368e-05 | gnorm 2.429 | train_wall 45 | gb_free 10 | wall 6906
2024-09-04 09:03:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:03:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:03:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:03:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000576
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22205.27734375Mb; avail=232824.62109375Mb
2024-09-04 09:03:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000564
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22205.27734375Mb; avail=232824.62109375Mb
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22205.27734375Mb; avail=232824.62109375Mb
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000150
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001026
2024-09-04 09:03:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22205.27734375Mb; avail=232824.62109375Mb
2024-09-04 09:03:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:03:56 | INFO | fairseq.trainer | begin training epoch 55
2024-09-04 09:03:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:04:02 | INFO | train_inner | epoch 055:      2 / 16 loss=9.952, nll_loss=7.733, ppl=212.7, wps=125, ups=0.03, wpb=4360.5, bsz=128, num_updates=866, lr=1.0392e-05, gnorm=2.815, train_wall=6, gb_free=10.7, wall=6912
2024-09-04 09:04:08 | INFO | train_inner | epoch 055:      4 / 16 loss=9.856, nll_loss=7.622, ppl=197.03, wps=1544.6, ups=0.35, wpb=4398.5, bsz=136, num_updates=868, lr=1.0416e-05, gnorm=2.356, train_wall=6, gb_free=9.4, wall=6917
2024-09-04 09:04:12 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.46 | nll_loss 6.906 | ppl 119.93 | wps 4701.1 | wpb 2350.9 | bsz 94.7 | num_updates 1134 | best_loss 9.46
2024-09-04 09:04:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1134 updates
2024-09-04 09:04:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:04:14 | INFO | train_inner | epoch 055:      6 / 16 loss=9.728, nll_loss=7.438, ppl=173.35, wps=1499.2, ups=0.33, wpb=4604.5, bsz=172, num_updates=870, lr=1.044e-05, gnorm=2.233, train_wall=6, gb_free=11, wall=6923
2024-09-04 09:04:19 | INFO | train_inner | epoch 055:      8 / 16 loss=9.53, nll_loss=7.187, ppl=145.75, wps=1462.4, ups=0.36, wpb=4037.5, bsz=156, num_updates=872, lr=1.0464e-05, gnorm=2.169, train_wall=6, gb_free=12.2, wall=6929
2024-09-04 09:04:26 | INFO | train_inner | epoch 055:     10 / 16 loss=9.846, nll_loss=7.602, ppl=194.28, wps=1433.8, ups=0.33, wpb=4374, bsz=128, num_updates=874, lr=1.0488e-05, gnorm=2.094, train_wall=6, gb_free=9.2, wall=6935
2024-09-04 09:04:31 | INFO | train_inner | epoch 055:     12 / 16 loss=10.046, nll_loss=7.85, ppl=230.78, wps=1486, ups=0.36, wpb=4124.5, bsz=92, num_updates=876, lr=1.0512e-05, gnorm=2.539, train_wall=6, gb_free=10.5, wall=6941
2024-09-04 09:04:37 | INFO | train_inner | epoch 055:     14 / 16 loss=10.086, nll_loss=7.911, ppl=240.65, wps=1507.2, ups=0.36, wpb=4210, bsz=84, num_updates=878, lr=1.0536e-05, gnorm=2.229, train_wall=6, gb_free=10.1, wall=6946
2024-09-04 09:04:42 | INFO | train_inner | epoch 055:     16 / 16 loss=9.778, nll_loss=7.511, ppl=182.35, wps=1425.8, ups=0.38, wpb=3782, bsz=121, num_updates=880, lr=1.056e-05, gnorm=2.361, train_wall=5, gb_free=12.3, wall=6952
2024-09-04 09:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39370.81640625Mb; avail=215659.0390625Mb
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39371.80078125Mb; avail=215658.0546875Mb
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012614
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39371.80078125Mb; avail=215658.546875Mb
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011227
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024935
2024-09-04 09:04:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39371.80078125Mb; avail=215658.546875Mb
2024-09-04 09:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:04:56 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 11.065 | nll_loss 9.057 | ppl 532.81 | wps 3828.1 | wpb 2070.5 | bsz 122.7 | num_updates 880 | best_loss 11.065
2024-09-04 09:04:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 880 updates
2024-09-04 09:04:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:05:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 54 @ 1134 updates, score 9.46) (writing took 62.86400059051812 seconds)
2024-09-04 09:05:15 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-09-04 09:05:15 | INFO | train | epoch 054 | loss 8.339 | nll_loss 5.584 | ppl 47.98 | wps 713.7 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 1134 | lr 1.3608e-05 | gnorm 2.08 | train_wall 54 | gb_free 10.7 | wall 7472
2024-09-04 09:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000720
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39399.96484375Mb; avail=215629.796875Mb
2024-09-04 09:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000058
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000651
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39399.96484375Mb; avail=215629.796875Mb
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39399.96484375Mb; avail=215629.796875Mb
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000239
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001270
2024-09-04 09:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39399.96484375Mb; avail=215629.796875Mb
2024-09-04 09:05:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:05:15 | INFO | fairseq.trainer | begin training epoch 55
2024-09-04 09:05:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:05:20 | INFO | train_inner | epoch 055:      2 / 21 loss=8.064, nll_loss=5.245, ppl=37.92, wps=112.3, ups=0.02, wpb=4804, bsz=136, num_updates=1136, lr=1.3632e-05, gnorm=2.42, train_wall=5, gb_free=14.2, wall=7477
2024-09-04 09:05:26 | INFO | train_inner | epoch 055:      4 / 21 loss=7.992, nll_loss=5.147, ppl=35.43, wps=1541.6, ups=0.36, wpb=4249.5, bsz=136, num_updates=1138, lr=1.3656e-05, gnorm=2.253, train_wall=6, gb_free=13.3, wall=7483
2024-09-04 09:05:30 | INFO | train_inner | epoch 055:      6 / 21 loss=8.239, nll_loss=5.459, ppl=43.98, wps=1762.3, ups=0.49, wpb=3564.5, bsz=73, num_updates=1140, lr=1.368e-05, gnorm=2.158, train_wall=4, gb_free=13.7, wall=7487
2024-09-04 09:05:36 | INFO | train_inner | epoch 055:      8 / 21 loss=8.299, nll_loss=5.535, ppl=46.38, wps=1717.2, ups=0.34, wpb=5073, bsz=112, num_updates=1142, lr=1.3704e-05, gnorm=1.989, train_wall=6, gb_free=13.9, wall=7493
2024-09-04 09:05:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:05:41 | INFO | train_inner | epoch 055:     10 / 21 loss=8.581, nll_loss=5.888, ppl=59.22, wps=2027, ups=0.41, wpb=4925.5, bsz=80, num_updates=1144, lr=1.3728e-05, gnorm=2.5, train_wall=5, gb_free=13, wall=7497
2024-09-04 09:05:46 | INFO | train_inner | epoch 055:     12 / 21 loss=8.388, nll_loss=5.65, ppl=50.23, wps=1644.2, ups=0.38, wpb=4287.5, bsz=76, num_updates=1146, lr=1.3752e-05, gnorm=2.637, train_wall=5, gb_free=13.2, wall=7503
2024-09-04 09:05:51 | INFO | train_inner | epoch 055:     14 / 21 loss=8.289, nll_loss=5.517, ppl=45.8, wps=1923.1, ups=0.42, wpb=4603.5, bsz=84, num_updates=1148, lr=1.3776e-05, gnorm=2.053, train_wall=5, gb_free=12.7, wall=7507
2024-09-04 09:05:56 | INFO | train_inner | epoch 055:     16 / 21 loss=8.62, nll_loss=5.93, ppl=60.97, wps=1489.8, ups=0.35, wpb=4308, bsz=68, num_updates=1150, lr=1.38e-05, gnorm=2.913, train_wall=6, gb_free=11.8, wall=7513
2024-09-04 09:06:01 | INFO | train_inner | epoch 055:     18 / 21 loss=8.298, nll_loss=5.53, ppl=46.2, wps=1930.3, ups=0.43, wpb=4537, bsz=92, num_updates=1152, lr=1.3824e-05, gnorm=2.376, train_wall=5, gb_free=12.6, wall=7518
2024-09-04 09:06:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 55 @ 880 updates, score 11.065) (writing took 65.97394709754735 seconds)
2024-09-04 09:06:02 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-09-04 09:06:02 | INFO | train | epoch 055 | loss 9.854 | nll_loss 7.608 | ppl 195.1 | wps 537.3 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 880 | lr 1.056e-05 | gnorm 2.349 | train_wall 46 | gb_free 12.3 | wall 7032
2024-09-04 09:06:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:06:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:06:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:06:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000728
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25156.9296875Mb; avail=229872.91015625Mb
2024-09-04 09:06:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000598
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25156.9296875Mb; avail=229872.91015625Mb
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25156.9296875Mb; avail=229872.91015625Mb
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000174
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001090
2024-09-04 09:06:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25156.9296875Mb; avail=229872.91015625Mb
2024-09-04 09:06:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:06:02 | INFO | fairseq.trainer | begin training epoch 56
2024-09-04 09:06:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:06:06 | INFO | train_inner | epoch 055:     20 / 21 loss=8.334, nll_loss=5.584, ppl=47.96, wps=2026.7, ups=0.42, wpb=4851, bsz=120, num_updates=1154, lr=1.3848e-05, gnorm=2.105, train_wall=5, gb_free=13.9, wall=7523
2024-09-04 09:06:09 | INFO | train_inner | epoch 056:      2 / 16 loss=9.919, nll_loss=7.693, ppl=206.91, wps=105.9, ups=0.02, wpb=4582, bsz=96, num_updates=882, lr=1.0584e-05, gnorm=2.697, train_wall=6, gb_free=9.9, wall=7038
2024-09-04 09:06:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25194.28515625Mb; avail=229835.51953125Mb
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000780
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25194.77734375Mb; avail=229835.02734375Mb
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012744
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25194.77734375Mb; avail=229835.02734375Mb
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011145
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025027
2024-09-04 09:06:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25194.77734375Mb; avail=229835.02734375Mb
2024-09-04 09:06:14 | INFO | train_inner | epoch 056:      4 / 16 loss=9.888, nll_loss=7.657, ppl=201.83, wps=1539.4, ups=0.37, wpb=4171.5, bsz=136, num_updates=884, lr=1.0608e-05, gnorm=2.484, train_wall=5, gb_free=8.9, wall=7043
2024-09-04 09:06:20 | INFO | train_inner | epoch 056:      6 / 16 loss=9.939, nll_loss=7.72, ppl=210.81, wps=1429.8, ups=0.36, wpb=4016, bsz=88, num_updates=886, lr=1.0632e-05, gnorm=2.402, train_wall=6, gb_free=13.4, wall=7049
2024-09-04 09:06:26 | INFO | train_inner | epoch 056:      8 / 16 loss=9.783, nll_loss=7.515, ppl=182.97, wps=1546.1, ups=0.33, wpb=4711, bsz=152, num_updates=888, lr=1.0656e-05, gnorm=2.031, train_wall=6, gb_free=8.5, wall=7055
2024-09-04 09:06:26 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.473 | nll_loss 6.926 | ppl 121.57 | wps 4764.9 | wpb 2350.9 | bsz 94.7 | num_updates 1155 | best_loss 9.46
2024-09-04 09:06:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1155 updates
2024-09-04 09:06:26 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:06:31 | INFO | train_inner | epoch 056:     10 / 16 loss=9.881, nll_loss=7.636, ppl=198.89, wps=1575.7, ups=0.35, wpb=4562.5, bsz=128, num_updates=890, lr=1.068e-05, gnorm=2.423, train_wall=6, gb_free=10.7, wall=7061
2024-09-04 09:06:37 | INFO | train_inner | epoch 056:     12 / 16 loss=9.76, nll_loss=7.476, ppl=177.99, wps=1453.5, ups=0.39, wpb=3702.5, bsz=109, num_updates=892, lr=1.0704e-05, gnorm=2.388, train_wall=5, gb_free=11.2, wall=7066
2024-09-04 09:06:42 | INFO | train_inner | epoch 056:     14 / 16 loss=9.835, nll_loss=7.577, ppl=190.95, wps=1490, ups=0.36, wpb=4173.5, bsz=124, num_updates=894, lr=1.0728e-05, gnorm=2.053, train_wall=6, gb_free=11.3, wall=7072
2024-09-04 09:06:48 | INFO | train_inner | epoch 056:     16 / 16 loss=9.211, nll_loss=6.762, ppl=108.51, wps=1372.7, ups=0.35, wpb=3972.5, bsz=184, num_updates=896, lr=1.0752e-05, gnorm=2.049, train_wall=6, gb_free=9.4, wall=7077
2024-09-04 09:06:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42360.75Mb; avail=212669.04296875Mb
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42360.75Mb; avail=212669.04296875Mb
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.013123
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42360.75Mb; avail=212669.04296875Mb
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011129
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025342
2024-09-04 09:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42360.75Mb; avail=212669.04296875Mb
2024-09-04 09:07:02 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 11.025 | nll_loss 8.994 | ppl 509.82 | wps 3838 | wpb 2070.5 | bsz 122.7 | num_updates 896 | best_loss 11.025
2024-09-04 09:07:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 896 updates
2024-09-04 09:07:02 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:07:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 55 @ 1155 updates, score 9.473) (writing took 41.5658123716712 seconds)
2024-09-04 09:07:08 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-09-04 09:07:08 | INFO | train | epoch 055 | loss 8.318 | nll_loss 5.557 | ppl 47.08 | wps 846.1 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1155 | lr 1.386e-05 | gnorm 2.35 | train_wall 54 | gb_free 12.6 | wall 7585
2024-09-04 09:07:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:07:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:07:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:07:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000699
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38661.16796875Mb; avail=216368.08984375Mb
2024-09-04 09:07:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000595
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38662.64453125Mb; avail=216367.10546875Mb
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38663.13671875Mb; avail=216366.61328125Mb
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001065
2024-09-04 09:07:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38663.62890625Mb; avail=216366.12109375Mb
2024-09-04 09:07:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:07:08 | INFO | fairseq.trainer | begin training epoch 56
2024-09-04 09:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:07:11 | INFO | train_inner | epoch 056:      1 / 21 loss=8.414, nll_loss=5.666, ppl=50.77, wps=147.8, ups=0.03, wpb=4800.5, bsz=64, num_updates=1156, lr=1.3872e-05, gnorm=2.116, train_wall=6, gb_free=12.7, wall=7588
2024-09-04 09:07:16 | INFO | train_inner | epoch 056:      3 / 21 loss=8.416, nll_loss=5.675, ppl=51.08, wps=1809.5, ups=0.39, wpb=4587, bsz=84, num_updates=1158, lr=1.3896e-05, gnorm=2.53, train_wall=5, gb_free=14.1, wall=7593
2024-09-04 09:07:35 | INFO | train_inner | epoch 056:      5 / 21 loss=8.304, nll_loss=5.524, ppl=46.01, wps=422.9, ups=0.1, wpb=4055, bsz=73, num_updates=1160, lr=1.392e-05, gnorm=2.47, train_wall=19, gb_free=12.6, wall=7612
2024-09-04 09:07:40 | INFO | train_inner | epoch 056:      7 / 21 loss=8.432, nll_loss=5.691, ppl=51.68, wps=1853.8, ups=0.42, wpb=4381.5, bsz=80, num_updates=1162, lr=1.3944e-05, gnorm=2.319, train_wall=5, gb_free=12.7, wall=7617
2024-09-04 09:07:46 | INFO | train_inner | epoch 056:      9 / 21 loss=8.247, nll_loss=5.487, ppl=44.84, wps=1654.5, ups=0.35, wpb=4795.5, bsz=108, num_updates=1164, lr=1.3968e-05, gnorm=2.444, train_wall=6, gb_free=13.2, wall=7622
2024-09-04 09:07:51 | INFO | train_inner | epoch 056:     11 / 21 loss=7.983, nll_loss=5.139, ppl=35.23, wps=1993.1, ups=0.4, wpb=4982.5, bsz=148, num_updates=1166, lr=1.3992e-05, gnorm=2.127, train_wall=5, gb_free=12.2, wall=7627
2024-09-04 09:07:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:07:56 | INFO | train_inner | epoch 056:     13 / 21 loss=8.279, nll_loss=5.498, ppl=45.19, wps=1563.4, ups=0.35, wpb=4478.5, bsz=112, num_updates=1168, lr=1.4016e-05, gnorm=2.386, train_wall=6, gb_free=14, wall=7633
2024-09-04 09:08:01 | INFO | train_inner | epoch 056:     15 / 21 loss=8.407, nll_loss=5.652, ppl=50.28, wps=2066.9, ups=0.45, wpb=4602, bsz=80, num_updates=1170, lr=1.404e-05, gnorm=1.997, train_wall=4, gb_free=12.6, wall=7638
2024-09-04 09:08:06 | INFO | train_inner | epoch 056:     17 / 21 loss=7.877, nll_loss=4.998, ppl=31.96, wps=1558.2, ups=0.35, wpb=4491, bsz=136, num_updates=1172, lr=1.4064e-05, gnorm=2.224, train_wall=6, gb_free=12.2, wall=7643
2024-09-04 09:08:12 | INFO | train_inner | epoch 056:     19 / 21 loss=8.361, nll_loss=5.621, ppl=49.22, wps=1973.7, ups=0.39, wpb=5009, bsz=104, num_updates=1174, lr=1.4088e-05, gnorm=2.463, train_wall=5, gb_free=12.8, wall=7648
2024-09-04 09:08:17 | INFO | train_inner | epoch 056:     21 / 21 loss=8.462, nll_loss=5.735, ppl=53.27, wps=1678.2, ups=0.39, wpb=4296, bsz=68, num_updates=1176, lr=1.4112e-05, gnorm=2.682, train_wall=5, gb_free=15.4, wall=7654
2024-09-04 09:08:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21820.05078125Mb; avail=233209.8828125Mb
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000635
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21820.05078125Mb; avail=233209.8828125Mb
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012659
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21820.05078125Mb; avail=233209.8828125Mb
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011230
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024882
2024-09-04 09:08:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21820.05078125Mb; avail=233209.8828125Mb
2024-09-04 09:08:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 56 @ 896 updates, score 11.025) (writing took 77.32351055461913 seconds)
2024-09-04 09:08:20 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-09-04 09:08:20 | INFO | train | epoch 056 | loss 9.783 | nll_loss 7.512 | ppl 182.55 | wps 493.8 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 896 | lr 1.0752e-05 | gnorm 2.316 | train_wall 45 | gb_free 9.4 | wall 7169
2024-09-04 09:08:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:08:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:08:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:08:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000617
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21849.1484375Mb; avail=233180.7421875Mb
2024-09-04 09:08:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000642
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21849.1484375Mb; avail=233180.7421875Mb
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000052
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21849.1484375Mb; avail=233180.7421875Mb
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000179
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001213
2024-09-04 09:08:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21849.1484375Mb; avail=233180.7421875Mb
2024-09-04 09:08:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:08:20 | INFO | fairseq.trainer | begin training epoch 57
2024-09-04 09:08:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:08:26 | INFO | train_inner | epoch 057:      2 / 16 loss=9.808, nll_loss=7.537, ppl=185.71, wps=95.6, ups=0.02, wpb=4663.5, bsz=120, num_updates=898, lr=1.0776e-05, gnorm=2.512, train_wall=6, gb_free=9.3, wall=7175
2024-09-04 09:08:31 | INFO | train_inner | epoch 057:      4 / 16 loss=9.979, nll_loss=7.758, ppl=216.46, wps=1524.9, ups=0.39, wpb=3943.5, bsz=76, num_updates=900, lr=1.08e-05, gnorm=2.564, train_wall=5, gb_free=13.8, wall=7180
2024-09-04 09:08:34 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.409 | nll_loss 6.832 | ppl 113.94 | wps 4758 | wpb 2350.9 | bsz 94.7 | num_updates 1176 | best_loss 9.409
2024-09-04 09:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1176 updates
2024-09-04 09:08:34 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:08:36 | INFO | train_inner | epoch 057:      6 / 16 loss=9.927, nll_loss=7.695, ppl=207.15, wps=1479.4, ups=0.41, wpb=3581.5, bsz=73, num_updates=902, lr=1.0824e-05, gnorm=2.368, train_wall=5, gb_free=11.2, wall=7185
2024-09-04 09:08:42 | INFO | train_inner | epoch 057:      8 / 16 loss=9.785, nll_loss=7.502, ppl=181.25, wps=1499.7, ups=0.33, wpb=4497, bsz=148, num_updates=904, lr=1.0848e-05, gnorm=2.355, train_wall=6, gb_free=9.9, wall=7191
2024-09-04 09:08:47 | INFO | train_inner | epoch 057:     10 / 16 loss=9.337, nll_loss=6.932, ppl=122.12, wps=1430.4, ups=0.35, wpb=4098, bsz=164, num_updates=906, lr=1.0872e-05, gnorm=2.606, train_wall=6, gb_free=11.4, wall=7197
2024-09-04 09:08:53 | INFO | train_inner | epoch 057:     12 / 16 loss=9.505, nll_loss=7.146, ppl=141.6, wps=1481.2, ups=0.33, wpb=4514, bsz=180, num_updates=908, lr=1.0896e-05, gnorm=2.274, train_wall=6, gb_free=9.8, wall=7203
2024-09-04 09:08:59 | INFO | train_inner | epoch 057:     14 / 16 loss=9.797, nll_loss=7.51, ppl=182.32, wps=1449.4, ups=0.36, wpb=4079, bsz=108, num_updates=910, lr=1.092e-05, gnorm=2.638, train_wall=6, gb_free=10.5, wall=7209
2024-09-04 09:09:05 | INFO | train_inner | epoch 057:     16 / 16 loss=9.745, nll_loss=7.445, ppl=174.3, wps=1536.7, ups=0.34, wpb=4515, bsz=148, num_updates=912, lr=1.0944e-05, gnorm=2.606, train_wall=6, gb_free=10.3, wall=7214
2024-09-04 09:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39012.3359375Mb; avail=216017.48828125Mb
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000769
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39012.3359375Mb; avail=216017.48828125Mb
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.016547
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39012.3359375Mb; avail=216017.48828125Mb
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.014686
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.032457
2024-09-04 09:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39013.5546875Mb; avail=216016.26953125Mb
2024-09-04 09:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:09:19 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 11.048 | nll_loss 9.034 | ppl 524.24 | wps 3818.9 | wpb 2070.5 | bsz 122.7 | num_updates 912 | best_loss 11.025
2024-09-04 09:09:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 912 updates
2024-09-04 09:09:19 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 56 @ 1176 updates, score 9.409) (writing took 63.82806802075356 seconds)
2024-09-04 09:09:38 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-09-04 09:09:38 | INFO | train | epoch 056 | loss 8.281 | nll_loss 5.507 | ppl 45.47 | wps 639.3 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 1176 | lr 1.4112e-05 | gnorm 2.332 | train_wall 69 | gb_free 15.4 | wall 7735
2024-09-04 09:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000670
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42297.25Mb; avail=212732.59375Mb
2024-09-04 09:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000612
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42297.25Mb; avail=212732.59375Mb
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42297.25Mb; avail=212732.59375Mb
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000188
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001103
2024-09-04 09:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42297.25Mb; avail=212732.59375Mb
2024-09-04 09:09:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:09:38 | INFO | fairseq.trainer | begin training epoch 57
2024-09-04 09:09:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:09:43 | INFO | train_inner | epoch 057:      2 / 21 loss=8.356, nll_loss=5.582, ppl=47.9, wps=104.8, ups=0.02, wpb=4507, bsz=88, num_updates=1178, lr=1.4136e-05, gnorm=2.373, train_wall=5, gb_free=14.5, wall=7740
2024-09-04 09:09:47 | INFO | train_inner | epoch 057:      4 / 21 loss=8.453, nll_loss=5.716, ppl=52.55, wps=1976.9, ups=0.42, wpb=4695.5, bsz=68, num_updates=1180, lr=1.416e-05, gnorm=2.526, train_wall=5, gb_free=14.8, wall=7744
2024-09-04 09:09:53 | INFO | train_inner | epoch 057:      6 / 21 loss=8.309, nll_loss=5.549, ppl=46.82, wps=1710.2, ups=0.35, wpb=4905.5, bsz=100, num_updates=1182, lr=1.4184e-05, gnorm=2.468, train_wall=6, gb_free=13, wall=7750
2024-09-04 09:09:57 | INFO | train_inner | epoch 057:      8 / 21 loss=8.37, nll_loss=5.643, ppl=49.98, wps=1924.2, ups=0.47, wpb=4128, bsz=57, num_updates=1184, lr=1.4208e-05, gnorm=3.122, train_wall=4, gb_free=11.5, wall=7754
2024-09-04 09:10:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:10:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 57 @ 912 updates, score 11.048) (writing took 42.48128858022392 seconds)
2024-09-04 09:10:02 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-09-04 09:10:02 | INFO | train | epoch 057 | loss 9.73 | nll_loss 7.434 | ppl 172.92 | wps 663.2 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 912 | lr 1.0944e-05 | gnorm 2.491 | train_wall 45 | gb_free 10.3 | wall 7271
2024-09-04 09:10:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:10:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:10:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:10:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000687
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28044.109375Mb; avail=226985.73046875Mb
2024-09-04 09:10:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000588
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28044.109375Mb; avail=226985.73046875Mb
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28044.109375Mb; avail=226985.73046875Mb
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000158
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001048
2024-09-04 09:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28044.109375Mb; avail=226985.73046875Mb
2024-09-04 09:10:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:10:02 | INFO | fairseq.trainer | begin training epoch 58
2024-09-04 09:10:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:10:03 | INFO | train_inner | epoch 057:     10 / 21 loss=8.322, nll_loss=5.533, ppl=46.29, wps=1820.3, ups=0.39, wpb=4609, bsz=104, num_updates=1186, lr=1.4232e-05, gnorm=2.672, train_wall=5, gb_free=14.7, wall=7759
2024-09-04 09:10:06 | INFO | train_inner | epoch 058:      2 / 16 loss=9.732, nll_loss=7.448, ppl=174.55, wps=98.6, ups=0.03, wpb=3031, bsz=69, num_updates=914, lr=1.0968e-05, gnorm=2.801, train_wall=4, gb_free=14.6, wall=7276
2024-09-04 09:10:08 | INFO | train_inner | epoch 057:     12 / 21 loss=8.264, nll_loss=5.47, ppl=44.34, wps=1713.1, ups=0.35, wpb=4892.5, bsz=80, num_updates=1188, lr=1.4256e-05, gnorm=2.947, train_wall=6, gb_free=12.2, wall=7765
2024-09-04 09:10:12 | INFO | train_inner | epoch 058:      4 / 16 loss=9.787, nll_loss=7.507, ppl=181.92, wps=1523.7, ups=0.33, wpb=4557.5, bsz=132, num_updates=916, lr=1.0992e-05, gnorm=2.718, train_wall=6, gb_free=8, wall=7282
2024-09-04 09:10:14 | INFO | train_inner | epoch 057:     14 / 21 loss=8.238, nll_loss=5.45, ppl=43.72, wps=1632.2, ups=0.36, wpb=4525.5, bsz=104, num_updates=1190, lr=1.428e-05, gnorm=2.884, train_wall=6, gb_free=13.6, wall=7771
2024-09-04 09:10:18 | INFO | train_inner | epoch 058:      6 / 16 loss=9.672, nll_loss=7.361, ppl=164.41, wps=1448.4, ups=0.35, wpb=4189, bsz=124, num_updates=918, lr=1.1016e-05, gnorm=2.369, train_wall=6, gb_free=10.2, wall=7288
2024-09-04 09:10:19 | INFO | train_inner | epoch 057:     16 / 21 loss=8.251, nll_loss=5.494, ppl=45.08, wps=2024.1, ups=0.42, wpb=4816, bsz=124, num_updates=1192, lr=1.4304e-05, gnorm=3.494, train_wall=5, gb_free=11.7, wall=7775
2024-09-04 09:10:24 | INFO | train_inner | epoch 057:     18 / 21 loss=8.084, nll_loss=5.228, ppl=37.48, wps=1833.8, ups=0.4, wpb=4581, bsz=120, num_updates=1194, lr=1.4328e-05, gnorm=2.685, train_wall=5, gb_free=12.5, wall=7780
2024-09-04 09:10:24 | INFO | train_inner | epoch 058:      8 / 16 loss=9.83, nll_loss=7.561, ppl=188.78, wps=1503.7, ups=0.36, wpb=4211, bsz=108, num_updates=920, lr=1.104e-05, gnorm=2.659, train_wall=6, gb_free=9.9, wall=7293
2024-09-04 09:10:29 | INFO | train_inner | epoch 057:     20 / 21 loss=7.996, nll_loss=5.137, ppl=35.18, wps=1505.4, ups=0.35, wpb=4297, bsz=128, num_updates=1196, lr=1.4352e-05, gnorm=2.344, train_wall=6, gb_free=16.4, wall=7786
2024-09-04 09:10:30 | INFO | train_inner | epoch 058:     10 / 16 loss=9.663, nll_loss=7.335, ppl=161.5, wps=1502.6, ups=0.34, wpb=4465.5, bsz=140, num_updates=922, lr=1.1064e-05, gnorm=2.698, train_wall=6, gb_free=9.2, wall=7299
2024-09-04 09:10:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28088.46484375Mb; avail=226941.33203125Mb
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000747
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28088.46484375Mb; avail=226941.33203125Mb
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012825
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28088.46484375Mb; avail=226941.33203125Mb
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011291
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025241
2024-09-04 09:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28088.95703125Mb; avail=226940.83984375Mb
2024-09-04 09:10:36 | INFO | train_inner | epoch 058:     12 / 16 loss=9.75, nll_loss=7.457, ppl=175.71, wps=1562, ups=0.34, wpb=4594, bsz=120, num_updates=924, lr=1.1088e-05, gnorm=3.005, train_wall=6, gb_free=9.7, wall=7305
2024-09-04 09:10:41 | INFO | train_inner | epoch 058:     14 / 16 loss=9.782, nll_loss=7.476, ppl=177.99, wps=1561.3, ups=0.34, wpb=4626, bsz=124, num_updates=926, lr=1.1112e-05, gnorm=2.405, train_wall=6, gb_free=9.1, wall=7311
2024-09-04 09:10:48 | INFO | train_inner | epoch 058:     16 / 16 loss=9.167, nll_loss=6.685, ppl=102.86, wps=1363.9, ups=0.32, wpb=4217.5, bsz=200, num_updates=928, lr=1.1136e-05, gnorm=2.743, train_wall=6, gb_free=8.1, wall=7317
2024-09-04 09:10:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28119.65625Mb; avail=226910.10546875Mb
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000736
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28119.65625Mb; avail=226910.10546875Mb
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012591
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28120.1484375Mb; avail=226909.61328125Mb
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011210
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024906
2024-09-04 09:10:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28120.1484375Mb; avail=226909.61328125Mb
2024-09-04 09:10:49 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.445 | nll_loss 6.903 | ppl 119.71 | wps 4679.1 | wpb 2350.9 | bsz 94.7 | num_updates 1197 | best_loss 9.409
2024-09-04 09:10:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1197 updates
2024-09-04 09:10:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:11:02 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 10.991 | nll_loss 8.947 | ppl 493.36 | wps 3827.6 | wpb 2070.5 | bsz 122.7 | num_updates 928 | best_loss 10.991
2024-09-04 09:11:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 928 updates
2024-09-04 09:11:02 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:11:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:11:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 57 @ 1197 updates, score 9.445) (writing took 44.37150888051838 seconds)
2024-09-04 09:11:33 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-09-04 09:11:33 | INFO | train | epoch 057 | loss 8.268 | nll_loss 5.484 | ppl 44.77 | wps 827.9 | ups 0.18 | wpb 4556.3 | bsz 96.9 | num_updates 1197 | lr 1.4364e-05 | gnorm 2.738 | train_wall 54 | gb_free 16.8 | wall 7850
2024-09-04 09:11:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:11:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:11:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:11:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000591
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36281.5546875Mb; avail=218747.89453125Mb
2024-09-04 09:11:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000059
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000608
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36282.046875Mb; avail=218748.38671875Mb
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36281.5546875Mb; avail=218748.38671875Mb
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000176
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001149
2024-09-04 09:11:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36281.5546875Mb; avail=218748.38671875Mb
2024-09-04 09:11:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:11:33 | INFO | fairseq.trainer | begin training epoch 58
2024-09-04 09:11:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:11:36 | INFO | train_inner | epoch 058:      1 / 21 loss=8.249, nll_loss=5.468, ppl=44.27, wps=128.6, ups=0.03, wpb=4284, bsz=92, num_updates=1198, lr=1.4376e-05, gnorm=2.535, train_wall=5, gb_free=12.4, wall=7853
2024-09-04 09:11:42 | INFO | train_inner | epoch 058:      3 / 21 loss=8.183, nll_loss=5.382, ppl=41.69, wps=1641.2, ups=0.33, wpb=4930, bsz=116, num_updates=1200, lr=1.44e-05, gnorm=1.989, train_wall=6, gb_free=13.7, wall=7859
2024-09-04 09:11:46 | INFO | train_inner | epoch 058:      5 / 21 loss=8.156, nll_loss=5.356, ppl=40.96, wps=1877.2, ups=0.45, wpb=4197.5, bsz=81, num_updates=1202, lr=1.4424e-05, gnorm=2.2, train_wall=4, gb_free=12.8, wall=7863
2024-09-04 09:11:52 | INFO | train_inner | epoch 058:      7 / 21 loss=8.245, nll_loss=5.459, ppl=43.97, wps=1743.6, ups=0.34, wpb=5064.5, bsz=100, num_updates=1204, lr=1.4448e-05, gnorm=1.971, train_wall=6, gb_free=14.4, wall=7869
2024-09-04 09:11:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:11:58 | INFO | train_inner | epoch 058:      9 / 21 loss=8.548, nll_loss=5.828, ppl=56.79, wps=1679.1, ups=0.37, wpb=4577, bsz=56, num_updates=1206, lr=1.4472e-05, gnorm=2.316, train_wall=5, gb_free=14.9, wall=7875
2024-09-04 09:12:03 | INFO | train_inner | epoch 058:     11 / 21 loss=8.261, nll_loss=5.464, ppl=44.15, wps=1586.9, ups=0.36, wpb=4449.5, bsz=108, num_updates=1208, lr=1.4496e-05, gnorm=2.184, train_wall=6, gb_free=14.5, wall=7880
2024-09-04 09:12:09 | INFO | train_inner | epoch 058:     13 / 21 loss=7.981, nll_loss=5.139, ppl=35.24, wps=1524.2, ups=0.35, wpb=4406, bsz=140, num_updates=1210, lr=1.452e-05, gnorm=2.731, train_wall=6, gb_free=11.8, wall=7886
2024-09-04 09:12:15 | INFO | train_inner | epoch 058:     15 / 21 loss=7.981, nll_loss=5.114, ppl=34.63, wps=1572.2, ups=0.33, wpb=4778.5, bsz=132, num_updates=1212, lr=1.4544e-05, gnorm=2.38, train_wall=6, gb_free=11.7, wall=7892
2024-09-04 09:12:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 58 @ 928 updates, score 10.991) (writing took 77.33017328102142 seconds)
2024-09-04 09:12:19 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-09-04 09:12:19 | INFO | train | epoch 058 | loss 9.674 | nll_loss 7.354 | ppl 163.63 | wps 492.7 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 928 | lr 1.1136e-05 | gnorm 2.675 | train_wall 46 | gb_free 8.1 | wall 7409
2024-09-04 09:12:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:12:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:12:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:12:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000608
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22425.328125Mb; avail=232604.64453125Mb
2024-09-04 09:12:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000056
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000584
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22425.328125Mb; avail=232604.64453125Mb
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22425.328125Mb; avail=232604.64453125Mb
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000154
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001067
2024-09-04 09:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22425.328125Mb; avail=232604.64453125Mb
2024-09-04 09:12:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:12:20 | INFO | fairseq.trainer | begin training epoch 59
2024-09-04 09:12:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:12:20 | INFO | train_inner | epoch 058:     17 / 21 loss=8.308, nll_loss=5.517, ppl=45.79, wps=1749.4, ups=0.4, wpb=4322.5, bsz=76, num_updates=1214, lr=1.4568e-05, gnorm=2.349, train_wall=5, gb_free=12.4, wall=7897
2024-09-04 09:12:24 | INFO | train_inner | epoch 058:     19 / 21 loss=8.342, nll_loss=5.579, ppl=47.82, wps=2020, ups=0.45, wpb=4455, bsz=80, num_updates=1216, lr=1.4592e-05, gnorm=3.333, train_wall=4, gb_free=13.6, wall=7901
2024-09-04 09:12:25 | INFO | train_inner | epoch 059:      2 / 16 loss=9.602, nll_loss=7.268, ppl=154.15, wps=86.5, ups=0.02, wpb=4215.5, bsz=144, num_updates=930, lr=1.116e-05, gnorm=2.322, train_wall=6, gb_free=9.6, wall=7415
2024-09-04 09:12:29 | INFO | train_inner | epoch 058:     21 / 21 loss=8.343, nll_loss=5.566, ppl=47.38, wps=1920.8, ups=0.45, wpb=4260.5, bsz=80, num_updates=1218, lr=1.4616e-05, gnorm=2.506, train_wall=4, gb_free=16.5, wall=7906
2024-09-04 09:12:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22466.87109375Mb; avail=232562.5703125Mb
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000755
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22467.36328125Mb; avail=232562.5703125Mb
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012754
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22467.36328125Mb; avail=232562.5703125Mb
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011481
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025361
2024-09-04 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22467.36328125Mb; avail=232562.5703125Mb
2024-09-04 09:12:31 | INFO | train_inner | epoch 059:      4 / 16 loss=9.41, nll_loss=7.02, ppl=129.8, wps=1536.5, ups=0.34, wpb=4470, bsz=176, num_updates=932, lr=1.1184e-05, gnorm=2.99, train_wall=6, gb_free=9.7, wall=7420
2024-09-04 09:12:36 | INFO | train_inner | epoch 059:      6 / 16 loss=9.918, nll_loss=7.655, ppl=201.54, wps=1456.7, ups=0.37, wpb=3908, bsz=68, num_updates=934, lr=1.1208e-05, gnorm=3.069, train_wall=5, gb_free=15, wall=7426
2024-09-04 09:12:42 | INFO | train_inner | epoch 059:      8 / 16 loss=9.777, nll_loss=7.487, ppl=179.44, wps=1537.2, ups=0.34, wpb=4568, bsz=132, num_updates=936, lr=1.1232e-05, gnorm=3.11, train_wall=6, gb_free=8.3, wall=7432
2024-09-04 09:12:47 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.402 | nll_loss 6.817 | ppl 112.74 | wps 4598.9 | wpb 2350.9 | bsz 94.7 | num_updates 1218 | best_loss 9.402
2024-09-04 09:12:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1218 updates
2024-09-04 09:12:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:12:48 | INFO | train_inner | epoch 059:     10 / 16 loss=9.393, nll_loss=6.984, ppl=126.59, wps=1479.1, ups=0.34, wpb=4413.5, bsz=168, num_updates=938, lr=1.1256e-05, gnorm=2.124, train_wall=6, gb_free=8.7, wall=7438
2024-09-04 09:12:53 | INFO | train_inner | epoch 059:     12 / 16 loss=9.623, nll_loss=7.295, ppl=157, wps=1465.3, ups=0.42, wpb=3482, bsz=101, num_updates=940, lr=1.128e-05, gnorm=2.748, train_wall=5, gb_free=10.4, wall=7442
2024-09-04 09:12:59 | INFO | train_inner | epoch 059:     14 / 16 loss=9.728, nll_loss=7.413, ppl=170.42, wps=1543.4, ups=0.35, wpb=4396, bsz=128, num_updates=942, lr=1.1304e-05, gnorm=2.771, train_wall=6, gb_free=8.3, wall=7448
2024-09-04 09:13:05 | INFO | train_inner | epoch 059:     16 / 16 loss=9.66, nll_loss=7.318, ppl=159.51, wps=1522.1, ups=0.34, wpb=4438.5, bsz=100, num_updates=944, lr=1.1328e-05, gnorm=2.085, train_wall=6, gb_free=10.3, wall=7454
2024-09-04 09:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39618.9375Mb; avail=215410.9609375Mb
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39619.4296875Mb; avail=215410.46875Mb
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012671
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39619.6640625Mb; avail=215410.234375Mb
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011182
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024979
2024-09-04 09:13:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39619.171875Mb; avail=215410.7265625Mb
2024-09-04 09:13:19 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 10.98 | nll_loss 8.914 | ppl 482.27 | wps 3836.8 | wpb 2070.5 | bsz 122.7 | num_updates 944 | best_loss 10.98
2024-09-04 09:13:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 944 updates
2024-09-04 09:13:19 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:13:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 58 @ 1218 updates, score 9.402) (writing took 63.14664224628359 seconds)
2024-09-04 09:13:50 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-09-04 09:13:50 | INFO | train | epoch 058 | loss 8.232 | nll_loss 5.437 | ppl 43.33 | wps 700.7 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 1218 | lr 1.4616e-05 | gnorm 2.405 | train_wall 56 | gb_free 16.5 | wall 7987
2024-09-04 09:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000780
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40928.734375Mb; avail=214101.19140625Mb
2024-09-04 09:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000609
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40928.734375Mb; avail=214101.19140625Mb
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40928.734375Mb; avail=214101.19140625Mb
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000181
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001113
2024-09-04 09:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40928.2421875Mb; avail=214101.19140625Mb
2024-09-04 09:13:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:13:50 | INFO | fairseq.trainer | begin training epoch 59
2024-09-04 09:13:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:13:56 | INFO | train_inner | epoch 059:      2 / 21 loss=8.103, nll_loss=5.259, ppl=38.28, wps=110, ups=0.02, wpb=4776, bsz=116, num_updates=1220, lr=1.464e-05, gnorm=2.318, train_wall=6, gb_free=10.7, wall=7993
2024-09-04 09:13:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:14:01 | INFO | train_inner | epoch 059:      4 / 21 loss=8.431, nll_loss=5.678, ppl=51.21, wps=1651.1, ups=0.35, wpb=4656.5, bsz=68, num_updates=1222, lr=1.4664e-05, gnorm=2.137, train_wall=6, gb_free=14.6, wall=7998
2024-09-04 09:14:06 | INFO | train_inner | epoch 059:      6 / 21 loss=8.407, nll_loss=5.677, ppl=51.18, wps=2013.8, ups=0.45, wpb=4472, bsz=76, num_updates=1224, lr=1.4688e-05, gnorm=2.558, train_wall=4, gb_free=17.1, wall=8003
2024-09-04 09:14:11 | INFO | train_inner | epoch 059:      8 / 21 loss=8.134, nll_loss=5.316, ppl=39.85, wps=1571.8, ups=0.36, wpb=4381.5, bsz=104, num_updates=1226, lr=1.4712e-05, gnorm=2.628, train_wall=6, gb_free=13.6, wall=8008
2024-09-04 09:14:16 | INFO | train_inner | epoch 059:     10 / 21 loss=8.059, nll_loss=5.228, ppl=37.49, wps=1853.5, ups=0.42, wpb=4464, bsz=140, num_updates=1228, lr=1.4736e-05, gnorm=2.495, train_wall=5, gb_free=13.5, wall=8013
2024-09-04 09:14:21 | INFO | train_inner | epoch 059:     12 / 21 loss=8.211, nll_loss=5.415, ppl=42.66, wps=1815, ups=0.44, wpb=4097.5, bsz=65, num_updates=1230, lr=1.476e-05, gnorm=2.459, train_wall=5, gb_free=12.5, wall=8018
2024-09-04 09:14:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 59 @ 944 updates, score 10.98) (writing took 62.576274920254946 seconds)
2024-09-04 09:14:22 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-09-04 09:14:22 | INFO | train | epoch 059 | loss 9.636 | nll_loss 7.301 | ppl 157.65 | wps 555.4 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 944 | lr 1.1328e-05 | gnorm 2.653 | train_wall 45 | gb_free 10.3 | wall 7531
2024-09-04 09:14:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:14:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:14:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:14:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000785
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29420.640625Mb; avail=225609.24609375Mb
2024-09-04 09:14:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000608
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29420.640625Mb; avail=225609.24609375Mb
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29420.640625Mb; avail=225609.24609375Mb
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001100
2024-09-04 09:14:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29420.640625Mb; avail=225609.24609375Mb
2024-09-04 09:14:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:14:22 | INFO | fairseq.trainer | begin training epoch 60
2024-09-04 09:14:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:14:26 | INFO | train_inner | epoch 059:     14 / 21 loss=8.239, nll_loss=5.442, ppl=43.48, wps=1688, ups=0.35, wpb=4813, bsz=88, num_updates=1232, lr=1.4784e-05, gnorm=2.572, train_wall=6, gb_free=12.7, wall=8023
2024-09-04 09:14:28 | INFO | train_inner | epoch 060:      2 / 16 loss=9.642, nll_loss=7.297, ppl=157.26, wps=109.1, ups=0.02, wpb=4527.5, bsz=116, num_updates=946, lr=1.1352e-05, gnorm=2.475, train_wall=6, gb_free=10.1, wall=7537
2024-09-04 09:14:31 | INFO | train_inner | epoch 059:     16 / 21 loss=7.986, nll_loss=5.136, ppl=35.15, wps=1882.4, ups=0.4, wpb=4663, bsz=140, num_updates=1234, lr=1.4808e-05, gnorm=2.294, train_wall=5, gb_free=12.8, wall=8028
2024-09-04 09:14:33 | INFO | train_inner | epoch 060:      4 / 16 loss=9.606, nll_loss=7.258, ppl=153.05, wps=1501.7, ups=0.37, wpb=4101, bsz=120, num_updates=948, lr=1.1376e-05, gnorm=2.44, train_wall=5, gb_free=10.4, wall=7542
2024-09-04 09:14:36 | INFO | train_inner | epoch 059:     18 / 21 loss=8.319, nll_loss=5.55, ppl=46.84, wps=2019.6, ups=0.44, wpb=4592, bsz=80, num_updates=1236, lr=1.4832e-05, gnorm=2.282, train_wall=5, gb_free=13.7, wall=8033
2024-09-04 09:14:39 | INFO | train_inner | epoch 060:      6 / 16 loss=9.625, nll_loss=7.282, ppl=155.68, wps=1528.5, ups=0.33, wpb=4593, bsz=128, num_updates=950, lr=1.14e-05, gnorm=2.488, train_wall=6, gb_free=9.8, wall=7548
2024-09-04 09:14:42 | INFO | train_inner | epoch 059:     20 / 21 loss=8.16, nll_loss=5.33, ppl=40.21, wps=1614.6, ups=0.35, wpb=4566, bsz=96, num_updates=1238, lr=1.4856e-05, gnorm=2.448, train_wall=6, gb_free=11.4, wall=8038
2024-09-04 09:14:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29471.75390625Mb; avail=225558.09375Mb
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000728
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29471.75390625Mb; avail=225558.09375Mb
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012703
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29471.75390625Mb; avail=225558.09375Mb
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011155
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024938
2024-09-04 09:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29472.24609375Mb; avail=225557.6015625Mb
2024-09-04 09:14:44 | INFO | train_inner | epoch 060:      8 / 16 loss=9.749, nll_loss=7.458, ppl=175.84, wps=1551.7, ups=0.4, wpb=3883, bsz=100, num_updates=952, lr=1.1424e-05, gnorm=3.217, train_wall=5, gb_free=9.4, wall=7553
2024-09-04 09:14:50 | INFO | train_inner | epoch 060:     10 / 16 loss=9.416, nll_loss=7.021, ppl=129.87, wps=1519.8, ups=0.34, wpb=4487.5, bsz=172, num_updates=954, lr=1.1448e-05, gnorm=2.392, train_wall=6, gb_free=8.6, wall=7559
2024-09-04 09:14:55 | INFO | train_inner | epoch 060:     12 / 16 loss=9.616, nll_loss=7.265, ppl=153.83, wps=1506.9, ups=0.42, wpb=3596, bsz=89, num_updates=956, lr=1.1472e-05, gnorm=2.954, train_wall=5, gb_free=16, wall=7564
2024-09-04 09:15:01 | INFO | train_inner | epoch 060:     14 / 16 loss=9.292, nll_loss=6.846, ppl=115.05, wps=1407.1, ups=0.32, wpb=4367.5, bsz=164, num_updates=958, lr=1.1496e-05, gnorm=2.479, train_wall=6, gb_free=10.5, wall=7570
2024-09-04 09:15:01 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.406 | nll_loss 6.844 | ppl 114.87 | wps 4686.8 | wpb 2350.9 | bsz 94.7 | num_updates 1239 | best_loss 9.402
2024-09-04 09:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1239 updates
2024-09-04 09:15:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:15:07 | INFO | train_inner | epoch 060:     16 / 16 loss=9.656, nll_loss=7.324, ppl=160.23, wps=1509.9, ups=0.35, wpb=4336, bsz=128, num_updates=960, lr=1.152e-05, gnorm=2.991, train_wall=6, gb_free=12, wall=7576
2024-09-04 09:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29990.6953125Mb; avail=225039.109375Mb
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000739
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29991.6796875Mb; avail=225038.125Mb
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012613
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30011.85546875Mb; avail=225017.9453125Mb
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011316
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025034
2024-09-04 09:15:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30030.55859375Mb; avail=224999.2421875Mb
2024-09-04 09:15:21 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 10.929 | nll_loss 8.844 | ppl 459.39 | wps 3830.2 | wpb 2070.5 | bsz 122.7 | num_updates 960 | best_loss 10.929
2024-09-04 09:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 960 updates
2024-09-04 09:15:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:15:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:15:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 59 @ 1239 updates, score 9.406) (writing took 49.236833167262375 seconds)
2024-09-04 09:15:51 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-09-04 09:15:51 | INFO | train | epoch 059 | loss 8.199 | nll_loss 5.395 | ppl 42.08 | wps 792.1 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1239 | lr 1.4868e-05 | gnorm 2.393 | train_wall 54 | gb_free 13.6 | wall 8107
2024-09-04 09:15:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:15:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:15:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:15:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000565
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44549.4375Mb; avail=210463.81640625Mb
2024-09-04 09:15:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000554
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44549.4375Mb; avail=210463.81640625Mb
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44549.9296875Mb; avail=210463.32421875Mb
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000160
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001016
2024-09-04 09:15:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44549.4375Mb; avail=210463.32421875Mb
2024-09-04 09:15:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:15:51 | INFO | fairseq.trainer | begin training epoch 60
2024-09-04 09:15:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:15:53 | INFO | train_inner | epoch 060:      1 / 21 loss=7.881, nll_loss=4.986, ppl=31.68, wps=132.8, ups=0.03, wpb=4776, bsz=128, num_updates=1240, lr=1.488e-05, gnorm=1.984, train_wall=5, gb_free=13.1, wall=8110
2024-09-04 09:15:59 | INFO | train_inner | epoch 060:      3 / 21 loss=8.189, nll_loss=5.39, ppl=41.93, wps=1611.6, ups=0.33, wpb=4841.5, bsz=100, num_updates=1242, lr=1.4904e-05, gnorm=2.477, train_wall=6, gb_free=11.1, wall=8116
2024-09-04 09:16:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:16:05 | INFO | train_inner | epoch 060:      5 / 21 loss=8.354, nll_loss=5.593, ppl=48.28, wps=1748.6, ups=0.38, wpb=4565.5, bsz=72, num_updates=1244, lr=1.4928e-05, gnorm=2.527, train_wall=5, gb_free=14.4, wall=8122
2024-09-04 09:16:09 | INFO | train_inner | epoch 060:      7 / 21 loss=8.432, nll_loss=5.694, ppl=51.76, wps=1890.7, ups=0.45, wpb=4190, bsz=41, num_updates=1246, lr=1.4952e-05, gnorm=2.486, train_wall=4, gb_free=11.9, wall=8126
2024-09-04 09:16:15 | INFO | train_inner | epoch 060:      9 / 21 loss=8.221, nll_loss=5.404, ppl=42.35, wps=1743.7, ups=0.35, wpb=5036.5, bsz=112, num_updates=1248, lr=1.4976e-05, gnorm=2.105, train_wall=6, gb_free=13.3, wall=8132
2024-09-04 09:16:21 | INFO | train_inner | epoch 060:     11 / 21 loss=7.953, nll_loss=5.105, ppl=34.42, wps=1662.9, ups=0.35, wpb=4698, bsz=148, num_updates=1250, lr=1.5e-05, gnorm=2.594, train_wall=6, gb_free=13.1, wall=8137
2024-09-04 09:16:25 | INFO | train_inner | epoch 060:     13 / 21 loss=8.251, nll_loss=5.459, ppl=43.99, wps=1864.7, ups=0.43, wpb=4363.5, bsz=88, num_updates=1252, lr=1.5024e-05, gnorm=2.161, train_wall=5, gb_free=12.4, wall=8142
2024-09-04 09:16:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 60 @ 960 updates, score 10.929) (writing took 67.46928804088384 seconds)
2024-09-04 09:16:29 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-09-04 09:16:29 | INFO | train | epoch 060 | loss 9.572 | nll_loss 7.214 | ppl 148.48 | wps 533.7 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 960 | lr 1.152e-05 | gnorm 2.679 | train_wall 45 | gb_free 12 | wall 7658
2024-09-04 09:16:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:16:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:16:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:16:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000749
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32350.65234375Mb; avail=222662.671875Mb
2024-09-04 09:16:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000629
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32350.65234375Mb; avail=222662.671875Mb
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32350.65234375Mb; avail=222662.671875Mb
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000176
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001135
2024-09-04 09:16:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32350.65234375Mb; avail=222662.671875Mb
2024-09-04 09:16:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:16:29 | INFO | fairseq.trainer | begin training epoch 61
2024-09-04 09:16:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:16:31 | INFO | train_inner | epoch 060:     15 / 21 loss=8.218, nll_loss=5.4, ppl=42.23, wps=1504.7, ups=0.35, wpb=4341, bsz=92, num_updates=1254, lr=1.5048e-05, gnorm=2.691, train_wall=6, gb_free=14.6, wall=8148
2024-09-04 09:16:34 | INFO | train_inner | epoch 061:      2 / 16 loss=9.685, nll_loss=7.347, ppl=162.8, wps=105.9, ups=0.02, wpb=4645, bsz=108, num_updates=962, lr=1.1544e-05, gnorm=2.643, train_wall=6, gb_free=9.9, wall=7664
2024-09-04 09:16:37 | INFO | train_inner | epoch 060:     17 / 21 loss=8.309, nll_loss=5.532, ppl=46.27, wps=1716.1, ups=0.35, wpb=4875, bsz=80, num_updates=1256, lr=1.5072e-05, gnorm=2.001, train_wall=6, gb_free=13.6, wall=8154
2024-09-04 09:16:40 | INFO | train_inner | epoch 061:      4 / 16 loss=9.594, nll_loss=7.231, ppl=150.22, wps=1542.4, ups=0.35, wpb=4410.5, bsz=144, num_updates=964, lr=1.1568e-05, gnorm=2.63, train_wall=6, gb_free=9, wall=7670
2024-09-04 09:16:41 | INFO | train_inner | epoch 060:     19 / 21 loss=7.831, nll_loss=4.937, ppl=30.63, wps=1880.2, ups=0.43, wpb=4356.5, bsz=116, num_updates=1258, lr=1.5096e-05, gnorm=2.309, train_wall=5, gb_free=13, wall=8158
2024-09-04 09:16:46 | INFO | train_inner | epoch 061:      6 / 16 loss=9.637, nll_loss=7.301, ppl=157.68, wps=1475.8, ups=0.34, wpb=4367, bsz=124, num_updates=966, lr=1.1592e-05, gnorm=2.631, train_wall=6, gb_free=9.4, wall=7676
2024-09-04 09:16:46 | INFO | train_inner | epoch 060:     21 / 21 loss=8.218, nll_loss=5.427, ppl=43.03, wps=1750.1, ups=0.42, wpb=4157, bsz=84, num_updates=1260, lr=1.512e-05, gnorm=2.329, train_wall=5, gb_free=16, wall=8163
2024-09-04 09:16:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32404.54296875Mb; avail=222608.0859375Mb
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32405.03515625Mb; avail=222608.0859375Mb
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012795
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32405.03515625Mb; avail=222608.0859375Mb
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011191
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025092
2024-09-04 09:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32405.03515625Mb; avail=222608.0859375Mb
2024-09-04 09:16:51 | INFO | train_inner | epoch 061:      8 / 16 loss=9.383, nll_loss=6.962, ppl=124.65, wps=1455.2, ups=0.4, wpb=3608, bsz=125, num_updates=968, lr=1.1616e-05, gnorm=2.245, train_wall=5, gb_free=9.9, wall=7680
2024-09-04 09:16:57 | INFO | train_inner | epoch 061:     10 / 16 loss=9.102, nll_loss=6.605, ppl=97.36, wps=1381.7, ups=0.34, wpb=4020, bsz=156, num_updates=970, lr=1.164e-05, gnorm=2.562, train_wall=6, gb_free=8.5, wall=7686
2024-09-04 09:17:04 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.44 | nll_loss 6.868 | ppl 116.8 | wps 4585.8 | wpb 2350.9 | bsz 94.7 | num_updates 1260 | best_loss 9.402
2024-09-04 09:17:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1260 updates
2024-09-04 09:17:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:17:17 | INFO | train_inner | epoch 061:     12 / 16 loss=9.766, nll_loss=7.459, ppl=176, wps=389, ups=0.1, wpb=3893.5, bsz=84, num_updates=972, lr=1.1664e-05, gnorm=3.295, train_wall=20, gb_free=11.6, wall=7706
2024-09-04 09:17:22 | INFO | train_inner | epoch 061:     14 / 16 loss=9.534, nll_loss=7.136, ppl=140.68, wps=1538.4, ups=0.35, wpb=4347, bsz=132, num_updates=974, lr=1.1688e-05, gnorm=3.009, train_wall=6, gb_free=10.6, wall=7712
2024-09-04 09:17:28 | INFO | train_inner | epoch 061:     16 / 16 loss=9.531, nll_loss=7.137, ppl=140.72, wps=1523.7, ups=0.33, wpb=4600.5, bsz=144, num_updates=976, lr=1.1712e-05, gnorm=2.362, train_wall=6, gb_free=11, wall=7718
2024-09-04 09:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37745.95703125Mb; avail=217267.80859375Mb
2024-09-04 09:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000632
2024-09-04 09:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37745.95703125Mb; avail=217267.80859375Mb
2024-09-04 09:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012574
2024-09-04 09:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37745.95703125Mb; avail=217267.31640625Mb
2024-09-04 09:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011271
2024-09-04 09:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024831
2024-09-04 09:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37745.90234375Mb; avail=217267.37109375Mb
2024-09-04 09:17:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 60 @ 1260 updates, score 9.44) (writing took 38.39716521371156 seconds)
2024-09-04 09:17:42 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-09-04 09:17:42 | INFO | train | epoch 060 | loss 8.171 | nll_loss 5.36 | ppl 41.07 | wps 855.9 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1260 | lr 1.512e-05 | gnorm 2.355 | train_wall 55 | gb_free 16 | wall 8219
2024-09-04 09:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000688
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23830.44140625Mb; avail=231182.8203125Mb
2024-09-04 09:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000599
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23830.44140625Mb; avail=231182.8203125Mb
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23830.44140625Mb; avail=231182.8203125Mb
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000167
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001064
2024-09-04 09:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23830.44140625Mb; avail=231182.8203125Mb
2024-09-04 09:17:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:17:42 | INFO | fairseq.trainer | begin training epoch 61
2024-09-04 09:17:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:17:43 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 10.964 | nll_loss 8.881 | ppl 471.39 | wps 3839 | wpb 2070.5 | bsz 122.7 | num_updates 976 | best_loss 10.929
2024-09-04 09:17:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 976 updates
2024-09-04 09:17:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:17:47 | INFO | train_inner | epoch 061:      2 / 21 loss=8.035, nll_loss=5.17, ppl=36.01, wps=150.5, ups=0.03, wpb=4599.5, bsz=116, num_updates=1262, lr=1.5144e-05, gnorm=2.329, train_wall=5, gb_free=12.1, wall=8224
2024-09-04 09:17:52 | INFO | train_inner | epoch 061:      4 / 21 loss=7.909, nll_loss=5.049, ppl=33.1, wps=1453.2, ups=0.4, wpb=3595, bsz=89, num_updates=1264, lr=1.5168e-05, gnorm=2.524, train_wall=5, gb_free=13, wall=8229
2024-09-04 09:17:57 | INFO | train_inner | epoch 061:      6 / 21 loss=8.244, nll_loss=5.477, ppl=44.55, wps=1714.9, ups=0.39, wpb=4363.5, bsz=84, num_updates=1266, lr=1.5192e-05, gnorm=2.676, train_wall=5, gb_free=13.9, wall=8234
2024-09-04 09:18:03 | INFO | train_inner | epoch 061:      8 / 21 loss=8.305, nll_loss=5.515, ppl=45.74, wps=1736.1, ups=0.37, wpb=4691.5, bsz=88, num_updates=1268, lr=1.5216e-05, gnorm=2.121, train_wall=5, gb_free=15.2, wall=8240
2024-09-04 09:18:09 | INFO | train_inner | epoch 061:     10 / 21 loss=8.159, nll_loss=5.312, ppl=39.72, wps=1600.5, ups=0.33, wpb=4784, bsz=84, num_updates=1270, lr=1.524e-05, gnorm=2.058, train_wall=6, gb_free=11.7, wall=8246
2024-09-04 09:18:14 | INFO | train_inner | epoch 061:     12 / 21 loss=8.238, nll_loss=5.447, ppl=43.63, wps=1737.4, ups=0.36, wpb=4778.5, bsz=80, num_updates=1272, lr=1.5264e-05, gnorm=2.201, train_wall=5, gb_free=10.8, wall=8251
2024-09-04 09:18:20 | INFO | train_inner | epoch 061:     14 / 21 loss=8.212, nll_loss=5.41, ppl=42.53, wps=1637.4, ups=0.37, wpb=4454, bsz=76, num_updates=1274, lr=1.5288e-05, gnorm=2.158, train_wall=5, gb_free=13.7, wall=8257
2024-09-04 09:18:24 | INFO | train_inner | epoch 061:     16 / 21 loss=7.945, nll_loss=5.079, ppl=33.8, wps=1884.3, ups=0.41, wpb=4592.5, bsz=128, num_updates=1276, lr=1.5312e-05, gnorm=2.426, train_wall=5, gb_free=16.5, wall=8261
2024-09-04 09:18:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:18:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 61 @ 976 updates, score 10.964) (writing took 42.5119808036834 seconds)
2024-09-04 09:18:25 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-09-04 09:18:25 | INFO | train | epoch 061 | loss 9.535 | nll_loss 7.154 | ppl 142.45 | wps 579.8 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 976 | lr 1.1712e-05 | gnorm 2.672 | train_wall 60 | gb_free 11 | wall 7775
2024-09-04 09:18:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:18:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:18:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:18:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000660
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26400.80078125Mb; avail=228612.453125Mb
2024-09-04 09:18:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000598
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26400.80078125Mb; avail=228612.453125Mb
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26400.80078125Mb; avail=228612.453125Mb
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000155
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001051
2024-09-04 09:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26400.80078125Mb; avail=228612.453125Mb
2024-09-04 09:18:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:18:26 | INFO | fairseq.trainer | begin training epoch 62
2024-09-04 09:18:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:18:30 | INFO | train_inner | epoch 061:     18 / 21 loss=7.985, nll_loss=5.116, ppl=34.69, wps=1502.5, ups=0.34, wpb=4361, bsz=120, num_updates=1278, lr=1.5336e-05, gnorm=2.313, train_wall=6, gb_free=13.1, wall=8267
2024-09-04 09:18:31 | INFO | train_inner | epoch 062:      2 / 16 loss=9.287, nll_loss=6.842, ppl=114.7, wps=126.5, ups=0.03, wpb=3956.5, bsz=136, num_updates=978, lr=1.1736e-05, gnorm=2.452, train_wall=5, gb_free=10.4, wall=7781
2024-09-04 09:18:37 | INFO | train_inner | epoch 062:      4 / 16 loss=9.569, nll_loss=7.209, ppl=147.98, wps=1533, ups=0.34, wpb=4547, bsz=128, num_updates=980, lr=1.176e-05, gnorm=2.439, train_wall=6, gb_free=10.2, wall=7786
2024-09-04 09:18:43 | INFO | train_inner | epoch 062:      6 / 16 loss=9.365, nll_loss=6.951, ppl=123.76, wps=1533.6, ups=0.34, wpb=4569, bsz=152, num_updates=982, lr=1.1784e-05, gnorm=2.519, train_wall=6, gb_free=9.2, wall=7792
2024-09-04 09:18:45 | INFO | train_inner | epoch 061:     20 / 21 loss=8.313, nll_loss=5.531, ppl=46.24, wps=664.9, ups=0.13, wpb=5070, bsz=88, num_updates=1280, lr=1.536e-05, gnorm=2.106, train_wall=15, gb_free=12.4, wall=8282
2024-09-04 09:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20893.2890625Mb; avail=234120.11328125Mb
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000644
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20893.2890625Mb; avail=234120.11328125Mb
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012667
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20893.2890625Mb; avail=234120.11328125Mb
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011276
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024997
2024-09-04 09:18:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20893.2890625Mb; avail=234120.11328125Mb
2024-09-04 09:18:49 | INFO | train_inner | epoch 062:      8 / 16 loss=9.651, nll_loss=7.3, ppl=157.6, wps=1508.4, ups=0.35, wpb=4278, bsz=112, num_updates=984, lr=1.1808e-05, gnorm=2.933, train_wall=6, gb_free=9.7, wall=7798
2024-09-04 09:18:54 | INFO | train_inner | epoch 062:     10 / 16 loss=9.74, nll_loss=7.427, ppl=172.14, wps=1574.1, ups=0.36, wpb=4327.5, bsz=84, num_updates=986, lr=1.1832e-05, gnorm=3.239, train_wall=5, gb_free=12, wall=7804
2024-09-04 09:19:00 | INFO | train_inner | epoch 062:     12 / 16 loss=9.515, nll_loss=7.11, ppl=138.16, wps=1439.3, ups=0.34, wpb=4295, bsz=116, num_updates=988, lr=1.1856e-05, gnorm=2.273, train_wall=6, gb_free=10.1, wall=7810
2024-09-04 09:19:05 | INFO | train_inner | epoch 062:     14 / 16 loss=9.511, nll_loss=7.125, ppl=139.56, wps=1301.9, ups=0.39, wpb=3368, bsz=81, num_updates=990, lr=1.188e-05, gnorm=2.861, train_wall=5, gb_free=9.1, wall=7815
2024-09-04 09:19:05 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.421 | nll_loss 6.848 | ppl 115.21 | wps 4720.3 | wpb 2350.9 | bsz 94.7 | num_updates 1281 | best_loss 9.402
2024-09-04 09:19:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1281 updates
2024-09-04 09:19:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:19:12 | INFO | train_inner | epoch 062:     16 / 16 loss=9.04, nll_loss=6.524, ppl=92.05, wps=1418.9, ups=0.31, wpb=4550.5, bsz=208, num_updates=992, lr=1.1904e-05, gnorm=2.226, train_wall=6, gb_free=9, wall=7821
2024-09-04 09:19:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24095.328125Mb; avail=230917.73046875Mb
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000732
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24095.328125Mb; avail=230917.73046875Mb
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012508
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24095.328125Mb; avail=230917.73046875Mb
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011250
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024872
2024-09-04 09:19:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24095.328125Mb; avail=230917.73046875Mb
2024-09-04 09:19:26 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 10.964 | nll_loss 8.889 | ppl 474.07 | wps 3812.3 | wpb 2070.5 | bsz 122.7 | num_updates 992 | best_loss 10.929
2024-09-04 09:19:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 992 updates
2024-09-04 09:19:26 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:19:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 61 @ 1281 updates, score 9.421) (writing took 39.56908936984837 seconds)
2024-09-04 09:19:45 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-09-04 09:19:45 | INFO | train | epoch 061 | loss 8.131 | nll_loss 5.307 | ppl 39.58 | wps 780.6 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1281 | lr 1.5372e-05 | gnorm 2.302 | train_wall 66 | gb_free 11.3 | wall 8342
2024-09-04 09:19:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:19:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:19:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:19:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000663
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39364.03515625Mb; avail=215649.79296875Mb
2024-09-04 09:19:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000597
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39364.03515625Mb; avail=215649.30078125Mb
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000026
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39364.03515625Mb; avail=215649.30078125Mb
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000164
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001052
2024-09-04 09:19:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39364.03515625Mb; avail=215649.30078125Mb
2024-09-04 09:19:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:19:45 | INFO | fairseq.trainer | begin training epoch 62
2024-09-04 09:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:19:47 | INFO | train_inner | epoch 062:      1 / 21 loss=8.105, nll_loss=5.282, ppl=38.92, wps=156.9, ups=0.03, wpb=4837.5, bsz=96, num_updates=1282, lr=1.5384e-05, gnorm=2.623, train_wall=5, gb_free=14.4, wall=8344
2024-09-04 09:19:53 | INFO | train_inner | epoch 062:      3 / 21 loss=8.357, nll_loss=5.6, ppl=48.52, wps=1605.3, ups=0.35, wpb=4604.5, bsz=68, num_updates=1284, lr=1.5408e-05, gnorm=2.373, train_wall=6, gb_free=15.9, wall=8350
2024-09-04 09:19:58 | INFO | train_inner | epoch 062:      5 / 21 loss=8.131, nll_loss=5.303, ppl=39.48, wps=2046.6, ups=0.41, wpb=4986, bsz=92, num_updates=1286, lr=1.5432e-05, gnorm=2.588, train_wall=5, gb_free=12.9, wall=8355
2024-09-04 09:20:03 | INFO | train_inner | epoch 062:      7 / 21 loss=7.843, nll_loss=4.955, ppl=31.01, wps=1592.1, ups=0.36, wpb=4469.5, bsz=172, num_updates=1288, lr=1.5456e-05, gnorm=2.924, train_wall=6, gb_free=13.9, wall=8360
2024-09-04 09:20:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:20:08 | INFO | train_inner | epoch 062:      9 / 21 loss=8.173, nll_loss=5.33, ppl=40.23, wps=2041.8, ups=0.4, wpb=5124.5, bsz=112, num_updates=1290, lr=1.548e-05, gnorm=2.969, train_wall=5, gb_free=11.4, wall=8365
2024-09-04 09:20:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 62 @ 992 updates, score 10.964) (writing took 43.02269050292671 seconds)
2024-09-04 09:20:09 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-09-04 09:20:09 | INFO | train | epoch 062 | loss 9.457 | nll_loss 7.058 | ppl 133.25 | wps 653.3 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 992 | lr 1.1904e-05 | gnorm 2.618 | train_wall 46 | gb_free 9 | wall 7879
2024-09-04 09:20:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:20:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:20:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:20:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000710
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24149.0234375Mb; avail=230864.23828125Mb
2024-09-04 09:20:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000595
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24149.0234375Mb; avail=230864.23828125Mb
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24149.0234375Mb; avail=230864.23828125Mb
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000158
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001052
2024-09-04 09:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24149.0234375Mb; avail=230864.23828125Mb
2024-09-04 09:20:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:20:09 | INFO | fairseq.trainer | begin training epoch 63
2024-09-04 09:20:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:20:14 | INFO | train_inner | epoch 062:     11 / 21 loss=8.219, nll_loss=5.41, ppl=42.52, wps=1482, ups=0.35, wpb=4260.5, bsz=84, num_updates=1292, lr=1.5504e-05, gnorm=2.508, train_wall=6, gb_free=12.2, wall=8371
2024-09-04 09:20:20 | INFO | train_inner | epoch 062:     13 / 21 loss=8.339, nll_loss=5.581, ppl=47.87, wps=1667, ups=0.36, wpb=4644, bsz=64, num_updates=1294, lr=1.5528e-05, gnorm=2.698, train_wall=6, gb_free=14.2, wall=8377
2024-09-04 09:20:24 | INFO | train_inner | epoch 063:      2 / 16 loss=9.624, nll_loss=7.269, ppl=154.23, wps=106.7, ups=0.03, wpb=3872.5, bsz=88, num_updates=994, lr=1.1928e-05, gnorm=2.761, train_wall=15, gb_free=8.7, wall=7894
2024-09-04 09:20:25 | INFO | train_inner | epoch 062:     15 / 21 loss=8.022, nll_loss=5.186, ppl=36.39, wps=1776, ups=0.38, wpb=4703, bsz=112, num_updates=1296, lr=1.5552e-05, gnorm=2.478, train_wall=5, gb_free=12.9, wall=8382
2024-09-04 09:20:30 | INFO | train_inner | epoch 063:      4 / 16 loss=9.053, nll_loss=6.517, ppl=91.61, wps=1524.1, ups=0.34, wpb=4480.5, bsz=196, num_updates=996, lr=1.1952e-05, gnorm=2.464, train_wall=6, gb_free=8.6, wall=7900
2024-09-04 09:20:31 | INFO | train_inner | epoch 062:     17 / 21 loss=8.257, nll_loss=5.452, ppl=43.77, wps=1668.8, ups=0.36, wpb=4649.5, bsz=84, num_updates=1298, lr=1.5576e-05, gnorm=2.126, train_wall=6, gb_free=12.4, wall=8388
2024-09-04 09:20:35 | INFO | train_inner | epoch 062:     19 / 21 loss=8.297, nll_loss=5.507, ppl=45.47, wps=1540, ups=0.41, wpb=3761, bsz=41, num_updates=1300, lr=1.56e-05, gnorm=2.716, train_wall=5, gb_free=11.4, wall=8392
2024-09-04 09:20:36 | INFO | train_inner | epoch 063:      6 / 16 loss=9.541, nll_loss=7.137, ppl=140.77, wps=1590, ups=0.34, wpb=4714, bsz=132, num_updates=998, lr=1.1976e-05, gnorm=2.456, train_wall=6, gb_free=10.9, wall=7906
2024-09-04 09:20:40 | INFO | train_inner | epoch 062:     21 / 21 loss=7.626, nll_loss=4.685, ppl=25.73, wps=1823.6, ups=0.42, wpb=4352.5, bsz=156, num_updates=1302, lr=1.5624e-05, gnorm=2.453, train_wall=5, gb_free=12.2, wall=8397
2024-09-04 09:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16642.21484375Mb; avail=238370.78515625Mb
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000750
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16642.70703125Mb; avail=238370.78515625Mb
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012694
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16642.70703125Mb; avail=238370.78515625Mb
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011180
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024980
2024-09-04 09:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16642.70703125Mb; avail=238370.78515625Mb
2024-09-04 09:20:42 | INFO | train_inner | epoch 063:      8 / 16 loss=9.447, nll_loss=7.028, ppl=130.49, wps=1531.5, ups=0.33, wpb=4626.5, bsz=148, num_updates=1000, lr=1.2e-05, gnorm=2.502, train_wall=6, gb_free=9.4, wall=7912
2024-09-04 09:20:48 | INFO | train_inner | epoch 063:     10 / 16 loss=9.706, nll_loss=7.38, ppl=166.62, wps=1412.3, ups=0.36, wpb=3918.5, bsz=64, num_updates=1002, lr=1.2024e-05, gnorm=3.674, train_wall=6, gb_free=10.6, wall=7917
2024-09-04 09:20:53 | INFO | train_inner | epoch 063:     12 / 16 loss=9.491, nll_loss=7.118, ppl=138.89, wps=1367.6, ups=0.4, wpb=3415, bsz=85, num_updates=1004, lr=1.2048e-05, gnorm=3.206, train_wall=5, gb_free=16.6, wall=7922
2024-09-04 09:20:58 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.403 | nll_loss 6.829 | ppl 113.66 | wps 4695.1 | wpb 2350.9 | bsz 94.7 | num_updates 1302 | best_loss 9.402
2024-09-04 09:20:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1302 updates
2024-09-04 09:20:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:20:59 | INFO | train_inner | epoch 063:     14 / 16 loss=9.363, nll_loss=6.913, ppl=120.49, wps=1446.3, ups=0.33, wpb=4415, bsz=148, num_updates=1006, lr=1.2072e-05, gnorm=2.759, train_wall=6, gb_free=9.9, wall=7928
2024-09-04 09:21:05 | INFO | train_inner | epoch 063:     16 / 16 loss=9.332, nll_loss=6.869, ppl=116.9, wps=1431.1, ups=0.32, wpb=4449.5, bsz=156, num_updates=1008, lr=1.2096e-05, gnorm=2.18, train_wall=6, gb_free=9.1, wall=7934
2024-09-04 09:21:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21058.3671875Mb; avail=233955.22265625Mb
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000664
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21059.84375Mb; avail=233953.74609375Mb
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012656
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21089.375Mb; avail=233923.72265625Mb
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011256
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024978
2024-09-04 09:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21115.953125Mb; avail=233897.63671875Mb
2024-09-04 09:21:19 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 10.984 | nll_loss 8.916 | ppl 483.14 | wps 3804.6 | wpb 2070.5 | bsz 122.7 | num_updates 1008 | best_loss 10.929
2024-09-04 09:21:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1008 updates
2024-09-04 09:21:19 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:21:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:21:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 62 @ 1302 updates, score 9.403) (writing took 38.51992561947554 seconds)
2024-09-04 09:21:36 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-09-04 09:21:36 | INFO | train | epoch 062 | loss 8.135 | nll_loss 5.311 | ppl 39.71 | wps 860.2 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1302 | lr 1.5624e-05 | gnorm 2.59 | train_wall 55 | gb_free 12.2 | wall 8453
2024-09-04 09:21:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:21:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:21:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:21:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000694
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36397.1171875Mb; avail=218616.22265625Mb
2024-09-04 09:21:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000612
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36396.625Mb; avail=218616.71484375Mb
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36396.625Mb; avail=218616.71484375Mb
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001087
2024-09-04 09:21:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36396.625Mb; avail=218616.71484375Mb
2024-09-04 09:21:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:21:36 | INFO | fairseq.trainer | begin training epoch 63
2024-09-04 09:21:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:21:41 | INFO | train_inner | epoch 063:      2 / 21 loss=8.244, nll_loss=5.436, ppl=43.29, wps=160.4, ups=0.03, wpb=4880.5, bsz=84, num_updates=1304, lr=1.5648e-05, gnorm=2.234, train_wall=5, gb_free=12.5, wall=8458
2024-09-04 09:21:57 | INFO | train_inner | epoch 063:      4 / 21 loss=8.194, nll_loss=5.371, ppl=41.37, wps=631.3, ups=0.12, wpb=5082.5, bsz=104, num_updates=1306, lr=1.5672e-05, gnorm=2.448, train_wall=16, gb_free=12.5, wall=8474
2024-09-04 09:22:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:22:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 63 @ 1008 updates, score 10.984) (writing took 42.65917157474905 seconds)
2024-09-04 09:22:02 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-09-04 09:22:02 | INFO | train | epoch 063 | loss 9.437 | nll_loss 7.017 | ppl 129.53 | wps 600.1 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 1008 | lr 1.2096e-05 | gnorm 2.75 | train_wall 56 | gb_free 9.1 | wall 7992
2024-09-04 09:22:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:22:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:22:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:22:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000654
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16604.88671875Mb; avail=238408.55078125Mb
2024-09-04 09:22:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000585
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16604.88671875Mb; avail=238408.55078125Mb
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16604.88671875Mb; avail=238408.55078125Mb
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000153
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001029
2024-09-04 09:22:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16604.88671875Mb; avail=238408.55078125Mb
2024-09-04 09:22:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:22:02 | INFO | fairseq.trainer | begin training epoch 64
2024-09-04 09:22:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:22:03 | INFO | train_inner | epoch 063:      6 / 21 loss=8.043, nll_loss=5.201, ppl=36.8, wps=1585.2, ups=0.36, wpb=4352, bsz=92, num_updates=1308, lr=1.5696e-05, gnorm=2.353, train_wall=5, gb_free=13.7, wall=8480
2024-09-04 09:22:08 | INFO | train_inner | epoch 064:      2 / 16 loss=9.497, nll_loss=7.111, ppl=138.23, wps=137.8, ups=0.03, wpb=4337, bsz=116, num_updates=1010, lr=1.212e-05, gnorm=3.051, train_wall=6, gb_free=10, wall=7997
2024-09-04 09:22:08 | INFO | train_inner | epoch 063:      8 / 21 loss=8.339, nll_loss=5.592, ppl=48.23, wps=1655.7, ups=0.36, wpb=4573, bsz=76, num_updates=1310, lr=1.572e-05, gnorm=2.462, train_wall=6, gb_free=14.3, wall=8485
2024-09-04 09:22:13 | INFO | train_inner | epoch 063:     10 / 21 loss=8.074, nll_loss=5.219, ppl=37.26, wps=1949.9, ups=0.44, wpb=4403, bsz=80, num_updates=1312, lr=1.5744e-05, gnorm=2.415, train_wall=5, gb_free=13.1, wall=8490
2024-09-04 09:22:14 | INFO | train_inner | epoch 064:      4 / 16 loss=8.985, nll_loss=6.441, ppl=86.88, wps=1395.5, ups=0.35, wpb=4000, bsz=168, num_updates=1012, lr=1.2144e-05, gnorm=2.552, train_wall=6, gb_free=9.3, wall=8003
2024-09-04 09:22:17 | INFO | train_inner | epoch 063:     12 / 21 loss=7.628, nll_loss=4.662, ppl=25.32, wps=1853.3, ups=0.43, wpb=4298, bsz=148, num_updates=1314, lr=1.5768e-05, gnorm=2.453, train_wall=5, gb_free=13.9, wall=8494
2024-09-04 09:22:19 | INFO | train_inner | epoch 064:      6 / 16 loss=9.532, nll_loss=7.152, ppl=142.22, wps=1479.4, ups=0.38, wpb=3843, bsz=100, num_updates=1014, lr=1.2168e-05, gnorm=2.717, train_wall=5, gb_free=11.6, wall=8008
2024-09-04 09:22:23 | INFO | train_inner | epoch 063:     14 / 21 loss=8.21, nll_loss=5.427, ppl=43.02, wps=1654.2, ups=0.34, wpb=4822, bsz=72, num_updates=1316, lr=1.5792e-05, gnorm=2.626, train_wall=6, gb_free=12.2, wall=8500
2024-09-04 09:22:25 | INFO | train_inner | epoch 064:      8 / 16 loss=9.251, nll_loss=6.781, ppl=109.94, wps=1552.6, ups=0.34, wpb=4610, bsz=172, num_updates=1016, lr=1.2192e-05, gnorm=2.237, train_wall=6, gb_free=10.6, wall=8014
2024-09-04 09:22:28 | INFO | train_inner | epoch 063:     16 / 21 loss=8.178, nll_loss=5.359, ppl=41.05, wps=1905.2, ups=0.43, wpb=4423, bsz=92, num_updates=1318, lr=1.5816e-05, gnorm=2.745, train_wall=5, gb_free=15.5, wall=8505
2024-09-04 09:22:30 | INFO | train_inner | epoch 064:     10 / 16 loss=9.274, nll_loss=6.806, ppl=111.86, wps=1400.3, ups=0.39, wpb=3553, bsz=105, num_updates=1018, lr=1.2216e-05, gnorm=2.667, train_wall=5, gb_free=14, wall=8019
2024-09-04 09:22:34 | INFO | train_inner | epoch 063:     18 / 21 loss=8.198, nll_loss=5.384, ppl=41.76, wps=1565.7, ups=0.34, wpb=4611.5, bsz=80, num_updates=1320, lr=1.584e-05, gnorm=2.75, train_wall=6, gb_free=11.9, wall=8511
2024-09-04 09:22:36 | INFO | train_inner | epoch 064:     12 / 16 loss=9.416, nll_loss=6.998, ppl=127.79, wps=1551.7, ups=0.34, wpb=4566, bsz=136, num_updates=1020, lr=1.224e-05, gnorm=2.895, train_wall=6, gb_free=11.3, wall=8025
2024-09-04 09:22:38 | INFO | train_inner | epoch 063:     20 / 21 loss=7.74, nll_loss=4.788, ppl=27.63, wps=1918.3, ups=0.45, wpb=4233, bsz=145, num_updates=1322, lr=1.5864e-05, gnorm=2.644, train_wall=4, gb_free=13.6, wall=8515
2024-09-04 09:22:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16653.84765625Mb; avail=238359.73046875Mb
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000624
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16653.84765625Mb; avail=238359.73046875Mb
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012669
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16653.9453125Mb; avail=238359.73046875Mb
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011208
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024860
2024-09-04 09:22:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16653.9453125Mb; avail=238359.73046875Mb
2024-09-04 09:22:42 | INFO | train_inner | epoch 064:     14 / 16 loss=9.567, nll_loss=7.181, ppl=145.14, wps=1445.5, ups=0.31, wpb=4592, bsz=100, num_updates=1022, lr=1.2264e-05, gnorm=2.922, train_wall=6, gb_free=9.2, wall=8032
2024-09-04 09:22:48 | INFO | train_inner | epoch 064:     16 / 16 loss=9.365, nll_loss=6.91, ppl=120.29, wps=1400.9, ups=0.32, wpb=4390.5, bsz=120, num_updates=1024, lr=1.2288e-05, gnorm=2.247, train_wall=6, gb_free=8.8, wall=8038
2024-09-04 09:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16673.5625Mb; avail=238339.8125Mb
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16673.5625Mb; avail=238339.8125Mb
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012671
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16673.5625Mb; avail=238339.8125Mb
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011324
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025112
2024-09-04 09:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16673.5625Mb; avail=238339.8125Mb
2024-09-04 09:22:58 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.39 | nll_loss 6.81 | ppl 112.18 | wps 4927.2 | wpb 2350.9 | bsz 94.7 | num_updates 1323 | best_loss 9.39
2024-09-04 09:22:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1323 updates
2024-09-04 09:22:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:23:03 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10.941 | nll_loss 8.853 | ppl 462.38 | wps 3804.5 | wpb 2070.5 | bsz 122.7 | num_updates 1024 | best_loss 10.929
2024-09-04 09:23:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1024 updates
2024-09-04 09:23:03 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:23:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:23:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:23:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 64 @ 1024 updates, score 10.941) (writing took 48.884410271421075 seconds)
2024-09-04 09:23:52 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-09-04 09:23:52 | INFO | train | epoch 064 | loss 9.365 | nll_loss 6.928 | ppl 121.76 | wps 618.2 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1024 | lr 1.2288e-05 | gnorm 2.661 | train_wall 46 | gb_free 8.8 | wall 8101
2024-09-04 09:23:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:23:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:23:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:23:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000569
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19496.8671875Mb; avail=235515.8046875Mb
2024-09-04 09:23:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000045
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000542
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19496.8671875Mb; avail=235516.296875Mb
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19496.8671875Mb; avail=235516.296875Mb
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000149
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000976
2024-09-04 09:23:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19496.8671875Mb; avail=235516.296875Mb
2024-09-04 09:23:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:23:52 | INFO | fairseq.trainer | begin training epoch 65
2024-09-04 09:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:23:58 | INFO | train_inner | epoch 065:      2 / 16 loss=8.94, nll_loss=6.372, ppl=82.84, wps=128.7, ups=0.03, wpb=4480, bsz=204, num_updates=1026, lr=1.2312e-05, gnorm=2.101, train_wall=6, gb_free=10.2, wall=8107
2024-09-04 09:24:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 63 @ 1323 updates, score 9.39) (writing took 62.95381594821811 seconds)
2024-09-04 09:24:01 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-09-04 09:24:01 | INFO | train | epoch 063 | loss 8.1 | nll_loss 5.265 | ppl 38.44 | wps 661.9 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 1323 | lr 1.5876e-05 | gnorm 2.563 | train_wall 65 | gb_free 14.3 | wall 8598
2024-09-04 09:24:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:24:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:24:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:24:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000753
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19542.89453125Mb; avail=235470.69140625Mb
2024-09-04 09:24:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000069
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000622
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19542.89453125Mb; avail=235470.69140625Mb
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19542.89453125Mb; avail=235470.69140625Mb
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000179
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001145
2024-09-04 09:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19542.89453125Mb; avail=235470.69140625Mb
2024-09-04 09:24:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:24:01 | INFO | fairseq.trainer | begin training epoch 64
2024-09-04 09:24:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:24:03 | INFO | train_inner | epoch 064:      1 / 21 loss=7.852, nll_loss=4.959, ppl=31.1, wps=105.7, ups=0.02, wpb=4506, bsz=148, num_updates=1324, lr=1.5888e-05, gnorm=2.763, train_wall=6, gb_free=12.7, wall=8600
2024-09-04 09:24:04 | INFO | train_inner | epoch 065:      4 / 16 loss=9.199, nll_loss=6.715, ppl=105.06, wps=1448, ups=0.33, wpb=4416.5, bsz=160, num_updates=1028, lr=1.2336e-05, gnorm=2.233, train_wall=6, gb_free=9, wall=8114
2024-09-04 09:24:09 | INFO | train_inner | epoch 065:      6 / 16 loss=9.316, nll_loss=6.858, ppl=116.02, wps=1422.5, ups=0.42, wpb=3370.5, bsz=105, num_updates=1030, lr=1.236e-05, gnorm=2.666, train_wall=5, gb_free=15.2, wall=8118
2024-09-04 09:24:09 | INFO | train_inner | epoch 064:      3 / 21 loss=7.983, nll_loss=5.139, ppl=35.23, wps=1580.4, ups=0.33, wpb=4799.5, bsz=128, num_updates=1326, lr=1.5912e-05, gnorm=2.2, train_wall=6, gb_free=13, wall=8606
2024-09-04 09:24:14 | INFO | train_inner | epoch 064:      5 / 21 loss=8.248, nll_loss=5.438, ppl=43.35, wps=1916.3, ups=0.41, wpb=4714.5, bsz=72, num_updates=1328, lr=1.5936e-05, gnorm=2.144, train_wall=5, gb_free=14.1, wall=8611
2024-09-04 09:24:14 | INFO | train_inner | epoch 065:      8 / 16 loss=9.407, nll_loss=6.977, ppl=125.96, wps=1570.8, ups=0.36, wpb=4408.5, bsz=108, num_updates=1032, lr=1.2384e-05, gnorm=3.042, train_wall=6, gb_free=10.3, wall=8124
2024-09-04 09:24:20 | INFO | train_inner | epoch 065:     10 / 16 loss=9.537, nll_loss=7.157, ppl=142.71, wps=1402.4, ups=0.34, wpb=4119, bsz=88, num_updates=1034, lr=1.2408e-05, gnorm=3.996, train_wall=6, gb_free=12.4, wall=8130
2024-09-04 09:24:20 | INFO | train_inner | epoch 064:      7 / 21 loss=7.59, nll_loss=4.605, ppl=24.33, wps=1377, ups=0.34, wpb=4052, bsz=144, num_updates=1330, lr=1.596e-05, gnorm=2.414, train_wall=6, gb_free=12.3, wall=8617
2024-09-04 09:24:26 | INFO | train_inner | epoch 064:      9 / 21 loss=8.076, nll_loss=5.244, ppl=37.91, wps=1651.3, ups=0.35, wpb=4757.5, bsz=96, num_updates=1332, lr=1.5984e-05, gnorm=2.687, train_wall=6, gb_free=12.4, wall=8623
2024-09-04 09:24:26 | INFO | train_inner | epoch 065:     12 / 16 loss=9.308, nll_loss=6.842, ppl=114.7, wps=1508.8, ups=0.34, wpb=4445, bsz=116, num_updates=1036, lr=1.2432e-05, gnorm=3.098, train_wall=6, gb_free=10, wall=8136
2024-09-04 09:24:31 | INFO | train_inner | epoch 064:     11 / 21 loss=8.333, nll_loss=5.577, ppl=47.74, wps=1821.3, ups=0.41, wpb=4393, bsz=52, num_updates=1334, lr=1.6008e-05, gnorm=2.843, train_wall=5, gb_free=12, wall=8628
2024-09-04 09:24:32 | INFO | train_inner | epoch 065:     14 / 16 loss=9.368, nll_loss=6.921, ppl=121.19, wps=1497.5, ups=0.33, wpb=4604.5, bsz=148, num_updates=1038, lr=1.2456e-05, gnorm=2.697, train_wall=6, gb_free=10.8, wall=8142
2024-09-04 09:24:36 | INFO | train_inner | epoch 064:     13 / 21 loss=8.176, nll_loss=5.36, ppl=41.07, wps=1713.6, ups=0.38, wpb=4492, bsz=76, num_updates=1336, lr=1.6032e-05, gnorm=2.859, train_wall=5, gb_free=14.2, wall=8633
2024-09-04 09:24:38 | INFO | train_inner | epoch 065:     16 / 16 loss=9.529, nll_loss=7.128, ppl=139.92, wps=1493.4, ups=0.37, wpb=4047.5, bsz=88, num_updates=1040, lr=1.248e-05, gnorm=2.88, train_wall=5, gb_free=12.1, wall=8147
2024-09-04 09:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19581.55859375Mb; avail=235431.88671875Mb
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000719
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19581.55859375Mb; avail=235431.88671875Mb
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012852
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19581.4609375Mb; avail=235431.88671875Mb
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011081
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025033
2024-09-04 09:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19581.4609375Mb; avail=235431.88671875Mb
2024-09-04 09:24:42 | INFO | train_inner | epoch 064:     15 / 21 loss=8.018, nll_loss=5.136, ppl=35.17, wps=1535.1, ups=0.37, wpb=4182, bsz=85, num_updates=1338, lr=1.6056e-05, gnorm=2.51, train_wall=5, gb_free=12.5, wall=8639
2024-09-04 09:24:46 | INFO | train_inner | epoch 064:     17 / 21 loss=8.102, nll_loss=5.27, ppl=38.58, wps=1942.5, ups=0.44, wpb=4460.5, bsz=100, num_updates=1340, lr=1.608e-05, gnorm=2.17, train_wall=5, gb_free=13.8, wall=8643
2024-09-04 09:24:52 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 10.892 | nll_loss 8.767 | ppl 435.78 | wps 3808.4 | wpb 2070.5 | bsz 122.7 | num_updates 1040 | best_loss 10.892
2024-09-04 09:24:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 1040 updates
2024-09-04 09:24:52 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:24:52 | INFO | train_inner | epoch 064:     19 / 21 loss=8.188, nll_loss=5.381, ppl=41.67, wps=1674.7, ups=0.32, wpb=5170.5, bsz=80, num_updates=1342, lr=1.6104e-05, gnorm=2.584, train_wall=6, gb_free=12.6, wall=8649
2024-09-04 09:24:58 | INFO | train_inner | epoch 064:     21 / 21 loss=8.12, nll_loss=5.287, ppl=39.03, wps=1563.4, ups=0.35, wpb=4476, bsz=80, num_updates=1344, lr=1.6128e-05, gnorm=2.487, train_wall=6, gb_free=12.3, wall=8655
2024-09-04 09:24:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20672.03515625Mb; avail=234341.46484375Mb
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000765
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20673.01953125Mb; avail=234340.48046875Mb
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012774
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20693.19921875Mb; avail=234320.30078125Mb
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011327
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025234
2024-09-04 09:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20710.84375Mb; avail=234302.6015625Mb
2024-09-04 09:25:16 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.421 | nll_loss 6.854 | ppl 115.66 | wps 4538.3 | wpb 2350.9 | bsz 94.7 | num_updates 1344 | best_loss 9.39
2024-09-04 09:25:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1344 updates
2024-09-04 09:25:16 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 65 @ 1040 updates, score 10.892) (writing took 63.598112143576145 seconds)
2024-09-04 09:25:56 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-09-04 09:25:56 | INFO | train | epoch 065 | loss 9.321 | nll_loss 6.866 | ppl 116.62 | wps 546.1 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 1040 | lr 1.248e-05 | gnorm 2.839 | train_wall 46 | gb_free 12.1 | wall 8225
2024-09-04 09:25:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:25:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:25:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:25:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000718
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38606.03125Mb; avail=216411.24609375Mb
2024-09-04 09:25:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000611
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38606.5234375Mb; avail=216406.81640625Mb
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38607.5078125Mb; avail=216405.83203125Mb
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001084
2024-09-04 09:25:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38609.4765625Mb; avail=216403.37109375Mb
2024-09-04 09:25:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:25:56 | INFO | fairseq.trainer | begin training epoch 66
2024-09-04 09:25:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:25:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:25:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 64 @ 1344 updates, score 9.421) (writing took 43.17106976173818 seconds)
2024-09-04 09:25:59 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-09-04 09:25:59 | INFO | train | epoch 064 | loss 8.06 | nll_loss 5.214 | ppl 37.13 | wps 807.6 | ups 0.18 | wpb 4556.3 | bsz 96.9 | num_updates 1344 | lr 1.6128e-05 | gnorm 2.465 | train_wall 57 | gb_free 12.3 | wall 8716
2024-09-04 09:25:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:25:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:25:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:25:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000709
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24716.25Mb; avail=230297.15234375Mb
2024-09-04 09:25:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000047
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000579
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24716.25Mb; avail=230297.15234375Mb
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24716.25Mb; avail=230297.15234375Mb
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001041
2024-09-04 09:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24716.25Mb; avail=230297.15234375Mb
2024-09-04 09:25:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:25:59 | INFO | fairseq.trainer | begin training epoch 65
2024-09-04 09:25:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:26:02 | INFO | train_inner | epoch 066:      2 / 16 loss=9.295, nll_loss=6.811, ppl=112.3, wps=109.9, ups=0.02, wpb=4628.5, bsz=140, num_updates=1042, lr=1.2504e-05, gnorm=2.711, train_wall=6, gb_free=10.9, wall=8232
2024-09-04 09:26:08 | INFO | train_inner | epoch 066:      4 / 16 loss=8.969, nll_loss=6.401, ppl=84.53, wps=1368.6, ups=0.35, wpb=3959, bsz=144, num_updates=1044, lr=1.2528e-05, gnorm=2.971, train_wall=6, gb_free=10.6, wall=8237
2024-09-04 09:26:13 | INFO | train_inner | epoch 066:      6 / 16 loss=9.45, nll_loss=7.051, ppl=132.58, wps=1545.6, ups=0.36, wpb=4304, bsz=96, num_updates=1046, lr=1.2552e-05, gnorm=3.919, train_wall=6, gb_free=14, wall=8243
2024-09-04 09:26:14 | INFO | train_inner | epoch 065:      2 / 21 loss=7.673, nll_loss=4.753, ppl=26.96, wps=102.9, ups=0.03, wpb=3929, bsz=121, num_updates=1346, lr=1.6152e-05, gnorm=2.689, train_wall=15, gb_free=13.3, wall=8731
2024-09-04 09:26:19 | INFO | train_inner | epoch 066:      8 / 16 loss=9.383, nll_loss=6.962, ppl=124.71, wps=1501.4, ups=0.35, wpb=4340.5, bsz=92, num_updates=1048, lr=1.2576e-05, gnorm=3.819, train_wall=6, gb_free=11.7, wall=8249
2024-09-04 09:26:19 | INFO | train_inner | epoch 065:      4 / 21 loss=8, nll_loss=5.119, ppl=34.74, wps=1911.9, ups=0.4, wpb=4770, bsz=108, num_updates=1348, lr=1.6176e-05, gnorm=2.898, train_wall=5, gb_free=13.6, wall=8736
2024-09-04 09:26:24 | INFO | train_inner | epoch 065:      6 / 21 loss=7.683, nll_loss=4.72, ppl=26.35, wps=1778.5, ups=0.44, wpb=4007, bsz=120, num_updates=1350, lr=1.62e-05, gnorm=3.068, train_wall=4, gb_free=16.5, wall=8741
2024-09-04 09:26:25 | INFO | train_inner | epoch 066:     10 / 16 loss=9.241, nll_loss=6.761, ppl=108.44, wps=1547.1, ups=0.33, wpb=4658, bsz=184, num_updates=1050, lr=1.26e-05, gnorm=2.853, train_wall=6, gb_free=10.6, wall=8255
2024-09-04 09:26:30 | INFO | train_inner | epoch 065:      8 / 21 loss=8.315, nll_loss=5.54, ppl=46.52, wps=1626.8, ups=0.36, wpb=4561.5, bsz=60, num_updates=1352, lr=1.6224e-05, gnorm=2.707, train_wall=6, gb_free=12.3, wall=8746
2024-09-04 09:26:30 | INFO | train_inner | epoch 066:     12 / 16 loss=9.369, nll_loss=6.9, ppl=119.44, wps=1363.7, ups=0.42, wpb=3218.5, bsz=73, num_updates=1052, lr=1.2624e-05, gnorm=3.24, train_wall=5, gb_free=14.6, wall=8259
2024-09-04 09:26:35 | INFO | train_inner | epoch 065:     10 / 21 loss=7.851, nll_loss=4.971, ppl=31.36, wps=1547.5, ups=0.35, wpb=4478.5, bsz=124, num_updates=1354, lr=1.6248e-05, gnorm=2.558, train_wall=6, gb_free=12.3, wall=8752
2024-09-04 09:26:36 | INFO | train_inner | epoch 066:     14 / 16 loss=9.378, nll_loss=6.944, ppl=123.15, wps=1469.9, ups=0.31, wpb=4666.5, bsz=140, num_updates=1054, lr=1.2648e-05, gnorm=3.558, train_wall=6, gb_free=9.2, wall=8266
2024-09-04 09:26:41 | INFO | train_inner | epoch 065:     12 / 21 loss=8.264, nll_loss=5.455, ppl=43.88, wps=1692.2, ups=0.37, wpb=4631.5, bsz=72, num_updates=1356, lr=1.6272e-05, gnorm=2.04, train_wall=5, gb_free=12.9, wall=8758
2024-09-04 09:26:42 | INFO | train_inner | epoch 066:     16 / 16 loss=9.267, nll_loss=6.804, ppl=111.71, wps=1488.4, ups=0.36, wpb=4116.5, bsz=148, num_updates=1056, lr=1.2672e-05, gnorm=3.277, train_wall=6, gb_free=9.9, wall=8271
2024-09-04 09:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22091.5625Mb; avail=232921.78515625Mb
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000745
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22091.5625Mb; avail=232921.78515625Mb
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012793
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22091.5625Mb; avail=232921.78515625Mb
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011129
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025047
2024-09-04 09:26:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22091.5625Mb; avail=232921.78515625Mb
2024-09-04 09:26:46 | INFO | train_inner | epoch 065:     14 / 21 loss=8.164, nll_loss=5.347, ppl=40.71, wps=1595.6, ups=0.37, wpb=4361.5, bsz=64, num_updates=1358, lr=1.6296e-05, gnorm=2.811, train_wall=5, gb_free=12.4, wall=8763
2024-09-04 09:26:52 | INFO | train_inner | epoch 065:     16 / 21 loss=8.207, nll_loss=5.383, ppl=41.72, wps=1680.6, ups=0.34, wpb=4989.5, bsz=84, num_updates=1360, lr=1.632e-05, gnorm=2.23, train_wall=6, gb_free=12.1, wall=8769
2024-09-04 09:26:56 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 10.952 | nll_loss 8.847 | ppl 460.45 | wps 3812 | wpb 2070.5 | bsz 122.7 | num_updates 1056 | best_loss 10.892
2024-09-04 09:26:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 1056 updates
2024-09-04 09:26:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:26:58 | INFO | train_inner | epoch 065:     18 / 21 loss=8.236, nll_loss=5.423, ppl=42.89, wps=1640.6, ups=0.33, wpb=4999, bsz=84, num_updates=1362, lr=1.6344e-05, gnorm=2.481, train_wall=6, gb_free=12.7, wall=8775
2024-09-04 09:27:04 | INFO | train_inner | epoch 065:     20 / 21 loss=7.968, nll_loss=5.098, ppl=34.26, wps=1657.3, ups=0.35, wpb=4733, bsz=124, num_updates=1364, lr=1.6368e-05, gnorm=2.187, train_wall=6, gb_free=12.5, wall=8781
2024-09-04 09:27:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32436.46875Mb; avail=222576.87890625Mb
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000666
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32437.9453125Mb; avail=222575.40234375Mb
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012678
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32466.09765625Mb; avail=222547.34765625Mb
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011305
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025022
2024-09-04 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32491.69140625Mb; avail=222521.75390625Mb
2024-09-04 09:27:24 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.407 | nll_loss 6.822 | ppl 113.17 | wps 4665.6 | wpb 2350.9 | bsz 94.7 | num_updates 1365 | best_loss 9.39
2024-09-04 09:27:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 1365 updates
2024-09-04 09:27:24 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:27:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:27:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 66 @ 1056 updates, score 10.952) (writing took 49.08712179027498 seconds)
2024-09-04 09:27:45 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-09-04 09:27:45 | INFO | train | epoch 066 | loss 9.295 | nll_loss 6.832 | ppl 113.93 | wps 619.2 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1056 | lr 1.2672e-05 | gnorm 3.293 | train_wall 46 | gb_free 9.9 | wall 8335
2024-09-04 09:27:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:27:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:27:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:27:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000585
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34399.73828125Mb; avail=220613.46484375Mb
2024-09-04 09:27:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000566
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34399.73828125Mb; avail=220613.46484375Mb
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34399.73828125Mb; avail=220613.46484375Mb
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000148
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001012
2024-09-04 09:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34400.23046875Mb; avail=220613.46484375Mb
2024-09-04 09:27:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:27:45 | INFO | fairseq.trainer | begin training epoch 67
2024-09-04 09:27:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:27:50 | INFO | train_inner | epoch 067:      2 / 16 loss=9.32, nll_loss=6.856, ppl=115.84, wps=106.7, ups=0.03, wpb=3667.5, bsz=85, num_updates=1058, lr=1.2696e-05, gnorm=3.135, train_wall=5, gb_free=10.1, wall=8340
2024-09-04 09:27:56 | INFO | train_inner | epoch 067:      4 / 16 loss=9.313, nll_loss=6.868, ppl=116.77, wps=1572.1, ups=0.35, wpb=4477.5, bsz=124, num_updates=1060, lr=1.272e-05, gnorm=2.923, train_wall=6, gb_free=13.9, wall=8346
2024-09-04 09:28:02 | INFO | train_inner | epoch 067:      6 / 16 loss=9.297, nll_loss=6.817, ppl=112.77, wps=1468.8, ups=0.34, wpb=4347.5, bsz=124, num_updates=1062, lr=1.2744e-05, gnorm=2.211, train_wall=6, gb_free=9.7, wall=8352
2024-09-04 09:28:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:28:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 65 @ 1365 updates, score 9.407) (writing took 42.8112664911896 seconds)
2024-09-04 09:28:07 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-09-04 09:28:07 | INFO | train | epoch 065 | loss 8.042 | nll_loss 5.188 | ppl 36.45 | wps 749.1 | ups 0.16 | wpb 4556.3 | bsz 96.9 | num_updates 1365 | lr 1.638e-05 | gnorm 2.544 | train_wall 67 | gb_free 11.7 | wall 8844
2024-09-04 09:28:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:28:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:28:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:28:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000659
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19308.59765625Mb; avail=235705.1328125Mb
2024-09-04 09:28:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000577
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19308.59765625Mb; avail=235705.1328125Mb
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19308.59765625Mb; avail=235705.1328125Mb
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000161
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001020
2024-09-04 09:28:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19308.59765625Mb; avail=235705.1328125Mb
2024-09-04 09:28:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:28:07 | INFO | fairseq.trainer | begin training epoch 66
2024-09-04 09:28:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:28:08 | INFO | train_inner | epoch 067:      8 / 16 loss=8.905, nll_loss=6.308, ppl=79.24, wps=1409.6, ups=0.34, wpb=4149, bsz=156, num_updates=1064, lr=1.2768e-05, gnorm=2.228, train_wall=6, gb_free=9.2, wall=8357
2024-09-04 09:28:09 | INFO | train_inner | epoch 066:      1 / 21 loss=7.719, nll_loss=4.795, ppl=27.75, wps=146, ups=0.03, wpb=4776.5, bsz=144, num_updates=1366, lr=1.6392e-05, gnorm=2.185, train_wall=5, gb_free=11.9, wall=8846
2024-09-04 09:28:14 | INFO | train_inner | epoch 067:     10 / 16 loss=9.244, nll_loss=6.776, ppl=109.58, wps=1497.8, ups=0.34, wpb=4466.5, bsz=124, num_updates=1066, lr=1.2792e-05, gnorm=2.419, train_wall=6, gb_free=10.6, wall=8363
2024-09-04 09:28:15 | INFO | train_inner | epoch 066:      3 / 21 loss=8.233, nll_loss=5.417, ppl=42.73, wps=1688.5, ups=0.34, wpb=4944.5, bsz=84, num_updates=1368, lr=1.6416e-05, gnorm=2.182, train_wall=6, gb_free=13.1, wall=8852
2024-09-04 09:28:19 | INFO | train_inner | epoch 067:     12 / 16 loss=9.308, nll_loss=6.862, ppl=116.31, wps=1418.9, ups=0.38, wpb=3735.5, bsz=100, num_updates=1068, lr=1.2816e-05, gnorm=2.446, train_wall=5, gb_free=9.4, wall=8369
2024-09-04 09:28:20 | INFO | train_inner | epoch 066:      5 / 21 loss=7.92, nll_loss=5.032, ppl=32.71, wps=1940.9, ups=0.42, wpb=4661, bsz=116, num_updates=1370, lr=1.644e-05, gnorm=1.965, train_wall=5, gb_free=14.4, wall=8857
2024-09-04 09:28:25 | INFO | train_inner | epoch 066:      7 / 21 loss=7.832, nll_loss=4.912, ppl=30.12, wps=1788.5, ups=0.42, wpb=4260.5, bsz=116, num_updates=1372, lr=1.6464e-05, gnorm=2.289, train_wall=5, gb_free=12.5, wall=8862
2024-09-04 09:28:26 | INFO | train_inner | epoch 067:     14 / 16 loss=9.231, nll_loss=6.731, ppl=106.2, wps=1428.5, ups=0.32, wpb=4523.5, bsz=132, num_updates=1070, lr=1.284e-05, gnorm=2.474, train_wall=6, gb_free=11.1, wall=8375
2024-09-04 09:28:30 | INFO | train_inner | epoch 066:      9 / 21 loss=8.183, nll_loss=5.404, ppl=42.33, wps=1541.8, ups=0.38, wpb=4017.5, bsz=61, num_updates=1374, lr=1.6488e-05, gnorm=2.929, train_wall=5, gb_free=17.3, wall=8867
2024-09-04 09:28:32 | INFO | train_inner | epoch 067:     16 / 16 loss=9.07, nll_loss=6.526, ppl=92.14, wps=1468, ups=0.32, wpb=4524.5, bsz=172, num_updates=1072, lr=1.2864e-05, gnorm=2.069, train_wall=6, gb_free=10.5, wall=8381
2024-09-04 09:28:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19359.77734375Mb; avail=235653.81640625Mb
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000757
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19360.26953125Mb; avail=235653.32421875Mb
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012890
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19360.26953125Mb; avail=235653.32421875Mb
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011247
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025274
2024-09-04 09:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19360.26953125Mb; avail=235653.32421875Mb
2024-09-04 09:28:34 | INFO | train_inner | epoch 066:     11 / 21 loss=8.185, nll_loss=5.368, ppl=41.3, wps=1926.3, ups=0.47, wpb=4068.5, bsz=64, num_updates=1376, lr=1.6512e-05, gnorm=3.304, train_wall=4, gb_free=12, wall=8871
2024-09-04 09:28:40 | INFO | train_inner | epoch 066:     13 / 21 loss=7.95, nll_loss=5.025, ppl=32.56, wps=1481.4, ups=0.35, wpb=4249, bsz=84, num_updates=1378, lr=1.6536e-05, gnorm=2.343, train_wall=6, gb_free=13.1, wall=8877
2024-09-04 09:28:45 | INFO | train_inner | epoch 066:     15 / 21 loss=8.217, nll_loss=5.38, ppl=41.65, wps=1707, ups=0.37, wpb=4593, bsz=52, num_updates=1380, lr=1.656e-05, gnorm=2.474, train_wall=5, gb_free=15.1, wall=8882
2024-09-04 09:28:46 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.884 | nll_loss 8.769 | ppl 436.18 | wps 3807.5 | wpb 2070.5 | bsz 122.7 | num_updates 1072 | best_loss 10.884
2024-09-04 09:28:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 1072 updates
2024-09-04 09:28:46 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:28:51 | INFO | train_inner | epoch 066:     17 / 21 loss=7.896, nll_loss=5.026, ppl=32.58, wps=1592.1, ups=0.33, wpb=4824.5, bsz=132, num_updates=1382, lr=1.6584e-05, gnorm=2.4, train_wall=6, gb_free=12.5, wall=8888
2024-09-04 09:28:57 | INFO | train_inner | epoch 066:     19 / 21 loss=7.853, nll_loss=4.982, ppl=31.6, wps=1887, ups=0.39, wpb=4885, bsz=140, num_updates=1384, lr=1.6608e-05, gnorm=2.659, train_wall=5, gb_free=11.4, wall=8894
2024-09-04 09:29:03 | INFO | train_inner | epoch 066:     21 / 21 loss=8.195, nll_loss=5.357, ppl=40.98, wps=1548.1, ups=0.31, wpb=4941.5, bsz=80, num_updates=1386, lr=1.6632e-05, gnorm=2.133, train_wall=6, gb_free=11.1, wall=8900
2024-09-04 09:29:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36495.21875Mb; avail=218518.234375Mb
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36495.21875Mb; avail=218518.234375Mb
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012783
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36495.21875Mb; avail=218518.234375Mb
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011149
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025032
2024-09-04 09:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36495.21875Mb; avail=218517.7421875Mb
2024-09-04 09:29:19 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.464 | nll_loss 6.868 | ppl 116.79 | wps 4964.2 | wpb 2350.9 | bsz 94.7 | num_updates 1386 | best_loss 9.39
2024-09-04 09:29:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 1386 updates
2024-09-04 09:29:19 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:29:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:29:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 67 @ 1072 updates, score 10.884) (writing took 63.1500786729157 seconds)
2024-09-04 09:29:49 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-09-04 09:29:49 | INFO | train | epoch 067 | loss 9.209 | nll_loss 6.715 | ppl 105.03 | wps 546.5 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 1072 | lr 1.2864e-05 | gnorm 2.488 | train_wall 46 | gb_free 10.5 | wall 8459
2024-09-04 09:29:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:29:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:29:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:29:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000773
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36532.3359375Mb; avail=218481.11328125Mb
2024-09-04 09:29:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000599
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36532.3359375Mb; avail=218481.11328125Mb
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36531.84375Mb; avail=218481.60546875Mb
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000194
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001103
2024-09-04 09:29:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36531.84375Mb; avail=218481.60546875Mb
2024-09-04 09:29:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:29:49 | INFO | fairseq.trainer | begin training epoch 68
2024-09-04 09:29:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:29:55 | INFO | train_inner | epoch 068:      2 / 16 loss=9.16, nll_loss=6.656, ppl=100.84, wps=102.8, ups=0.02, wpb=4288, bsz=124, num_updates=1074, lr=1.2888e-05, gnorm=2.271, train_wall=6, gb_free=10.7, wall=8465
2024-09-04 09:30:01 | INFO | train_inner | epoch 068:      4 / 16 loss=9.298, nll_loss=6.813, ppl=112.44, wps=1592.5, ups=0.37, wpb=4309, bsz=112, num_updates=1076, lr=1.2912e-05, gnorm=2.53, train_wall=5, gb_free=13.6, wall=8470
2024-09-04 09:30:05 | INFO | train_inner | epoch 068:      6 / 16 loss=9.25, nll_loss=6.792, ppl=110.82, wps=1464.1, ups=0.41, wpb=3585, bsz=85, num_updates=1078, lr=1.2936e-05, gnorm=2.734, train_wall=5, gb_free=10.5, wall=8475
2024-09-04 09:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:30:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 66 @ 1386 updates, score 9.464) (writing took 48.30540804378688 seconds)
2024-09-04 09:30:08 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-09-04 09:30:08 | INFO | train | epoch 066 | loss 8.02 | nll_loss 5.158 | ppl 35.7 | wps 791.9 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1386 | lr 1.6632e-05 | gnorm 2.459 | train_wall 56 | gb_free 11.1 | wall 8965
2024-09-04 09:30:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:30:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:30:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:30:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000575
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19510.9140625Mb; avail=235502.234375Mb
2024-09-04 09:30:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000559
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19510.9140625Mb; avail=235502.234375Mb
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000026
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19510.9140625Mb; avail=235502.234375Mb
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000155
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001003
2024-09-04 09:30:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19510.9140625Mb; avail=235502.234375Mb
2024-09-04 09:30:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:30:08 | INFO | fairseq.trainer | begin training epoch 67
2024-09-04 09:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:30:11 | INFO | train_inner | epoch 068:      8 / 16 loss=9.241, nll_loss=6.758, ppl=108.21, wps=1540.2, ups=0.34, wpb=4576, bsz=128, num_updates=1080, lr=1.296e-05, gnorm=2.461, train_wall=6, gb_free=11.1, wall=8481
2024-09-04 09:30:14 | INFO | train_inner | epoch 067:      2 / 21 loss=8.177, nll_loss=5.344, ppl=40.62, wps=151.8, ups=0.03, wpb=5369, bsz=92, num_updates=1388, lr=1.6656e-05, gnorm=2.721, train_wall=6, gb_free=11.7, wall=8971
2024-09-04 09:30:17 | INFO | train_inner | epoch 068:     10 / 16 loss=9.358, nll_loss=6.9, ppl=119.46, wps=1429.9, ups=0.34, wpb=4207, bsz=84, num_updates=1082, lr=1.2984e-05, gnorm=2.948, train_wall=6, gb_free=10.2, wall=8487
2024-09-04 09:30:18 | INFO | train_inner | epoch 067:      4 / 21 loss=7.994, nll_loss=5.143, ppl=35.33, wps=1929.3, ups=0.44, wpb=4370.5, bsz=96, num_updates=1390, lr=1.668e-05, gnorm=2.751, train_wall=5, gb_free=16.4, wall=8975
2024-09-04 09:30:23 | INFO | train_inner | epoch 067:      6 / 21 loss=8.104, nll_loss=5.264, ppl=38.43, wps=1938.9, ups=0.41, wpb=4689.5, bsz=76, num_updates=1392, lr=1.6704e-05, gnorm=2.301, train_wall=5, gb_free=13.5, wall=8980
2024-09-04 09:30:24 | INFO | train_inner | epoch 068:     12 / 16 loss=9.072, nll_loss=6.555, ppl=94.01, wps=1470, ups=0.31, wpb=4708, bsz=176, num_updates=1084, lr=1.3008e-05, gnorm=2.574, train_wall=6, gb_free=9.5, wall=8493
2024-09-04 09:30:29 | INFO | train_inner | epoch 067:      8 / 21 loss=8.227, nll_loss=5.416, ppl=42.69, wps=1605.5, ups=0.36, wpb=4457.5, bsz=64, num_updates=1394, lr=1.6728e-05, gnorm=2.426, train_wall=6, gb_free=12.3, wall=8986
2024-09-04 09:30:29 | INFO | train_inner | epoch 068:     14 / 16 loss=8.686, nll_loss=6.026, ppl=65.18, wps=1383.3, ups=0.36, wpb=3861, bsz=180, num_updates=1086, lr=1.3032e-05, gnorm=2.424, train_wall=6, gb_free=9.8, wall=8499
2024-09-04 09:30:34 | INFO | train_inner | epoch 067:     10 / 21 loss=7.989, nll_loss=5.106, ppl=34.43, wps=1630.3, ups=0.36, wpb=4588, bsz=108, num_updates=1396, lr=1.6752e-05, gnorm=2.72, train_wall=6, gb_free=14.3, wall=8991
2024-09-04 09:30:35 | INFO | train_inner | epoch 068:     16 / 16 loss=9.264, nll_loss=6.791, ppl=110.73, wps=1473, ups=0.34, wpb=4357.5, bsz=128, num_updates=1088, lr=1.3056e-05, gnorm=2.764, train_wall=6, gb_free=8.8, wall=8505
2024-09-04 09:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19541.609375Mb; avail=235471.7890625Mb
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000736
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19541.609375Mb; avail=235471.7890625Mb
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012634
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19541.609375Mb; avail=235471.7890625Mb
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011224
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025012
2024-09-04 09:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19541.609375Mb; avail=235471.7890625Mb
2024-09-04 09:30:40 | INFO | train_inner | epoch 067:     12 / 21 loss=7.66, nll_loss=4.716, ppl=26.28, wps=1563.2, ups=0.32, wpb=4826, bsz=140, num_updates=1398, lr=1.6776e-05, gnorm=2.125, train_wall=6, gb_free=12.8, wall=8997
2024-09-04 09:30:45 | INFO | train_inner | epoch 067:     14 / 21 loss=7.922, nll_loss=5.03, ppl=32.68, wps=1842.2, ups=0.42, wpb=4347.5, bsz=92, num_updates=1400, lr=1.68e-05, gnorm=2.544, train_wall=5, gb_free=13.7, wall=9002
2024-09-04 09:30:50 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.914 | nll_loss 8.8 | ppl 445.67 | wps 3805 | wpb 2070.5 | bsz 122.7 | num_updates 1088 | best_loss 10.884
2024-09-04 09:30:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 1088 updates
2024-09-04 09:30:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:30:51 | INFO | train_inner | epoch 067:     16 / 21 loss=7.99, nll_loss=5.149, ppl=35.49, wps=1466.3, ups=0.37, wpb=3959, bsz=57, num_updates=1402, lr=1.6824e-05, gnorm=2.829, train_wall=5, gb_free=17.8, wall=9008
2024-09-04 09:30:55 | INFO | train_inner | epoch 067:     18 / 21 loss=8.171, nll_loss=5.389, ppl=41.91, wps=1866.7, ups=0.45, wpb=4191.5, bsz=80, num_updates=1404, lr=1.6848e-05, gnorm=2.937, train_wall=4, gb_free=14.2, wall=9012
2024-09-04 09:31:01 | INFO | train_inner | epoch 067:     20 / 21 loss=7.74, nll_loss=4.775, ppl=27.37, wps=1605.3, ups=0.34, wpb=4746.5, bsz=144, num_updates=1406, lr=1.6872e-05, gnorm=2.197, train_wall=6, gb_free=12.7, wall=9018
2024-09-04 09:31:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36684.7109375Mb; avail=218328.7890625Mb
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000731
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36685.203125Mb; avail=218328.296875Mb
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012776
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36684.7109375Mb; avail=218328.7890625Mb
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011176
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025057
2024-09-04 09:31:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36684.7109375Mb; avail=218328.7890625Mb
2024-09-04 09:31:21 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.371 | nll_loss 6.741 | ppl 106.95 | wps 4775.3 | wpb 2350.9 | bsz 94.7 | num_updates 1407 | best_loss 9.371
2024-09-04 09:31:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 1407 updates
2024-09-04 09:31:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:31:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:31:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 68 @ 1088 updates, score 10.914) (writing took 38.83875690679997 seconds)
2024-09-04 09:31:29 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-09-04 09:31:29 | INFO | train | epoch 068 | loss 9.17 | nll_loss 6.666 | ppl 101.54 | wps 683.5 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1088 | lr 1.3056e-05 | gnorm 2.588 | train_wall 46 | gb_free 8.8 | wall 8558
2024-09-04 09:31:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:31:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:31:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:31:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000725
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29640.8984375Mb; avail=225372.01171875Mb
2024-09-04 09:31:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000615
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29642.375Mb; avail=225371.02734375Mb
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29642.375Mb; avail=225371.02734375Mb
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001089
2024-09-04 09:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29643.359375Mb; avail=225370.04296875Mb
2024-09-04 09:31:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:31:29 | INFO | fairseq.trainer | begin training epoch 69
2024-09-04 09:31:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:31:33 | INFO | train_inner | epoch 069:      2 / 16 loss=9.221, nll_loss=6.726, ppl=105.83, wps=109.3, ups=0.03, wpb=3161, bsz=61, num_updates=1090, lr=1.308e-05, gnorm=3.298, train_wall=4, gb_free=15.6, wall=8563
2024-09-04 09:31:39 | INFO | train_inner | epoch 069:      4 / 16 loss=8.922, nll_loss=6.324, ppl=80.12, wps=1490, ups=0.33, wpb=4533, bsz=164, num_updates=1092, lr=1.3104e-05, gnorm=2.548, train_wall=6, gb_free=8.3, wall=8569
2024-09-04 09:31:44 | INFO | train_inner | epoch 069:      6 / 16 loss=9.306, nll_loss=6.827, ppl=113.54, wps=1517.9, ups=0.38, wpb=4032.5, bsz=108, num_updates=1094, lr=1.3128e-05, gnorm=2.498, train_wall=5, gb_free=12.7, wall=8574
2024-09-04 09:31:51 | INFO | train_inner | epoch 069:      8 / 16 loss=9.254, nll_loss=6.76, ppl=108.37, wps=1560.3, ups=0.33, wpb=4795, bsz=144, num_updates=1096, lr=1.3152e-05, gnorm=2.786, train_wall=6, gb_free=8.9, wall=8580
2024-09-04 09:32:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:32:12 | INFO | train_inner | epoch 069:     10 / 16 loss=9.063, nll_loss=6.536, ppl=92.8, wps=413.1, ups=0.09, wpb=4381.5, bsz=144, num_updates=1098, lr=1.3176e-05, gnorm=2.678, train_wall=21, gb_free=10.3, wall=8601
2024-09-04 09:32:17 | INFO | train_inner | epoch 069:     12 / 16 loss=9.192, nll_loss=6.707, ppl=104.46, wps=1552.1, ups=0.36, wpb=4274.5, bsz=96, num_updates=1100, lr=1.32e-05, gnorm=2.939, train_wall=5, gb_free=11.1, wall=8607
2024-09-04 09:32:23 | INFO | train_inner | epoch 069:     14 / 16 loss=9.17, nll_loss=6.665, ppl=101.49, wps=1448.9, ups=0.34, wpb=4314, bsz=128, num_updates=1102, lr=1.3224e-05, gnorm=2.805, train_wall=6, gb_free=9.5, wall=8613
2024-09-04 09:32:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 67 @ 1407 updates, score 9.371) (writing took 66.68620021734387 seconds)
2024-09-04 09:32:28 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-09-04 09:32:28 | INFO | train | epoch 067 | loss 7.989 | nll_loss 5.121 | ppl 34.79 | wps 683.8 | ups 0.15 | wpb 4556.3 | bsz 96.9 | num_updates 1407 | lr 1.6884e-05 | gnorm 2.546 | train_wall 56 | gb_free 13.1 | wall 9105
2024-09-04 09:32:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:32:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:32:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:32:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000712
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17161.765625Mb; avail=237868.17578125Mb
2024-09-04 09:32:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000062
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000592
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17161.765625Mb; avail=237868.17578125Mb
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17161.765625Mb; avail=237868.17578125Mb
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000177
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001069
2024-09-04 09:32:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17161.765625Mb; avail=237868.17578125Mb
2024-09-04 09:32:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:32:28 | INFO | fairseq.trainer | begin training epoch 68
2024-09-04 09:32:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:32:29 | INFO | train_inner | epoch 069:     16 / 16 loss=8.955, nll_loss=6.379, ppl=83.25, wps=1443.7, ups=0.33, wpb=4400, bsz=172, num_updates=1104, lr=1.3248e-05, gnorm=2.618, train_wall=6, gb_free=10.5, wall=8619
2024-09-04 09:32:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17195.6796875Mb; avail=237834.21875Mb
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000636
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17195.6796875Mb; avail=237834.21875Mb
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012586
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17195.6796875Mb; avail=237834.21875Mb
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011322
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024901
2024-09-04 09:32:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17195.6796875Mb; avail=237834.21875Mb
2024-09-04 09:32:31 | INFO | train_inner | epoch 068:      1 / 21 loss=7.909, nll_loss=4.995, ppl=31.9, wps=97.8, ups=0.02, wpb=4377.5, bsz=108, num_updates=1408, lr=1.6896e-05, gnorm=2.472, train_wall=6, gb_free=16.7, wall=9107
2024-09-04 09:32:36 | INFO | train_inner | epoch 068:      3 / 21 loss=7.916, nll_loss=5.026, ppl=32.58, wps=2020.3, ups=0.4, wpb=5047.5, bsz=108, num_updates=1410, lr=1.692e-05, gnorm=2.602, train_wall=5, gb_free=10.8, wall=9112
2024-09-04 09:32:41 | INFO | train_inner | epoch 068:      5 / 21 loss=7.799, nll_loss=4.916, ppl=30.18, wps=1510, ups=0.38, wpb=3968, bsz=81, num_updates=1412, lr=1.6944e-05, gnorm=2.927, train_wall=5, gb_free=12.7, wall=9118
2024-09-04 09:32:44 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.878 | nll_loss 8.76 | ppl 433.39 | wps 3824.3 | wpb 2070.5 | bsz 122.7 | num_updates 1104 | best_loss 10.878
2024-09-04 09:32:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 1104 updates
2024-09-04 09:32:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:32:45 | INFO | train_inner | epoch 068:      7 / 21 loss=8.058, nll_loss=5.221, ppl=37.3, wps=1944.8, ups=0.43, wpb=4567.5, bsz=88, num_updates=1414, lr=1.6968e-05, gnorm=2.507, train_wall=5, gb_free=13.6, wall=9122
2024-09-04 09:32:50 | INFO | train_inner | epoch 068:      9 / 21 loss=7.638, nll_loss=4.669, ppl=25.45, wps=1826, ups=0.41, wpb=4459.5, bsz=152, num_updates=1416, lr=1.6992e-05, gnorm=2.143, train_wall=5, gb_free=12, wall=9127
2024-09-04 09:32:57 | INFO | train_inner | epoch 068:     11 / 21 loss=7.985, nll_loss=5.099, ppl=34.28, wps=1629.4, ups=0.31, wpb=5188.5, bsz=88, num_updates=1418, lr=1.7016e-05, gnorm=2.224, train_wall=6, gb_free=11.3, wall=9134
2024-09-04 09:33:02 | INFO | train_inner | epoch 068:     13 / 21 loss=7.981, nll_loss=5.112, ppl=34.59, wps=1671.8, ups=0.36, wpb=4658, bsz=100, num_updates=1420, lr=1.704e-05, gnorm=2.19, train_wall=6, gb_free=13.9, wall=9139
2024-09-04 09:33:08 | INFO | train_inner | epoch 068:     15 / 21 loss=8.204, nll_loss=5.387, ppl=41.83, wps=1615.8, ups=0.35, wpb=4640.5, bsz=64, num_updates=1422, lr=1.7064e-05, gnorm=2.581, train_wall=6, gb_free=13.8, wall=9145
2024-09-04 09:33:13 | INFO | train_inner | epoch 068:     17 / 21 loss=7.685, nll_loss=4.776, ppl=27.39, wps=1892.5, ups=0.4, wpb=4763.5, bsz=152, num_updates=1424, lr=1.7088e-05, gnorm=2.432, train_wall=5, gb_free=11.5, wall=9150
2024-09-04 09:33:19 | INFO | train_inner | epoch 068:     19 / 21 loss=8.061, nll_loss=5.19, ppl=36.52, wps=1557.8, ups=0.37, wpb=4249.5, bsz=80, num_updates=1426, lr=1.7112e-05, gnorm=2.559, train_wall=5, gb_free=13.4, wall=9155
2024-09-04 09:33:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:33:23 | INFO | train_inner | epoch 068:     21 / 21 loss=8.21, nll_loss=5.368, ppl=41.3, wps=1915.2, ups=0.45, wpb=4217, bsz=64, num_updates=1428, lr=1.7136e-05, gnorm=2.712, train_wall=4, gb_free=13.9, wall=9160
2024-09-04 09:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22241.85546875Mb; avail=232788.04296875Mb
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000746
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22241.85546875Mb; avail=232788.04296875Mb
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012801
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22060.23828125Mb; avail=232969.16796875Mb
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011427
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025340
2024-09-04 09:33:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22054.82421875Mb; avail=232975.07421875Mb
2024-09-04 09:33:39 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.413 | nll_loss 6.847 | ppl 115.14 | wps 4958.4 | wpb 2350.9 | bsz 94.7 | num_updates 1428 | best_loss 9.371
2024-09-04 09:33:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 1428 updates
2024-09-04 09:33:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:33:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 69 @ 1104 updates, score 10.878) (writing took 62.633413639850914 seconds)
2024-09-04 09:33:46 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-09-04 09:33:46 | INFO | train | epoch 069 | loss 9.131 | nll_loss 6.609 | ppl 97.63 | wps 491.5 | ups 0.12 | wpb 4236.4 | bsz 127.1 | num_updates 1104 | lr 1.3248e-05 | gnorm 2.771 | train_wall 61 | gb_free 10.5 | wall 8696
2024-09-04 09:33:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:33:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:33:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:33:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000720
2024-09-04 09:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38359.55859375Mb; avail=216670.296875Mb
2024-09-04 09:33:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 09:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000686
2024-09-04 09:33:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38361.03515625Mb; avail=216668.8203125Mb
2024-09-04 09:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-09-04 09:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38361.52734375Mb; avail=216668.328125Mb
2024-09-04 09:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000182
2024-09-04 09:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001220
2024-09-04 09:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38362.51171875Mb; avail=216667.34375Mb
2024-09-04 09:33:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:33:47 | INFO | fairseq.trainer | begin training epoch 70
2024-09-04 09:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:33:52 | INFO | train_inner | epoch 070:      2 / 16 loss=9.152, nll_loss=6.63, ppl=99.03, wps=109.9, ups=0.02, wpb=4554, bsz=140, num_updates=1106, lr=1.3272e-05, gnorm=2.484, train_wall=6, gb_free=11.1, wall=8702
2024-09-04 09:33:58 | INFO | train_inner | epoch 070:      4 / 16 loss=9.199, nll_loss=6.699, ppl=103.9, wps=1455.8, ups=0.36, wpb=3997.5, bsz=92, num_updates=1108, lr=1.3296e-05, gnorm=3.336, train_wall=5, gb_free=10.3, wall=8707
2024-09-04 09:34:04 | INFO | train_inner | epoch 070:      6 / 16 loss=9.097, nll_loss=6.578, ppl=95.54, wps=1545.9, ups=0.33, wpb=4633.5, bsz=144, num_updates=1110, lr=1.332e-05, gnorm=3.015, train_wall=6, gb_free=9, wall=8713
2024-09-04 09:34:08 | INFO | train_inner | epoch 070:      8 / 16 loss=9.194, nll_loss=6.69, ppl=103.24, wps=1474.7, ups=0.42, wpb=3482.5, bsz=89, num_updates=1112, lr=1.3344e-05, gnorm=2.73, train_wall=5, gb_free=14.5, wall=8718
2024-09-04 09:34:15 | INFO | train_inner | epoch 070:     10 / 16 loss=8.784, nll_loss=6.161, ppl=71.56, wps=1437.3, ups=0.33, wpb=4375.5, bsz=160, num_updates=1114, lr=1.3368e-05, gnorm=2.693, train_wall=6, gb_free=11.3, wall=8724
2024-09-04 09:34:20 | INFO | train_inner | epoch 070:     12 / 16 loss=9.037, nll_loss=6.502, ppl=90.67, wps=1491.4, ups=0.34, wpb=4411, bsz=164, num_updates=1116, lr=1.3392e-05, gnorm=2.789, train_wall=6, gb_free=11.9, wall=8730
2024-09-04 09:34:26 | INFO | train_inner | epoch 070:     14 / 16 loss=9.11, nll_loss=6.584, ppl=95.92, wps=1498.5, ups=0.34, wpb=4469.5, bsz=128, num_updates=1118, lr=1.3416e-05, gnorm=2.746, train_wall=6, gb_free=11.8, wall=8736
2024-09-04 09:34:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 68 @ 1428 updates, score 9.413) (writing took 52.05509118549526 seconds)
2024-09-04 09:34:31 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-09-04 09:34:31 | INFO | train | epoch 068 | loss 7.955 | nll_loss 5.076 | ppl 33.73 | wps 773.2 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1428 | lr 1.7136e-05 | gnorm 2.492 | train_wall 55 | gb_free 13.9 | wall 9228
2024-09-04 09:34:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:34:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:34:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:34:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000611
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23656.27734375Mb; avail=231373.62109375Mb
2024-09-04 09:34:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000549
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23656.27734375Mb; avail=231373.62109375Mb
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23656.27734375Mb; avail=231373.62109375Mb
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000159
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000993
2024-09-04 09:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23656.27734375Mb; avail=231373.62109375Mb
2024-09-04 09:34:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:34:31 | INFO | fairseq.trainer | begin training epoch 69
2024-09-04 09:34:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:34:32 | INFO | train_inner | epoch 070:     16 / 16 loss=9.176, nll_loss=6.667, ppl=101.62, wps=1465.7, ups=0.37, wpb=3968, bsz=100, num_updates=1120, lr=1.344e-05, gnorm=2.688, train_wall=5, gb_free=8.9, wall=8741
2024-09-04 09:34:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23687.30078125Mb; avail=231342.5546875Mb
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23687.30078125Mb; avail=231342.5546875Mb
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012845
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23687.30078125Mb; avail=231342.5546875Mb
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011326
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025275
2024-09-04 09:34:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23687.79296875Mb; avail=231342.0625Mb
2024-09-04 09:34:37 | INFO | train_inner | epoch 069:      2 / 21 loss=7.909, nll_loss=5.009, ppl=32.21, wps=113.9, ups=0.03, wpb=4189.5, bsz=80, num_updates=1430, lr=1.716e-05, gnorm=2.435, train_wall=5, gb_free=12.5, wall=9233
2024-09-04 09:34:41 | INFO | train_inner | epoch 069:      4 / 21 loss=7.929, nll_loss=5.048, ppl=33.09, wps=1776.9, ups=0.44, wpb=4003, bsz=61, num_updates=1432, lr=1.7184e-05, gnorm=2.631, train_wall=4, gb_free=13.7, wall=9238
2024-09-04 09:34:46 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.855 | nll_loss 8.719 | ppl 421.33 | wps 3814.8 | wpb 2070.5 | bsz 122.7 | num_updates 1120 | best_loss 10.855
2024-09-04 09:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 1120 updates
2024-09-04 09:34:46 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:34:47 | INFO | train_inner | epoch 069:      6 / 21 loss=7.541, nll_loss=4.584, ppl=23.98, wps=1436.4, ups=0.34, wpb=4228, bsz=152, num_updates=1434, lr=1.7208e-05, gnorm=3.542, train_wall=6, gb_free=13.1, wall=9244
2024-09-04 09:34:52 | INFO | train_inner | epoch 069:      8 / 21 loss=8.357, nll_loss=5.597, ppl=48.42, wps=2026.6, ups=0.43, wpb=4690.5, bsz=48, num_updates=1436, lr=1.7232e-05, gnorm=2.751, train_wall=5, gb_free=12.4, wall=9248
2024-09-04 09:34:57 | INFO | train_inner | epoch 069:     10 / 21 loss=8.046, nll_loss=5.18, ppl=36.25, wps=1768.5, ups=0.34, wpb=5211.5, bsz=100, num_updates=1438, lr=1.7256e-05, gnorm=2.733, train_wall=6, gb_free=11.3, wall=9254
2024-09-04 09:35:02 | INFO | train_inner | epoch 069:     12 / 21 loss=7.821, nll_loss=4.877, ppl=29.39, wps=1891.9, ups=0.43, wpb=4395.5, bsz=104, num_updates=1440, lr=1.728e-05, gnorm=2.579, train_wall=5, gb_free=12.9, wall=9259
2024-09-04 09:35:08 | INFO | train_inner | epoch 069:     14 / 21 loss=7.942, nll_loss=5.048, ppl=33.08, wps=1622.2, ups=0.35, wpb=4589.5, bsz=100, num_updates=1442, lr=1.7304e-05, gnorm=2.538, train_wall=6, gb_free=13.5, wall=9265
2024-09-04 09:35:13 | INFO | train_inner | epoch 069:     16 / 21 loss=8.104, nll_loss=5.264, ppl=38.42, wps=1668.4, ups=0.36, wpb=4618, bsz=72, num_updates=1444, lr=1.7328e-05, gnorm=2.333, train_wall=6, gb_free=12.6, wall=9270
2024-09-04 09:35:19 | INFO | train_inner | epoch 069:     18 / 21 loss=8.005, nll_loss=5.147, ppl=35.42, wps=1683.5, ups=0.37, wpb=4514, bsz=108, num_updates=1446, lr=1.7352e-05, gnorm=2.683, train_wall=5, gb_free=12.9, wall=9276
2024-09-04 09:35:24 | INFO | train_inner | epoch 069:     20 / 21 loss=8.04, nll_loss=5.192, ppl=36.55, wps=2016.5, ups=0.41, wpb=4963, bsz=88, num_updates=1448, lr=1.7376e-05, gnorm=2.738, train_wall=5, gb_free=11.2, wall=9280
2024-09-04 09:35:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31417.41796875Mb; avail=223617.3515625Mb
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000752
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31416.92578125Mb; avail=223612.4296875Mb
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012849
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31417.41796875Mb; avail=223611.9375Mb
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011261
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025293
2024-09-04 09:35:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31416.92578125Mb; avail=223612.921875Mb
2024-09-04 09:35:44 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.379 | nll_loss 6.807 | ppl 111.99 | wps 4948.5 | wpb 2350.9 | bsz 94.7 | num_updates 1449 | best_loss 9.371
2024-09-04 09:35:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 1449 updates
2024-09-04 09:35:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:35:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 70 @ 1120 updates, score 10.855) (writing took 62.73051527980715 seconds)
2024-09-04 09:35:49 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-09-04 09:35:49 | INFO | train | epoch 070 | loss 9.089 | nll_loss 6.558 | ppl 94.24 | wps 553 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 1120 | lr 1.344e-05 | gnorm 2.81 | train_wall 45 | gb_free 8.9 | wall 8819
2024-09-04 09:35:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:35:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:35:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:35:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000757
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35794.078125Mb; avail=219235.703125Mb
2024-09-04 09:35:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000069
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000586
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35794.078125Mb; avail=219235.703125Mb
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35794.078125Mb; avail=219235.703125Mb
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000170
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001073
2024-09-04 09:35:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35794.078125Mb; avail=219235.703125Mb
2024-09-04 09:35:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:35:49 | INFO | fairseq.trainer | begin training epoch 71
2024-09-04 09:35:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:35:55 | INFO | train_inner | epoch 071:      2 / 16 loss=9.2, nll_loss=6.694, ppl=103.57, wps=101.2, ups=0.02, wpb=4204.5, bsz=92, num_updates=1122, lr=1.3464e-05, gnorm=2.863, train_wall=6, gb_free=10.6, wall=8824
2024-09-04 09:36:10 | INFO | train_inner | epoch 071:      4 / 16 loss=9.068, nll_loss=6.517, ppl=91.56, wps=516.3, ups=0.13, wpb=3967, bsz=120, num_updates=1124, lr=1.3488e-05, gnorm=2.7, train_wall=15, gb_free=12, wall=8840
2024-09-04 09:36:16 | INFO | train_inner | epoch 071:      6 / 16 loss=9.03, nll_loss=6.472, ppl=88.79, wps=1556.9, ups=0.33, wpb=4657.5, bsz=160, num_updates=1126, lr=1.3512e-05, gnorm=2.76, train_wall=6, gb_free=9.8, wall=8846
2024-09-04 09:36:22 | INFO | train_inner | epoch 071:      8 / 16 loss=9.236, nll_loss=6.763, ppl=108.6, wps=1597.1, ups=0.36, wpb=4383.5, bsz=92, num_updates=1128, lr=1.3536e-05, gnorm=3.28, train_wall=5, gb_free=10.9, wall=8851
2024-09-04 09:36:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:36:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 69 @ 1449 updates, score 9.379) (writing took 41.99829902779311 seconds)
2024-09-04 09:36:26 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-09-04 09:36:26 | INFO | train | epoch 069 | loss 7.948 | nll_loss 5.067 | ppl 33.52 | wps 838 | ups 0.18 | wpb 4556.3 | bsz 96.9 | num_updates 1449 | lr 1.7388e-05 | gnorm 2.68 | train_wall 55 | gb_free 11.1 | wall 9343
2024-09-04 09:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000703
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25338.28125Mb; avail=229691.66015625Mb
2024-09-04 09:36:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000552
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25338.28125Mb; avail=229691.66015625Mb
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25338.28125Mb; avail=229691.66015625Mb
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001017
2024-09-04 09:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25338.28125Mb; avail=229691.66015625Mb
2024-09-04 09:36:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:36:26 | INFO | fairseq.trainer | begin training epoch 70
2024-09-04 09:36:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:36:28 | INFO | train_inner | epoch 071:     10 / 16 loss=8.619, nll_loss=5.962, ppl=62.35, wps=1521, ups=0.34, wpb=4467.5, bsz=220, num_updates=1130, lr=1.356e-05, gnorm=2.78, train_wall=6, gb_free=10.4, wall=8857
2024-09-04 09:36:29 | INFO | train_inner | epoch 070:      1 / 21 loss=7.734, nll_loss=4.795, ppl=27.76, wps=141.5, ups=0.03, wpb=4598, bsz=140, num_updates=1450, lr=1.74e-05, gnorm=2.82, train_wall=6, gb_free=13, wall=9345
2024-09-04 09:36:33 | INFO | train_inner | epoch 071:     12 / 16 loss=9.085, nll_loss=6.524, ppl=92, wps=1390.3, ups=0.38, wpb=3615.5, bsz=105, num_updates=1132, lr=1.3584e-05, gnorm=3.551, train_wall=5, gb_free=10.7, wall=8862
2024-09-04 09:36:33 | INFO | train_inner | epoch 070:      3 / 21 loss=8.134, nll_loss=5.316, ppl=39.83, wps=1967.1, ups=0.44, wpb=4515.5, bsz=72, num_updates=1452, lr=1.7424e-05, gnorm=2.663, train_wall=5, gb_free=14.1, wall=9350
2024-09-04 09:36:38 | INFO | train_inner | epoch 071:     14 / 16 loss=9.156, nll_loss=6.642, ppl=99.86, wps=1492.7, ups=0.37, wpb=4032, bsz=108, num_updates=1134, lr=1.3608e-05, gnorm=3.382, train_wall=5, gb_free=8.6, wall=8868
2024-09-04 09:36:39 | INFO | train_inner | epoch 070:      5 / 21 loss=7.887, nll_loss=4.991, ppl=31.81, wps=1723.6, ups=0.34, wpb=5130.5, bsz=112, num_updates=1454, lr=1.7448e-05, gnorm=2.395, train_wall=6, gb_free=12.8, wall=9356
2024-09-04 09:36:44 | INFO | train_inner | epoch 070:      7 / 21 loss=7.937, nll_loss=5.056, ppl=33.27, wps=2036, ups=0.41, wpb=4931.5, bsz=92, num_updates=1456, lr=1.7472e-05, gnorm=2.621, train_wall=5, gb_free=12.6, wall=9361
2024-09-04 09:36:44 | INFO | train_inner | epoch 071:     16 / 16 loss=9.072, nll_loss=6.544, ppl=93.34, wps=1548, ups=0.34, wpb=4564, bsz=120, num_updates=1136, lr=1.3632e-05, gnorm=3.193, train_wall=6, gb_free=10, wall=8874
2024-09-04 09:36:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25369.07421875Mb; avail=229660.82421875Mb
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25369.07421875Mb; avail=229660.82421875Mb
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012499
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25369.07421875Mb; avail=229660.82421875Mb
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011249
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024714
2024-09-04 09:36:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25369.07421875Mb; avail=229660.82421875Mb
2024-09-04 09:36:48 | INFO | train_inner | epoch 070:      9 / 21 loss=7.975, nll_loss=5.128, ppl=34.96, wps=1916.7, ups=0.51, wpb=3751, bsz=69, num_updates=1458, lr=1.7496e-05, gnorm=3.192, train_wall=4, gb_free=13.2, wall=9365
2024-09-04 09:36:54 | INFO | train_inner | epoch 070:     11 / 21 loss=7.824, nll_loss=4.9, ppl=29.86, wps=1600.6, ups=0.34, wpb=4690, bsz=108, num_updates=1460, lr=1.752e-05, gnorm=2.358, train_wall=6, gb_free=12.5, wall=9371
2024-09-04 09:36:59 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.881 | nll_loss 8.758 | ppl 432.8 | wps 3825 | wpb 2070.5 | bsz 122.7 | num_updates 1136 | best_loss 10.855
2024-09-04 09:36:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 1136 updates
2024-09-04 09:36:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:36:59 | INFO | train_inner | epoch 070:     13 / 21 loss=7.997, nll_loss=5.112, ppl=34.58, wps=1870.1, ups=0.41, wpb=4602, bsz=96, num_updates=1462, lr=1.7544e-05, gnorm=2.348, train_wall=5, gb_free=11.1, wall=9376
2024-09-04 09:37:05 | INFO | train_inner | epoch 070:     15 / 21 loss=7.72, nll_loss=4.792, ppl=27.7, wps=1493.5, ups=0.33, wpb=4524.5, bsz=132, num_updates=1464, lr=1.7568e-05, gnorm=2.297, train_wall=6, gb_free=13.5, wall=9382
2024-09-04 09:37:11 | INFO | train_inner | epoch 070:     17 / 21 loss=7.478, nll_loss=4.483, ppl=22.36, wps=1448.7, ups=0.34, wpb=4246.5, bsz=128, num_updates=1466, lr=1.7592e-05, gnorm=2.175, train_wall=6, gb_free=12.5, wall=9387
2024-09-04 09:37:17 | INFO | train_inner | epoch 070:     19 / 21 loss=7.986, nll_loss=5.111, ppl=34.55, wps=1609.6, ups=0.32, wpb=4989, bsz=96, num_updates=1468, lr=1.7616e-05, gnorm=2.245, train_wall=6, gb_free=12, wall=9394
2024-09-04 09:37:21 | INFO | train_inner | epoch 070:     21 / 21 loss=8.047, nll_loss=5.194, ppl=36.6, wps=1824.3, ups=0.42, wpb=4301, bsz=76, num_updates=1470, lr=1.764e-05, gnorm=2.712, train_wall=5, gb_free=14.2, wall=9398
2024-09-04 09:37:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42502.58984375Mb; avail=212527.2421875Mb
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000761
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42503.08203125Mb; avail=212526.75Mb
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012827
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42503.31640625Mb; avail=212526.515625Mb
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011420
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025473
2024-09-04 09:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42503.31640625Mb; avail=212526.515625Mb
2024-09-04 09:37:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:37:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 71 @ 1136 updates, score 10.881) (writing took 39.040535021573305 seconds)
2024-09-04 09:37:38 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-09-04 09:37:38 | INFO | train | epoch 071 | loss 9.055 | nll_loss 6.511 | ppl 91.19 | wps 624.2 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1136 | lr 1.3632e-05 | gnorm 3.064 | train_wall 55 | gb_free 10 | wall 8927
2024-09-04 09:37:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:37:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:37:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:37:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000629
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33151.5546875Mb; avail=221878.37890625Mb
2024-09-04 09:37:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000572
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33151.5546875Mb; avail=221878.37890625Mb
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33151.5546875Mb; avail=221878.37890625Mb
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000154
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001021
2024-09-04 09:37:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33151.5546875Mb; avail=221878.37890625Mb
2024-09-04 09:37:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:37:38 | INFO | fairseq.trainer | begin training epoch 72
2024-09-04 09:37:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:37:39 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.388 | nll_loss 6.808 | ppl 112.05 | wps 4530.9 | wpb 2350.9 | bsz 94.7 | num_updates 1470 | best_loss 9.371
2024-09-04 09:37:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 1470 updates
2024-09-04 09:37:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:37:43 | INFO | train_inner | epoch 072:      2 / 16 loss=9.053, nll_loss=6.499, ppl=90.45, wps=139.9, ups=0.03, wpb=4145.5, bsz=120, num_updates=1138, lr=1.3656e-05, gnorm=2.76, train_wall=6, gb_free=10, wall=8933
2024-09-04 09:37:48 | INFO | train_inner | epoch 072:      4 / 16 loss=8.983, nll_loss=6.412, ppl=85.13, wps=1480.3, ups=0.39, wpb=3814, bsz=121, num_updates=1140, lr=1.368e-05, gnorm=2.679, train_wall=5, gb_free=8.4, wall=8938
2024-09-04 09:37:54 | INFO | train_inner | epoch 072:      6 / 16 loss=8.614, nll_loss=5.947, ppl=61.67, wps=1481.9, ups=0.33, wpb=4433, bsz=192, num_updates=1142, lr=1.3704e-05, gnorm=2.621, train_wall=6, gb_free=9.6, wall=8944
2024-09-04 09:38:00 | INFO | train_inner | epoch 072:      8 / 16 loss=9.235, nll_loss=6.75, ppl=107.61, wps=1476.8, ups=0.38, wpb=3860.5, bsz=80, num_updates=1144, lr=1.3728e-05, gnorm=3.021, train_wall=5, gb_free=10, wall=8949
2024-09-04 09:38:05 | INFO | train_inner | epoch 072:     10 / 16 loss=9.094, nll_loss=6.56, ppl=94.32, wps=1523.8, ups=0.36, wpb=4271, bsz=120, num_updates=1146, lr=1.3752e-05, gnorm=2.743, train_wall=6, gb_free=14, wall=8955
2024-09-04 09:38:11 | INFO | train_inner | epoch 072:     12 / 16 loss=8.798, nll_loss=6.186, ppl=72.81, wps=1477.5, ups=0.34, wpb=4289, bsz=164, num_updates=1148, lr=1.3776e-05, gnorm=2.378, train_wall=6, gb_free=13.4, wall=8961
2024-09-04 09:38:17 | INFO | train_inner | epoch 072:     14 / 16 loss=9.093, nll_loss=6.563, ppl=94.55, wps=1560.6, ups=0.35, wpb=4518.5, bsz=104, num_updates=1150, lr=1.38e-05, gnorm=2.957, train_wall=6, gb_free=10.3, wall=8966
2024-09-04 09:38:23 | INFO | train_inner | epoch 072:     16 / 16 loss=9.055, nll_loss=6.488, ppl=89.74, wps=1466.9, ups=0.32, wpb=4560, bsz=116, num_updates=1152, lr=1.3824e-05, gnorm=2.618, train_wall=6, gb_free=11, wall=8973
2024-09-04 09:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45108.08984375Mb; avail=209921.7578125Mb
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000729
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45108.08984375Mb; avail=209921.7578125Mb
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012610
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45108.0390625Mb; avail=209921.7890625Mb
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011097
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024794
2024-09-04 09:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45108.0390625Mb; avail=209921.7890625Mb
2024-09-04 09:38:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 70 @ 1470 updates, score 9.388) (writing took 48.57601009402424 seconds)
2024-09-04 09:38:28 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-09-04 09:38:28 | INFO | train | epoch 070 | loss 7.909 | nll_loss 5.021 | ppl 32.47 | wps 781.4 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1470 | lr 1.764e-05 | gnorm 2.538 | train_wall 56 | gb_free 14.2 | wall 9465
2024-09-04 09:38:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:38:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:38:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:38:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000575
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31653.515625Mb; avail=223376.37890625Mb
2024-09-04 09:38:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000056
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000538
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31653.515625Mb; avail=223376.37890625Mb
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31653.515625Mb; avail=223376.37890625Mb
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000159
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000984
2024-09-04 09:38:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31653.515625Mb; avail=223376.37890625Mb
2024-09-04 09:38:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:38:28 | INFO | fairseq.trainer | begin training epoch 71
2024-09-04 09:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:38:33 | INFO | train_inner | epoch 071:      2 / 21 loss=7.596, nll_loss=4.612, ppl=24.46, wps=134.9, ups=0.03, wpb=4856, bsz=156, num_updates=1472, lr=1.7664e-05, gnorm=2.433, train_wall=5, gb_free=13, wall=9470
2024-09-04 09:38:38 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.885 | nll_loss 8.748 | ppl 430.05 | wps 3805.2 | wpb 2070.5 | bsz 122.7 | num_updates 1152 | best_loss 10.855
2024-09-04 09:38:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 1152 updates
2024-09-04 09:38:38 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:38:38 | INFO | train_inner | epoch 071:      4 / 21 loss=8.152, nll_loss=5.321, ppl=39.97, wps=1697.5, ups=0.4, wpb=4247.5, bsz=60, num_updates=1474, lr=1.7688e-05, gnorm=2.872, train_wall=5, gb_free=13.4, wall=9475
2024-09-04 09:38:43 | INFO | train_inner | epoch 071:      6 / 21 loss=7.878, nll_loss=5.008, ppl=32.17, wps=1906.9, ups=0.44, wpb=4345, bsz=92, num_updates=1476, lr=1.7712e-05, gnorm=2.985, train_wall=5, gb_free=13, wall=9480
2024-09-04 09:38:48 | INFO | train_inner | epoch 071:      8 / 21 loss=7.496, nll_loss=4.481, ppl=22.34, wps=1502.4, ups=0.38, wpb=3945, bsz=125, num_updates=1478, lr=1.7736e-05, gnorm=2.672, train_wall=5, gb_free=12.7, wall=9485
2024-09-04 09:38:53 | INFO | train_inner | epoch 071:     10 / 21 loss=7.795, nll_loss=4.865, ppl=29.15, wps=1889.1, ups=0.39, wpb=4795.5, bsz=104, num_updates=1480, lr=1.776e-05, gnorm=2.69, train_wall=5, gb_free=12.2, wall=9490
2024-09-04 09:38:59 | INFO | train_inner | epoch 071:     12 / 21 loss=7.91, nll_loss=5.015, ppl=32.33, wps=1663.2, ups=0.35, wpb=4796, bsz=96, num_updates=1482, lr=1.7784e-05, gnorm=2.649, train_wall=6, gb_free=11.8, wall=9496
2024-09-04 09:39:04 | INFO | train_inner | epoch 071:     14 / 21 loss=7.814, nll_loss=4.895, ppl=29.76, wps=1964.9, ups=0.43, wpb=4611.5, bsz=116, num_updates=1484, lr=1.7808e-05, gnorm=2.324, train_wall=5, gb_free=16.4, wall=9501
2024-09-04 09:39:10 | INFO | train_inner | epoch 071:     16 / 21 loss=7.888, nll_loss=4.986, ppl=31.69, wps=1654.4, ups=0.33, wpb=5086, bsz=104, num_updates=1486, lr=1.7832e-05, gnorm=2.365, train_wall=6, gb_free=11.1, wall=9507
2024-09-04 09:39:15 | INFO | train_inner | epoch 071:     18 / 21 loss=8.065, nll_loss=5.251, ppl=38.08, wps=1984.4, ups=0.43, wpb=4621.5, bsz=60, num_updates=1488, lr=1.7856e-05, gnorm=2.729, train_wall=5, gb_free=12.5, wall=9512
2024-09-04 09:39:20 | INFO | train_inner | epoch 071:     20 / 21 loss=8.055, nll_loss=5.159, ppl=35.73, wps=1535.2, ups=0.35, wpb=4345.5, bsz=76, num_updates=1490, lr=1.788e-05, gnorm=2.544, train_wall=6, gb_free=12.8, wall=9517
2024-09-04 09:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:39:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 72 @ 1152 updates, score 10.885) (writing took 43.56537539418787 seconds)
2024-09-04 09:39:21 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-09-04 09:39:21 | INFO | train | epoch 072 | loss 8.987 | nll_loss 6.421 | ppl 85.66 | wps 654.1 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1152 | lr 1.3824e-05 | gnorm 2.722 | train_wall 45 | gb_free 11 | wall 9031
2024-09-04 09:39:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:39:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:39:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:39:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000808
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28670.71484375Mb; avail=226359.26953125Mb
2024-09-04 09:39:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000045
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000557
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28670.71484375Mb; avail=226359.26953125Mb
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28670.71484375Mb; avail=226359.26953125Mb
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000148
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000990
2024-09-04 09:39:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28670.71484375Mb; avail=226359.26953125Mb
2024-09-04 09:39:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:39:21 | INFO | fairseq.trainer | begin training epoch 73
2024-09-04 09:39:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:39:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28709.6796875Mb; avail=226320.26171875Mb
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000745
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28709.6796875Mb; avail=226320.26171875Mb
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012806
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28709.6796875Mb; avail=226320.26171875Mb
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011275
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025190
2024-09-04 09:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28709.6796875Mb; avail=226320.26171875Mb
2024-09-04 09:39:27 | INFO | train_inner | epoch 073:      2 / 16 loss=9.005, nll_loss=6.447, ppl=87.26, wps=135.5, ups=0.03, wpb=4319, bsz=140, num_updates=1154, lr=1.3848e-05, gnorm=2.928, train_wall=5, gb_free=10.1, wall=9036
2024-09-04 09:39:32 | INFO | train_inner | epoch 073:      4 / 16 loss=9.044, nll_loss=6.499, ppl=90.46, wps=1582, ups=0.36, wpb=4372, bsz=116, num_updates=1156, lr=1.3872e-05, gnorm=3.108, train_wall=6, gb_free=10.5, wall=9042
2024-09-04 09:39:38 | INFO | train_inner | epoch 073:      6 / 16 loss=8.994, nll_loss=6.435, ppl=86.53, wps=1492.2, ups=0.34, wpb=4358.5, bsz=152, num_updates=1158, lr=1.3896e-05, gnorm=3.206, train_wall=6, gb_free=11.8, wall=9048
2024-09-04 09:39:39 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.405 | nll_loss 6.841 | ppl 114.67 | wps 4945.9 | wpb 2350.9 | bsz 94.7 | num_updates 1491 | best_loss 9.371
2024-09-04 09:39:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 1491 updates
2024-09-04 09:39:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:39:44 | INFO | train_inner | epoch 073:      8 / 16 loss=9.148, nll_loss=6.611, ppl=97.72, wps=1547.7, ups=0.34, wpb=4522.5, bsz=100, num_updates=1160, lr=1.392e-05, gnorm=3.039, train_wall=6, gb_free=12.5, wall=9054
2024-09-04 09:39:49 | INFO | train_inner | epoch 073:     10 / 16 loss=9.003, nll_loss=6.444, ppl=87.05, wps=1494.7, ups=0.37, wpb=4035, bsz=100, num_updates=1162, lr=1.3944e-05, gnorm=3.409, train_wall=5, gb_free=8.9, wall=9059
2024-09-04 09:39:54 | INFO | train_inner | epoch 073:     12 / 16 loss=9.148, nll_loss=6.626, ppl=98.76, wps=1483.3, ups=0.4, wpb=3704.5, bsz=85, num_updates=1164, lr=1.3968e-05, gnorm=3.488, train_wall=5, gb_free=16, wall=9064
2024-09-04 09:40:00 | INFO | train_inner | epoch 073:     14 / 16 loss=8.992, nll_loss=6.431, ppl=86.27, wps=1488.9, ups=0.34, wpb=4375, bsz=128, num_updates=1166, lr=1.3992e-05, gnorm=3.21, train_wall=6, gb_free=11, wall=9070
2024-09-04 09:40:06 | INFO | train_inner | epoch 073:     16 / 16 loss=8.481, nll_loss=5.796, ppl=55.55, wps=1422.7, ups=0.34, wpb=4205, bsz=196, num_updates=1168, lr=1.4016e-05, gnorm=3.308, train_wall=6, gb_free=10.1, wall=9076
2024-09-04 09:40:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45853.01171875Mb; avail=209176.89453125Mb
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000725
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45852.51953125Mb; avail=209176.89453125Mb
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012789
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45853.0078125Mb; avail=209176.89453125Mb
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011272
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025145
2024-09-04 09:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45853.5Mb; avail=209176.40234375Mb
2024-09-04 09:40:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:40:21 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.839 | nll_loss 8.688 | ppl 412.34 | wps 3820.5 | wpb 2070.5 | bsz 122.7 | num_updates 1168 | best_loss 10.839
2024-09-04 09:40:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 1168 updates
2024-09-04 09:40:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:40:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 71 @ 1491 updates, score 9.405) (writing took 42.3119993833825 seconds)
2024-09-04 09:40:21 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-09-04 09:40:21 | INFO | train | epoch 071 | loss 7.881 | nll_loss 4.981 | ppl 31.57 | wps 843.2 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1491 | lr 1.7892e-05 | gnorm 2.641 | train_wall 54 | gb_free 13.3 | wall 9578
2024-09-04 09:40:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:40:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:40:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:40:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000698
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34942.9375Mb; avail=220086.95703125Mb
2024-09-04 09:40:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000051
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000593
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34943.4296875Mb; avail=220086.46484375Mb
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34943.921875Mb; avail=220085.97265625Mb
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001042
2024-09-04 09:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34943.921875Mb; avail=220085.48046875Mb
2024-09-04 09:40:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:40:22 | INFO | fairseq.trainer | begin training epoch 72
2024-09-04 09:40:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:40:24 | INFO | train_inner | epoch 072:      1 / 21 loss=8.07, nll_loss=5.212, ppl=37.07, wps=149.2, ups=0.03, wpb=4790.5, bsz=76, num_updates=1492, lr=1.7904e-05, gnorm=2.637, train_wall=5, gb_free=11.1, wall=9581
2024-09-04 09:40:30 | INFO | train_inner | epoch 072:      3 / 21 loss=7.873, nll_loss=4.994, ppl=31.88, wps=1670.3, ups=0.36, wpb=4583, bsz=100, num_updates=1494, lr=1.7928e-05, gnorm=2.623, train_wall=5, gb_free=13, wall=9587
2024-09-04 09:40:35 | INFO | train_inner | epoch 072:      5 / 21 loss=7.867, nll_loss=4.956, ppl=31.04, wps=1742.5, ups=0.4, wpb=4315.5, bsz=73, num_updates=1496, lr=1.7952e-05, gnorm=2.831, train_wall=5, gb_free=11.9, wall=9592
2024-09-04 09:40:40 | INFO | train_inner | epoch 072:      7 / 21 loss=7.758, nll_loss=4.857, ppl=28.99, wps=1850.8, ups=0.43, wpb=4318.5, bsz=88, num_updates=1498, lr=1.7976e-05, gnorm=2.521, train_wall=5, gb_free=13, wall=9597
2024-09-04 09:40:45 | INFO | train_inner | epoch 072:      9 / 21 loss=7.79, nll_loss=4.892, ppl=29.68, wps=1671.3, ups=0.37, wpb=4474.5, bsz=108, num_updates=1500, lr=1.8e-05, gnorm=2.476, train_wall=5, gb_free=13.3, wall=9602
2024-09-04 09:40:50 | INFO | train_inner | epoch 072:     11 / 21 loss=7.568, nll_loss=4.528, ppl=23.08, wps=1756.7, ups=0.38, wpb=4668, bsz=140, num_updates=1502, lr=1.8024e-05, gnorm=2.282, train_wall=5, gb_free=11.9, wall=9607
2024-09-04 09:40:56 | INFO | train_inner | epoch 072:     13 / 21 loss=8.039, nll_loss=5.156, ppl=35.64, wps=1684.2, ups=0.35, wpb=4770, bsz=76, num_updates=1504, lr=1.8048e-05, gnorm=2.395, train_wall=6, gb_free=13.6, wall=9613
2024-09-04 09:41:01 | INFO | train_inner | epoch 072:     15 / 21 loss=7.791, nll_loss=4.884, ppl=29.52, wps=1685.4, ups=0.4, wpb=4205, bsz=96, num_updates=1506, lr=1.8072e-05, gnorm=2.786, train_wall=5, gb_free=13.1, wall=9618
2024-09-04 09:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:41:06 | INFO | train_inner | epoch 072:     17 / 21 loss=7.894, nll_loss=5.003, ppl=32.06, wps=1691.1, ups=0.38, wpb=4465.5, bsz=96, num_updates=1508, lr=1.8096e-05, gnorm=2.377, train_wall=5, gb_free=14.1, wall=9623
2024-09-04 09:41:11 | INFO | train_inner | epoch 072:     19 / 21 loss=7.901, nll_loss=5.024, ppl=32.55, wps=1941.6, ups=0.4, wpb=4899.5, bsz=120, num_updates=1510, lr=1.812e-05, gnorm=2.925, train_wall=5, gb_free=11.7, wall=9628
2024-09-04 09:41:17 | INFO | train_inner | epoch 072:     21 / 21 loss=8.001, nll_loss=5.107, ppl=34.46, wps=1573.7, ups=0.35, wpb=4542.5, bsz=72, num_updates=1512, lr=1.8144e-05, gnorm=2.368, train_wall=6, gb_free=12.8, wall=9634
2024-09-04 09:41:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38946.4609375Mb; avail=216083.3828125Mb
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38945.96875Mb; avail=216083.3828125Mb
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012621
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38946.6953125Mb; avail=216083.1484375Mb
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011313
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025017
2024-09-04 09:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38946.203125Mb; avail=216083.1484375Mb
2024-09-04 09:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 73 @ 1168 updates, score 10.839) (writing took 66.98913199454546 seconds)
2024-09-04 09:41:28 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-09-04 09:41:28 | INFO | train | epoch 073 | loss 8.976 | nll_loss 6.41 | ppl 85.06 | wps 536 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 1168 | lr 1.4016e-05 | gnorm 3.212 | train_wall 45 | gb_free 10.1 | wall 9157
2024-09-04 09:41:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:41:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:41:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:41:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000721
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38985.21875Mb; avail=216044.55078125Mb
2024-09-04 09:41:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000577
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38985.21875Mb; avail=216044.55078125Mb
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38985.21875Mb; avail=216044.55078125Mb
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000167
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001056
2024-09-04 09:41:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38985.21875Mb; avail=216044.55078125Mb
2024-09-04 09:41:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:41:28 | INFO | fairseq.trainer | begin training epoch 74
2024-09-04 09:41:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:41:34 | INFO | train_inner | epoch 074:      2 / 16 loss=8.876, nll_loss=6.257, ppl=76.47, wps=107.7, ups=0.02, wpb=4714, bsz=192, num_updates=1170, lr=1.404e-05, gnorm=2.463, train_wall=6, gb_free=10.6, wall=9163
2024-09-04 09:41:34 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.397 | nll_loss 6.836 | ppl 114.25 | wps 4706.6 | wpb 2350.9 | bsz 94.7 | num_updates 1512 | best_loss 9.371
2024-09-04 09:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 1512 updates
2024-09-04 09:41:34 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:41:39 | INFO | train_inner | epoch 074:      4 / 16 loss=9.153, nll_loss=6.621, ppl=98.42, wps=1537.7, ups=0.38, wpb=4024.5, bsz=76, num_updates=1172, lr=1.4064e-05, gnorm=2.832, train_wall=5, gb_free=14.2, wall=9169
2024-09-04 09:41:45 | INFO | train_inner | epoch 074:      6 / 16 loss=8.981, nll_loss=6.41, ppl=85.01, wps=1588.6, ups=0.34, wpb=4700.5, bsz=128, num_updates=1174, lr=1.4088e-05, gnorm=2.873, train_wall=6, gb_free=10, wall=9174
2024-09-04 09:41:51 | INFO | train_inner | epoch 074:      8 / 16 loss=8.761, nll_loss=6.154, ppl=71.23, wps=1539, ups=0.34, wpb=4568.5, bsz=176, num_updates=1176, lr=1.4112e-05, gnorm=2.989, train_wall=6, gb_free=9.3, wall=9180
2024-09-04 09:41:57 | INFO | train_inner | epoch 074:     10 / 16 loss=9.02, nll_loss=6.447, ppl=87.22, wps=1497.8, ups=0.35, wpb=4322.5, bsz=120, num_updates=1178, lr=1.4136e-05, gnorm=3.233, train_wall=6, gb_free=10.5, wall=9186
2024-09-04 09:42:03 | INFO | train_inner | epoch 074:     12 / 16 loss=9.061, nll_loss=6.506, ppl=90.87, wps=1455, ups=0.34, wpb=4288.5, bsz=92, num_updates=1180, lr=1.416e-05, gnorm=2.998, train_wall=6, gb_free=8.2, wall=9192
2024-09-04 09:42:08 | INFO | train_inner | epoch 074:     14 / 16 loss=8.63, nll_loss=5.964, ppl=62.42, wps=1379.7, ups=0.37, wpb=3754.5, bsz=136, num_updates=1182, lr=1.4184e-05, gnorm=2.75, train_wall=5, gb_free=14, wall=9197
2024-09-04 09:42:13 | INFO | train_inner | epoch 074:     16 / 16 loss=8.871, nll_loss=6.271, ppl=77.22, wps=1425.7, ups=0.41, wpb=3518.5, bsz=97, num_updates=1184, lr=1.4208e-05, gnorm=3.091, train_wall=5, gb_free=10.5, wall=9202
2024-09-04 09:42:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=48793.390625Mb; avail=206236.48828125Mb
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000733
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48793.390625Mb; avail=206236.48828125Mb
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012733
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48793.390625Mb; avail=206236.48828125Mb
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011229
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025050
2024-09-04 09:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48793.390625Mb; avail=206236.48828125Mb
2024-09-04 09:42:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:42:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 72 @ 1512 updates, score 9.397) (writing took 48.594509788788855 seconds)
2024-09-04 09:42:23 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-09-04 09:42:23 | INFO | train | epoch 072 | loss 7.855 | nll_loss 4.948 | ppl 30.86 | wps 787.7 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1512 | lr 1.8144e-05 | gnorm 2.548 | train_wall 55 | gb_free 12.8 | wall 9700
2024-09-04 09:42:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:42:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:42:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:42:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000596
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39668.3125Mb; avail=215361.5859375Mb
2024-09-04 09:42:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000056
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000559
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39668.3125Mb; avail=215361.5859375Mb
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39668.3125Mb; avail=215361.5859375Mb
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000158
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001019
2024-09-04 09:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39668.3125Mb; avail=215361.5859375Mb
2024-09-04 09:42:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:42:23 | INFO | fairseq.trainer | begin training epoch 73
2024-09-04 09:42:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:42:27 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.873 | nll_loss 8.743 | ppl 428.49 | wps 3831.6 | wpb 2070.5 | bsz 122.7 | num_updates 1184 | best_loss 10.839
2024-09-04 09:42:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 1184 updates
2024-09-04 09:42:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:42:29 | INFO | train_inner | epoch 073:      2 / 21 loss=7.645, nll_loss=4.709, ppl=26.16, wps=124.5, ups=0.03, wpb=4462.5, bsz=128, num_updates=1514, lr=1.8168e-05, gnorm=2.857, train_wall=6, gb_free=12.7, wall=9706
2024-09-04 09:42:34 | INFO | train_inner | epoch 073:      4 / 21 loss=7.768, nll_loss=4.838, ppl=28.61, wps=1950.1, ups=0.42, wpb=4687.5, bsz=104, num_updates=1516, lr=1.8192e-05, gnorm=2.793, train_wall=5, gb_free=13.5, wall=9710
2024-09-04 09:42:39 | INFO | train_inner | epoch 073:      6 / 21 loss=7.84, nll_loss=4.92, ppl=30.28, wps=1616.5, ups=0.36, wpb=4550, bsz=88, num_updates=1518, lr=1.8216e-05, gnorm=2.537, train_wall=6, gb_free=12.5, wall=9716
2024-09-04 09:42:45 | INFO | train_inner | epoch 073:      8 / 21 loss=7.821, nll_loss=4.91, ppl=30.07, wps=1732, ups=0.33, wpb=5247.5, bsz=104, num_updates=1520, lr=1.824e-05, gnorm=2.506, train_wall=6, gb_free=12.8, wall=9722
2024-09-04 09:42:51 | INFO | train_inner | epoch 073:     10 / 21 loss=7.6, nll_loss=4.647, ppl=25.05, wps=1616.2, ups=0.34, wpb=4722, bsz=136, num_updates=1522, lr=1.8264e-05, gnorm=2.517, train_wall=6, gb_free=11.8, wall=9728
2024-09-04 09:42:57 | INFO | train_inner | epoch 073:     12 / 21 loss=7.691, nll_loss=4.72, ppl=26.36, wps=1664.2, ups=0.37, wpb=4519.5, bsz=116, num_updates=1524, lr=1.8288e-05, gnorm=2.281, train_wall=5, gb_free=12, wall=9733
2024-09-04 09:43:03 | INFO | train_inner | epoch 073:     14 / 21 loss=7.768, nll_loss=4.839, ppl=28.61, wps=1539.2, ups=0.32, wpb=4831, bsz=136, num_updates=1526, lr=1.8312e-05, gnorm=2.282, train_wall=6, gb_free=12.6, wall=9740
2024-09-04 09:43:07 | INFO | train_inner | epoch 073:     16 / 21 loss=8.113, nll_loss=5.307, ppl=39.59, wps=1939.6, ups=0.47, wpb=4155.5, bsz=48, num_updates=1528, lr=1.8336e-05, gnorm=3.379, train_wall=4, gb_free=14.5, wall=9744
2024-09-04 09:43:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 74 @ 1184 updates, score 10.873) (writing took 42.44305794034153 seconds)
2024-09-04 09:43:10 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-09-04 09:43:10 | INFO | train | epoch 074 | loss 8.922 | nll_loss 6.332 | ppl 80.57 | wps 664.2 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1184 | lr 1.4208e-05 | gnorm 2.904 | train_wall 45 | gb_free 10.5 | wall 9259
2024-09-04 09:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000728
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44681.67578125Mb; avail=210348.10546875Mb
2024-09-04 09:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000599
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44681.67578125Mb; avail=210348.10546875Mb
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44681.67578125Mb; avail=210348.10546875Mb
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000163
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001057
2024-09-04 09:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44681.67578125Mb; avail=210348.10546875Mb
2024-09-04 09:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:43:10 | INFO | fairseq.trainer | begin training epoch 75
2024-09-04 09:43:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:43:12 | INFO | train_inner | epoch 073:     18 / 21 loss=7.956, nll_loss=5.052, ppl=33.18, wps=1630.6, ups=0.39, wpb=4183, bsz=80, num_updates=1530, lr=1.836e-05, gnorm=2.646, train_wall=5, gb_free=14.4, wall=9749
2024-09-04 09:43:15 | INFO | train_inner | epoch 075:      2 / 16 loss=8.869, nll_loss=6.267, ppl=76.99, wps=119.4, ups=0.03, wpb=3699.5, bsz=121, num_updates=1186, lr=1.4232e-05, gnorm=3.003, train_wall=5, gb_free=10.6, wall=9264
2024-09-04 09:43:17 | INFO | train_inner | epoch 073:     20 / 21 loss=8.018, nll_loss=5.116, ppl=34.67, wps=1766.4, ups=0.44, wpb=4059, bsz=49, num_updates=1532, lr=1.8384e-05, gnorm=2.684, train_wall=5, gb_free=12.7, wall=9754
2024-09-04 09:43:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44722.63671875Mb; avail=210307.21875Mb
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000745
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44722.63671875Mb; avail=210307.21875Mb
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012832
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44722.63671875Mb; avail=210307.21875Mb
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011301
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025228
2024-09-04 09:43:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44722.63671875Mb; avail=210307.21875Mb
2024-09-04 09:43:21 | INFO | train_inner | epoch 075:      4 / 16 loss=8.72, nll_loss=6.071, ppl=67.25, wps=1448.6, ups=0.33, wpb=4455, bsz=160, num_updates=1188, lr=1.4256e-05, gnorm=2.856, train_wall=6, gb_free=9.8, wall=9271
2024-09-04 09:43:27 | INFO | train_inner | epoch 075:      6 / 16 loss=8.879, nll_loss=6.281, ppl=77.74, wps=1540.5, ups=0.34, wpb=4532.5, bsz=160, num_updates=1190, lr=1.428e-05, gnorm=2.614, train_wall=6, gb_free=8.9, wall=9276
2024-09-04 09:43:37 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.388 | nll_loss 6.856 | ppl 115.81 | wps 4674.3 | wpb 2350.9 | bsz 94.7 | num_updates 1533 | best_loss 9.371
2024-09-04 09:43:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 1533 updates
2024-09-04 09:43:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:43:47 | INFO | train_inner | epoch 075:      8 / 16 loss=9.163, nll_loss=6.669, ppl=101.76, wps=426.6, ups=0.1, wpb=4346, bsz=96, num_updates=1192, lr=1.4304e-05, gnorm=3.974, train_wall=20, gb_free=9, wall=9297
2024-09-04 09:43:53 | INFO | train_inner | epoch 075:     10 / 16 loss=8.925, nll_loss=6.305, ppl=79.08, wps=1492.1, ups=0.35, wpb=4276.5, bsz=108, num_updates=1194, lr=1.4328e-05, gnorm=3.011, train_wall=6, gb_free=10.2, wall=9303
2024-09-04 09:43:59 | INFO | train_inner | epoch 075:     12 / 16 loss=8.502, nll_loss=5.778, ppl=54.86, wps=1390, ups=0.34, wpb=4044, bsz=160, num_updates=1196, lr=1.4352e-05, gnorm=2.708, train_wall=6, gb_free=8.9, wall=9308
2024-09-04 09:44:04 | INFO | train_inner | epoch 075:     14 / 16 loss=8.9, nll_loss=6.31, ppl=79.35, wps=1474.2, ups=0.37, wpb=3975, bsz=104, num_updates=1198, lr=1.4376e-05, gnorm=3.481, train_wall=5, gb_free=10, wall=9314
2024-09-04 09:44:10 | INFO | train_inner | epoch 075:     16 / 16 loss=8.979, nll_loss=6.429, ppl=86.18, wps=1510, ups=0.33, wpb=4563, bsz=108, num_updates=1200, lr=1.44e-05, gnorm=3.277, train_wall=6, gb_free=9.5, wall=9320
2024-09-04 09:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44183.4375Mb; avail=210846.4453125Mb
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44183.4375Mb; avail=210846.4453125Mb
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012820
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44182.9453125Mb; avail=210846.9375Mb
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011163
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024983
2024-09-04 09:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44182.9453125Mb; avail=210846.9375Mb
2024-09-04 09:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:44:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 73 @ 1533 updates, score 9.388) (writing took 39.64321459084749 seconds)
2024-09-04 09:44:16 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-09-04 09:44:16 | INFO | train | epoch 073 | loss 7.828 | nll_loss 4.913 | ppl 30.13 | wps 844.4 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1533 | lr 1.8396e-05 | gnorm 2.665 | train_wall 56 | gb_free 14.9 | wall 9813
2024-09-04 09:44:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:44:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:44:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:44:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000693
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34942.2421875Mb; avail=220087.61328125Mb
2024-09-04 09:44:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000059
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000582
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34942.2421875Mb; avail=220087.61328125Mb
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34942.2421875Mb; avail=220087.61328125Mb
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000169
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001043
2024-09-04 09:44:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34942.2421875Mb; avail=220087.61328125Mb
2024-09-04 09:44:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:44:16 | INFO | fairseq.trainer | begin training epoch 74
2024-09-04 09:44:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:44:19 | INFO | train_inner | epoch 074:      1 / 21 loss=7.919, nll_loss=5.053, ppl=33.2, wps=147.2, ups=0.03, wpb=4562.5, bsz=84, num_updates=1534, lr=1.8408e-05, gnorm=3.377, train_wall=5, gb_free=12.8, wall=9816
2024-09-04 09:44:25 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.85 | nll_loss 8.716 | ppl 420.44 | wps 3829.6 | wpb 2070.5 | bsz 122.7 | num_updates 1200 | best_loss 10.839
2024-09-04 09:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 1200 updates
2024-09-04 09:44:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:44:25 | INFO | train_inner | epoch 074:      3 / 21 loss=7.662, nll_loss=4.699, ppl=25.98, wps=1652.7, ups=0.32, wpb=5160, bsz=128, num_updates=1536, lr=1.8432e-05, gnorm=2.467, train_wall=6, gb_free=10.8, wall=9822
2024-09-04 09:44:30 | INFO | train_inner | epoch 074:      5 / 21 loss=7.913, nll_loss=5.035, ppl=32.79, wps=1691.5, ups=0.39, wpb=4383, bsz=72, num_updates=1538, lr=1.8456e-05, gnorm=2.791, train_wall=5, gb_free=15.4, wall=9827
2024-09-04 09:44:36 | INFO | train_inner | epoch 074:      7 / 21 loss=7.952, nll_loss=5.083, ppl=33.89, wps=1682.5, ups=0.36, wpb=4716.5, bsz=84, num_updates=1540, lr=1.848e-05, gnorm=2.671, train_wall=6, gb_free=11.4, wall=9833
2024-09-04 09:44:40 | INFO | train_inner | epoch 074:      9 / 21 loss=7.872, nll_loss=4.968, ppl=31.31, wps=1984.9, ups=0.47, wpb=4245.5, bsz=88, num_updates=1542, lr=1.8504e-05, gnorm=2.536, train_wall=4, gb_free=17, wall=9837
2024-09-04 09:44:46 | INFO | train_inner | epoch 074:     11 / 21 loss=7.718, nll_loss=4.746, ppl=26.84, wps=1773.2, ups=0.36, wpb=4975, bsz=108, num_updates=1544, lr=1.8528e-05, gnorm=2.205, train_wall=6, gb_free=12.6, wall=9843
2024-09-04 09:44:51 | INFO | train_inner | epoch 074:     13 / 21 loss=7.614, nll_loss=4.621, ppl=24.6, wps=1755.5, ups=0.37, wpb=4701.5, bsz=136, num_updates=1546, lr=1.8552e-05, gnorm=2.034, train_wall=5, gb_free=11.2, wall=9848
2024-09-04 09:44:57 | INFO | train_inner | epoch 074:     15 / 21 loss=7.82, nll_loss=4.938, ppl=30.66, wps=1573.8, ups=0.35, wpb=4524.5, bsz=92, num_updates=1548, lr=1.8576e-05, gnorm=2.393, train_wall=6, gb_free=11.9, wall=9854
2024-09-04 09:45:02 | INFO | train_inner | epoch 074:     17 / 21 loss=8.052, nll_loss=5.207, ppl=36.94, wps=2043, ups=0.39, wpb=5227.5, bsz=80, num_updates=1550, lr=1.86e-05, gnorm=2.456, train_wall=5, gb_free=13.2, wall=9859
2024-09-04 09:45:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 75 @ 1200 updates, score 10.85) (writing took 39.37907260656357 seconds)
2024-09-04 09:45:04 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-09-04 09:45:04 | INFO | train | epoch 075 | loss 8.87 | nll_loss 6.268 | ppl 77.06 | wps 593.1 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 1200 | lr 1.44e-05 | gnorm 3.116 | train_wall 60 | gb_free 9.5 | wall 9374
2024-09-04 09:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000641
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37630.32421875Mb; avail=217399.53125Mb
2024-09-04 09:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000583
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37630.32421875Mb; avail=217399.53125Mb
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37630.32421875Mb; avail=217399.53125Mb
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000158
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001053
2024-09-04 09:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37630.32421875Mb; avail=217399.53125Mb
2024-09-04 09:45:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:45:04 | INFO | fairseq.trainer | begin training epoch 76
2024-09-04 09:45:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:45:07 | INFO | train_inner | epoch 074:     19 / 21 loss=7.419, nll_loss=4.38, ppl=20.81, wps=1692.1, ups=0.41, wpb=4136, bsz=132, num_updates=1552, lr=1.8624e-05, gnorm=2.318, train_wall=5, gb_free=12.5, wall=9864
2024-09-04 09:45:10 | INFO | train_inner | epoch 076:      2 / 16 loss=8.586, nll_loss=5.907, ppl=60.02, wps=144.9, ups=0.03, wpb=4337.5, bsz=156, num_updates=1202, lr=1.4424e-05, gnorm=2.41, train_wall=6, gb_free=10.5, wall=9380
2024-09-04 09:45:12 | INFO | train_inner | epoch 074:     21 / 21 loss=7.978, nll_loss=5.087, ppl=33.98, wps=1529.5, ups=0.42, wpb=3632.5, bsz=41, num_updates=1554, lr=1.8648e-05, gnorm=3.895, train_wall=5, gb_free=11.6, wall=9869
2024-09-04 09:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37665.4921875Mb; avail=217364.31640625Mb
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37665.984375Mb; avail=217363.82421875Mb
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.013014
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37665.984375Mb; avail=217363.82421875Mb
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011224
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025338
2024-09-04 09:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37665.984375Mb; avail=217363.82421875Mb
2024-09-04 09:45:16 | INFO | train_inner | epoch 076:      4 / 16 loss=8.763, nll_loss=6.136, ppl=70.32, wps=1569.6, ups=0.35, wpb=4440, bsz=152, num_updates=1204, lr=1.4448e-05, gnorm=2.525, train_wall=6, gb_free=10.3, wall=9385
2024-09-04 09:45:21 | INFO | train_inner | epoch 076:      6 / 16 loss=8.897, nll_loss=6.309, ppl=79.26, wps=1486.4, ups=0.39, wpb=3777, bsz=109, num_updates=1206, lr=1.4472e-05, gnorm=2.986, train_wall=5, gb_free=14.7, wall=9390
2024-09-04 09:45:27 | INFO | train_inner | epoch 076:      8 / 16 loss=8.564, nll_loss=5.832, ppl=56.96, wps=1487.1, ups=0.34, wpb=4346.5, bsz=172, num_updates=1208, lr=1.4496e-05, gnorm=2.962, train_wall=6, gb_free=10.9, wall=9396
2024-09-04 09:45:28 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.334 | nll_loss 6.715 | ppl 105.07 | wps 4921.8 | wpb 2350.9 | bsz 94.7 | num_updates 1554 | best_loss 9.334
2024-09-04 09:45:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 1554 updates
2024-09-04 09:45:28 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:45:33 | INFO | train_inner | epoch 076:     10 / 16 loss=8.826, nll_loss=6.218, ppl=74.44, wps=1530.1, ups=0.34, wpb=4500, bsz=156, num_updates=1210, lr=1.452e-05, gnorm=3.437, train_wall=6, gb_free=11.5, wall=9402
2024-09-04 09:45:39 | INFO | train_inner | epoch 076:     12 / 16 loss=8.975, nll_loss=6.428, ppl=86.08, wps=1448.3, ups=0.34, wpb=4300, bsz=104, num_updates=1212, lr=1.4544e-05, gnorm=3.694, train_wall=6, gb_free=11.9, wall=9408
2024-09-04 09:45:44 | INFO | train_inner | epoch 076:     14 / 16 loss=8.957, nll_loss=6.366, ppl=82.49, wps=1480.4, ups=0.35, wpb=4205, bsz=88, num_updates=1214, lr=1.4568e-05, gnorm=2.848, train_wall=6, gb_free=8.8, wall=9414
2024-09-04 09:45:49 | INFO | train_inner | epoch 076:     16 / 16 loss=9.173, nll_loss=6.633, ppl=99.26, wps=1527.1, ups=0.38, wpb=3985.5, bsz=80, num_updates=1216, lr=1.4592e-05, gnorm=4.082, train_wall=5, gb_free=9.5, wall=9419
2024-09-04 09:45:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=49513.3984375Mb; avail=205516.5Mb
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000717
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49513.3984375Mb; avail=205516.5Mb
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012632
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49513.3984375Mb; avail=205516.5Mb
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011110
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024824
2024-09-04 09:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49513.3984375Mb; avail=205516.5Mb
2024-09-04 09:46:04 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.854 | nll_loss 8.673 | ppl 408.08 | wps 3816.4 | wpb 2070.5 | bsz 122.7 | num_updates 1216 | best_loss 10.839
2024-09-04 09:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 1216 updates
2024-09-04 09:46:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt
2024-09-04 09:46:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_best.pt (epoch 74 @ 1554 updates, score 9.334) (writing took 73.51202213950455 seconds)
2024-09-04 09:46:42 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-09-04 09:46:42 | INFO | train | epoch 074 | loss 7.798 | nll_loss 4.877 | ppl 29.39 | wps 657.9 | ups 0.14 | wpb 4556.3 | bsz 96.9 | num_updates 1554 | lr 1.8648e-05 | gnorm 2.633 | train_wall 55 | gb_free 11.6 | wall 9959
2024-09-04 09:46:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:46:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:46:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:46:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000588
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=49312.96875Mb; avail=205716.9296875Mb
2024-09-04 09:46:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000570
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49312.96875Mb; avail=205716.9296875Mb
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49312.96875Mb; avail=205716.9296875Mb
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000164
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001037
2024-09-04 09:46:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49312.96875Mb; avail=205716.9296875Mb
2024-09-04 09:46:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:46:42 | INFO | fairseq.trainer | begin training epoch 75
2024-09-04 09:46:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:46:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:46:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 76 @ 1216 updates, score 10.854) (writing took 42.26189721468836 seconds)
2024-09-04 09:46:46 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-09-04 09:46:46 | INFO | train | epoch 076 | loss 8.837 | nll_loss 6.222 | ppl 74.64 | wps 663.7 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1216 | lr 1.4592e-05 | gnorm 3.118 | train_wall 45 | gb_free 9.5 | wall 9476
2024-09-04 09:46:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:46:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:46:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:46:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000698
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37683.68359375Mb; avail=217346.171875Mb
2024-09-04 09:46:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000597
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37683.68359375Mb; avail=217346.171875Mb
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37683.68359375Mb; avail=217346.171875Mb
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000162
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001060
2024-09-04 09:46:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37683.68359375Mb; avail=217346.171875Mb
2024-09-04 09:46:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:46:46 | INFO | fairseq.trainer | begin training epoch 77
2024-09-04 09:46:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:46:47 | INFO | train_inner | epoch 075:      2 / 21 loss=7.636, nll_loss=4.675, ppl=25.54, wps=77.4, ups=0.02, wpb=3675, bsz=85, num_updates=1556, lr=1.8672e-05, gnorm=3.093, train_wall=5, gb_free=13.2, wall=9964
2024-09-04 09:46:51 | INFO | train_inner | epoch 075:      4 / 21 loss=7.809, nll_loss=4.895, ppl=29.76, wps=1906.7, ups=0.41, wpb=4680.5, bsz=80, num_updates=1558, lr=1.8696e-05, gnorm=2.608, train_wall=5, gb_free=12.4, wall=9968
2024-09-04 09:46:52 | INFO | train_inner | epoch 077:      2 / 16 loss=8.875, nll_loss=6.232, ppl=75.18, wps=138.8, ups=0.03, wpb=4326.5, bsz=104, num_updates=1218, lr=1.4616e-05, gnorm=3.252, train_wall=5, gb_free=11.4, wall=9481
2024-09-04 09:46:57 | INFO | train_inner | epoch 075:      6 / 21 loss=7.948, nll_loss=5.111, ppl=34.55, wps=1658.2, ups=0.37, wpb=4426, bsz=68, num_updates=1560, lr=1.872e-05, gnorm=3.152, train_wall=5, gb_free=13.9, wall=9974
2024-09-04 09:46:58 | INFO | train_inner | epoch 077:      4 / 16 loss=8.375, nll_loss=5.637, ppl=49.77, wps=1484.3, ups=0.35, wpb=4243, bsz=208, num_updates=1220, lr=1.464e-05, gnorm=3.27, train_wall=6, gb_free=12.2, wall=9487
2024-09-04 09:47:02 | INFO | train_inner | epoch 075:      8 / 21 loss=7.752, nll_loss=4.786, ppl=27.58, wps=2070.1, ups=0.38, wpb=5386.5, bsz=132, num_updates=1562, lr=1.8744e-05, gnorm=2.835, train_wall=5, gb_free=11.6, wall=9979
2024-09-04 09:47:03 | INFO | train_inner | epoch 077:      6 / 16 loss=8.961, nll_loss=6.409, ppl=85, wps=1454.2, ups=0.37, wpb=3976, bsz=100, num_updates=1222, lr=1.4664e-05, gnorm=4.445, train_wall=5, gb_free=10.4, wall=9492
2024-09-04 09:47:08 | INFO | train_inner | epoch 075:     10 / 21 loss=7.655, nll_loss=4.657, ppl=25.23, wps=1644.1, ups=0.36, wpb=4581.5, bsz=120, num_updates=1564, lr=1.8768e-05, gnorm=2.231, train_wall=6, gb_free=12.2, wall=9985
2024-09-04 09:47:09 | INFO | train_inner | epoch 077:      8 / 16 loss=8.971, nll_loss=6.393, ppl=84.04, wps=1594.1, ups=0.34, wpb=4696.5, bsz=112, num_updates=1224, lr=1.4688e-05, gnorm=3.615, train_wall=6, gb_free=10.9, wall=9498
2024-09-04 09:47:13 | INFO | train_inner | epoch 075:     12 / 21 loss=7.86, nll_loss=4.959, ppl=31.11, wps=1772, ups=0.39, wpb=4568.5, bsz=80, num_updates=1566, lr=1.8792e-05, gnorm=2.567, train_wall=5, gb_free=12.5, wall=9990
2024-09-04 09:47:14 | INFO | train_inner | epoch 077:     10 / 16 loss=8.785, nll_loss=6.158, ppl=71.43, wps=1416.9, ups=0.39, wpb=3656.5, bsz=117, num_updates=1226, lr=1.4712e-05, gnorm=3.242, train_wall=5, gb_free=13.7, wall=9504
2024-09-04 09:47:18 | INFO | train_inner | epoch 075:     14 / 21 loss=7.753, nll_loss=4.843, ppl=28.7, wps=1859.6, ups=0.41, wpb=4501.5, bsz=108, num_updates=1568, lr=1.8816e-05, gnorm=2.63, train_wall=5, gb_free=11.3, wall=9995
2024-09-04 09:47:23 | INFO | train_inner | epoch 075:     16 / 21 loss=7.985, nll_loss=5.118, ppl=34.72, wps=1614.2, ups=0.36, wpb=4516, bsz=64, num_updates=1570, lr=1.884e-05, gnorm=2.581, train_wall=6, gb_free=12.9, wall=10000
2024-09-04 09:47:28 | INFO | train_inner | epoch 075:     18 / 21 loss=7.957, nll_loss=5.086, ppl=33.96, wps=1771.9, ups=0.4, wpb=4440.5, bsz=76, num_updates=1572, lr=1.8864e-05, gnorm=2.717, train_wall=5, gb_free=14.3, wall=10005
2024-09-04 09:47:29 | INFO | train_inner | epoch 077:     12 / 16 loss=8.911, nll_loss=6.302, ppl=78.88, wps=533, ups=0.13, wpb=4118.5, bsz=108, num_updates=1228, lr=1.4736e-05, gnorm=3.088, train_wall=15, gb_free=10.2, wall=9519
2024-09-04 09:47:34 | INFO | train_inner | epoch 075:     20 / 21 loss=7.54, nll_loss=4.578, ppl=23.88, wps=1666.7, ups=0.36, wpb=4610.5, bsz=148, num_updates=1574, lr=1.8888e-05, gnorm=3.201, train_wall=6, gb_free=13.3, wall=10011
2024-09-04 09:47:36 | INFO | train_inner | epoch 077:     14 / 16 loss=8.64, nll_loss=5.994, ppl=63.74, wps=1472.5, ups=0.33, wpb=4457.5, bsz=172, num_updates=1230, lr=1.476e-05, gnorm=2.92, train_wall=6, gb_free=8.7, wall=9525
2024-09-04 09:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29949.578125Mb; avail=225080.36328125Mb
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000759
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29950.0703125Mb; avail=225079.87109375Mb
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012846
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29950.0703125Mb; avail=225079.87109375Mb
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011227
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025201
2024-09-04 09:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29950.0703125Mb; avail=225079.87109375Mb
2024-09-04 09:47:41 | INFO | train_inner | epoch 077:     16 / 16 loss=8.936, nll_loss=6.355, ppl=81.86, wps=1493.8, ups=0.34, wpb=4417, bsz=96, num_updates=1232, lr=1.4784e-05, gnorm=2.758, train_wall=6, gb_free=11.1, wall=9531
2024-09-04 09:47:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29984.21875Mb; avail=225045.67578125Mb
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29984.21875Mb; avail=225045.67578125Mb
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012678
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29984.21875Mb; avail=225045.67578125Mb
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011135
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024787
2024-09-04 09:47:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29984.21875Mb; avail=225045.67578125Mb
2024-09-04 09:47:54 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.372 | nll_loss 6.718 | ppl 105.26 | wps 4701 | wpb 2350.9 | bsz 94.7 | num_updates 1575 | best_loss 9.334
2024-09-04 09:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 1575 updates
2024-09-04 09:47:54 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:47:56 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.846 | nll_loss 8.711 | ppl 418.97 | wps 3836.5 | wpb 2070.5 | bsz 122.7 | num_updates 1232 | best_loss 10.839
2024-09-04 09:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 1232 updates
2024-09-04 09:47:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:48:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:48:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 75 @ 1575 updates, score 9.372) (writing took 42.94814146216959 seconds)
2024-09-04 09:48:37 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-09-04 09:48:37 | INFO | train | epoch 075 | loss 7.785 | nll_loss 4.86 | ppl 29.03 | wps 826.7 | ups 0.18 | wpb 4556.3 | bsz 96.9 | num_updates 1575 | lr 1.89e-05 | gnorm 2.757 | train_wall 55 | gb_free 12.6 | wall 10074
2024-09-04 09:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000705
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=55151.64453125Mb; avail=199886.7734375Mb
2024-09-04 09:48:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000064
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000590
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55147.26953125Mb; avail=199881.3046875Mb
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55148.25390625Mb; avail=199880.8125Mb
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000168
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001049
2024-09-04 09:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55152.21484375Mb; avail=199875.375Mb
2024-09-04 09:48:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:48:38 | INFO | fairseq.trainer | begin training epoch 76
2024-09-04 09:48:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:48:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:48:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 77 @ 1232 updates, score 10.846) (writing took 43.41839137580246 seconds)
2024-09-04 09:48:39 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-09-04 09:48:39 | INFO | train | epoch 077 | loss 8.807 | nll_loss 6.186 | ppl 72.8 | wps 599.5 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 1232 | lr 1.4784e-05 | gnorm 3.324 | train_wall 55 | gb_free 11.1 | wall 9589
2024-09-04 09:48:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:48:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:48:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:48:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000666
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39966.8046875Mb; avail=215062.12109375Mb
2024-09-04 09:48:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000063
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000594
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39958.9921875Mb; avail=215069.44140625Mb
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39958.9921875Mb; avail=215069.44140625Mb
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000157
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001040
2024-09-04 09:48:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39958.9921875Mb; avail=215070.42578125Mb
2024-09-04 09:48:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:48:39 | INFO | fairseq.trainer | begin training epoch 78
2024-09-04 09:48:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:48:40 | INFO | train_inner | epoch 076:      1 / 21 loss=7.883, nll_loss=4.887, ppl=29.59, wps=157.5, ups=0.03, wpb=5224.5, bsz=96, num_updates=1576, lr=1.8912e-05, gnorm=2.596, train_wall=6, gb_free=12.8, wall=10077
2024-09-04 09:48:44 | INFO | train_inner | epoch 078:      2 / 16 loss=8.728, nll_loss=6.091, ppl=68.19, wps=108.8, ups=0.03, wpb=3410, bsz=81, num_updates=1234, lr=1.4808e-05, gnorm=3.432, train_wall=5, gb_free=10.5, wall=9594
2024-09-04 09:48:46 | INFO | train_inner | epoch 076:      3 / 21 loss=7.503, nll_loss=4.486, ppl=22.41, wps=1609.5, ups=0.34, wpb=4755.5, bsz=164, num_updates=1578, lr=1.8936e-05, gnorm=2.736, train_wall=6, gb_free=11.6, wall=10083
2024-09-04 09:48:50 | INFO | train_inner | epoch 078:      4 / 16 loss=8.746, nll_loss=6.105, ppl=68.82, wps=1596.7, ups=0.35, wpb=4584.5, bsz=136, num_updates=1236, lr=1.4832e-05, gnorm=3.133, train_wall=6, gb_free=10, wall=9599
2024-09-04 09:48:51 | INFO | train_inner | epoch 076:      5 / 21 loss=7.601, nll_loss=4.633, ppl=24.82, wps=1724.3, ups=0.43, wpb=3999.5, bsz=97, num_updates=1580, lr=1.896e-05, gnorm=3.219, train_wall=5, gb_free=11.1, wall=10088
2024-09-04 09:48:56 | INFO | train_inner | epoch 078:      6 / 16 loss=8.902, nll_loss=6.281, ppl=77.79, wps=1567.9, ups=0.33, wpb=4691, bsz=124, num_updates=1238, lr=1.4856e-05, gnorm=3.121, train_wall=6, gb_free=11.5, wall=9605
2024-09-04 09:48:56 | INFO | train_inner | epoch 076:      7 / 21 loss=7.762, nll_loss=4.882, ppl=29.49, wps=1597.4, ups=0.38, wpb=4245, bsz=108, num_updates=1582, lr=1.8984e-05, gnorm=3.07, train_wall=5, gb_free=12.7, wall=10093
2024-09-04 09:49:01 | INFO | train_inner | epoch 078:      8 / 16 loss=8.839, nll_loss=6.229, ppl=74.99, wps=1486.2, ups=0.38, wpb=3901.5, bsz=92, num_updates=1240, lr=1.488e-05, gnorm=3.954, train_wall=5, gb_free=8.8, wall=9611
2024-09-04 09:49:02 | INFO | train_inner | epoch 076:      9 / 21 loss=7.466, nll_loss=4.451, ppl=21.87, wps=1574.8, ups=0.34, wpb=4669, bsz=124, num_updates=1584, lr=1.9008e-05, gnorm=2.674, train_wall=6, gb_free=13, wall=10099
2024-09-04 09:49:07 | INFO | train_inner | epoch 076:     11 / 21 loss=7.838, nll_loss=4.901, ppl=29.89, wps=1874.2, ups=0.43, wpb=4398, bsz=96, num_updates=1586, lr=1.9032e-05, gnorm=3.697, train_wall=5, gb_free=14.6, wall=10104
2024-09-04 09:49:07 | INFO | train_inner | epoch 078:     10 / 16 loss=8.19, nll_loss=5.402, ppl=42.29, wps=1378.9, ups=0.33, wpb=4199, bsz=228, num_updates=1242, lr=1.4904e-05, gnorm=2.802, train_wall=6, gb_free=9, wall=9617
2024-09-04 09:49:12 | INFO | train_inner | epoch 076:     13 / 21 loss=7.707, nll_loss=4.755, ppl=27, wps=1683.8, ups=0.34, wpb=4974.5, bsz=104, num_updates=1588, lr=1.9056e-05, gnorm=2.653, train_wall=6, gb_free=13.8, wall=10109
2024-09-04 09:49:13 | INFO | train_inner | epoch 078:     12 / 16 loss=9.028, nll_loss=6.442, ppl=86.93, wps=1533.6, ups=0.35, wpb=4404.5, bsz=96, num_updates=1244, lr=1.4928e-05, gnorm=3.294, train_wall=6, gb_free=11.2, wall=9622
2024-09-04 09:49:17 | INFO | train_inner | epoch 076:     15 / 21 loss=7.863, nll_loss=4.962, ppl=31.18, wps=1976.1, ups=0.45, wpb=4438.5, bsz=80, num_updates=1590, lr=1.908e-05, gnorm=2.935, train_wall=4, gb_free=14.6, wall=10114
2024-09-04 09:49:19 | INFO | train_inner | epoch 078:     14 / 16 loss=8.958, nll_loss=6.378, ppl=83.16, wps=1488.1, ups=0.35, wpb=4212.5, bsz=104, num_updates=1246, lr=1.4952e-05, gnorm=3.501, train_wall=6, gb_free=9.3, wall=9628
2024-09-04 09:49:22 | INFO | train_inner | epoch 076:     17 / 21 loss=8.052, nll_loss=5.189, ppl=36.48, wps=1929.2, ups=0.39, wpb=4918.5, bsz=72, num_updates=1592, lr=1.9104e-05, gnorm=2.716, train_wall=5, gb_free=13.1, wall=10119
2024-09-04 09:49:25 | INFO | train_inner | epoch 078:     16 / 16 loss=8.732, nll_loss=6.077, ppl=67.52, wps=1486.3, ups=0.33, wpb=4488.5, bsz=156, num_updates=1248, lr=1.4976e-05, gnorm=2.723, train_wall=6, gb_free=10.9, wall=9634
2024-09-04 09:49:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40006.859375Mb; avail=215022.9921875Mb
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000764
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40006.859375Mb; avail=215022.9921875Mb
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012585
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40006.859375Mb; avail=215022.9921875Mb
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011350
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025064
2024-09-04 09:49:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40006.859375Mb; avail=215022.9921875Mb
2024-09-04 09:49:27 | INFO | train_inner | epoch 076:     19 / 21 loss=7.962, nll_loss=5.07, ppl=33.59, wps=1616.2, ups=0.37, wpb=4369.5, bsz=60, num_updates=1594, lr=1.9128e-05, gnorm=2.713, train_wall=5, gb_free=12.6, wall=10124
2024-09-04 09:49:33 | INFO | train_inner | epoch 076:     21 / 21 loss=7.809, nll_loss=4.915, ppl=30.16, wps=1562.7, ups=0.36, wpb=4303, bsz=72, num_updates=1596, lr=1.9152e-05, gnorm=2.817, train_wall=5, gb_free=12.3, wall=10130
2024-09-04 09:49:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40041.73828125Mb; avail=214988.0703125Mb
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000757
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40041.73828125Mb; avail=214988.0703125Mb
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012846
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40041.73828125Mb; avail=214988.0703125Mb
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011186
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025175
2024-09-04 09:49:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40042.23046875Mb; avail=214987.578125Mb
2024-09-04 09:49:39 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.859 | nll_loss 8.712 | ppl 419.4 | wps 3825.9 | wpb 2070.5 | bsz 122.7 | num_updates 1248 | best_loss 10.839
2024-09-04 09:49:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 1248 updates
2024-09-04 09:49:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:49:50 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.376 | nll_loss 6.774 | ppl 109.48 | wps 4714.1 | wpb 2350.9 | bsz 94.7 | num_updates 1596 | best_loss 9.334
2024-09-04 09:49:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 1596 updates
2024-09-04 09:49:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:50:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:50:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 78 @ 1248 updates, score 10.859) (writing took 38.81124881654978 seconds)
2024-09-04 09:50:18 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-09-04 09:50:18 | INFO | train | epoch 078 | loss 8.769 | nll_loss 6.129 | ppl 69.99 | wps 687.2 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1248 | lr 1.4976e-05 | gnorm 3.245 | train_wall 45 | gb_free 10.9 | wall 9687
2024-09-04 09:50:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:50:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:50:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:50:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000700
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43273.4375Mb; avail=211756.41015625Mb
2024-09-04 09:50:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000566
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43273.4375Mb; avail=211756.41015625Mb
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43273.4375Mb; avail=211756.41015625Mb
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000175
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001039
2024-09-04 09:50:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43273.4375Mb; avail=211756.41015625Mb
2024-09-04 09:50:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:50:18 | INFO | fairseq.trainer | begin training epoch 79
2024-09-04 09:50:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:50:23 | INFO | train_inner | epoch 079:      2 / 16 loss=8.732, nll_loss=6.1, ppl=68.58, wps=141.5, ups=0.03, wpb=4155.5, bsz=136, num_updates=1250, lr=1.5e-05, gnorm=3.605, train_wall=5, gb_free=11.1, wall=9693
2024-09-04 09:50:29 | INFO | train_inner | epoch 079:      4 / 16 loss=8.869, nll_loss=6.227, ppl=74.92, wps=1472.5, ups=0.35, wpb=4244, bsz=104, num_updates=1252, lr=1.5024e-05, gnorm=2.854, train_wall=6, gb_free=10.8, wall=9699
2024-09-04 09:50:34 | INFO | train_inner | epoch 079:      6 / 16 loss=8.868, nll_loss=6.24, ppl=75.57, wps=1480.4, ups=0.39, wpb=3789, bsz=76, num_updates=1254, lr=1.5048e-05, gnorm=3.18, train_wall=5, gb_free=17.3, wall=9704
2024-09-04 09:50:40 | INFO | train_inner | epoch 079:      8 / 16 loss=8.925, nll_loss=6.343, ppl=81.18, wps=1535, ups=0.34, wpb=4539.5, bsz=92, num_updates=1256, lr=1.5072e-05, gnorm=3.207, train_wall=6, gb_free=9.4, wall=9710
2024-09-04 09:50:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:50:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 76 @ 1596 updates, score 9.376) (writing took 53.212962250225246 seconds)
2024-09-04 09:50:44 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-09-04 09:50:44 | INFO | train | epoch 076 | loss 7.774 | nll_loss 4.84 | ppl 28.64 | wps 759 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1596 | lr 1.9152e-05 | gnorm 2.904 | train_wall 55 | gb_free 12.3 | wall 10200
2024-09-04 09:50:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:50:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:50:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:50:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000597
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32184.71875Mb; avail=222845.1796875Mb
2024-09-04 09:50:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000580
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32184.71875Mb; avail=222845.1796875Mb
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32184.71875Mb; avail=222845.1796875Mb
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000166
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001071
2024-09-04 09:50:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32184.71875Mb; avail=222845.1796875Mb
2024-09-04 09:50:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:50:44 | INFO | fairseq.trainer | begin training epoch 77
2024-09-04 09:50:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:50:45 | INFO | train_inner | epoch 079:     10 / 16 loss=8.635, nll_loss=5.976, ppl=62.92, wps=1478.8, ups=0.4, wpb=3705.5, bsz=133, num_updates=1258, lr=1.5096e-05, gnorm=2.564, train_wall=5, gb_free=10.4, wall=9715
2024-09-04 09:50:48 | INFO | train_inner | epoch 077:      2 / 21 loss=7.826, nll_loss=4.908, ppl=30.02, wps=116.1, ups=0.03, wpb=4377.5, bsz=72, num_updates=1598, lr=1.9176e-05, gnorm=2.477, train_wall=5, gb_free=12.8, wall=10205
2024-09-04 09:50:51 | INFO | train_inner | epoch 079:     12 / 16 loss=8.631, nll_loss=5.965, ppl=62.48, wps=1534.3, ups=0.33, wpb=4666, bsz=168, num_updates=1260, lr=1.512e-05, gnorm=2.904, train_wall=6, gb_free=9.6, wall=9721
2024-09-04 09:50:54 | INFO | train_inner | epoch 077:      4 / 21 loss=7.604, nll_loss=4.617, ppl=24.53, wps=1681.6, ups=0.38, wpb=4431, bsz=112, num_updates=1600, lr=1.92e-05, gnorm=2.594, train_wall=5, gb_free=13.2, wall=10211
2024-09-04 09:50:57 | INFO | train_inner | epoch 079:     14 / 16 loss=8.838, nll_loss=6.212, ppl=74.12, wps=1501.6, ups=0.35, wpb=4314.5, bsz=104, num_updates=1262, lr=1.5144e-05, gnorm=3.046, train_wall=6, gb_free=11.3, wall=9727
2024-09-04 09:50:59 | INFO | train_inner | epoch 077:      6 / 21 loss=7.72, nll_loss=4.776, ppl=27.39, wps=1603.6, ups=0.39, wpb=4149, bsz=92, num_updates=1602, lr=1.9224e-05, gnorm=2.621, train_wall=5, gb_free=14.6, wall=10216
2024-09-04 09:51:03 | INFO | train_inner | epoch 079:     16 / 16 loss=8.341, nll_loss=5.601, ppl=48.55, wps=1459.7, ups=0.33, wpb=4477.5, bsz=204, num_updates=1264, lr=1.5168e-05, gnorm=2.776, train_wall=6, gb_free=8.7, wall=9733
2024-09-04 09:51:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32227.50390625Mb; avail=222802.3515625Mb
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000713
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32227.50390625Mb; avail=222802.3515625Mb
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012694
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32227.50390625Mb; avail=222802.3515625Mb
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011240
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025024
2024-09-04 09:51:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32227.50390625Mb; avail=222802.3515625Mb
2024-09-04 09:51:05 | INFO | train_inner | epoch 077:      8 / 21 loss=7.59, nll_loss=4.653, ppl=25.16, wps=1475.1, ups=0.33, wpb=4524, bsz=120, num_updates=1604, lr=1.9248e-05, gnorm=3.095, train_wall=6, gb_free=12.4, wall=10222
2024-09-04 09:51:10 | INFO | train_inner | epoch 077:     10 / 21 loss=7.596, nll_loss=4.589, ppl=24.06, wps=1558.2, ups=0.38, wpb=4125, bsz=104, num_updates=1606, lr=1.9272e-05, gnorm=2.761, train_wall=5, gb_free=13.9, wall=10227
2024-09-04 09:51:17 | INFO | train_inner | epoch 077:     12 / 21 loss=7.845, nll_loss=4.926, ppl=30.4, wps=1593.4, ups=0.32, wpb=4986, bsz=92, num_updates=1608, lr=1.9296e-05, gnorm=2.758, train_wall=6, gb_free=14, wall=10233
2024-09-04 09:51:18 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.863 | nll_loss 8.74 | ppl 427.62 | wps 3821.3 | wpb 2070.5 | bsz 122.7 | num_updates 1264 | best_loss 10.839
2024-09-04 09:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 1264 updates
2024-09-04 09:51:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:51:22 | INFO | train_inner | epoch 077:     14 / 21 loss=8.011, nll_loss=5.152, ppl=35.55, wps=1874.4, ups=0.38, wpb=4874.5, bsz=64, num_updates=1610, lr=1.932e-05, gnorm=2.854, train_wall=5, gb_free=12.4, wall=10239
2024-09-04 09:51:27 | INFO | train_inner | epoch 077:     16 / 21 loss=7.716, nll_loss=4.766, ppl=27.21, wps=1667.2, ups=0.4, wpb=4203, bsz=73, num_updates=1612, lr=1.9344e-05, gnorm=2.502, train_wall=5, gb_free=17, wall=10244
2024-09-04 09:51:33 | INFO | train_inner | epoch 077:     18 / 21 loss=7.84, nll_loss=4.922, ppl=30.31, wps=1658, ups=0.33, wpb=4957.5, bsz=92, num_updates=1614, lr=1.9368e-05, gnorm=2.448, train_wall=6, gb_free=11.6, wall=10250
2024-09-04 09:51:39 | INFO | train_inner | epoch 077:     20 / 21 loss=7.455, nll_loss=4.442, ppl=21.74, wps=1688.6, ups=0.34, wpb=4989.5, bsz=152, num_updates=1616, lr=1.9392e-05, gnorm=2.762, train_wall=6, gb_free=11.5, wall=10256
2024-09-04 09:51:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41943.4453125Mb; avail=213085.79296875Mb
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41944.4296875Mb; avail=213085.79296875Mb
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012639
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41943.9375Mb; avail=213085.79296875Mb
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011198
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024977
2024-09-04 09:51:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41943.9375Mb; avail=213085.79296875Mb
2024-09-04 09:51:59 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.464 | nll_loss 6.934 | ppl 122.24 | wps 4718.5 | wpb 2350.9 | bsz 94.7 | num_updates 1617 | best_loss 9.334
2024-09-04 09:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 1617 updates
2024-09-04 09:51:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 79 @ 1264 updates, score 10.863) (writing took 43.02686502225697 seconds)
2024-09-04 09:52:01 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-09-04 09:52:01 | INFO | train | epoch 079 | loss 8.728 | nll_loss 6.08 | ppl 67.66 | wps 659.1 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1264 | lr 1.5168e-05 | gnorm 3.017 | train_wall 45 | gb_free 8.7 | wall 9790
2024-09-04 09:52:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:52:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:52:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:52:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000586
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26750.2890625Mb; avail=228279.609375Mb
2024-09-04 09:52:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000046
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000537
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26750.2890625Mb; avail=228279.609375Mb
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26750.2890625Mb; avail=228279.609375Mb
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000145
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000964
2024-09-04 09:52:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26750.2890625Mb; avail=228279.609375Mb
2024-09-04 09:52:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:52:01 | INFO | fairseq.trainer | begin training epoch 80
2024-09-04 09:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:52:06 | INFO | train_inner | epoch 080:      2 / 16 loss=8.845, nll_loss=6.252, ppl=76.22, wps=131.2, ups=0.03, wpb=4144, bsz=80, num_updates=1266, lr=1.5192e-05, gnorm=3.4, train_wall=5, gb_free=10.1, wall=9796
2024-09-04 09:52:12 | INFO | train_inner | epoch 080:      4 / 16 loss=8.324, nll_loss=5.552, ppl=46.93, wps=1479.1, ups=0.35, wpb=4167.5, bsz=168, num_updates=1268, lr=1.5216e-05, gnorm=3.059, train_wall=6, gb_free=11.1, wall=9801
2024-09-04 09:52:17 | INFO | train_inner | epoch 080:      6 / 16 loss=8.683, nll_loss=6.009, ppl=64.39, wps=1394.1, ups=0.39, wpb=3559, bsz=93, num_updates=1270, lr=1.524e-05, gnorm=3.456, train_wall=5, gb_free=8.5, wall=9807
2024-09-04 09:52:23 | INFO | train_inner | epoch 080:      8 / 16 loss=8.708, nll_loss=6.066, ppl=66.99, wps=1571.3, ups=0.34, wpb=4611.5, bsz=152, num_updates=1272, lr=1.5264e-05, gnorm=2.857, train_wall=6, gb_free=9.5, wall=9812
2024-09-04 09:52:29 | INFO | train_inner | epoch 080:     10 / 16 loss=8.811, nll_loss=6.175, ppl=72.25, wps=1520.6, ups=0.33, wpb=4649.5, bsz=124, num_updates=1274, lr=1.5288e-05, gnorm=3.076, train_wall=6, gb_free=8.9, wall=9819
2024-09-04 09:52:35 | INFO | train_inner | epoch 080:     12 / 16 loss=8.749, nll_loss=6.106, ppl=68.87, wps=1398.4, ups=0.35, wpb=3957, bsz=92, num_updates=1276, lr=1.5312e-05, gnorm=2.517, train_wall=6, gb_free=11.2, wall=9824
2024-09-04 09:52:41 | INFO | train_inner | epoch 080:     14 / 16 loss=8.599, nll_loss=5.909, ppl=60.07, wps=1533.4, ups=0.33, wpb=4704.5, bsz=176, num_updates=1278, lr=1.5336e-05, gnorm=2.73, train_wall=6, gb_free=9.6, wall=9830
2024-09-04 09:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:52:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 77 @ 1617 updates, score 9.464) (writing took 43.42709910031408 seconds)
2024-09-04 09:52:42 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-09-04 09:52:42 | INFO | train | epoch 077 | loss 7.724 | nll_loss 4.779 | ppl 27.46 | wps 804.4 | ups 0.18 | wpb 4556.3 | bsz 96.9 | num_updates 1617 | lr 1.9404e-05 | gnorm 2.663 | train_wall 58 | gb_free 12.4 | wall 10319
2024-09-04 09:52:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:52:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:52:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:52:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000695
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32759.421875Mb; avail=222270.4765625Mb
2024-09-04 09:52:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000552
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32759.421875Mb; avail=222270.4765625Mb
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32759.421875Mb; avail=222270.4765625Mb
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000166
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001000
2024-09-04 09:52:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32759.421875Mb; avail=222270.4765625Mb
2024-09-04 09:52:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:52:43 | INFO | fairseq.trainer | begin training epoch 78
2024-09-04 09:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:52:46 | INFO | train_inner | epoch 078:      1 / 21 loss=7.832, nll_loss=4.93, ppl=30.49, wps=133.3, ups=0.03, wpb=4471, bsz=80, num_updates=1618, lr=1.9416e-05, gnorm=2.718, train_wall=6, gb_free=12.5, wall=10323
2024-09-04 09:52:46 | INFO | train_inner | epoch 080:     16 / 16 loss=8.678, nll_loss=6.014, ppl=64.63, wps=1453.4, ups=0.35, wpb=4098.5, bsz=132, num_updates=1280, lr=1.536e-05, gnorm=2.702, train_wall=6, gb_free=13.4, wall=9836
2024-09-04 09:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32800.08203125Mb; avail=222229.7734375Mb
2024-09-04 09:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000750
2024-09-04 09:52:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32800.57421875Mb; avail=222229.28125Mb
2024-09-04 09:52:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012715
2024-09-04 09:52:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32800.5703125Mb; avail=222229.28125Mb
2024-09-04 09:52:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011118
2024-09-04 09:52:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024938
2024-09-04 09:52:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32800.5703125Mb; avail=222229.28125Mb
2024-09-04 09:52:51 | INFO | train_inner | epoch 078:      3 / 21 loss=7.812, nll_loss=4.885, ppl=29.55, wps=2017.9, ups=0.39, wpb=5239, bsz=100, num_updates=1620, lr=1.944e-05, gnorm=2.695, train_wall=5, gb_free=12.5, wall=10328
2024-09-04 09:52:56 | INFO | train_inner | epoch 078:      5 / 21 loss=7.531, nll_loss=4.542, ppl=23.29, wps=1956.5, ups=0.41, wpb=4825.5, bsz=128, num_updates=1622, lr=1.9464e-05, gnorm=2.377, train_wall=5, gb_free=11.7, wall=10333
2024-09-04 09:53:01 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.853 | nll_loss 8.732 | ppl 425.17 | wps 3823.9 | wpb 2070.5 | bsz 122.7 | num_updates 1280 | best_loss 10.839
2024-09-04 09:53:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 1280 updates
2024-09-04 09:53:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:53:02 | INFO | train_inner | epoch 078:      7 / 21 loss=7.595, nll_loss=4.617, ppl=24.54, wps=1560.7, ups=0.34, wpb=4540.5, bsz=116, num_updates=1624, lr=1.9488e-05, gnorm=2.48, train_wall=6, gb_free=13, wall=10339
2024-09-04 09:53:06 | INFO | train_inner | epoch 078:      9 / 21 loss=7.723, nll_loss=4.798, ppl=27.83, wps=1970.8, ups=0.42, wpb=4666, bsz=80, num_updates=1626, lr=1.9512e-05, gnorm=2.606, train_wall=5, gb_free=11.5, wall=10343
2024-09-04 09:53:11 | INFO | train_inner | epoch 078:     11 / 21 loss=7.723, nll_loss=4.745, ppl=26.82, wps=1429.2, ups=0.41, wpb=3521.5, bsz=53, num_updates=1628, lr=1.9536e-05, gnorm=2.673, train_wall=5, gb_free=20, wall=10348
2024-09-04 09:53:17 | INFO | train_inner | epoch 078:     13 / 21 loss=7.655, nll_loss=4.68, ppl=25.64, wps=1569.2, ups=0.35, wpb=4422, bsz=112, num_updates=1630, lr=1.956e-05, gnorm=2.552, train_wall=6, gb_free=12.3, wall=10354
2024-09-04 09:53:22 | INFO | train_inner | epoch 078:     15 / 21 loss=7.401, nll_loss=4.382, ppl=20.86, wps=1881.4, ups=0.42, wpb=4444.5, bsz=124, num_updates=1632, lr=1.9584e-05, gnorm=2.716, train_wall=5, gb_free=12.1, wall=10359
2024-09-04 09:53:37 | INFO | train_inner | epoch 078:     17 / 21 loss=7.702, nll_loss=4.769, ppl=27.26, wps=574.6, ups=0.13, wpb=4484, bsz=100, num_updates=1634, lr=1.9608e-05, gnorm=2.438, train_wall=16, gb_free=12.2, wall=10374
2024-09-04 09:53:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:53:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 80 @ 1280 updates, score 10.853) (writing took 39.00924052298069 seconds)
2024-09-04 09:53:40 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-09-04 09:53:40 | INFO | train | epoch 080 | loss 8.675 | nll_loss 6.011 | ppl 64.49 | wps 683.3 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1280 | lr 1.536e-05 | gnorm 2.975 | train_wall 46 | gb_free 13.4 | wall 9889
2024-09-04 09:53:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:53:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:53:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:53:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000694
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23290.4375Mb; avail=231739.4609375Mb
2024-09-04 09:53:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000616
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23290.4375Mb; avail=231739.4609375Mb
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23290.4375Mb; avail=231739.4609375Mb
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000177
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001154
2024-09-04 09:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23290.4375Mb; avail=231739.4609375Mb
2024-09-04 09:53:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:53:40 | INFO | fairseq.trainer | begin training epoch 81
2024-09-04 09:53:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:53:42 | INFO | train_inner | epoch 078:     19 / 21 loss=7.819, nll_loss=4.893, ppl=29.71, wps=1942.1, ups=0.42, wpb=4635.5, bsz=76, num_updates=1636, lr=1.9632e-05, gnorm=2.311, train_wall=5, gb_free=14.1, wall=10379
2024-09-04 09:53:46 | INFO | train_inner | epoch 081:      2 / 16 loss=8.382, nll_loss=5.638, ppl=49.81, wps=143.9, ups=0.03, wpb=4274, bsz=152, num_updates=1282, lr=1.5384e-05, gnorm=2.69, train_wall=6, gb_free=10.5, wall=9895
2024-09-04 09:53:48 | INFO | train_inner | epoch 078:     21 / 21 loss=7.782, nll_loss=4.866, ppl=29.17, wps=1666.8, ups=0.35, wpb=4815.5, bsz=92, num_updates=1638, lr=1.9656e-05, gnorm=2.683, train_wall=6, gb_free=11.6, wall=10385
2024-09-04 09:53:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23333.66015625Mb; avail=231696.23828125Mb
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000633
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23333.66015625Mb; avail=231696.23828125Mb
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012688
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23333.66015625Mb; avail=231696.23828125Mb
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011260
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024928
2024-09-04 09:53:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23333.66015625Mb; avail=231696.23828125Mb
2024-09-04 09:53:52 | INFO | train_inner | epoch 081:      4 / 16 loss=8.557, nll_loss=5.87, ppl=58.48, wps=1570, ups=0.34, wpb=4646.5, bsz=168, num_updates=1284, lr=1.5408e-05, gnorm=3.06, train_wall=6, gb_free=11, wall=9901
2024-09-04 09:53:58 | INFO | train_inner | epoch 081:      6 / 16 loss=8.555, nll_loss=5.871, ppl=58.52, wps=1521.9, ups=0.34, wpb=4432, bsz=144, num_updates=1286, lr=1.5432e-05, gnorm=2.95, train_wall=6, gb_free=12.5, wall=9907
2024-09-04 09:54:02 | INFO | train_inner | epoch 081:      8 / 16 loss=8.958, nll_loss=6.376, ppl=83.08, wps=1447.1, ups=0.47, wpb=3078, bsz=45, num_updates=1288, lr=1.5456e-05, gnorm=4.397, train_wall=4, gb_free=11.2, wall=9911
2024-09-04 09:54:04 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.36 | nll_loss 6.748 | ppl 107.47 | wps 5003.2 | wpb 2350.9 | bsz 94.7 | num_updates 1638 | best_loss 9.334
2024-09-04 09:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 1638 updates
2024-09-04 09:54:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:54:07 | INFO | train_inner | epoch 081:     10 / 16 loss=8.844, nll_loss=6.161, ppl=71.55, wps=1549.7, ups=0.36, wpb=4299.5, bsz=100, num_updates=1290, lr=1.548e-05, gnorm=3.163, train_wall=6, gb_free=13.9, wall=9917
2024-09-04 09:54:13 | INFO | train_inner | epoch 081:     12 / 16 loss=8.689, nll_loss=6.004, ppl=64.18, wps=1460.1, ups=0.33, wpb=4359, bsz=132, num_updates=1292, lr=1.5504e-05, gnorm=2.928, train_wall=6, gb_free=10.7, wall=9923
2024-09-04 09:54:20 | INFO | train_inner | epoch 081:     14 / 16 loss=8.693, nll_loss=6.05, ppl=66.25, wps=1494.6, ups=0.32, wpb=4717.5, bsz=156, num_updates=1294, lr=1.5528e-05, gnorm=2.987, train_wall=6, gb_free=10.1, wall=9929
2024-09-04 09:54:25 | INFO | train_inner | epoch 081:     16 / 16 loss=8.645, nll_loss=6.002, ppl=64.09, wps=1446.3, ups=0.35, wpb=4085, bsz=120, num_updates=1296, lr=1.5552e-05, gnorm=3.372, train_wall=6, gb_free=14, wall=9935
2024-09-04 09:54:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40465.0625Mb; avail=214564.7890625Mb
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000731
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40465.0625Mb; avail=214564.7890625Mb
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012810
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40465.0625Mb; avail=214564.7890625Mb
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011081
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024966
2024-09-04 09:54:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40466.046875Mb; avail=214564.296875Mb
2024-09-04 09:54:40 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.923 | nll_loss 8.838 | ppl 457.57 | wps 3823.1 | wpb 2070.5 | bsz 122.7 | num_updates 1296 | best_loss 10.839
2024-09-04 09:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 1296 updates
2024-09-04 09:54:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:54:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:54:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 78 @ 1638 updates, score 9.36) (writing took 39.71940967999399 seconds)
2024-09-04 09:54:44 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-09-04 09:54:44 | INFO | train | epoch 078 | loss 7.688 | nll_loss 4.737 | ppl 26.68 | wps 787.7 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1638 | lr 1.9656e-05 | gnorm 2.587 | train_wall 65 | gb_free 11.6 | wall 10441
2024-09-04 09:54:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:54:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:54:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:54:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000639
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34710.33984375Mb; avail=220319.50390625Mb
2024-09-04 09:54:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000589
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34710.33984375Mb; avail=220319.50390625Mb
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34710.33984375Mb; avail=220319.50390625Mb
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000160
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001035
2024-09-04 09:54:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34710.33984375Mb; avail=220319.50390625Mb
2024-09-04 09:54:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:54:44 | INFO | fairseq.trainer | begin training epoch 79
2024-09-04 09:54:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:54:50 | INFO | train_inner | epoch 079:      2 / 21 loss=7.718, nll_loss=4.762, ppl=27.13, wps=158.9, ups=0.03, wpb=4900, bsz=112, num_updates=1640, lr=1.968e-05, gnorm=2.448, train_wall=5, gb_free=13.8, wall=10446
2024-09-04 09:54:54 | INFO | train_inner | epoch 079:      4 / 21 loss=7.336, nll_loss=4.255, ppl=19.09, wps=1799.1, ups=0.45, wpb=3964.5, bsz=113, num_updates=1642, lr=1.9704e-05, gnorm=2.391, train_wall=4, gb_free=16.6, wall=10451
2024-09-04 09:54:59 | INFO | train_inner | epoch 079:      6 / 21 loss=7.528, nll_loss=4.544, ppl=23.34, wps=1906.8, ups=0.42, wpb=4509.5, bsz=112, num_updates=1644, lr=1.9728e-05, gnorm=2.509, train_wall=5, gb_free=14.5, wall=10456
2024-09-04 09:55:04 | INFO | train_inner | epoch 079:      8 / 21 loss=7.964, nll_loss=5.123, ppl=34.84, wps=1572.2, ups=0.36, wpb=4318, bsz=56, num_updates=1646, lr=1.9752e-05, gnorm=3.139, train_wall=5, gb_free=12, wall=10461
2024-09-04 09:55:09 | INFO | train_inner | epoch 079:     10 / 21 loss=7.717, nll_loss=4.758, ppl=27.06, wps=1956, ups=0.43, wpb=4596, bsz=80, num_updates=1648, lr=1.9776e-05, gnorm=2.642, train_wall=5, gb_free=14.5, wall=10466
2024-09-04 09:55:14 | INFO | train_inner | epoch 079:     12 / 21 loss=7.755, nll_loss=4.817, ppl=28.19, wps=1585.1, ups=0.36, wpb=4417.5, bsz=76, num_updates=1650, lr=1.98e-05, gnorm=2.911, train_wall=6, gb_free=12.5, wall=10471
2024-09-04 09:55:19 | INFO | train_inner | epoch 079:     14 / 21 loss=7.866, nll_loss=4.964, ppl=31.22, wps=1940.8, ups=0.43, wpb=4500, bsz=64, num_updates=1652, lr=1.9824e-05, gnorm=3.486, train_wall=5, gb_free=13.1, wall=10476
2024-09-04 09:55:25 | INFO | train_inner | epoch 079:     16 / 21 loss=7.744, nll_loss=4.789, ppl=27.65, wps=1510.9, ups=0.33, wpb=4642.5, bsz=104, num_updates=1654, lr=1.9848e-05, gnorm=2.781, train_wall=6, gb_free=12.9, wall=10482
2024-09-04 09:55:31 | INFO | train_inner | epoch 079:     18 / 21 loss=7.583, nll_loss=4.618, ppl=24.56, wps=1649.8, ups=0.36, wpb=4601, bsz=112, num_updates=1656, lr=1.9872e-05, gnorm=2.614, train_wall=6, gb_free=12.2, wall=10488
2024-09-04 09:55:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 81 @ 1296 updates, score 10.923) (writing took 51.86182955186814 seconds)
2024-09-04 09:55:32 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-09-04 09:55:32 | INFO | train | epoch 081 | loss 8.654 | nll_loss 5.982 | ppl 63.2 | wps 606.7 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 1296 | lr 1.5552e-05 | gnorm 3.193 | train_wall 45 | gb_free 14 | wall 10001
2024-09-04 09:55:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:55:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:55:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:55:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000586
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28067.3671875Mb; avail=226962.56640625Mb
2024-09-04 09:55:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000055
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000532
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28067.3671875Mb; avail=226962.56640625Mb
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28067.3671875Mb; avail=226962.56640625Mb
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000150
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000978
2024-09-04 09:55:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28067.3671875Mb; avail=226962.56640625Mb
2024-09-04 09:55:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:55:32 | INFO | fairseq.trainer | begin training epoch 82
2024-09-04 09:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:55:36 | INFO | train_inner | epoch 079:     20 / 21 loss=7.742, nll_loss=4.806, ppl=27.98, wps=2077.1, ups=0.39, wpb=5332.5, bsz=116, num_updates=1658, lr=1.9896e-05, gnorm=2.66, train_wall=5, gb_free=13, wall=10493
2024-09-04 09:55:38 | INFO | train_inner | epoch 082:      2 / 16 loss=8.224, nll_loss=5.456, ppl=43.9, wps=117.6, ups=0.03, wpb=4267.5, bsz=180, num_updates=1298, lr=1.5576e-05, gnorm=3.002, train_wall=6, gb_free=9.3, wall=10007
2024-09-04 09:55:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28105.28515625Mb; avail=226924.61328125Mb
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000753
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28105.77734375Mb; avail=226924.12109375Mb
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012911
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28105.77734375Mb; avail=226924.12109375Mb
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011370
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025400
2024-09-04 09:55:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28105.77734375Mb; avail=226924.12109375Mb
2024-09-04 09:55:44 | INFO | train_inner | epoch 082:      4 / 16 loss=8.768, nll_loss=6.131, ppl=70.11, wps=1502.4, ups=0.36, wpb=4231.5, bsz=112, num_updates=1300, lr=1.56e-05, gnorm=2.966, train_wall=6, gb_free=8.8, wall=10013
2024-09-04 09:55:49 | INFO | train_inner | epoch 082:      6 / 16 loss=8.768, nll_loss=6.149, ppl=70.97, wps=1556.6, ups=0.38, wpb=4131.5, bsz=92, num_updates=1302, lr=1.5624e-05, gnorm=3.232, train_wall=5, gb_free=10.2, wall=10018
2024-09-04 09:55:55 | INFO | train_inner | epoch 082:      8 / 16 loss=8.595, nll_loss=5.875, ppl=58.68, wps=1539.1, ups=0.34, wpb=4506, bsz=156, num_updates=1304, lr=1.5648e-05, gnorm=2.598, train_wall=6, gb_free=12, wall=10024
2024-09-04 09:55:57 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.389 | nll_loss 6.768 | ppl 109.02 | wps 4634.4 | wpb 2350.9 | bsz 94.7 | num_updates 1659 | best_loss 9.334
2024-09-04 09:55:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 1659 updates
2024-09-04 09:55:57 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:56:00 | INFO | train_inner | epoch 082:     10 / 16 loss=8.729, nll_loss=6.053, ppl=66.38, wps=1546.1, ups=0.35, wpb=4386, bsz=112, num_updates=1306, lr=1.5672e-05, gnorm=3.668, train_wall=6, gb_free=9.7, wall=10030
2024-09-04 09:56:06 | INFO | train_inner | epoch 082:     12 / 16 loss=8.596, nll_loss=5.938, ppl=61.31, wps=1587.1, ups=0.34, wpb=4601, bsz=164, num_updates=1308, lr=1.5696e-05, gnorm=3.183, train_wall=6, gb_free=10.2, wall=10036
2024-09-04 09:56:12 | INFO | train_inner | epoch 082:     14 / 16 loss=8.538, nll_loss=5.85, ppl=57.7, wps=1504, ups=0.35, wpb=4288, bsz=124, num_updates=1310, lr=1.572e-05, gnorm=2.592, train_wall=6, gb_free=12.1, wall=10041
2024-09-04 09:56:17 | INFO | train_inner | epoch 082:     16 / 16 loss=8.675, nll_loss=5.992, ppl=63.65, wps=1380.8, ups=0.4, wpb=3480, bsz=77, num_updates=1312, lr=1.5744e-05, gnorm=3.168, train_wall=5, gb_free=14.8, wall=10046
2024-09-04 09:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45248.19140625Mb; avail=209781.62890625Mb
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000736
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45248.68359375Mb; avail=209781.13671875Mb
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012595
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45248.19140625Mb; avail=209781.62890625Mb
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011194
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024889
2024-09-04 09:56:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45248.19140625Mb; avail=209781.13671875Mb
2024-09-04 09:56:31 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.808 | nll_loss 8.668 | ppl 406.62 | wps 3824.1 | wpb 2070.5 | bsz 122.7 | num_updates 1312 | best_loss 10.808
2024-09-04 09:56:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 1312 updates
2024-09-04 09:56:31 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:56:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:56:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 79 @ 1659 updates, score 9.389) (writing took 39.43189042713493 seconds)
2024-09-04 09:56:36 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-09-04 09:56:36 | INFO | train | epoch 079 | loss 7.68 | nll_loss 4.723 | ppl 26.41 | wps 851.1 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1659 | lr 1.9908e-05 | gnorm 2.72 | train_wall 55 | gb_free 12.6 | wall 10553
2024-09-04 09:56:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:56:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:56:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:56:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000683
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34683.60546875Mb; avail=220346.2421875Mb
2024-09-04 09:56:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000579
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34683.60546875Mb; avail=220346.2421875Mb
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34683.60546875Mb; avail=220346.2421875Mb
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000167
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001042
2024-09-04 09:56:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34683.60546875Mb; avail=220346.2421875Mb
2024-09-04 09:56:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:56:36 | INFO | fairseq.trainer | begin training epoch 80
2024-09-04 09:56:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:56:40 | INFO | train_inner | epoch 080:      1 / 21 loss=7.323, nll_loss=4.238, ppl=18.87, wps=139.9, ups=0.03, wpb=4476.5, bsz=148, num_updates=1660, lr=1.992e-05, gnorm=2.126, train_wall=7, gb_free=11.8, wall=10557
2024-09-04 09:56:45 | INFO | train_inner | epoch 080:      3 / 21 loss=7.772, nll_loss=4.855, ppl=28.95, wps=1966.6, ups=0.43, wpb=4581, bsz=96, num_updates=1662, lr=1.9944e-05, gnorm=2.719, train_wall=5, gb_free=17.3, wall=10561
2024-09-04 09:56:50 | INFO | train_inner | epoch 080:      5 / 21 loss=7.849, nll_loss=4.947, ppl=30.85, wps=1719.3, ups=0.38, wpb=4525, bsz=60, num_updates=1664, lr=1.9968e-05, gnorm=2.853, train_wall=5, gb_free=12.5, wall=10567
2024-09-04 09:56:56 | INFO | train_inner | epoch 080:      7 / 21 loss=7.249, nll_loss=4.159, ppl=17.86, wps=1608.2, ups=0.34, wpb=4678, bsz=148, num_updates=1666, lr=1.9992e-05, gnorm=2.308, train_wall=6, gb_free=15.1, wall=10573
2024-09-04 09:57:01 | INFO | train_inner | epoch 080:      9 / 21 loss=7.607, nll_loss=4.631, ppl=24.78, wps=1603.2, ups=0.35, wpb=4602, bsz=116, num_updates=1668, lr=2.0016e-05, gnorm=2.714, train_wall=6, gb_free=12.7, wall=10578
2024-09-04 09:57:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 09:57:16 | INFO | train_inner | epoch 080:     11 / 21 loss=7.725, nll_loss=4.755, ppl=27, wps=616.9, ups=0.14, wpb=4477, bsz=72, num_updates=1670, lr=2.004e-05, gnorm=2.607, train_wall=15, gb_free=12.8, wall=10593
2024-09-04 09:57:22 | INFO | train_inner | epoch 080:     13 / 21 loss=7.696, nll_loss=4.752, ppl=26.94, wps=1592.2, ups=0.34, wpb=4700.5, bsz=88, num_updates=1672, lr=2.0064e-05, gnorm=2.44, train_wall=6, gb_free=12, wall=10599
2024-09-04 09:57:26 | INFO | train_inner | epoch 080:     15 / 21 loss=7.578, nll_loss=4.605, ppl=24.33, wps=1559.7, ups=0.43, wpb=3655, bsz=69, num_updates=1674, lr=2.0088e-05, gnorm=2.269, train_wall=5, gb_free=12.6, wall=10603
2024-09-04 09:57:32 | INFO | train_inner | epoch 080:     17 / 21 loss=7.435, nll_loss=4.426, ppl=21.5, wps=1752.1, ups=0.35, wpb=5004, bsz=144, num_updates=1676, lr=2.0112e-05, gnorm=2.512, train_wall=6, gb_free=10.8, wall=10609
2024-09-04 09:57:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 82 @ 1312 updates, score 10.808) (writing took 65.20209114905447 seconds)
2024-09-04 09:57:37 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2024-09-04 09:57:37 | INFO | train | epoch 082 | loss 8.609 | nll_loss 5.928 | ppl 60.89 | wps 542.7 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 1312 | lr 1.5744e-05 | gnorm 3.051 | train_wall 45 | gb_free 14.8 | wall 10126
2024-09-04 09:57:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:57:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:57:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:57:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000747
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24508.91015625Mb; avail=230521.0234375Mb
2024-09-04 09:57:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000050
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000659
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24508.91015625Mb; avail=230521.0234375Mb
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24508.91015625Mb; avail=230521.0234375Mb
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000180
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001139
2024-09-04 09:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24508.91015625Mb; avail=230521.0234375Mb
2024-09-04 09:57:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:57:37 | INFO | fairseq.trainer | begin training epoch 83
2024-09-04 09:57:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:57:37 | INFO | train_inner | epoch 080:     19 / 21 loss=7.871, nll_loss=4.98, ppl=31.56, wps=1977.8, ups=0.42, wpb=4661, bsz=64, num_updates=1678, lr=2.0136e-05, gnorm=2.95, train_wall=5, gb_free=13.3, wall=10614
2024-09-04 09:57:42 | INFO | train_inner | epoch 083:      2 / 16 loss=8.76, nll_loss=6.124, ppl=69.76, wps=102.7, ups=0.02, wpb=4384, bsz=104, num_updates=1314, lr=1.5768e-05, gnorm=3.09, train_wall=6, gb_free=10, wall=10132
2024-09-04 09:57:43 | INFO | train_inner | epoch 080:     21 / 21 loss=7.87, nll_loss=4.963, ppl=31.2, wps=1597, ups=0.35, wpb=4540.5, bsz=84, num_updates=1680, lr=2.016e-05, gnorm=2.704, train_wall=6, gb_free=12, wall=10620
2024-09-04 09:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24546.74609375Mb; avail=230483.1484375Mb
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000640
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24546.74609375Mb; avail=230483.1484375Mb
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012715
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24546.74609375Mb; avail=230483.1484375Mb
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011152
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024870
2024-09-04 09:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24546.74609375Mb; avail=230483.1484375Mb
2024-09-04 09:57:48 | INFO | train_inner | epoch 083:      4 / 16 loss=8.524, nll_loss=5.846, ppl=57.51, wps=1481.9, ups=0.33, wpb=4454, bsz=152, num_updates=1316, lr=1.5792e-05, gnorm=3.168, train_wall=6, gb_free=9.8, wall=10138
2024-09-04 09:57:54 | INFO | train_inner | epoch 083:      6 / 16 loss=8.343, nll_loss=5.593, ppl=48.26, wps=1461.3, ups=0.34, wpb=4246, bsz=148, num_updates=1318, lr=1.5816e-05, gnorm=3.375, train_wall=6, gb_free=9.9, wall=10144
2024-09-04 09:57:59 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.372 | nll_loss 6.779 | ppl 109.83 | wps 4969.5 | wpb 2350.9 | bsz 94.7 | num_updates 1680 | best_loss 9.334
2024-09-04 09:57:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 1680 updates
2024-09-04 09:57:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:58:00 | INFO | train_inner | epoch 083:      8 / 16 loss=8.368, nll_loss=5.638, ppl=49.8, wps=1512.7, ups=0.34, wpb=4423.5, bsz=176, num_updates=1320, lr=1.584e-05, gnorm=3.128, train_wall=6, gb_free=10.3, wall=10149
2024-09-04 09:58:06 | INFO | train_inner | epoch 083:     10 / 16 loss=8.869, nll_loss=6.223, ppl=74.7, wps=1558, ups=0.35, wpb=4488, bsz=88, num_updates=1322, lr=1.5864e-05, gnorm=3.201, train_wall=6, gb_free=10.5, wall=10155
2024-09-04 09:58:11 | INFO | train_inner | epoch 083:     12 / 16 loss=8.502, nll_loss=5.799, ppl=55.68, wps=1419, ups=0.39, wpb=3596, bsz=105, num_updates=1324, lr=1.5888e-05, gnorm=3.071, train_wall=5, gb_free=15.7, wall=10160
2024-09-04 09:58:16 | INFO | train_inner | epoch 083:     14 / 16 loss=8.514, nll_loss=5.827, ppl=56.75, wps=1575.9, ups=0.36, wpb=4414.5, bsz=160, num_updates=1326, lr=1.5912e-05, gnorm=2.913, train_wall=6, gb_free=14.3, wall=10166
2024-09-04 09:58:22 | INFO | train_inner | epoch 083:     16 / 16 loss=8.747, nll_loss=6.104, ppl=68.77, wps=1486.7, ups=0.38, wpb=3885.5, bsz=84, num_updates=1328, lr=1.5936e-05, gnorm=3.267, train_wall=5, gb_free=10.1, wall=10171
2024-09-04 09:58:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41671.89453125Mb; avail=213357.9609375Mb
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000738
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41672.38671875Mb; avail=213357.46875Mb
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012607
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41672.9765625Mb; avail=213356.87890625Mb
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011187
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024879
2024-09-04 09:58:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41672.9765625Mb; avail=213356.87890625Mb
2024-09-04 09:58:36 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.809 | nll_loss 8.617 | ppl 392.66 | wps 3830.9 | wpb 2070.5 | bsz 122.7 | num_updates 1328 | best_loss 10.808
2024-09-04 09:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 1328 updates
2024-09-04 09:58:36 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:58:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:58:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 80 @ 1680 updates, score 9.372) (writing took 38.12415813840926 seconds)
2024-09-04 09:58:37 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-09-04 09:58:37 | INFO | train | epoch 080 | loss 7.649 | nll_loss 4.686 | ppl 25.74 | wps 791.9 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1680 | lr 2.016e-05 | gnorm 2.593 | train_wall 66 | gb_free 12 | wall 10674
2024-09-04 09:58:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 09:58:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 09:58:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 09:58:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000641
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30573.33203125Mb; avail=224456.5078125Mb
2024-09-04 09:58:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000571
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30573.33203125Mb; avail=224456.5078125Mb
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30573.33203125Mb; avail=224456.5078125Mb
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000164
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001033
2024-09-04 09:58:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30573.33203125Mb; avail=224456.5078125Mb
2024-09-04 09:58:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 09:58:37 | INFO | fairseq.trainer | begin training epoch 81
2024-09-04 09:58:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:58:43 | INFO | train_inner | epoch 081:      2 / 21 loss=7.713, nll_loss=4.745, ppl=26.82, wps=142.5, ups=0.03, wpb=4274.5, bsz=68, num_updates=1682, lr=2.0184e-05, gnorm=2.81, train_wall=5, gb_free=14.2, wall=10680
2024-09-04 09:58:48 | INFO | train_inner | epoch 081:      4 / 21 loss=7.749, nll_loss=4.809, ppl=28.02, wps=1586.8, ups=0.38, wpb=4198, bsz=76, num_updates=1684, lr=2.0208e-05, gnorm=2.662, train_wall=5, gb_free=12, wall=10685
2024-09-04 09:58:53 | INFO | train_inner | epoch 081:      6 / 21 loss=7.232, nll_loss=4.171, ppl=18.01, wps=1617.4, ups=0.37, wpb=4415, bsz=136, num_updates=1686, lr=2.0232e-05, gnorm=2.457, train_wall=5, gb_free=14.1, wall=10690
2024-09-04 09:58:59 | INFO | train_inner | epoch 081:      8 / 21 loss=7.405, nll_loss=4.397, ppl=21.07, wps=1804.7, ups=0.38, wpb=4803.5, bsz=144, num_updates=1688, lr=2.0256e-05, gnorm=2.931, train_wall=5, gb_free=12.1, wall=10696
2024-09-04 09:59:04 | INFO | train_inner | epoch 081:     10 / 21 loss=7.746, nll_loss=4.789, ppl=27.64, wps=1931.7, ups=0.4, wpb=4783.5, bsz=92, num_updates=1690, lr=2.028e-05, gnorm=2.587, train_wall=5, gb_free=12.6, wall=10701
2024-09-04 09:59:09 | INFO | train_inner | epoch 081:     12 / 21 loss=7.826, nll_loss=4.888, ppl=29.62, wps=1624, ups=0.35, wpb=4669, bsz=72, num_updates=1692, lr=2.0304e-05, gnorm=2.811, train_wall=6, gb_free=11.8, wall=10706
2024-09-04 09:59:14 | INFO | train_inner | epoch 081:     14 / 21 loss=7.713, nll_loss=4.791, ppl=27.69, wps=1909.6, ups=0.47, wpb=4043, bsz=53, num_updates=1694, lr=2.0328e-05, gnorm=3.308, train_wall=4, gb_free=12.1, wall=10711
2024-09-04 09:59:19 | INFO | train_inner | epoch 081:     16 / 21 loss=7.567, nll_loss=4.589, ppl=24.07, wps=1766.3, ups=0.37, wpb=4769, bsz=120, num_updates=1696, lr=2.0352e-05, gnorm=2.685, train_wall=5, gb_free=12.1, wall=10716
2024-09-04 09:59:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 09:59:24 | INFO | train_inner | epoch 081:     18 / 21 loss=7.674, nll_loss=4.73, ppl=26.54, wps=1739.2, ups=0.37, wpb=4692.5, bsz=88, num_updates=1698, lr=2.0376e-05, gnorm=2.895, train_wall=5, gb_free=11.9, wall=10721
2024-09-04 09:59:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 83 @ 1328 updates, score 10.809) (writing took 48.782480262219906 seconds)
2024-09-04 09:59:25 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2024-09-04 09:59:25 | INFO | train | epoch 083 | loss 8.579 | nll_loss 5.895 | ppl 59.51 | wps 626.2 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1328 | lr 1.5936e-05 | gnorm 3.151 | train_wall 45 | gb_free 10.1 | wall 10234
2024-09-04 09:59:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 09:59:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 09:59:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 09:59:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000577
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28233.64453125Mb; avail=226796.4453125Mb
2024-09-04 09:59:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000552
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28233.64453125Mb; avail=226796.4453125Mb
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28233.64453125Mb; avail=226796.4453125Mb
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000146
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000986
2024-09-04 09:59:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28233.64453125Mb; avail=226796.4453125Mb
2024-09-04 09:59:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 09:59:25 | INFO | fairseq.trainer | begin training epoch 84
2024-09-04 09:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 09:59:29 | INFO | train_inner | epoch 081:     20 / 21 loss=7.859, nll_loss=4.907, ppl=30, wps=1806, ups=0.39, wpb=4576.5, bsz=76, num_updates=1700, lr=2.04e-05, gnorm=2.624, train_wall=5, gb_free=12.8, wall=10726
2024-09-04 09:59:31 | INFO | train_inner | epoch 084:      2 / 16 loss=8.428, nll_loss=5.682, ppl=51.32, wps=123.1, ups=0.03, wpb=4258, bsz=148, num_updates=1330, lr=1.596e-05, gnorm=2.669, train_wall=6, gb_free=11.4, wall=10240
2024-09-04 09:59:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28273.203125Mb; avail=226756.69140625Mb
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000734
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28273.203125Mb; avail=226756.69140625Mb
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012757
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28273.203125Mb; avail=226756.69140625Mb
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011461
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025308
2024-09-04 09:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28273.203125Mb; avail=226756.69140625Mb
2024-09-04 09:59:37 | INFO | train_inner | epoch 084:      4 / 16 loss=8.676, nll_loss=6.004, ppl=64.18, wps=1564.9, ups=0.35, wpb=4517, bsz=100, num_updates=1332, lr=1.5984e-05, gnorm=3.143, train_wall=6, gb_free=9.2, wall=10246
2024-09-04 09:59:42 | INFO | train_inner | epoch 084:      6 / 16 loss=8.596, nll_loss=5.934, ppl=61.15, wps=1498.7, ups=0.38, wpb=3995, bsz=108, num_updates=1334, lr=1.6008e-05, gnorm=3.511, train_wall=5, gb_free=10, wall=10251
2024-09-04 09:59:48 | INFO | train_inner | epoch 084:      8 / 16 loss=8.604, nll_loss=5.958, ppl=62.15, wps=1547, ups=0.33, wpb=4743.5, bsz=172, num_updates=1336, lr=1.6032e-05, gnorm=3.412, train_wall=6, gb_free=9.7, wall=10258
2024-09-04 09:59:50 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.397 | nll_loss 6.816 | ppl 112.71 | wps 4664.3 | wpb 2350.9 | bsz 94.7 | num_updates 1701 | best_loss 9.334
2024-09-04 09:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 1701 updates
2024-09-04 09:59:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 09:59:53 | INFO | train_inner | epoch 084:     10 / 16 loss=8.507, nll_loss=5.776, ppl=54.81, wps=1448.8, ups=0.4, wpb=3592.5, bsz=77, num_updates=1338, lr=1.6056e-05, gnorm=3.174, train_wall=5, gb_free=11.3, wall=10262
2024-09-04 09:59:59 | INFO | train_inner | epoch 084:     12 / 16 loss=8.691, nll_loss=5.997, ppl=63.87, wps=1504.5, ups=0.35, wpb=4315.5, bsz=108, num_updates=1340, lr=1.608e-05, gnorm=3.219, train_wall=6, gb_free=10.5, wall=10268
2024-09-04 10:00:04 | INFO | train_inner | epoch 084:     14 / 16 loss=8.632, nll_loss=5.95, ppl=61.84, wps=1511.5, ups=0.36, wpb=4251.5, bsz=100, num_updates=1342, lr=1.6104e-05, gnorm=3.028, train_wall=6, gb_free=9.8, wall=10274
2024-09-04 10:00:10 | INFO | train_inner | epoch 084:     16 / 16 loss=8.025, nll_loss=5.179, ppl=36.22, wps=1382.8, ups=0.33, wpb=4218.5, bsz=204, num_updates=1344, lr=1.6128e-05, gnorm=2.648, train_wall=6, gb_free=9.5, wall=10280
2024-09-04 10:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45402.83203125Mb; avail=209627.06640625Mb
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000731
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45403.32421875Mb; avail=209626.57421875Mb
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012506
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45403.32421875Mb; avail=209626.57421875Mb
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011210
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024796
2024-09-04 10:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45403.32421875Mb; avail=209626.08203125Mb
2024-09-04 10:00:25 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.816 | nll_loss 8.665 | ppl 405.78 | wps 3823.1 | wpb 2070.5 | bsz 122.7 | num_updates 1344 | best_loss 10.808
2024-09-04 10:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 1344 updates
2024-09-04 10:00:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:00:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 10:00:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 81 @ 1701 updates, score 9.397) (writing took 39.109310076572 seconds)
2024-09-04 10:00:29 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-09-04 10:00:29 | INFO | train | epoch 081 | loss 7.629 | nll_loss 4.656 | ppl 25.22 | wps 853.4 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1701 | lr 2.0412e-05 | gnorm 2.772 | train_wall 55 | gb_free 11.3 | wall 10786
2024-09-04 10:00:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 10:00:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 10:00:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 10:00:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000701
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44069.22265625Mb; avail=210960.13671875Mb
2024-09-04 10:00:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000061
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000574
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44070.69921875Mb; avail=210959.15234375Mb
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44070.69921875Mb; avail=210959.15234375Mb
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000167
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001036
2024-09-04 10:00:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44071.68359375Mb; avail=210958.16796875Mb
2024-09-04 10:00:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 10:00:29 | INFO | fairseq.trainer | begin training epoch 82
2024-09-04 10:00:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:00:33 | INFO | train_inner | epoch 082:      1 / 21 loss=7.59, nll_loss=4.598, ppl=24.22, wps=161.8, ups=0.03, wpb=5117, bsz=132, num_updates=1702, lr=2.0424e-05, gnorm=2.691, train_wall=6, gb_free=14.2, wall=10790
2024-09-04 10:00:38 | INFO | train_inner | epoch 082:      3 / 21 loss=7.646, nll_loss=4.707, ppl=26.13, wps=1573.1, ups=0.35, wpb=4474.5, bsz=88, num_updates=1704, lr=2.0448e-05, gnorm=2.984, train_wall=6, gb_free=12.6, wall=10795
2024-09-04 10:00:53 | INFO | train_inner | epoch 082:      5 / 21 loss=7.911, nll_loss=4.991, ppl=31.8, wps=530.8, ups=0.14, wpb=3849.5, bsz=45, num_updates=1706, lr=2.0472e-05, gnorm=3.614, train_wall=14, gb_free=13.5, wall=10810
2024-09-04 10:00:58 | INFO | train_inner | epoch 082:      7 / 21 loss=7.73, nll_loss=4.816, ppl=28.16, wps=1760.3, ups=0.41, wpb=4254.5, bsz=80, num_updates=1708, lr=2.0496e-05, gnorm=3.094, train_wall=5, gb_free=13.9, wall=10815
2024-09-04 10:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:01:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 84 @ 1344 updates, score 10.816) (writing took 38.014353641308844 seconds)
2024-09-04 10:01:03 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2024-09-04 10:01:03 | INFO | train | epoch 084 | loss 8.523 | nll_loss 5.814 | ppl 56.27 | wps 690.9 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1344 | lr 1.6128e-05 | gnorm 3.1 | train_wall 45 | gb_free 9.5 | wall 10332
2024-09-04 10:01:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:01:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:01:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:01:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000759
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28697.73828125Mb; avail=226332.16015625Mb
2024-09-04 10:01:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000052
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000629
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28697.73828125Mb; avail=226332.16015625Mb
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28697.73828125Mb; avail=226332.16015625Mb
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000173
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001109
2024-09-04 10:01:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28697.73828125Mb; avail=226332.16015625Mb
2024-09-04 10:01:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:01:03 | INFO | fairseq.trainer | begin training epoch 85
2024-09-04 10:01:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:01:03 | INFO | train_inner | epoch 082:      9 / 21 loss=7.132, nll_loss=4.015, ppl=16.17, wps=1680.5, ups=0.36, wpb=4611.5, bsz=172, num_updates=1710, lr=2.052e-05, gnorm=2.578, train_wall=5, gb_free=13, wall=10820
2024-09-04 10:01:08 | INFO | train_inner | epoch 082:     11 / 21 loss=7.785, nll_loss=4.856, ppl=28.96, wps=1944.7, ups=0.45, wpb=4322.5, bsz=80, num_updates=1712, lr=2.0544e-05, gnorm=3.098, train_wall=4, gb_free=12.8, wall=10825
2024-09-04 10:01:09 | INFO | train_inner | epoch 085:      2 / 16 loss=8.512, nll_loss=5.852, ppl=57.76, wps=152.8, ups=0.03, wpb=4477.5, bsz=148, num_updates=1346, lr=1.6152e-05, gnorm=3.313, train_wall=6, gb_free=10, wall=10339
2024-09-04 10:01:14 | INFO | train_inner | epoch 082:     13 / 21 loss=7.561, nll_loss=4.558, ppl=23.56, wps=1539.3, ups=0.33, wpb=4615, bsz=116, num_updates=1714, lr=2.0568e-05, gnorm=2.752, train_wall=6, gb_free=11.7, wall=10831
2024-09-04 10:01:15 | INFO | train_inner | epoch 085:      4 / 16 loss=8.661, nll_loss=5.972, ppl=62.76, wps=1583.4, ups=0.34, wpb=4615.5, bsz=108, num_updates=1348, lr=1.6176e-05, gnorm=3.133, train_wall=6, gb_free=10.4, wall=10344
2024-09-04 10:01:18 | INFO | train_inner | epoch 082:     15 / 21 loss=7.711, nll_loss=4.759, ppl=27.07, wps=2030.5, ups=0.41, wpb=4897.5, bsz=88, num_updates=1716, lr=2.0592e-05, gnorm=3.27, train_wall=5, gb_free=15, wall=10835
2024-09-04 10:01:21 | INFO | train_inner | epoch 085:      6 / 16 loss=8.307, nll_loss=5.552, ppl=46.91, wps=1481.3, ups=0.33, wpb=4516.5, bsz=176, num_updates=1350, lr=1.62e-05, gnorm=2.578, train_wall=6, gb_free=9, wall=10350
2024-09-04 10:01:23 | INFO | train_inner | epoch 082:     17 / 21 loss=7.635, nll_loss=4.645, ppl=25.02, wps=1927.7, ups=0.41, wpb=4671, bsz=100, num_updates=1718, lr=2.0616e-05, gnorm=2.582, train_wall=5, gb_free=13.2, wall=10840
2024-09-04 10:01:26 | INFO | train_inner | epoch 085:      8 / 16 loss=8.778, nll_loss=6.148, ppl=70.91, wps=1479.9, ups=0.4, wpb=3740, bsz=64, num_updates=1352, lr=1.6224e-05, gnorm=3.557, train_wall=5, gb_free=11.4, wall=10356
2024-09-04 10:01:29 | INFO | train_inner | epoch 082:     19 / 21 loss=7.59, nll_loss=4.627, ppl=24.7, wps=1618, ups=0.36, wpb=4536, bsz=104, num_updates=1720, lr=2.064e-05, gnorm=2.809, train_wall=6, gb_free=12.7, wall=10846
2024-09-04 10:01:32 | INFO | train_inner | epoch 085:     10 / 16 loss=8.138, nll_loss=5.321, ppl=39.99, wps=1458.6, ups=0.34, wpb=4240.5, bsz=168, num_updates=1354, lr=1.6248e-05, gnorm=3.309, train_wall=6, gb_free=12.1, wall=10361
2024-09-04 10:01:34 | INFO | train_inner | epoch 082:     21 / 21 loss=7.571, nll_loss=4.575, ppl=23.84, wps=2032.4, ups=0.4, wpb=5108.5, bsz=104, num_updates=1722, lr=2.0664e-05, gnorm=2.46, train_wall=5, gb_free=12.4, wall=10851
2024-09-04 10:01:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28737.5234375Mb; avail=226292.3203125Mb
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000631
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28737.5234375Mb; avail=226292.3203125Mb
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012712
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28737.5234375Mb; avail=226292.3203125Mb
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011257
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025024
2024-09-04 10:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28737.5234375Mb; avail=226292.3203125Mb
2024-09-04 10:01:37 | INFO | train_inner | epoch 085:     12 / 16 loss=8.496, nll_loss=5.763, ppl=54.3, wps=1440.7, ups=0.42, wpb=3463.5, bsz=93, num_updates=1356, lr=1.6272e-05, gnorm=3.445, train_wall=5, gb_free=11.2, wall=10366
2024-09-04 10:01:43 | INFO | train_inner | epoch 085:     14 / 16 loss=8.468, nll_loss=5.753, ppl=53.94, wps=1490.2, ups=0.34, wpb=4415, bsz=148, num_updates=1358, lr=1.6296e-05, gnorm=3.394, train_wall=6, gb_free=9.6, wall=10372
2024-09-04 10:01:48 | INFO | train_inner | epoch 085:     16 / 16 loss=8.581, nll_loss=5.887, ppl=59.2, wps=1529.7, ups=0.35, wpb=4423, bsz=112, num_updates=1360, lr=1.632e-05, gnorm=3.144, train_wall=6, gb_free=8.8, wall=10378
2024-09-04 10:01:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28785.46484375Mb; avail=226244.34765625Mb
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000741
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28785.95703125Mb; avail=226243.85546875Mb
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012535
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28785.95703125Mb; avail=226243.85546875Mb
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011302
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024936
2024-09-04 10:01:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28785.95703125Mb; avail=226243.85546875Mb
2024-09-04 10:01:50 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.435 | nll_loss 6.87 | ppl 116.96 | wps 4973.1 | wpb 2350.9 | bsz 94.7 | num_updates 1722 | best_loss 9.334
2024-09-04 10:01:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 1722 updates
2024-09-04 10:01:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 10:02:03 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.813 | nll_loss 8.638 | ppl 398.36 | wps 3821.9 | wpb 2070.5 | bsz 122.7 | num_updates 1360 | best_loss 10.808
2024-09-04 10:02:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 1360 updates
2024-09-04 10:02:03 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:02:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 10:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 82 @ 1722 updates, score 9.435) (writing took 39.77251334954053 seconds)
2024-09-04 10:02:30 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2024-09-04 10:02:30 | INFO | train | epoch 082 | loss 7.634 | nll_loss 4.663 | ppl 25.33 | wps 791.3 | ups 0.17 | wpb 4556.3 | bsz 96.9 | num_updates 1722 | lr 2.0664e-05 | gnorm 2.914 | train_wall 64 | gb_free 12.4 | wall 10907
2024-09-04 10:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 10:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 10:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 10:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000655
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=53373.28125Mb; avail=201656.65625Mb
2024-09-04 10:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000585
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53373.28125Mb; avail=201656.65625Mb
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53373.28125Mb; avail=201656.65625Mb
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000164
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001035
2024-09-04 10:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53373.28125Mb; avail=201656.65625Mb
2024-09-04 10:02:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 10:02:30 | INFO | fairseq.trainer | begin training epoch 83
2024-09-04 10:02:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:02:35 | INFO | train_inner | epoch 083:      2 / 21 loss=7.636, nll_loss=4.674, ppl=25.53, wps=153.5, ups=0.03, wpb=4685.5, bsz=108, num_updates=1724, lr=2.0688e-05, gnorm=2.604, train_wall=5, gb_free=12.3, wall=10912
2024-09-04 10:02:41 | INFO | train_inner | epoch 083:      4 / 21 loss=7.198, nll_loss=4.135, ppl=17.57, wps=1573.7, ups=0.35, wpb=4502.5, bsz=128, num_updates=1726, lr=2.0712e-05, gnorm=2.684, train_wall=6, gb_free=12.9, wall=10918
2024-09-04 10:02:46 | INFO | train_inner | epoch 083:      6 / 21 loss=7.602, nll_loss=4.642, ppl=24.96, wps=1927.8, ups=0.41, wpb=4673.5, bsz=100, num_updates=1728, lr=2.0736e-05, gnorm=3.166, train_wall=5, gb_free=12.4, wall=10923
2024-09-04 10:02:50 | INFO | train_inner | epoch 083:      8 / 21 loss=7.767, nll_loss=4.802, ppl=27.9, wps=1881.8, ups=0.44, wpb=4299.5, bsz=60, num_updates=1730, lr=2.076e-05, gnorm=2.916, train_wall=5, gb_free=12.8, wall=10927
2024-09-04 10:02:56 | INFO | train_inner | epoch 083:     10 / 21 loss=7.401, nll_loss=4.362, ppl=20.56, wps=1572.6, ups=0.35, wpb=4543, bsz=128, num_updates=1732, lr=2.0784e-05, gnorm=2.787, train_wall=6, gb_free=12.1, wall=10933
2024-09-04 10:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:03:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 85 @ 1360 updates, score 10.813) (writing took 57.59923141077161 seconds)
2024-09-04 10:03:00 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2024-09-04 10:03:00 | INFO | train | epoch 085 | loss 8.489 | nll_loss 5.777 | ppl 54.84 | wps 576.8 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 1360 | lr 1.632e-05 | gnorm 3.234 | train_wall 45 | gb_free 8.8 | wall 10450
2024-09-04 10:03:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:03:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:03:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:03:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000591
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38789.03515625Mb; avail=216240.85546875Mb
2024-09-04 10:03:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000046
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000540
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38789.03515625Mb; avail=216240.85546875Mb
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38789.03515625Mb; avail=216240.85546875Mb
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000150
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.000973
2024-09-04 10:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38789.03515625Mb; avail=216240.85546875Mb
2024-09-04 10:03:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:03:01 | INFO | fairseq.trainer | begin training epoch 86
2024-09-04 10:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:03:01 | INFO | train_inner | epoch 083:     12 / 21 loss=7.758, nll_loss=4.819, ppl=28.23, wps=1624.5, ups=0.37, wpb=4395, bsz=68, num_updates=1734, lr=2.0808e-05, gnorm=2.958, train_wall=5, gb_free=12.6, wall=10938
2024-09-04 10:03:06 | INFO | train_inner | epoch 086:      2 / 16 loss=8.539, nll_loss=5.841, ppl=57.32, wps=103.3, ups=0.03, wpb=3994.5, bsz=112, num_updates=1362, lr=1.6344e-05, gnorm=3.482, train_wall=5, gb_free=12.1, wall=10455
2024-09-04 10:03:06 | INFO | train_inner | epoch 083:     14 / 21 loss=7.442, nll_loss=4.426, ppl=21.49, wps=1915.5, ups=0.41, wpb=4717.5, bsz=124, num_updates=1736, lr=2.0832e-05, gnorm=2.585, train_wall=5, gb_free=11.9, wall=10943
2024-09-04 10:03:11 | INFO | train_inner | epoch 083:     16 / 21 loss=7.454, nll_loss=4.459, ppl=21.99, wps=1936.6, ups=0.41, wpb=4724, bsz=116, num_updates=1738, lr=2.0856e-05, gnorm=2.686, train_wall=5, gb_free=12.1, wall=10948
2024-09-04 10:03:11 | INFO | train_inner | epoch 086:      4 / 16 loss=8.56, nll_loss=5.877, ppl=58.76, wps=1481.7, ups=0.35, wpb=4174, bsz=92, num_updates=1364, lr=1.6368e-05, gnorm=3.334, train_wall=6, gb_free=10.8, wall=10461
2024-09-04 10:03:17 | INFO | train_inner | epoch 083:     18 / 21 loss=7.804, nll_loss=4.867, ppl=29.17, wps=1715, ups=0.36, wpb=4714.5, bsz=68, num_updates=1740, lr=2.088e-05, gnorm=2.717, train_wall=5, gb_free=11, wall=10954
2024-09-04 10:03:17 | INFO | train_inner | epoch 086:      6 / 16 loss=8.387, nll_loss=5.66, ppl=50.56, wps=1567.1, ups=0.34, wpb=4634.5, bsz=160, num_updates=1366, lr=1.6392e-05, gnorm=2.88, train_wall=6, gb_free=9.1, wall=10467
2024-09-04 10:03:22 | INFO | train_inner | epoch 083:     20 / 21 loss=7.651, nll_loss=4.693, ppl=25.87, wps=1805.1, ups=0.36, wpb=4999, bsz=96, num_updates=1742, lr=2.0904e-05, gnorm=2.525, train_wall=6, gb_free=12.2, wall=10959
2024-09-04 10:03:23 | INFO | train_inner | epoch 086:      8 / 16 loss=8.503, nll_loss=5.764, ppl=54.33, wps=1513.5, ups=0.35, wpb=4297.5, bsz=104, num_updates=1368, lr=1.6416e-05, gnorm=2.796, train_wall=6, gb_free=9.4, wall=10472
2024-09-04 10:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38835.20703125Mb; avail=216194.6484375Mb
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000752
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38835.69921875Mb; avail=216194.15625Mb
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012744
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38835.69921875Mb; avail=216194.15625Mb
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011301
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025161
2024-09-04 10:03:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38835.69921875Mb; avail=216194.15625Mb
2024-09-04 10:03:29 | INFO | train_inner | epoch 086:     10 / 16 loss=8.582, nll_loss=5.898, ppl=59.65, wps=1515.6, ups=0.33, wpb=4639.5, bsz=128, num_updates=1370, lr=1.644e-05, gnorm=3.085, train_wall=6, gb_free=11.5, wall=10479
2024-09-04 10:03:34 | INFO | train_inner | epoch 086:     12 / 16 loss=8.053, nll_loss=5.19, ppl=36.51, wps=1396.8, ups=0.4, wpb=3513, bsz=149, num_updates=1372, lr=1.6464e-05, gnorm=2.771, train_wall=5, gb_free=9.5, wall=10484
2024-09-04 10:03:40 | INFO | train_inner | epoch 086:     14 / 16 loss=8.45, nll_loss=5.746, ppl=53.68, wps=1477.7, ups=0.34, wpb=4363.5, bsz=140, num_updates=1374, lr=1.6488e-05, gnorm=3.353, train_wall=6, gb_free=9.8, wall=10489
2024-09-04 10:03:42 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.374 | nll_loss 6.771 | ppl 109.21 | wps 4706.5 | wpb 2350.9 | bsz 94.7 | num_updates 1743 | best_loss 9.334
2024-09-04 10:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 1743 updates
2024-09-04 10:03:42 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 10:03:46 | INFO | train_inner | epoch 086:     16 / 16 loss=8.54, nll_loss=5.858, ppl=58, wps=1522.3, ups=0.36, wpb=4275, bsz=132, num_updates=1376, lr=1.6512e-05, gnorm=3.228, train_wall=6, gb_free=11.1, wall=10495
2024-09-04 10:03:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44344.453125Mb; avail=210685.35546875Mb
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44345.4375Mb; avail=210684.37109375Mb
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012779
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44366.6015625Mb; avail=210663.20703125Mb
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011241
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025134
2024-09-04 10:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44385.3046875Mb; avail=210644.50390625Mb
2024-09-04 10:04:00 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.759 | nll_loss 8.549 | ppl 374.59 | wps 3801.4 | wpb 2070.5 | bsz 122.7 | num_updates 1376 | best_loss 10.759
2024-09-04 10:04:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 1376 updates
2024-09-04 10:04:00 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 10:04:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 10:04:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 83 @ 1743 updates, score 9.374) (writing took 44.218567934818566 seconds)
2024-09-04 10:04:26 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2024-09-04 10:04:26 | INFO | train | epoch 083 | loss 7.572 | nll_loss 4.589 | ppl 24.07 | wps 823.3 | ups 0.18 | wpb 4556.3 | bsz 96.9 | num_updates 1743 | lr 2.0916e-05 | gnorm 2.753 | train_wall 54 | gb_free 16.4 | wall 11023
2024-09-04 10:04:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-mr': 2034}; raw total size: 2034
2024-09-04 10:04:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-mr': 2034}; resampled total size: 2034
2024-09-04 10:04:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-mr': 1.0}
2024-09-04 10:04:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000570
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=57943.72265625Mb; avail=197078.16796875Mb
2024-09-04 10:04:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000065
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000570
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57943.72265625Mb; avail=197078.16796875Mb
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57943.72265625Mb; avail=197078.16796875Mb
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000156
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001015
2024-09-04 10:04:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=57943.72265625Mb; avail=197078.16796875Mb
2024-09-04 10:04:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 21
2024-09-04 10:04:27 | INFO | fairseq.trainer | begin training epoch 84
2024-09-04 10:04:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:04:30 | INFO | train_inner | epoch 084:      1 / 21 loss=7.424, nll_loss=4.383, ppl=20.87, wps=116, ups=0.03, wpb=3916, bsz=101, num_updates=1744, lr=2.0928e-05, gnorm=2.493, train_wall=6, gb_free=12.9, wall=11027
2024-09-04 10:04:34 | INFO | train_inner | epoch 084:      3 / 21 loss=7.396, nll_loss=4.385, ppl=20.9, wps=1921, ups=0.43, wpb=4469, bsz=124, num_updates=1746, lr=2.0952e-05, gnorm=2.341, train_wall=5, gb_free=13.7, wall=11031
2024-09-04 10:04:39 | INFO | train_inner | epoch 084:      5 / 21 loss=7.57, nll_loss=4.577, ppl=23.87, wps=1911.3, ups=0.4, wpb=4728.5, bsz=92, num_updates=1748, lr=2.0976e-05, gnorm=2.403, train_wall=5, gb_free=12.3, wall=11036
2024-09-04 10:04:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-09-04 10:04:44 | INFO | train_inner | epoch 084:      7 / 21 loss=7.307, nll_loss=4.227, ppl=18.73, wps=1629.3, ups=0.41, wpb=3975.5, bsz=117, num_updates=1750, lr=2.1e-05, gnorm=3.184, train_wall=5, gb_free=18.5, wall=11041
2024-09-04 10:04:49 | INFO | train_inner | epoch 084:      9 / 21 loss=7.752, nll_loss=4.816, ppl=28.16, wps=1832.4, ups=0.38, wpb=4819, bsz=64, num_updates=1752, lr=2.1024e-05, gnorm=2.796, train_wall=5, gb_free=14, wall=11046
2024-09-04 10:04:55 | INFO | train_inner | epoch 084:     11 / 21 loss=7.707, nll_loss=4.797, ppl=27.8, wps=1614, ups=0.36, wpb=4463.5, bsz=76, num_updates=1754, lr=2.1048e-05, gnorm=2.936, train_wall=6, gb_free=12.4, wall=11052
2024-09-04 10:05:00 | INFO | train_inner | epoch 084:     13 / 21 loss=7.196, nll_loss=4.069, ppl=16.78, wps=1716.6, ups=0.4, wpb=4318, bsz=148, num_updates=1756, lr=2.1072e-05, gnorm=2.385, train_wall=5, gb_free=12, wall=11057
2024-09-04 10:05:06 | INFO | train_inner | epoch 084:     15 / 21 loss=7.61, nll_loss=4.615, ppl=24.5, wps=1646.6, ups=0.35, wpb=4763, bsz=76, num_updates=1758, lr=2.1096e-05, gnorm=2.508, train_wall=6, gb_free=13.4, wall=11063
2024-09-04 10:05:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 86 @ 1376 updates, score 10.759) (writing took 66.73332978691906 seconds)
2024-09-04 10:05:07 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2024-09-04 10:05:07 | INFO | train | epoch 086 | loss 8.46 | nll_loss 5.741 | ppl 53.49 | wps 535.9 | ups 0.13 | wpb 4236.4 | bsz 127.1 | num_updates 1376 | lr 1.6512e-05 | gnorm 3.116 | train_wall 45 | gb_free 11.1 | wall 10576
2024-09-04 10:05:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:05:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:05:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:05:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000747
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46063.64453125Mb; avail=208958.26171875Mb
2024-09-04 10:05:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000595
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46063.64453125Mb; avail=208958.26171875Mb
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46063.64453125Mb; avail=208958.26171875Mb
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000181
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001072
2024-09-04 10:05:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46063.64453125Mb; avail=208958.26171875Mb
2024-09-04 10:05:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:05:07 | INFO | fairseq.trainer | begin training epoch 87
2024-09-04 10:05:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:05:10 | INFO | train_inner | epoch 084:     17 / 21 loss=7.693, nll_loss=4.766, ppl=27.2, wps=1955, ups=0.44, wpb=4476.5, bsz=76, num_updates=1760, lr=2.112e-05, gnorm=2.997, train_wall=5, gb_free=16.4, wall=11067
2024-09-04 10:05:13 | INFO | train_inner | epoch 087:      2 / 16 loss=8.336, nll_loss=5.537, ppl=46.44, wps=102.4, ups=0.02, wpb=4471.5, bsz=164, num_updates=1378, lr=1.6536e-05, gnorm=3.167, train_wall=6, gb_free=8.7, wall=10582
2024-09-04 10:05:16 | INFO | train_inner | epoch 084:     19 / 21 loss=7.576, nll_loss=4.604, ppl=24.32, wps=1707.8, ups=0.34, wpb=5014.5, bsz=104, num_updates=1762, lr=2.1144e-05, gnorm=2.308, train_wall=6, gb_free=12.4, wall=11073
2024-09-04 10:05:18 | INFO | train_inner | epoch 087:      4 / 16 loss=8.609, nll_loss=5.9, ppl=59.73, wps=1453.2, ups=0.41, wpb=3537.5, bsz=60, num_updates=1380, lr=1.656e-05, gnorm=4.244, train_wall=5, gb_free=9.7, wall=10587
2024-09-04 10:05:22 | INFO | train_inner | epoch 084:     21 / 21 loss=7.798, nll_loss=4.844, ppl=28.73, wps=1634, ups=0.36, wpb=4484.5, bsz=60, num_updates=1764, lr=2.1168e-05, gnorm=2.411, train_wall=5, gb_free=15.3, wall=11079
2024-09-04 10:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46102.94921875Mb; avail=208918.42578125Mb
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000746
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46103.44140625Mb; avail=208918.42578125Mb
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012749
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46103.44140625Mb; avail=208918.42578125Mb
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011283
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025141
2024-09-04 10:05:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46103.44140625Mb; avail=208918.42578125Mb
2024-09-04 10:05:23 | INFO | train_inner | epoch 087:      6 / 16 loss=8.124, nll_loss=5.315, ppl=39.8, wps=1492.3, ups=0.37, wpb=4075.5, bsz=180, num_updates=1382, lr=1.6584e-05, gnorm=2.936, train_wall=5, gb_free=10.4, wall=10593
2024-09-04 10:05:29 | INFO | train_inner | epoch 087:      8 / 16 loss=8.647, nll_loss=6.02, ppl=64.88, wps=1552.2, ups=0.35, wpb=4492.5, bsz=108, num_updates=1384, lr=1.6608e-05, gnorm=4.496, train_wall=6, gb_free=11, wall=10599
2024-09-04 10:05:35 | INFO | train_inner | epoch 087:     10 / 16 loss=8.569, nll_loss=5.843, ppl=57.42, wps=1584.7, ups=0.34, wpb=4595, bsz=116, num_updates=1386, lr=1.6632e-05, gnorm=3.526, train_wall=6, gb_free=11.4, wall=10604
2024-09-04 10:05:39 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.356 | nll_loss 6.748 | ppl 107.49 | wps 4674.6 | wpb 2350.9 | bsz 94.7 | num_updates 1764 | best_loss 9.334
2024-09-04 10:05:39 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2024-09-04 10:05:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 1764 updates
2024-09-04 10:05:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 10:05:41 | INFO | train_inner | epoch 087:     12 / 16 loss=8.58, nll_loss=5.875, ppl=58.68, wps=1504.4, ups=0.33, wpb=4548, bsz=108, num_updates=1388, lr=1.6656e-05, gnorm=3.841, train_wall=6, gb_free=8.7, wall=10610
2024-09-04 10:05:47 | INFO | train_inner | epoch 087:     14 / 16 loss=8.439, nll_loss=5.706, ppl=52.22, wps=1472.5, ups=0.33, wpb=4473, bsz=140, num_updates=1390, lr=1.668e-05, gnorm=3.481, train_wall=6, gb_free=10.4, wall=10616
2024-09-04 10:05:52 | INFO | train_inner | epoch 087:     16 / 16 loss=8.179, nll_loss=5.383, ppl=41.73, wps=1403.6, ups=0.38, wpb=3698.5, bsz=141, num_updates=1392, lr=1.6704e-05, gnorm=3.838, train_wall=5, gb_free=8.9, wall=10622
2024-09-04 10:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60838.109375Mb; avail=194183.265625Mb
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000758
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60840.078125Mb; avail=194181.7890625Mb
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012644
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60867.640625Mb; avail=194154.2265625Mb
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011167
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024948
2024-09-04 10:05:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60892.7421875Mb; avail=194129.125Mb
2024-09-04 10:06:07 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.832 | nll_loss 8.665 | ppl 405.94 | wps 3816.6 | wpb 2070.5 | bsz 122.7 | num_updates 1392 | best_loss 10.759
2024-09-04 10:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 1392 updates
2024-09-04 10:06:07 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:06:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt
2024-09-04 10:06:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B/checkpoint_last.pt (epoch 84 @ 1764 updates, score 9.356) (writing took 38.543733328580856 seconds)
2024-09-04 10:06:18 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2024-09-04 10:06:18 | INFO | train | epoch 084 | loss 7.553 | nll_loss 4.561 | ppl 23.6 | wps 859.5 | ups 0.19 | wpb 4556.3 | bsz 96.9 | num_updates 1764 | lr 2.1168e-05 | gnorm 2.617 | train_wall 55 | gb_free 15.3 | wall 11135
2024-09-04 10:06:18 | INFO | fairseq_cli.train | done training in 11134.5 seconds
2024-09-04 10:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 87 @ 1392 updates, score 10.832) (writing took 43.83540593273938 seconds)
2024-09-04 10:06:51 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2024-09-04 10:06:51 | INFO | train | epoch 087 | loss 8.441 | nll_loss 5.705 | ppl 52.15 | wps 653.7 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1392 | lr 1.6704e-05 | gnorm 3.691 | train_wall 45 | gb_free 8.9 | wall 10680
2024-09-04 10:06:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:06:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:06:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:06:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000697
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31157.5078125Mb; avail=223888.42578125Mb
2024-09-04 10:06:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000611
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31157.5078125Mb; avail=223888.42578125Mb
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31157.5078125Mb; avail=223888.42578125Mb
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000163
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001067
2024-09-04 10:06:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31157.5078125Mb; avail=223888.42578125Mb
2024-09-04 10:06:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:06:51 | INFO | fairseq.trainer | begin training epoch 88
2024-09-04 10:06:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:06:56 | INFO | train_inner | epoch 088:      2 / 16 loss=8.546, nll_loss=5.852, ppl=57.77, wps=137, ups=0.03, wpb=4389, bsz=100, num_updates=1394, lr=1.6728e-05, gnorm=3.652, train_wall=6, gb_free=8.9, wall=10686
2024-09-04 10:07:17 | INFO | train_inner | epoch 088:      4 / 16 loss=8.398, nll_loss=5.649, ppl=50.19, wps=424.8, ups=0.1, wpb=4367, bsz=152, num_updates=1396, lr=1.6752e-05, gnorm=3.157, train_wall=21, gb_free=10.6, wall=10706
2024-09-04 10:07:23 | INFO | train_inner | epoch 088:      6 / 16 loss=8.407, nll_loss=5.686, ppl=51.49, wps=1460.7, ups=0.36, wpb=4109, bsz=116, num_updates=1398, lr=1.6776e-05, gnorm=3.288, train_wall=6, gb_free=14, wall=10712
2024-09-04 10:07:28 | INFO | train_inner | epoch 088:      8 / 16 loss=8.114, nll_loss=5.274, ppl=38.68, wps=1459.5, ups=0.34, wpb=4315, bsz=172, num_updates=1400, lr=1.68e-05, gnorm=2.911, train_wall=6, gb_free=10.2, wall=10718
2024-09-04 10:07:33 | INFO | train_inner | epoch 088:     10 / 16 loss=8.407, nll_loss=5.679, ppl=51.24, wps=1471.6, ups=0.42, wpb=3506, bsz=73, num_updates=1402, lr=1.6824e-05, gnorm=3.737, train_wall=5, gb_free=10.4, wall=10723
2024-09-04 10:07:39 | INFO | train_inner | epoch 088:     12 / 16 loss=8.49, nll_loss=5.816, ppl=56.35, wps=1539.9, ups=0.32, wpb=4738.5, bsz=136, num_updates=1404, lr=1.6848e-05, gnorm=3.471, train_wall=6, gb_free=10.3, wall=10729
2024-09-04 10:07:45 | INFO | train_inner | epoch 088:     14 / 16 loss=8.349, nll_loss=5.557, ppl=47.07, wps=1535.9, ups=0.33, wpb=4649.5, bsz=160, num_updates=1406, lr=1.6872e-05, gnorm=2.856, train_wall=6, gb_free=10, wall=10735
2024-09-04 10:07:51 | INFO | train_inner | epoch 088:     16 / 16 loss=8.367, nll_loss=5.601, ppl=48.54, wps=1400.2, ups=0.37, wpb=3817.5, bsz=108, num_updates=1408, lr=1.6896e-05, gnorm=3.061, train_wall=5, gb_free=10.4, wall=10740
2024-09-04 10:07:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13187.73828125Mb; avail=241858.28515625Mb
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000628
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13187.73828125Mb; avail=241858.28515625Mb
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012594
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13187.73828125Mb; avail=241858.28515625Mb
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011326
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024927
2024-09-04 10:07:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13187.73828125Mb; avail=241858.28515625Mb
2024-09-04 10:08:05 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.765 | nll_loss 8.564 | ppl 378.45 | wps 3831 | wpb 2070.5 | bsz 122.7 | num_updates 1408 | best_loss 10.759
2024-09-04 10:08:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 1408 updates
2024-09-04 10:08:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 88 @ 1408 updates, score 10.765) (writing took 39.259666964411736 seconds)
2024-09-04 10:08:45 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2024-09-04 10:08:45 | INFO | train | epoch 088 | loss 8.386 | nll_loss 5.641 | ppl 49.88 | wps 594.8 | ups 0.14 | wpb 4236.4 | bsz 127.1 | num_updates 1408 | lr 1.6896e-05 | gnorm 3.267 | train_wall 60 | gb_free 10.4 | wall 10794
2024-09-04 10:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000664
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20094.96875Mb; avail=234951.05078125Mb
2024-09-04 10:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000574
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20094.96875Mb; avail=234951.05078125Mb
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20094.96875Mb; avail=234951.05078125Mb
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000155
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001013
2024-09-04 10:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20094.96875Mb; avail=234951.05078125Mb
2024-09-04 10:08:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:08:45 | INFO | fairseq.trainer | begin training epoch 89
2024-09-04 10:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:08:50 | INFO | train_inner | epoch 089:      2 / 16 loss=8.449, nll_loss=5.717, ppl=52.59, wps=143.7, ups=0.03, wpb=4275, bsz=80, num_updates=1410, lr=1.692e-05, gnorm=3.049, train_wall=6, gb_free=7.6, wall=10800
2024-09-04 10:08:55 | INFO | train_inner | epoch 089:      4 / 16 loss=8.27, nll_loss=5.517, ppl=45.79, wps=1485, ups=0.4, wpb=3753.5, bsz=97, num_updates=1412, lr=1.6944e-05, gnorm=3.317, train_wall=5, gb_free=9.9, wall=10805
2024-09-04 10:09:01 | INFO | train_inner | epoch 089:      6 / 16 loss=8.537, nll_loss=5.837, ppl=57.16, wps=1568.2, ups=0.34, wpb=4562, bsz=104, num_updates=1414, lr=1.6968e-05, gnorm=2.914, train_wall=6, gb_free=11.2, wall=10811
2024-09-04 10:09:07 | INFO | train_inner | epoch 089:      8 / 16 loss=8.3, nll_loss=5.518, ppl=45.82, wps=1506.1, ups=0.35, wpb=4312, bsz=160, num_updates=1416, lr=1.6992e-05, gnorm=2.873, train_wall=6, gb_free=11.7, wall=10816
2024-09-04 10:09:13 | INFO | train_inner | epoch 089:     10 / 16 loss=8.377, nll_loss=5.637, ppl=49.76, wps=1516.6, ups=0.33, wpb=4531, bsz=140, num_updates=1418, lr=1.7016e-05, gnorm=2.763, train_wall=6, gb_free=10.1, wall=10822
2024-09-04 10:09:19 | INFO | train_inner | epoch 089:     12 / 16 loss=8.234, nll_loss=5.447, ppl=43.63, wps=1369.8, ups=0.32, wpb=4230, bsz=152, num_updates=1420, lr=1.704e-05, gnorm=3.633, train_wall=6, gb_free=10.1, wall=10829
2024-09-04 10:09:25 | INFO | train_inner | epoch 089:     14 / 16 loss=8.548, nll_loss=5.814, ppl=56.26, wps=1534, ups=0.37, wpb=4167, bsz=96, num_updates=1422, lr=1.7064e-05, gnorm=3.431, train_wall=5, gb_free=9.4, wall=10834
2024-09-04 10:09:30 | INFO | train_inner | epoch 089:     16 / 16 loss=7.944, nll_loss=5.097, ppl=34.23, wps=1410.8, ups=0.35, wpb=4061, bsz=188, num_updates=1424, lr=1.7088e-05, gnorm=3.267, train_wall=6, gb_free=9.8, wall=10840
2024-09-04 10:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20137.328125Mb; avail=234908.65234375Mb
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000742
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20137.328125Mb; avail=234908.65234375Mb
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012779
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20137.328125Mb; avail=234908.65234375Mb
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011219
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025114
2024-09-04 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20137.328125Mb; avail=234908.65234375Mb
2024-09-04 10:09:45 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.76 | nll_loss 8.564 | ppl 378.45 | wps 3826.9 | wpb 2070.5 | bsz 122.7 | num_updates 1424 | best_loss 10.759
2024-09-04 10:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 1424 updates
2024-09-04 10:09:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:10:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:10:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 89 @ 1424 updates, score 10.76) (writing took 40.29403327591717 seconds)
2024-09-04 10:10:25 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2024-09-04 10:10:25 | INFO | train | epoch 089 | loss 8.337 | nll_loss 5.579 | ppl 47.8 | wps 674.2 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1424 | lr 1.7088e-05 | gnorm 3.156 | train_wall 46 | gb_free 9.8 | wall 10895
2024-09-04 10:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000711
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29140.3046875Mb; avail=225905.6484375Mb
2024-09-04 10:10:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000596
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29140.3046875Mb; avail=225905.6484375Mb
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29140.3046875Mb; avail=225905.6484375Mb
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000166
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001091
2024-09-04 10:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29140.3046875Mb; avail=225905.6484375Mb
2024-09-04 10:10:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:10:25 | INFO | fairseq.trainer | begin training epoch 90
2024-09-04 10:10:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:10:31 | INFO | train_inner | epoch 090:      2 / 16 loss=8.475, nll_loss=5.757, ppl=54.08, wps=142.5, ups=0.03, wpb=4323, bsz=108, num_updates=1426, lr=1.7112e-05, gnorm=3.614, train_wall=6, gb_free=10.4, wall=10901
2024-09-04 10:10:37 | INFO | train_inner | epoch 090:      4 / 16 loss=8.316, nll_loss=5.529, ppl=46.18, wps=1580.6, ups=0.36, wpb=4363.5, bsz=128, num_updates=1428, lr=1.7136e-05, gnorm=3.154, train_wall=6, gb_free=10.5, wall=10906
2024-09-04 10:10:42 | INFO | train_inner | epoch 090:      6 / 16 loss=8.503, nll_loss=5.766, ppl=54.43, wps=1537.1, ups=0.34, wpb=4575, bsz=104, num_updates=1430, lr=1.716e-05, gnorm=3.071, train_wall=6, gb_free=9.5, wall=10912
2024-09-04 10:10:59 | INFO | train_inner | epoch 090:      8 / 16 loss=8.471, nll_loss=5.772, ppl=54.65, wps=578.3, ups=0.12, wpb=4672, bsz=112, num_updates=1432, lr=1.7184e-05, gnorm=3.155, train_wall=16, gb_free=10.6, wall=10928
2024-09-04 10:11:04 | INFO | train_inner | epoch 090:     10 / 16 loss=8.187, nll_loss=5.427, ppl=43.01, wps=1474, ups=0.36, wpb=4084, bsz=136, num_updates=1434, lr=1.7208e-05, gnorm=3.234, train_wall=6, gb_free=14.5, wall=10934
2024-09-04 10:11:10 | INFO | train_inner | epoch 090:     12 / 16 loss=8.383, nll_loss=5.623, ppl=49.28, wps=1527.3, ups=0.34, wpb=4458, bsz=140, num_updates=1436, lr=1.7232e-05, gnorm=3.074, train_wall=6, gb_free=10.5, wall=10940
2024-09-04 10:11:15 | INFO | train_inner | epoch 090:     14 / 16 loss=8.278, nll_loss=5.478, ppl=44.56, wps=1338.1, ups=0.42, wpb=3211, bsz=93, num_updates=1438, lr=1.7256e-05, gnorm=3.917, train_wall=5, gb_free=9.1, wall=10944
2024-09-04 10:11:21 | INFO | train_inner | epoch 090:     16 / 16 loss=7.845, nll_loss=4.951, ppl=30.93, wps=1391.8, ups=0.33, wpb=4205, bsz=196, num_updates=1440, lr=1.728e-05, gnorm=3.1, train_wall=6, gb_free=10.1, wall=10950
2024-09-04 10:11:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17044.25390625Mb; avail=238001.8125Mb
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000615
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17044.25390625Mb; avail=238001.8125Mb
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012877
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17044.25390625Mb; avail=238001.8125Mb
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011180
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025021
2024-09-04 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17044.25390625Mb; avail=238001.8125Mb
2024-09-04 10:11:35 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.78 | nll_loss 8.593 | ppl 386.02 | wps 3825.5 | wpb 2070.5 | bsz 122.7 | num_updates 1440 | best_loss 10.759
2024-09-04 10:11:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 1440 updates
2024-09-04 10:11:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:12:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:12:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 90 @ 1440 updates, score 10.78) (writing took 39.058315764181316 seconds)
2024-09-04 10:12:14 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2024-09-04 10:12:14 | INFO | train | epoch 090 | loss 8.314 | nll_loss 5.547 | ppl 46.76 | wps 620 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1440 | lr 1.728e-05 | gnorm 3.29 | train_wall 55 | gb_free 10.1 | wall 11004
2024-09-04 10:12:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:12:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:12:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:12:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000655
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23956.921875Mb; avail=231089.14453125Mb
2024-09-04 10:12:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000582
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23956.921875Mb; avail=231089.14453125Mb
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000027
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23956.921875Mb; avail=231089.14453125Mb
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000155
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001021
2024-09-04 10:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23956.921875Mb; avail=231089.14453125Mb
2024-09-04 10:12:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:12:15 | INFO | fairseq.trainer | begin training epoch 91
2024-09-04 10:12:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:12:20 | INFO | train_inner | epoch 091:      2 / 16 loss=8.19, nll_loss=5.392, ppl=42, wps=135.4, ups=0.03, wpb=3996, bsz=140, num_updates=1442, lr=1.7304e-05, gnorm=3.249, train_wall=5, gb_free=14.3, wall=11009
2024-09-04 10:12:26 | INFO | train_inner | epoch 091:      4 / 16 loss=8.334, nll_loss=5.575, ppl=47.67, wps=1528.2, ups=0.32, wpb=4738, bsz=144, num_updates=1444, lr=1.7328e-05, gnorm=3.241, train_wall=6, gb_free=9.6, wall=11016
2024-09-04 10:12:32 | INFO | train_inner | epoch 091:      6 / 16 loss=8.202, nll_loss=5.412, ppl=42.59, wps=1483.7, ups=0.34, wpb=4378.5, bsz=156, num_updates=1446, lr=1.7352e-05, gnorm=2.824, train_wall=6, gb_free=10.1, wall=11021
2024-09-04 10:12:38 | INFO | train_inner | epoch 091:      8 / 16 loss=8.376, nll_loss=5.644, ppl=50.01, wps=1467.6, ups=0.34, wpb=4326.5, bsz=92, num_updates=1448, lr=1.7376e-05, gnorm=3.488, train_wall=6, gb_free=9.3, wall=11027
2024-09-04 10:12:44 | INFO | train_inner | epoch 091:     10 / 16 loss=8.231, nll_loss=5.425, ppl=42.95, wps=1492.4, ups=0.34, wpb=4382, bsz=124, num_updates=1450, lr=1.74e-05, gnorm=2.894, train_wall=6, gb_free=8.9, wall=11033
2024-09-04 10:12:49 | INFO | train_inner | epoch 091:     12 / 16 loss=8.542, nll_loss=5.815, ppl=56.31, wps=1541.6, ups=0.36, wpb=4232, bsz=80, num_updates=1452, lr=1.7424e-05, gnorm=3.688, train_wall=5, gb_free=11.6, wall=11039
2024-09-04 10:12:54 | INFO | train_inner | epoch 091:     14 / 16 loss=7.892, nll_loss=5.016, ppl=32.36, wps=1370.7, ups=0.41, wpb=3374, bsz=145, num_updates=1454, lr=1.7448e-05, gnorm=3.231, train_wall=5, gb_free=10.4, wall=11044
2024-09-04 10:13:00 | INFO | train_inner | epoch 091:     16 / 16 loss=8.327, nll_loss=5.58, ppl=47.83, wps=1543.3, ups=0.35, wpb=4464.5, bsz=136, num_updates=1456, lr=1.7472e-05, gnorm=3.12, train_wall=6, gb_free=10.2, wall=11049
2024-09-04 10:13:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24002.30859375Mb; avail=231043.71484375Mb
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000752
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24002.30859375Mb; avail=231043.71484375Mb
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012911
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24002.30859375Mb; avail=231043.71484375Mb
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011243
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.025272
2024-09-04 10:13:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24002.30859375Mb; avail=231043.71484375Mb
2024-09-04 10:13:14 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.767 | nll_loss 8.527 | ppl 368.89 | wps 3834.5 | wpb 2070.5 | bsz 122.7 | num_updates 1456 | best_loss 10.759
2024-09-04 10:13:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 1456 updates
2024-09-04 10:13:14 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:13:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:13:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 91 @ 1456 updates, score 10.767) (writing took 39.84754512086511 seconds)
2024-09-04 10:13:54 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2024-09-04 10:13:54 | INFO | train | epoch 091 | loss 8.273 | nll_loss 5.497 | ppl 45.16 | wps 679 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1456 | lr 1.7472e-05 | gnorm 3.217 | train_wall 45 | gb_free 10.2 | wall 11104
2024-09-04 10:13:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:13:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:13:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:13:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000698
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30908.82421875Mb; avail=224137.19921875Mb
2024-09-04 10:13:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000057
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000596
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30908.82421875Mb; avail=224137.19921875Mb
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30908.82421875Mb; avail=224137.19921875Mb
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000161
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001051
2024-09-04 10:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30908.82421875Mb; avail=224137.19921875Mb
2024-09-04 10:13:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:13:54 | INFO | fairseq.trainer | begin training epoch 92
2024-09-04 10:13:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:13:59 | INFO | train_inner | epoch 092:      2 / 16 loss=8.268, nll_loss=5.498, ppl=45.19, wps=126.8, ups=0.03, wpb=3774, bsz=108, num_updates=1458, lr=1.7496e-05, gnorm=4.004, train_wall=5, gb_free=11.4, wall=11109
2024-09-04 10:14:05 | INFO | train_inner | epoch 092:      4 / 16 loss=8.319, nll_loss=5.5, ppl=45.25, wps=1474.2, ups=0.35, wpb=4255, bsz=100, num_updates=1460, lr=1.752e-05, gnorm=3.217, train_wall=6, gb_free=8.9, wall=11115
2024-09-04 10:14:21 | INFO | train_inner | epoch 092:      6 / 16 loss=8.341, nll_loss=5.559, ppl=47.15, wps=526.4, ups=0.13, wpb=4103, bsz=104, num_updates=1462, lr=1.7544e-05, gnorm=3.268, train_wall=16, gb_free=11.7, wall=11130
2024-09-04 10:14:26 | INFO | train_inner | epoch 092:      8 / 16 loss=7.782, nll_loss=4.881, ppl=29.46, wps=1381, ups=0.39, wpb=3574.5, bsz=149, num_updates=1464, lr=1.7568e-05, gnorm=3.153, train_wall=5, gb_free=9.3, wall=11136
2024-09-04 10:14:32 | INFO | train_inner | epoch 092:     10 / 16 loss=8.251, nll_loss=5.522, ppl=45.94, wps=1501.9, ups=0.32, wpb=4701, bsz=164, num_updates=1466, lr=1.7592e-05, gnorm=3.384, train_wall=6, gb_free=9.3, wall=11142
2024-09-04 10:14:38 | INFO | train_inner | epoch 092:     12 / 16 loss=8.277, nll_loss=5.509, ppl=45.55, wps=1550.3, ups=0.33, wpb=4685, bsz=148, num_updates=1468, lr=1.7616e-05, gnorm=2.942, train_wall=6, gb_free=10.2, wall=11148
2024-09-04 10:14:44 | INFO | train_inner | epoch 092:     14 / 16 loss=8.336, nll_loss=5.561, ppl=47.21, wps=1527.2, ups=0.35, wpb=4366.5, bsz=112, num_updates=1470, lr=1.764e-05, gnorm=3.179, train_wall=6, gb_free=10.4, wall=11154
2024-09-04 10:14:50 | INFO | train_inner | epoch 092:     16 / 16 loss=8.238, nll_loss=5.429, ppl=43.08, wps=1486.9, ups=0.34, wpb=4432.5, bsz=132, num_updates=1472, lr=1.7664e-05, gnorm=3.132, train_wall=6, gb_free=10.4, wall=11159
2024-09-04 10:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14927.890625Mb; avail=240118.171875Mb
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000619
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14927.890625Mb; avail=240118.171875Mb
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012618
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14927.890625Mb; avail=240118.171875Mb
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011304
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024899
2024-09-04 10:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14928.1328125Mb; avail=240117.9296875Mb
2024-09-04 10:15:04 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.78 | nll_loss 8.579 | ppl 382.36 | wps 3833.9 | wpb 2070.5 | bsz 122.7 | num_updates 1472 | best_loss 10.759
2024-09-04 10:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 1472 updates
2024-09-04 10:15:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:15:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:15:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 92 @ 1472 updates, score 10.78) (writing took 40.08848502859473 seconds)
2024-09-04 10:15:45 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2024-09-04 10:15:45 | INFO | train | epoch 092 | loss 8.236 | nll_loss 5.445 | ppl 43.55 | wps 614.7 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1472 | lr 1.7664e-05 | gnorm 3.285 | train_wall 55 | gb_free 10.4 | wall 11214
2024-09-04 10:15:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:15:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:15:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:15:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000668
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21832.703125Mb; avail=233213.36328125Mb
2024-09-04 10:15:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000587
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21832.703125Mb; avail=233213.36328125Mb
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21832.703125Mb; avail=233213.36328125Mb
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000161
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001072
2024-09-04 10:15:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21832.703125Mb; avail=233213.36328125Mb
2024-09-04 10:15:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:15:45 | INFO | fairseq.trainer | begin training epoch 93
2024-09-04 10:15:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:15:51 | INFO | train_inner | epoch 093:      2 / 16 loss=7.82, nll_loss=4.896, ppl=29.77, wps=141.3, ups=0.03, wpb=4279.5, bsz=180, num_updates=1474, lr=1.7688e-05, gnorm=2.545, train_wall=6, gb_free=9.1, wall=11220
2024-09-04 10:15:56 | INFO | train_inner | epoch 093:      4 / 16 loss=8.277, nll_loss=5.523, ppl=46, wps=1558.6, ups=0.34, wpb=4534, bsz=124, num_updates=1476, lr=1.7712e-05, gnorm=3.067, train_wall=6, gb_free=10.9, wall=11226
2024-09-04 10:16:02 | INFO | train_inner | epoch 093:      6 / 16 loss=8.352, nll_loss=5.574, ppl=47.64, wps=1557.1, ups=0.34, wpb=4534.5, bsz=104, num_updates=1478, lr=1.7736e-05, gnorm=3.138, train_wall=6, gb_free=9.2, wall=11232
2024-09-04 10:16:08 | INFO | train_inner | epoch 093:      8 / 16 loss=8.337, nll_loss=5.597, ppl=48.4, wps=1493.4, ups=0.36, wpb=4094.5, bsz=96, num_updates=1480, lr=1.776e-05, gnorm=3.517, train_wall=5, gb_free=9.6, wall=11237
2024-09-04 10:16:13 | INFO | train_inner | epoch 093:     10 / 16 loss=8.14, nll_loss=5.338, ppl=40.46, wps=1551.2, ups=0.34, wpb=4514, bsz=140, num_updates=1482, lr=1.7784e-05, gnorm=2.836, train_wall=6, gb_free=10.2, wall=11243
2024-09-04 10:16:19 | INFO | train_inner | epoch 093:     12 / 16 loss=8.231, nll_loss=5.422, ppl=42.87, wps=1502.4, ups=0.36, wpb=4121, bsz=128, num_updates=1484, lr=1.7808e-05, gnorm=3.087, train_wall=5, gb_free=14.7, wall=11248
2024-09-04 10:16:25 | INFO | train_inner | epoch 093:     14 / 16 loss=8.302, nll_loss=5.515, ppl=45.74, wps=1394.4, ups=0.34, wpb=4080, bsz=84, num_updates=1486, lr=1.7832e-05, gnorm=3.422, train_wall=6, gb_free=11.3, wall=11254
2024-09-04 10:16:30 | INFO | train_inner | epoch 093:     16 / 16 loss=7.988, nll_loss=5.179, ppl=36.22, wps=1436.4, ups=0.38, wpb=3734, bsz=161, num_updates=1488, lr=1.7856e-05, gnorm=2.834, train_wall=5, gb_free=9.9, wall=11260
2024-09-04 10:16:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21917.8359375Mb; avail=233128.17578125Mb
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000747
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21918.328125Mb; avail=233127.68359375Mb
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012615
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21918.328125Mb; avail=233127.68359375Mb
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011203
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024926
2024-09-04 10:16:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21918.328125Mb; avail=233127.68359375Mb
2024-09-04 10:16:45 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.771 | nll_loss 8.563 | ppl 378.16 | wps 3834.1 | wpb 2070.5 | bsz 122.7 | num_updates 1488 | best_loss 10.759
2024-09-04 10:16:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 1488 updates
2024-09-04 10:16:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:17:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:17:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 93 @ 1488 updates, score 10.771) (writing took 39.12539908941835 seconds)
2024-09-04 10:17:24 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2024-09-04 10:17:24 | INFO | train | epoch 093 | loss 8.184 | nll_loss 5.384 | ppl 41.76 | wps 684.1 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1488 | lr 1.7856e-05 | gnorm 3.056 | train_wall 45 | gb_free 9.9 | wall 11313
2024-09-04 10:17:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:17:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:17:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:17:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000699
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31135.5078125Mb; avail=223910.51171875Mb
2024-09-04 10:17:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000060
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000575
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31135.5078125Mb; avail=223910.51171875Mb
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31135.5078125Mb; avail=223910.51171875Mb
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000164
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001032
2024-09-04 10:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31135.5078125Mb; avail=223910.51171875Mb
2024-09-04 10:17:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:17:24 | INFO | fairseq.trainer | begin training epoch 94
2024-09-04 10:17:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:17:30 | INFO | train_inner | epoch 094:      2 / 16 loss=8.126, nll_loss=5.287, ppl=39.05, wps=145.9, ups=0.03, wpb=4348, bsz=140, num_updates=1490, lr=1.788e-05, gnorm=2.574, train_wall=6, gb_free=9.8, wall=11319
2024-09-04 10:17:35 | INFO | train_inner | epoch 094:      4 / 16 loss=8.163, nll_loss=5.356, ppl=40.96, wps=1380.6, ups=0.38, wpb=3643.5, bsz=97, num_updates=1492, lr=1.7904e-05, gnorm=3.14, train_wall=5, gb_free=12.9, wall=11324
2024-09-04 10:17:41 | INFO | train_inner | epoch 094:      6 / 16 loss=8.135, nll_loss=5.324, ppl=40.07, wps=1611.5, ups=0.35, wpb=4631.5, bsz=128, num_updates=1494, lr=1.7928e-05, gnorm=2.834, train_wall=6, gb_free=10.3, wall=11330
2024-09-04 10:17:57 | INFO | train_inner | epoch 094:      8 / 16 loss=8.045, nll_loss=5.208, ppl=36.96, wps=576.7, ups=0.12, wpb=4665, bsz=200, num_updates=1496, lr=1.7952e-05, gnorm=2.424, train_wall=16, gb_free=9.3, wall=11346
2024-09-04 10:18:03 | INFO | train_inner | epoch 094:     10 / 16 loss=8.184, nll_loss=5.42, ppl=42.82, wps=1468.4, ups=0.35, wpb=4234.5, bsz=108, num_updates=1498, lr=1.7976e-05, gnorm=3.122, train_wall=6, gb_free=9.9, wall=11352
2024-09-04 10:18:08 | INFO | train_inner | epoch 094:     12 / 16 loss=8.263, nll_loss=5.462, ppl=44.09, wps=1562, ups=0.35, wpb=4481.5, bsz=116, num_updates=1500, lr=1.8e-05, gnorm=3.055, train_wall=6, gb_free=10.7, wall=11358
2024-09-04 10:18:14 | INFO | train_inner | epoch 094:     14 / 16 loss=8.263, nll_loss=5.431, ppl=43.15, wps=1550.1, ups=0.36, wpb=4343.5, bsz=108, num_updates=1502, lr=1.8024e-05, gnorm=3.304, train_wall=6, gb_free=10.3, wall=11363
2024-09-04 10:18:19 | INFO | train_inner | epoch 094:     16 / 16 loss=8.044, nll_loss=5.198, ppl=36.7, wps=1442.7, ups=0.41, wpb=3544, bsz=120, num_updates=1504, lr=1.8048e-05, gnorm=3.444, train_wall=5, gb_free=15, wall=11368
2024-09-04 10:18:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17289.9296875Mb; avail=237756.1328125Mb
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000624
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17289.9296875Mb; avail=237756.1328125Mb
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012715
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17289.9296875Mb; avail=237756.1328125Mb
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011162
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024844
2024-09-04 10:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17289.9296875Mb; avail=237756.1328125Mb
2024-09-04 10:18:33 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.796 | nll_loss 8.641 | ppl 399.15 | wps 3838.2 | wpb 2070.5 | bsz 122.7 | num_updates 1504 | best_loss 10.759
2024-09-04 10:18:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 1504 updates
2024-09-04 10:18:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:19:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 94 @ 1504 updates, score 10.796) (writing took 39.39631627313793 seconds)
2024-09-04 10:19:13 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2024-09-04 10:19:13 | INFO | train | epoch 094 | loss 8.154 | nll_loss 5.338 | ppl 40.44 | wps 621.3 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1504 | lr 1.8048e-05 | gnorm 2.987 | train_wall 55 | gb_free 15 | wall 11422
2024-09-04 10:19:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:19:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:19:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:19:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000654
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28684.2421875Mb; avail=226361.8203125Mb
2024-09-04 10:19:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000048
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000588
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28684.2421875Mb; avail=226361.8203125Mb
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28684.2421875Mb; avail=226361.8203125Mb
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000157
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001038
2024-09-04 10:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28684.2421875Mb; avail=226361.8203125Mb
2024-09-04 10:19:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:19:13 | INFO | fairseq.trainer | begin training epoch 95
2024-09-04 10:19:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:19:19 | INFO | train_inner | epoch 095:      2 / 16 loss=8.317, nll_loss=5.567, ppl=47.4, wps=147.1, ups=0.03, wpb=4401.5, bsz=84, num_updates=1506, lr=1.8072e-05, gnorm=3.186, train_wall=6, gb_free=10.1, wall=11428
2024-09-04 10:19:24 | INFO | train_inner | epoch 095:      4 / 16 loss=8.141, nll_loss=5.303, ppl=39.48, wps=1418.9, ups=0.41, wpb=3479, bsz=69, num_updates=1508, lr=1.8096e-05, gnorm=3.364, train_wall=5, gb_free=15.4, wall=11433
2024-09-04 10:19:29 | INFO | train_inner | epoch 095:      6 / 16 loss=7.875, nll_loss=4.988, ppl=31.73, wps=1457, ups=0.36, wpb=3993, bsz=144, num_updates=1510, lr=1.812e-05, gnorm=3.332, train_wall=5, gb_free=9.9, wall=11439
2024-09-04 10:19:35 | INFO | train_inner | epoch 095:      8 / 16 loss=7.93, nll_loss=5.05, ppl=33.13, wps=1503.2, ups=0.34, wpb=4411.5, bsz=164, num_updates=1512, lr=1.8144e-05, gnorm=2.662, train_wall=6, gb_free=9.4, wall=11444
2024-09-04 10:19:40 | INFO | train_inner | epoch 095:     10 / 16 loss=8.35, nll_loss=5.627, ppl=49.43, wps=1536, ups=0.37, wpb=4162, bsz=112, num_updates=1514, lr=1.8168e-05, gnorm=3.99, train_wall=5, gb_free=10.1, wall=11450
2024-09-04 10:19:46 | INFO | train_inner | epoch 095:     12 / 16 loss=8.173, nll_loss=5.363, ppl=41.15, wps=1469.2, ups=0.34, wpb=4373, bsz=132, num_updates=1516, lr=1.8192e-05, gnorm=3.409, train_wall=6, gb_free=9.5, wall=11456
2024-09-04 10:19:52 | INFO | train_inner | epoch 095:     14 / 16 loss=8.012, nll_loss=5.124, ppl=34.86, wps=1549.2, ups=0.33, wpb=4638.5, bsz=172, num_updates=1518, lr=1.8216e-05, gnorm=2.874, train_wall=6, gb_free=8.6, wall=11462
2024-09-04 10:19:58 | INFO | train_inner | epoch 095:     16 / 16 loss=8.153, nll_loss=5.344, ppl=40.62, wps=1492.1, ups=0.34, wpb=4433, bsz=140, num_updates=1520, lr=1.824e-05, gnorm=3.035, train_wall=6, gb_free=11.3, wall=11468
2024-09-04 10:19:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28741.71484375Mb; avail=226304.30078125Mb
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000749
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28741.71484375Mb; avail=226304.30078125Mb
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012663
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28741.71484375Mb; avail=226304.30078125Mb
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011051
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024819
2024-09-04 10:19:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28742.20703125Mb; avail=226303.80859375Mb
2024-09-04 10:20:13 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.776 | nll_loss 8.609 | ppl 390.51 | wps 3834.5 | wpb 2070.5 | bsz 122.7 | num_updates 1520 | best_loss 10.759
2024-09-04 10:20:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 1520 updates
2024-09-04 10:20:13 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:20:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:20:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 95 @ 1520 updates, score 10.776) (writing took 40.13943563681096 seconds)
2024-09-04 10:20:53 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2024-09-04 10:20:53 | INFO | train | epoch 095 | loss 8.119 | nll_loss 5.296 | ppl 39.28 | wps 676.5 | ups 0.16 | wpb 4236.4 | bsz 127.1 | num_updates 1520 | lr 1.824e-05 | gnorm 3.231 | train_wall 45 | gb_free 11.3 | wall 11522
2024-09-04 10:20:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 2034}; raw total size: 2034
2024-09-04 10:20:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 2034}; resampled total size: 2034
2024-09-04 10:20:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-09-04 10:20:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000705
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37943.7421875Mb; avail=217102.27734375Mb
2024-09-04 10:20:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000565
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37943.7421875Mb; avail=217102.27734375Mb
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000028
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37943.7421875Mb; avail=217102.27734375Mb
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000161
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001026
2024-09-04 10:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37943.7421875Mb; avail=217102.27734375Mb
2024-09-04 10:20:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 16
2024-09-04 10:20:53 | INFO | fairseq.trainer | begin training epoch 96
2024-09-04 10:20:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-09-04 10:20:59 | INFO | train_inner | epoch 096:      2 / 16 loss=7.83, nll_loss=4.946, ppl=30.83, wps=143.5, ups=0.03, wpb=4346.5, bsz=172, num_updates=1522, lr=1.8264e-05, gnorm=2.886, train_wall=6, gb_free=10.3, wall=11528
2024-09-04 10:21:04 | INFO | train_inner | epoch 096:      4 / 16 loss=8.199, nll_loss=5.392, ppl=41.99, wps=1512.7, ups=0.37, wpb=4063.5, bsz=100, num_updates=1524, lr=1.8288e-05, gnorm=3.214, train_wall=5, gb_free=10.8, wall=11534
2024-09-04 10:21:10 | INFO | train_inner | epoch 096:      6 / 16 loss=7.975, nll_loss=5.13, ppl=35.01, wps=1496.5, ups=0.32, wpb=4618.5, bsz=164, num_updates=1526, lr=1.8312e-05, gnorm=2.992, train_wall=6, gb_free=9.4, wall=11540
2024-09-04 10:21:26 | INFO | train_inner | epoch 096:      8 / 16 loss=8.151, nll_loss=5.371, ppl=41.39, wps=555.5, ups=0.13, wpb=4360, bsz=132, num_updates=1528, lr=1.8336e-05, gnorm=3.373, train_wall=16, gb_free=14.1, wall=11556
2024-09-04 10:21:31 | INFO | train_inner | epoch 096:     10 / 16 loss=8.362, nll_loss=5.582, ppl=47.91, wps=1424.4, ups=0.4, wpb=3547.5, bsz=49, num_updates=1530, lr=1.836e-05, gnorm=3.828, train_wall=5, gb_free=10.3, wall=11561
2024-09-04 10:21:37 | INFO | train_inner | epoch 096:     12 / 16 loss=8.2, nll_loss=5.35, ppl=40.78, wps=1561.3, ups=0.36, wpb=4289, bsz=104, num_updates=1532, lr=1.8384e-05, gnorm=3.349, train_wall=5, gb_free=10.1, wall=11566
2024-09-04 10:21:42 | INFO | train_inner | epoch 096:     14 / 16 loss=8.046, nll_loss=5.229, ppl=37.5, wps=1473.2, ups=0.35, wpb=4231, bsz=148, num_updates=1534, lr=1.8408e-05, gnorm=3.392, train_wall=6, gb_free=10.2, wall=11572
2024-09-04 10:21:48 | INFO | train_inner | epoch 096:     16 / 16 loss=8.043, nll_loss=5.198, ppl=36.71, wps=1475.6, ups=0.33, wpb=4435.5, bsz=148, num_updates=1536, lr=1.8432e-05, gnorm=3.04, train_wall=6, gb_free=8.7, wall=11578
2024-09-04 10:21:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17276.14453125Mb; avail=237769.91796875Mb
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000627
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17276.14453125Mb; avail=237769.91796875Mb
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.012842
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17276.14453125Mb; avail=237769.91796875Mb
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.011092
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.024931
2024-09-04 10:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17276.14453125Mb; avail=237769.91796875Mb
2024-09-04 10:22:03 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.767 | nll_loss 8.556 | ppl 376.48 | wps 3836.8 | wpb 2070.5 | bsz 122.7 | num_updates 1536 | best_loss 10.759
2024-09-04 10:22:03 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2024-09-04 10:22:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 1536 updates
2024-09-04 10:22:03 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:22:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-09-04 10:22:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Version2/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 96 @ 1536 updates, score 10.767) (writing took 40.335716566070914 seconds)
2024-09-04 10:22:43 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2024-09-04 10:22:43 | INFO | train | epoch 096 | loss 8.093 | nll_loss 5.265 | ppl 38.46 | wps 614.9 | ups 0.15 | wpb 4236.4 | bsz 127.1 | num_updates 1536 | lr 1.8432e-05 | gnorm 3.259 | train_wall 55 | gb_free 8.7 | wall 11633
2024-09-04 10:22:43 | INFO | fairseq_cli.train | done training in 11632.5 seconds
