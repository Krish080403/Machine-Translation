2024-08-11 13:49:49 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-08-11 13:49:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-08-11 13:49:49 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-08-11 13:49:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-08-11 13:49:58 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-08-11 13:49:58 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-08-11 13:49:58 | INFO | fairseq_cli.train | model: TransformerModel
2024-08-11 13:49:58 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-08-11 13:49:58 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-08-11 13:49:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-08-11 13:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-08-11 13:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10539.29296875Mb; avail=244558.97265625Mb
2024-08-11 13:49:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-08-11 13:49:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:mr-hi': 1}
2024-08-11 13:49:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-08-11 13:49:58 | INFO | fairseq.data.data_utils | loaded 1,168 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.hi-mr.mr
2024-08-11 13:49:58 | INFO | fairseq.data.data_utils | loaded 1,168 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.hi-mr.hi
2024-08-11 13:49:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin valid mr-hi 1168 examples
2024-08-11 13:49:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-08-11 13:49:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-08-11 13:49:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-08-11 13:49:59 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-08-11 13:49:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-08-11 13:49:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-08-11 13:49:59 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-08-11 13:49:59 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/1.2B_last_checkpoint.pt: optimizer, lr scheduler, meters, dataloader will be reset
2024-08-11 13:49:59 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/1.2B_last_checkpoint.pt
2024-08-11 13:50:05 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-08-11 13:50:06 | INFO | fairseq.trainer | Loaded checkpoint /home/krish/content/1.2B_last_checkpoint.pt (epoch 81 @ 0 updates)
2024-08-11 13:50:06 | INFO | fairseq.trainer | loading train data for epoch 1
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6876.16796875Mb; avail=248214.1015625Mb
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:mr-hi': 1}
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-08-11 13:50:06 | INFO | fairseq.data.data_utils | loaded 9,337 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/train.hi-mr.mr
2024-08-11 13:50:06 | INFO | fairseq.data.data_utils | loaded 9,337 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/train.hi-mr.hi
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin train mr-hi 9337 examples
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:mr-hi', 9337)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:mr-hi', 1.0)]
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000550
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6876.1640625Mb; avail=248214.1015625Mb
2024-08-11 13:50:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000097
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001149
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6876.1640625Mb; avail=248214.1015625Mb
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6876.1640625Mb; avail=248214.1015625Mb
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000811
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002297
2024-08-11 13:50:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6876.1640625Mb; avail=248214.1015625Mb
2024-08-11 13:50:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 62
2024-08-11 13:50:06 | INFO | fairseq.trainer | begin training epoch 1
2024-08-11 13:50:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 13:50:11 | INFO | train_inner | epoch 001:      2 / 62 loss=11.419, nll_loss=9.233, ppl=601.55, wps=1705.1, ups=0.45, wpb=4820, bsz=132, num_updates=2, lr=2.4e-08, gnorm=10.005, train_wall=5, gb_free=14.2, wall=12
2024-08-11 13:50:16 | INFO | train_inner | epoch 001:      4 / 62 loss=11.421, nll_loss=9.221, ppl=596.91, wps=2122.2, ups=0.44, wpb=4778, bsz=180, num_updates=4, lr=4.8e-08, gnorm=10.586, train_wall=4, gb_free=12.5, wall=17
2024-08-11 13:50:21 | INFO | train_inner | epoch 001:      6 / 62 loss=11.092, nll_loss=8.815, ppl=450.45, wps=1800.2, ups=0.38, wpb=4681, bsz=144, num_updates=6, lr=7.2e-08, gnorm=9.04, train_wall=5, gb_free=13.9, wall=22
2024-08-11 13:50:25 | INFO | train_inner | epoch 001:      8 / 62 loss=10.049, nll_loss=7.509, ppl=182.16, wps=1488.8, ups=0.42, wpb=3513, bsz=60, num_updates=8, lr=9.6e-08, gnorm=5.703, train_wall=5, gb_free=13.2, wall=27
2024-08-11 13:50:30 | INFO | train_inner | epoch 001:     10 / 62 loss=11.409, nll_loss=9.209, ppl=591.79, wps=2288.1, ups=0.41, wpb=5597, bsz=204, num_updates=10, lr=1.2e-07, gnorm=10.687, train_wall=5, gb_free=11.2, wall=32
2024-08-11 13:50:35 | INFO | train_inner | epoch 001:     12 / 62 loss=10.964, nll_loss=8.664, ppl=405.64, wps=2043, ups=0.39, wpb=5178.5, bsz=136, num_updates=12, lr=1.44e-07, gnorm=7.728, train_wall=5, gb_free=10.8, wall=37
2024-08-11 13:50:40 | INFO | train_inner | epoch 001:     14 / 62 loss=10.768, nll_loss=8.418, ppl=341.94, wps=1895.4, ups=0.4, wpb=4681, bsz=104, num_updates=14, lr=1.68e-07, gnorm=7.119, train_wall=5, gb_free=10, wall=42
2024-08-11 13:50:45 | INFO | train_inner | epoch 001:     16 / 62 loss=11.228, nll_loss=8.986, ppl=506.92, wps=2035.3, ups=0.44, wpb=4672, bsz=176, num_updates=16, lr=1.92e-07, gnorm=9.325, train_wall=5, gb_free=14.1, wall=46
2024-08-11 13:50:50 | INFO | train_inner | epoch 001:     18 / 62 loss=11.167, nll_loss=8.902, ppl=478.43, wps=2127, ups=0.39, wpb=5429.5, bsz=208, num_updates=18, lr=2.16e-07, gnorm=9.885, train_wall=5, gb_free=9.9, wall=51
2024-08-11 13:50:55 | INFO | train_inner | epoch 001:     20 / 62 loss=11.208, nll_loss=8.961, ppl=498.22, wps=2114.3, ups=0.42, wpb=5082, bsz=240, num_updates=20, lr=2.4e-07, gnorm=9.086, train_wall=5, gb_free=11.9, wall=56
2024-08-11 13:51:00 | INFO | train_inner | epoch 001:     22 / 62 loss=11.165, nll_loss=8.911, ppl=481.52, wps=2166.2, ups=0.4, wpb=5417.5, bsz=168, num_updates=22, lr=2.64e-07, gnorm=8.788, train_wall=5, gb_free=10.9, wall=61
2024-08-11 13:51:05 | INFO | train_inner | epoch 001:     24 / 62 loss=10.878, nll_loss=8.555, ppl=376.22, wps=1917.5, ups=0.42, wpb=4597, bsz=108, num_updates=24, lr=2.88e-07, gnorm=7.266, train_wall=5, gb_free=11.4, wall=66
2024-08-11 13:51:10 | INFO | train_inner | epoch 001:     26 / 62 loss=11.168, nll_loss=8.913, ppl=482.14, wps=2091.8, ups=0.41, wpb=5097.5, bsz=188, num_updates=26, lr=3.12e-07, gnorm=8.72, train_wall=5, gb_free=12.5, wall=71
2024-08-11 13:51:15 | INFO | train_inner | epoch 001:     28 / 62 loss=10.633, nll_loss=8.262, ppl=307.08, wps=1728.2, ups=0.4, wpb=4349, bsz=88, num_updates=28, lr=3.36e-07, gnorm=5.502, train_wall=5, gb_free=11.5, wall=76
2024-08-11 13:51:20 | INFO | train_inner | epoch 001:     30 / 62 loss=11.061, nll_loss=8.784, ppl=440.75, wps=2214.6, ups=0.41, wpb=5441, bsz=156, num_updates=30, lr=3.6e-07, gnorm=8.119, train_wall=5, gb_free=11.6, wall=81
2024-08-11 13:51:25 | INFO | train_inner | epoch 001:     32 / 62 loss=10.892, nll_loss=8.576, ppl=381.54, wps=1969.3, ups=0.39, wpb=5035.5, bsz=180, num_updates=32, lr=3.84e-07, gnorm=6.983, train_wall=5, gb_free=11.3, wall=86
2024-08-11 13:51:30 | INFO | train_inner | epoch 001:     34 / 62 loss=11.069, nll_loss=8.802, ppl=446.49, wps=2094.6, ups=0.41, wpb=5125, bsz=140, num_updates=34, lr=4.08e-07, gnorm=6.928, train_wall=5, gb_free=12.5, wall=91
2024-08-11 13:51:35 | INFO | train_inner | epoch 001:     36 / 62 loss=11.001, nll_loss=8.713, ppl=419.71, wps=2143.6, ups=0.38, wpb=5669.5, bsz=236, num_updates=36, lr=4.32e-07, gnorm=8.089, train_wall=5, gb_free=11.1, wall=96
2024-08-11 13:51:40 | INFO | train_inner | epoch 001:     38 / 62 loss=10.824, nll_loss=8.5, ppl=362.13, wps=1840.7, ups=0.41, wpb=4467, bsz=124, num_updates=38, lr=4.56e-07, gnorm=6.425, train_wall=5, gb_free=11.1, wall=101
2024-08-11 13:51:44 | INFO | train_inner | epoch 001:     40 / 62 loss=10.605, nll_loss=8.215, ppl=297.23, wps=1710.9, ups=0.43, wpb=3974.5, bsz=184, num_updates=40, lr=4.8e-07, gnorm=6.802, train_wall=5, gb_free=12.6, wall=105
2024-08-11 13:51:48 | INFO | train_inner | epoch 001:     42 / 62 loss=10.776, nll_loss=8.447, ppl=348.99, wps=1965.8, ups=0.49, wpb=4020.5, bsz=92.5, num_updates=42, lr=5.04e-07, gnorm=5.783, train_wall=4, gb_free=17.6, wall=110
2024-08-11 13:51:54 | INFO | train_inner | epoch 001:     44 / 62 loss=10.41, nll_loss=7.985, ppl=253.36, wps=1746.8, ups=0.39, wpb=4478.5, bsz=144, num_updates=44, lr=5.28e-07, gnorm=5.025, train_wall=5, gb_free=11.5, wall=115
2024-08-11 13:51:59 | INFO | train_inner | epoch 001:     46 / 62 loss=10.698, nll_loss=8.352, ppl=326.63, wps=1924.5, ups=0.39, wpb=4947.5, bsz=144, num_updates=46, lr=5.52e-07, gnorm=5.409, train_wall=5, gb_free=11.4, wall=120
2024-08-11 13:52:04 | INFO | train_inner | epoch 001:     48 / 62 loss=10.456, nll_loss=8.052, ppl=265.35, wps=1655, ups=0.4, wpb=4137, bsz=124, num_updates=48, lr=5.76e-07, gnorm=4.884, train_wall=5, gb_free=12.6, wall=125
2024-08-11 13:52:09 | INFO | train_inner | epoch 001:     50 / 62 loss=10.358, nll_loss=7.921, ppl=242.38, wps=1632.2, ups=0.4, wpb=4070.5, bsz=148, num_updates=50, lr=6e-07, gnorm=5.139, train_wall=5, gb_free=12.9, wall=130
2024-08-11 13:52:14 | INFO | train_inner | epoch 001:     52 / 62 loss=10.663, nll_loss=8.325, ppl=320.57, wps=1894.3, ups=0.41, wpb=4596.5, bsz=140, num_updates=52, lr=6.24e-07, gnorm=5.136, train_wall=5, gb_free=11.9, wall=135
2024-08-11 13:52:19 | INFO | train_inner | epoch 001:     54 / 62 loss=10.633, nll_loss=8.289, ppl=312.8, wps=2044.9, ups=0.4, wpb=5152, bsz=148, num_updates=54, lr=6.48e-07, gnorm=4.906, train_wall=5, gb_free=11.8, wall=140
2024-08-11 13:52:23 | INFO | train_inner | epoch 001:     56 / 62 loss=10.665, nll_loss=8.318, ppl=319.03, wps=1806.6, ups=0.42, wpb=4302, bsz=152, num_updates=56, lr=6.72e-07, gnorm=5.338, train_wall=5, gb_free=13.2, wall=144
2024-08-11 13:52:28 | INFO | train_inner | epoch 001:     58 / 62 loss=10.611, nll_loss=8.256, ppl=305.72, wps=2043.8, ups=0.39, wpb=5234.5, bsz=176, num_updates=58, lr=6.96e-07, gnorm=4.98, train_wall=5, gb_free=10.5, wall=150
2024-08-11 13:52:33 | INFO | train_inner | epoch 001:     60 / 62 loss=10.381, nll_loss=7.969, ppl=250.53, wps=1837.2, ups=0.4, wpb=4597, bsz=132, num_updates=60, lr=7.2e-07, gnorm=4.032, train_wall=5, gb_free=11.8, wall=155
2024-08-11 13:52:37 | INFO | train_inner | epoch 001:     62 / 62 loss=10.561, nll_loss=8.201, ppl=294.32, wps=2038.4, ups=0.52, wpb=3886, bsz=112, num_updates=62, lr=7.44e-07, gnorm=4.622, train_wall=4, gb_free=16.2, wall=158
2024-08-11 13:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7397.53125Mb; avail=247684.75390625Mb
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000550
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7397.53125Mb; avail=247684.75390625Mb
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004650
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7397.53125Mb; avail=247684.75390625Mb
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004130
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009749
2024-08-11 13:52:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7397.53125Mb; avail=247684.75390625Mb
2024-08-11 13:52:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.353 | nll_loss 7.828 | ppl 227.15 | wps 4793.2 | wpb 2061.8 | bsz 64.9 | num_updates 62
2024-08-11 13:52:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 62 updates
2024-08-11 13:52:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 13:53:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 13:53:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 1 @ 62 updates, score 10.353) (writing took 48.40996196586639 seconds)
2024-08-11 13:53:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-08-11 13:53:33 | INFO | train | epoch 001 | loss 10.872 | nll_loss 8.556 | ppl 376.46 | wps 1409.8 | ups 0.3 | wpb 4742.8 | bsz 150.6 | num_updates 62 | lr 7.44e-07 | gnorm 7.034 | train_wall 151 | gb_free 16.2 | wall 214
2024-08-11 13:53:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 13:53:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 13:53:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 13:53:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000756
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10560.109375Mb; avail=244522.15625Mb
2024-08-11 13:53:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000150
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001268
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10560.109375Mb; avail=244522.15625Mb
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000053
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10560.109375Mb; avail=244522.15625Mb
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000447
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002135
2024-08-11 13:53:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10560.109375Mb; avail=244522.15625Mb
2024-08-11 13:53:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 62
2024-08-11 13:53:33 | INFO | fairseq.trainer | begin training epoch 2
2024-08-11 13:53:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 13:53:38 | INFO | train_inner | epoch 002:      2 / 62 loss=10.347, nll_loss=7.928, ppl=243.62, wps=139.2, ups=0.03, wpb=4222, bsz=152, num_updates=64, lr=7.68e-07, gnorm=4.764, train_wall=5, gb_free=12.8, wall=219
2024-08-11 13:53:43 | INFO | train_inner | epoch 002:      4 / 62 loss=10.491, nll_loss=8.114, ppl=277.13, wps=2013.5, ups=0.4, wpb=4992.5, bsz=188, num_updates=66, lr=7.92e-07, gnorm=4.322, train_wall=5, gb_free=11.6, wall=224
2024-08-11 13:53:47 | INFO | train_inner | epoch 002:      6 / 62 loss=10.529, nll_loss=8.188, ppl=291.65, wps=2015.7, ups=0.5, wpb=4038, bsz=104.5, num_updates=68, lr=8.16e-07, gnorm=4.429, train_wall=4, gb_free=13.1, wall=228
2024-08-11 13:53:52 | INFO | train_inner | epoch 002:      8 / 62 loss=10.458, nll_loss=8.075, ppl=269.68, wps=2271.5, ups=0.41, wpb=5502, bsz=176, num_updates=70, lr=8.4e-07, gnorm=4.368, train_wall=5, gb_free=11.6, wall=233
2024-08-11 13:53:57 | INFO | train_inner | epoch 002:     10 / 62 loss=10.319, nll_loss=7.912, ppl=240.82, wps=1837.7, ups=0.42, wpb=4406.5, bsz=120, num_updates=72, lr=8.64e-07, gnorm=3.985, train_wall=5, gb_free=11.9, wall=238
2024-08-11 13:54:01 | INFO | train_inner | epoch 002:     12 / 62 loss=10.067, nll_loss=7.608, ppl=195.04, wps=1686.6, ups=0.41, wpb=4069.5, bsz=84, num_updates=74, lr=8.88e-07, gnorm=4.464, train_wall=5, gb_free=11.3, wall=243
2024-08-11 13:54:06 | INFO | train_inner | epoch 002:     14 / 62 loss=10.281, nll_loss=7.865, ppl=233.15, wps=2062.7, ups=0.4, wpb=5127.5, bsz=124, num_updates=76, lr=9.12e-07, gnorm=3.347, train_wall=5, gb_free=11.3, wall=247
2024-08-11 13:54:11 | INFO | train_inner | epoch 002:     16 / 62 loss=10.068, nll_loss=7.587, ppl=192.24, wps=1963.6, ups=0.39, wpb=5005, bsz=208, num_updates=78, lr=9.36e-07, gnorm=3.648, train_wall=5, gb_free=12.4, wall=253
2024-08-11 13:54:16 | INFO | train_inner | epoch 002:     18 / 62 loss=10.137, nll_loss=7.681, ppl=205.26, wps=1802.8, ups=0.41, wpb=4423.5, bsz=140, num_updates=80, lr=9.6e-07, gnorm=3.164, train_wall=5, gb_free=12.7, wall=257
2024-08-11 13:54:21 | INFO | train_inner | epoch 002:     20 / 62 loss=10.068, nll_loss=7.592, ppl=192.96, wps=2174.9, ups=0.39, wpb=5628.5, bsz=212, num_updates=82, lr=9.84e-07, gnorm=3.484, train_wall=5, gb_free=11.2, wall=263
2024-08-11 13:54:27 | INFO | train_inner | epoch 002:     22 / 62 loss=10.123, nll_loss=7.679, ppl=204.9, wps=1834.8, ups=0.4, wpb=4601, bsz=104, num_updates=84, lr=1.008e-06, gnorm=3.89, train_wall=5, gb_free=12.8, wall=268
2024-08-11 13:54:31 | INFO | train_inner | epoch 002:     24 / 62 loss=10.214, nll_loss=7.785, ppl=220.58, wps=1750.3, ups=0.45, wpb=3908.5, bsz=108, num_updates=86, lr=1.032e-06, gnorm=3.225, train_wall=4, gb_free=14.9, wall=272
2024-08-11 13:54:36 | INFO | train_inner | epoch 002:     26 / 62 loss=9.95, nll_loss=7.444, ppl=174.18, wps=1619.4, ups=0.43, wpb=3797, bsz=220, num_updates=88, lr=1.056e-06, gnorm=3.72, train_wall=5, gb_free=13.1, wall=277
2024-08-11 13:54:40 | INFO | train_inner | epoch 002:     28 / 62 loss=10.151, nll_loss=7.722, ppl=211.08, wps=2044.7, ups=0.42, wpb=4884, bsz=120, num_updates=90, lr=1.08e-06, gnorm=3.19, train_wall=5, gb_free=13.6, wall=282
2024-08-11 13:54:46 | INFO | train_inner | epoch 002:     30 / 62 loss=9.858, nll_loss=7.332, ppl=161.14, wps=1783.2, ups=0.38, wpb=4699, bsz=176, num_updates=92, lr=1.104e-06, gnorm=2.836, train_wall=5, gb_free=11, wall=287
2024-08-11 13:54:51 | INFO | train_inner | epoch 002:     32 / 62 loss=9.991, nll_loss=7.504, ppl=181.48, wps=2135.3, ups=0.38, wpb=5628.5, bsz=204, num_updates=94, lr=1.128e-06, gnorm=3.26, train_wall=5, gb_free=9.7, wall=292
2024-08-11 13:54:56 | INFO | train_inner | epoch 002:     34 / 62 loss=9.882, nll_loss=7.374, ppl=165.88, wps=1847, ups=0.4, wpb=4567.5, bsz=108, num_updates=96, lr=1.152e-06, gnorm=2.839, train_wall=5, gb_free=10.9, wall=297
2024-08-11 13:55:01 | INFO | train_inner | epoch 002:     36 / 62 loss=9.987, nll_loss=7.508, ppl=182.07, wps=1732.8, ups=0.42, wpb=4151, bsz=148, num_updates=98, lr=1.176e-06, gnorm=2.868, train_wall=5, gb_free=13.4, wall=302
2024-08-11 13:55:06 | INFO | train_inner | epoch 002:     38 / 62 loss=10.117, nll_loss=7.693, ppl=206.9, wps=1883, ups=0.42, wpb=4536.5, bsz=104, num_updates=100, lr=1.2e-06, gnorm=2.844, train_wall=5, gb_free=12.9, wall=307
2024-08-11 13:55:11 | INFO | train_inner | epoch 002:     40 / 62 loss=9.692, nll_loss=7.145, ppl=141.52, wps=1725.8, ups=0.37, wpb=4603.5, bsz=112, num_updates=102, lr=1.224e-06, gnorm=3.016, train_wall=5, gb_free=11.7, wall=312
2024-08-11 13:55:16 | INFO | train_inner | epoch 002:     42 / 62 loss=9.892, nll_loss=7.389, ppl=167.58, wps=2070.8, ups=0.38, wpb=5465, bsz=224, num_updates=104, lr=1.248e-06, gnorm=3.05, train_wall=5, gb_free=11.6, wall=317
2024-08-11 13:55:21 | INFO | train_inner | epoch 002:     44 / 62 loss=9.887, nll_loss=7.387, ppl=167.43, wps=1931.4, ups=0.37, wpb=5158, bsz=216, num_updates=106, lr=1.272e-06, gnorm=2.864, train_wall=5, gb_free=12.1, wall=323
2024-08-11 13:55:27 | INFO | train_inner | epoch 002:     46 / 62 loss=9.927, nll_loss=7.442, ppl=173.86, wps=2174.4, ups=0.38, wpb=5650, bsz=196, num_updates=108, lr=1.296e-06, gnorm=3.057, train_wall=5, gb_free=11.6, wall=328
2024-08-11 13:55:31 | INFO | train_inner | epoch 002:     48 / 62 loss=9.79, nll_loss=7.275, ppl=154.91, wps=1759.2, ups=0.42, wpb=4168, bsz=108, num_updates=110, lr=1.32e-06, gnorm=2.806, train_wall=5, gb_free=13.4, wall=333
2024-08-11 13:55:37 | INFO | train_inner | epoch 002:     50 / 62 loss=9.848, nll_loss=7.351, ppl=163.24, wps=1950.2, ups=0.39, wpb=4993, bsz=132, num_updates=112, lr=1.344e-06, gnorm=2.484, train_wall=5, gb_free=11, wall=338
2024-08-11 13:55:41 | INFO | train_inner | epoch 002:     52 / 62 loss=9.88, nll_loss=7.382, ppl=166.86, wps=2014.4, ups=0.42, wpb=4784, bsz=152, num_updates=114, lr=1.368e-06, gnorm=2.949, train_wall=5, gb_free=13.9, wall=342
2024-08-11 13:55:46 | INFO | train_inner | epoch 002:     54 / 62 loss=9.732, nll_loss=7.209, ppl=147.95, wps=1889.4, ups=0.4, wpb=4776, bsz=172, num_updates=116, lr=1.392e-06, gnorm=2.779, train_wall=5, gb_free=11.9, wall=348
2024-08-11 13:55:51 | INFO | train_inner | epoch 002:     56 / 62 loss=9.899, nll_loss=7.416, ppl=170.78, wps=1996.9, ups=0.39, wpb=5105, bsz=160, num_updates=118, lr=1.416e-06, gnorm=2.618, train_wall=5, gb_free=12.7, wall=353
2024-08-11 13:55:56 | INFO | train_inner | epoch 002:     58 / 62 loss=9.809, nll_loss=7.308, ppl=158.42, wps=2044.4, ups=0.4, wpb=5085.5, bsz=156, num_updates=120, lr=1.44e-06, gnorm=2.614, train_wall=5, gb_free=12.5, wall=358
2024-08-11 13:56:01 | INFO | train_inner | epoch 002:     60 / 62 loss=9.726, nll_loss=7.203, ppl=147.38, wps=1872.8, ups=0.42, wpb=4500.5, bsz=108, num_updates=122, lr=1.464e-06, gnorm=2.741, train_wall=5, gb_free=12.3, wall=362
2024-08-11 13:56:05 | INFO | train_inner | epoch 002:     62 / 62 loss=9.812, nll_loss=7.306, ppl=158.29, wps=2236.5, ups=0.49, wpb=4551, bsz=132, num_updates=124, lr=1.488e-06, gnorm=2.606, train_wall=4, gb_free=16.1, wall=366
2024-08-11 13:56:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10604.43359375Mb; avail=244477.80859375Mb
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000632
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10604.43359375Mb; avail=244477.80859375Mb
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004505
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10604.43359375Mb; avail=244477.80859375Mb
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004031
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009604
2024-08-11 13:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10604.92578125Mb; avail=244477.31640625Mb
2024-08-11 13:56:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.517 | nll_loss 6.848 | ppl 115.23 | wps 4780.1 | wpb 2061.8 | bsz 64.9 | num_updates 124 | best_loss 9.517
2024-08-11 13:56:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 124 updates
2024-08-11 13:56:13 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 13:56:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 13:57:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 2 @ 124 updates, score 9.517) (writing took 62.0114662759006 seconds)
2024-08-11 13:57:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-08-11 13:57:15 | INFO | train | epoch 002 | loss 10.028 | nll_loss 7.559 | ppl 188.57 | wps 1326.4 | ups 0.28 | wpb 4742.8 | bsz 150.6 | num_updates 124 | lr 1.488e-06 | gnorm 3.298 | train_wall 152 | gb_free 16.1 | wall 436
2024-08-11 13:57:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 13:57:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 13:57:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 13:57:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000844
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13681.07421875Mb; avail=241401.19140625Mb
2024-08-11 13:57:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000156
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001297
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13681.07421875Mb; avail=241401.19140625Mb
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000057
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13681.07421875Mb; avail=241401.19140625Mb
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000448
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002096
2024-08-11 13:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13681.07421875Mb; avail=241401.19140625Mb
2024-08-11 13:57:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 62
2024-08-11 13:57:15 | INFO | fairseq.trainer | begin training epoch 3
2024-08-11 13:57:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 13:57:20 | INFO | train_inner | epoch 003:      2 / 62 loss=9.667, nll_loss=7.128, ppl=139.87, wps=161.6, ups=0.03, wpb=6041, bsz=248, num_updates=126, lr=1.512e-06, gnorm=2.544, train_wall=5, gb_free=11.9, wall=441
2024-08-11 13:57:35 | INFO | train_inner | epoch 003:      4 / 62 loss=9.719, nll_loss=7.193, ppl=146.36, wps=679.9, ups=0.14, wpb=5000, bsz=120, num_updates=128, lr=1.536e-06, gnorm=2.448, train_wall=15, gb_free=10.8, wall=456
2024-08-11 13:57:40 | INFO | train_inner | epoch 003:      6 / 62 loss=9.673, nll_loss=7.138, ppl=140.82, wps=2073.5, ups=0.39, wpb=5294, bsz=188, num_updates=130, lr=1.56e-06, gnorm=2.429, train_wall=5, gb_free=11.3, wall=461
2024-08-11 13:57:45 | INFO | train_inner | epoch 003:      8 / 62 loss=9.678, nll_loss=7.133, ppl=140.36, wps=2031.7, ups=0.4, wpb=5089.5, bsz=148, num_updates=132, lr=1.584e-06, gnorm=2.259, train_wall=5, gb_free=12.9, wall=466
2024-08-11 13:57:50 | INFO | train_inner | epoch 003:     10 / 62 loss=9.467, nll_loss=6.874, ppl=117.33, wps=2063.2, ups=0.41, wpb=5092, bsz=256, num_updates=134, lr=1.608e-06, gnorm=2.555, train_wall=5, gb_free=12.2, wall=471
2024-08-11 13:57:55 | INFO | train_inner | epoch 003:     12 / 62 loss=9.562, nll_loss=7, ppl=128.01, wps=1990.8, ups=0.4, wpb=5017.5, bsz=140, num_updates=136, lr=1.632e-06, gnorm=2.229, train_wall=5, gb_free=11.5, wall=476
2024-08-11 13:57:59 | INFO | train_inner | epoch 003:     14 / 62 loss=9.575, nll_loss=7.042, ppl=131.74, wps=1774, ups=0.51, wpb=3482.5, bsz=68.5, num_updates=138, lr=1.656e-06, gnorm=3.505, train_wall=4, gb_free=17.3, wall=480
2024-08-11 13:58:03 | INFO | train_inner | epoch 003:     16 / 62 loss=9.648, nll_loss=7.104, ppl=137.56, wps=1979.9, ups=0.43, wpb=4617, bsz=144, num_updates=140, lr=1.68e-06, gnorm=2.385, train_wall=5, gb_free=12.1, wall=485
2024-08-11 13:58:08 | INFO | train_inner | epoch 003:     18 / 62 loss=9.5, nll_loss=6.934, ppl=122.27, wps=1651.5, ups=0.42, wpb=3938, bsz=88, num_updates=142, lr=1.704e-06, gnorm=2.903, train_wall=5, gb_free=11.5, wall=489
2024-08-11 13:58:13 | INFO | train_inner | epoch 003:     20 / 62 loss=9.428, nll_loss=6.851, ppl=115.43, wps=1712.9, ups=0.42, wpb=4042.5, bsz=100, num_updates=144, lr=1.728e-06, gnorm=3.081, train_wall=5, gb_free=15.9, wall=494
2024-08-11 13:58:18 | INFO | train_inner | epoch 003:     22 / 62 loss=9.314, nll_loss=6.704, ppl=104.29, wps=1583.7, ups=0.41, wpb=3821.5, bsz=184, num_updates=146, lr=1.752e-06, gnorm=2.213, train_wall=5, gb_free=12.6, wall=499
2024-08-11 13:58:23 | INFO | train_inner | epoch 003:     24 / 62 loss=9.364, nll_loss=6.778, ppl=109.73, wps=1416, ups=0.42, wpb=3363.5, bsz=104, num_updates=148, lr=1.776e-06, gnorm=3.199, train_wall=5, gb_free=12.8, wall=504
2024-08-11 13:58:28 | INFO | train_inner | epoch 003:     26 / 62 loss=9.412, nll_loss=6.836, ppl=114.21, wps=1960.5, ups=0.4, wpb=4957, bsz=152, num_updates=150, lr=1.8e-06, gnorm=2.35, train_wall=5, gb_free=12.1, wall=509
2024-08-11 13:58:33 | INFO | train_inner | epoch 003:     28 / 62 loss=9.497, nll_loss=6.939, ppl=122.74, wps=2113.3, ups=0.39, wpb=5374.5, bsz=200, num_updates=152, lr=1.824e-06, gnorm=2.29, train_wall=5, gb_free=13, wall=514
2024-08-11 13:58:37 | INFO | train_inner | epoch 003:     30 / 62 loss=9.475, nll_loss=6.912, ppl=120.45, wps=1966.1, ups=0.42, wpb=4694, bsz=120, num_updates=154, lr=1.848e-06, gnorm=2.755, train_wall=5, gb_free=13.8, wall=519
2024-08-11 13:58:43 | INFO | train_inner | epoch 003:     32 / 62 loss=9.51, nll_loss=6.966, ppl=125.02, wps=2024.5, ups=0.39, wpb=5134.5, bsz=120, num_updates=156, lr=1.872e-06, gnorm=2.077, train_wall=5, gb_free=12, wall=524
2024-08-11 13:58:48 | INFO | train_inner | epoch 003:     34 / 62 loss=9.385, nll_loss=6.796, ppl=111.13, wps=2089.7, ups=0.4, wpb=5248, bsz=212, num_updates=158, lr=1.896e-06, gnorm=2.275, train_wall=5, gb_free=13.2, wall=529
2024-08-11 13:58:52 | INFO | train_inner | epoch 003:     36 / 62 loss=9.346, nll_loss=6.751, ppl=107.72, wps=1846.3, ups=0.42, wpb=4412, bsz=140, num_updates=160, lr=1.92e-06, gnorm=2.379, train_wall=5, gb_free=14.3, wall=534
2024-08-11 13:58:57 | INFO | train_inner | epoch 003:     38 / 62 loss=9.448, nll_loss=6.878, ppl=117.58, wps=1818.2, ups=0.41, wpb=4448.5, bsz=104, num_updates=162, lr=1.944e-06, gnorm=2.12, train_wall=5, gb_free=12.3, wall=538
2024-08-11 13:59:02 | INFO | train_inner | epoch 003:     40 / 62 loss=9.264, nll_loss=6.643, ppl=99.96, wps=2044.6, ups=0.39, wpb=5200, bsz=144, num_updates=164, lr=1.968e-06, gnorm=2.156, train_wall=5, gb_free=11.1, wall=543
2024-08-11 13:59:07 | INFO | train_inner | epoch 003:     42 / 62 loss=9.341, nll_loss=6.736, ppl=106.57, wps=1807.1, ups=0.4, wpb=4523, bsz=128, num_updates=166, lr=1.992e-06, gnorm=2.494, train_wall=5, gb_free=12.3, wall=549
2024-08-11 13:59:12 | INFO | train_inner | epoch 003:     44 / 62 loss=9.266, nll_loss=6.655, ppl=100.76, wps=1672.5, ups=0.39, wpb=4280.5, bsz=72, num_updates=168, lr=2.016e-06, gnorm=2.416, train_wall=5, gb_free=11.3, wall=554
2024-08-11 13:59:18 | INFO | train_inner | epoch 003:     46 / 62 loss=9.331, nll_loss=6.717, ppl=105.18, wps=1931.3, ups=0.37, wpb=5164.5, bsz=168, num_updates=170, lr=2.04e-06, gnorm=2.084, train_wall=5, gb_free=11.9, wall=559
2024-08-11 13:59:23 | INFO | train_inner | epoch 003:     48 / 62 loss=9.335, nll_loss=6.729, ppl=106.08, wps=1897.7, ups=0.4, wpb=4762.5, bsz=164, num_updates=172, lr=2.064e-06, gnorm=2.082, train_wall=5, gb_free=12.6, wall=564
2024-08-11 13:59:28 | INFO | train_inner | epoch 003:     50 / 62 loss=9.421, nll_loss=6.838, ppl=114.39, wps=2185.4, ups=0.4, wpb=5480.5, bsz=192, num_updates=174, lr=2.088e-06, gnorm=2.142, train_wall=5, gb_free=10.8, wall=569
2024-08-11 13:59:33 | INFO | train_inner | epoch 003:     52 / 62 loss=9.257, nll_loss=6.618, ppl=98.26, wps=2065.6, ups=0.4, wpb=5179, bsz=156, num_updates=176, lr=2.112e-06, gnorm=1.987, train_wall=5, gb_free=13.7, wall=574
2024-08-11 13:59:37 | INFO | train_inner | epoch 003:     54 / 62 loss=9.314, nll_loss=6.7, ppl=103.97, wps=1704, ups=0.45, wpb=3814, bsz=128, num_updates=178, lr=2.136e-06, gnorm=2.257, train_wall=4, gb_free=15.1, wall=578
2024-08-11 13:59:43 | INFO | train_inner | epoch 003:     56 / 62 loss=9.168, nll_loss=6.521, ppl=91.85, wps=1816.6, ups=0.38, wpb=4720, bsz=180, num_updates=180, lr=2.16e-06, gnorm=2.091, train_wall=5, gb_free=11.4, wall=584
2024-08-11 13:59:48 | INFO | train_inner | epoch 003:     58 / 62 loss=9.22, nll_loss=6.588, ppl=96.19, wps=2159.4, ups=0.39, wpb=5606.5, bsz=212, num_updates=182, lr=2.184e-06, gnorm=1.812, train_wall=5, gb_free=10.6, wall=589
2024-08-11 13:59:53 | INFO | train_inner | epoch 003:     60 / 62 loss=9.358, nll_loss=6.749, ppl=107.54, wps=1985.5, ups=0.4, wpb=4985.5, bsz=148, num_updates=184, lr=2.208e-06, gnorm=1.898, train_wall=5, gb_free=11, wall=594
2024-08-11 13:59:57 | INFO | train_inner | epoch 003:     62 / 62 loss=9.109, nll_loss=6.456, ppl=87.81, wps=2017.7, ups=0.48, wpb=4244, bsz=140, num_updates=186, lr=2.232e-06, gnorm=2.039, train_wall=4, gb_free=15.4, wall=598
2024-08-11 13:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7855.01953125Mb; avail=247227.203125Mb
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000513
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7855.01953125Mb; avail=247227.203125Mb
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004461
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7855.01953125Mb; avail=247227.203125Mb
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003951
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009256
2024-08-11 13:59:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7855.01953125Mb; avail=247227.203125Mb
2024-08-11 14:00:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.996 | nll_loss 6.212 | ppl 74.15 | wps 4775.4 | wpb 2061.8 | bsz 64.9 | num_updates 186 | best_loss 8.996
2024-08-11 14:00:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 186 updates
2024-08-11 14:00:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 14:00:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 14:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 3 @ 186 updates, score 8.996) (writing took 57.119534271769226 seconds)
2024-08-11 14:01:02 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-08-11 14:01:02 | INFO | train | epoch 003 | loss 9.425 | nll_loss 6.84 | ppl 114.59 | wps 1297 | ups 0.27 | wpb 4742.8 | bsz 150.6 | num_updates 186 | lr 2.232e-06 | gnorm 2.369 | train_wall 162 | gb_free 15.4 | wall 663
2024-08-11 14:01:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 14:01:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 14:01:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 14:01:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000871
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12925.69140625Mb; avail=242156.48828125Mb
2024-08-11 14:01:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000148
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001382
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12925.69140625Mb; avail=242156.48828125Mb
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000060
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12925.69140625Mb; avail=242156.48828125Mb
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000482
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002312
2024-08-11 14:01:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12925.69140625Mb; avail=242156.48828125Mb
2024-08-11 14:01:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 62
2024-08-11 14:01:02 | INFO | fairseq.trainer | begin training epoch 4
2024-08-11 14:01:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 14:01:17 | INFO | train_inner | epoch 004:      2 / 62 loss=9.082, nll_loss=6.425, ppl=85.92, wps=124.5, ups=0.02, wpb=4982.5, bsz=136, num_updates=188, lr=2.256e-06, gnorm=1.992, train_wall=15, gb_free=11.6, wall=678
2024-08-11 14:01:21 | INFO | train_inner | epoch 004:      4 / 62 loss=9.044, nll_loss=6.388, ppl=83.77, wps=1482, ups=0.45, wpb=3311.5, bsz=152, num_updates=190, lr=2.28e-06, gnorm=2.395, train_wall=4, gb_free=14.2, wall=683
2024-08-11 14:01:26 | INFO | train_inner | epoch 004:      6 / 62 loss=9.153, nll_loss=6.507, ppl=90.94, wps=2333.2, ups=0.42, wpb=5558.5, bsz=236, num_updates=192, lr=2.304e-06, gnorm=2.033, train_wall=5, gb_free=13.8, wall=687
2024-08-11 14:01:31 | INFO | train_inner | epoch 004:      8 / 62 loss=9.078, nll_loss=6.424, ppl=85.87, wps=1984.9, ups=0.41, wpb=4857.5, bsz=144, num_updates=194, lr=2.328e-06, gnorm=2.139, train_wall=5, gb_free=12.2, wall=692
2024-08-11 14:01:36 | INFO | train_inner | epoch 004:     10 / 62 loss=9.109, nll_loss=6.456, ppl=87.81, wps=2139.6, ups=0.39, wpb=5512.5, bsz=184, num_updates=196, lr=2.352e-06, gnorm=1.742, train_wall=5, gb_free=13.1, wall=697
2024-08-11 14:01:41 | INFO | train_inner | epoch 004:     12 / 62 loss=8.931, nll_loss=6.235, ppl=75.32, wps=1659.9, ups=0.42, wpb=3960.5, bsz=76, num_updates=198, lr=2.376e-06, gnorm=2.391, train_wall=5, gb_free=11.5, wall=702
2024-08-11 14:01:46 | INFO | train_inner | epoch 004:     14 / 62 loss=8.985, nll_loss=6.296, ppl=78.57, wps=1931.4, ups=0.39, wpb=4953, bsz=172, num_updates=200, lr=2.4e-06, gnorm=1.884, train_wall=5, gb_free=11.2, wall=707
2024-08-11 14:01:51 | INFO | train_inner | epoch 004:     16 / 62 loss=9.164, nll_loss=6.523, ppl=91.94, wps=2139.6, ups=0.4, wpb=5331.5, bsz=152, num_updates=202, lr=2.424e-06, gnorm=1.863, train_wall=5, gb_free=12.1, wall=712
2024-08-11 14:01:56 | INFO | train_inner | epoch 004:     18 / 62 loss=9.14, nll_loss=6.488, ppl=89.77, wps=2090.5, ups=0.41, wpb=5061, bsz=124, num_updates=204, lr=2.448e-06, gnorm=2.078, train_wall=5, gb_free=13.5, wall=717
2024-08-11 14:02:01 | INFO | train_inner | epoch 004:     20 / 62 loss=9.03, nll_loss=6.352, ppl=81.69, wps=1962.4, ups=0.38, wpb=5146.5, bsz=200, num_updates=206, lr=2.472e-06, gnorm=1.85, train_wall=5, gb_free=11.4, wall=722
2024-08-11 14:02:06 | INFO | train_inner | epoch 004:     22 / 62 loss=9.078, nll_loss=6.411, ppl=85.07, wps=2313.1, ups=0.39, wpb=5966.5, bsz=216, num_updates=208, lr=2.496e-06, gnorm=1.883, train_wall=5, gb_free=11.6, wall=728
2024-08-11 14:02:11 | INFO | train_inner | epoch 004:     24 / 62 loss=9.095, nll_loss=6.437, ppl=86.65, wps=1816.4, ups=0.43, wpb=4224, bsz=96, num_updates=210, lr=2.52e-06, gnorm=2.092, train_wall=5, gb_free=11.8, wall=732
2024-08-11 14:02:16 | INFO | train_inner | epoch 004:     26 / 62 loss=9.077, nll_loss=6.414, ppl=85.25, wps=1788, ups=0.41, wpb=4406.5, bsz=136, num_updates=212, lr=2.544e-06, gnorm=2.068, train_wall=5, gb_free=11.4, wall=737
2024-08-11 14:02:21 | INFO | train_inner | epoch 004:     28 / 62 loss=9.121, nll_loss=6.461, ppl=88.12, wps=2278.5, ups=0.42, wpb=5478.5, bsz=180, num_updates=214, lr=2.568e-06, gnorm=2.038, train_wall=5, gb_free=13, wall=742
2024-08-11 14:02:25 | INFO | train_inner | epoch 004:     30 / 62 loss=9.146, nll_loss=6.501, ppl=90.6, wps=1786.8, ups=0.43, wpb=4202.5, bsz=108, num_updates=216, lr=2.592e-06, gnorm=1.989, train_wall=5, gb_free=15.2, wall=747
2024-08-11 14:02:30 | INFO | train_inner | epoch 004:     32 / 62 loss=9.017, nll_loss=6.319, ppl=79.86, wps=1987.9, ups=0.42, wpb=4774.5, bsz=140, num_updates=218, lr=2.616e-06, gnorm=1.978, train_wall=5, gb_free=13.1, wall=751
2024-08-11 14:02:34 | INFO | train_inner | epoch 004:     34 / 62 loss=8.96, nll_loss=6.27, ppl=77.17, wps=1559.2, ups=0.5, wpb=3136.5, bsz=68.5, num_updates=220, lr=2.64e-06, gnorm=2.672, train_wall=4, gb_free=15.7, wall=755
2024-08-11 14:02:39 | INFO | train_inner | epoch 004:     36 / 62 loss=9.114, nll_loss=6.453, ppl=87.61, wps=2388.9, ups=0.39, wpb=6197, bsz=208, num_updates=222, lr=2.664e-06, gnorm=2.075, train_wall=5, gb_free=12.6, wall=761
2024-08-11 14:02:45 | INFO | train_inner | epoch 004:     38 / 62 loss=9.109, nll_loss=6.438, ppl=86.68, wps=1804.2, ups=0.4, wpb=4538, bsz=140, num_updates=224, lr=2.688e-06, gnorm=2.025, train_wall=5, gb_free=13.1, wall=766
2024-08-11 14:02:49 | INFO | train_inner | epoch 004:     40 / 62 loss=8.952, nll_loss=6.258, ppl=76.51, wps=1967.3, ups=0.41, wpb=4852, bsz=120, num_updates=226, lr=2.712e-06, gnorm=1.79, train_wall=5, gb_free=11.7, wall=771
2024-08-11 14:02:54 | INFO | train_inner | epoch 004:     42 / 62 loss=9.046, nll_loss=6.373, ppl=82.87, wps=2131.5, ups=0.4, wpb=5287.5, bsz=152, num_updates=228, lr=2.736e-06, gnorm=1.758, train_wall=5, gb_free=12.9, wall=776
2024-08-11 14:02:59 | INFO | train_inner | epoch 004:     44 / 62 loss=8.798, nll_loss=6.061, ppl=66.78, wps=1685.7, ups=0.39, wpb=4268, bsz=240, num_updates=230, lr=2.76e-06, gnorm=2.116, train_wall=5, gb_free=12.1, wall=781
2024-08-11 14:03:04 | INFO | train_inner | epoch 004:     46 / 62 loss=8.848, nll_loss=6.131, ppl=70.06, wps=1871.1, ups=0.4, wpb=4657.5, bsz=124, num_updates=232, lr=2.784e-06, gnorm=2.026, train_wall=5, gb_free=12.8, wall=786
2024-08-11 14:03:10 | INFO | train_inner | epoch 004:     48 / 62 loss=8.805, nll_loss=6.072, ppl=67.29, wps=1979.3, ups=0.37, wpb=5322, bsz=188, num_updates=234, lr=2.808e-06, gnorm=1.784, train_wall=5, gb_free=11.6, wall=791
2024-08-11 14:03:15 | INFO | train_inner | epoch 004:     50 / 62 loss=9.021, nll_loss=6.341, ppl=81.04, wps=1698.6, ups=0.41, wpb=4119, bsz=124, num_updates=236, lr=2.832e-06, gnorm=2.004, train_wall=5, gb_free=12.9, wall=796
2024-08-11 14:03:20 | INFO | train_inner | epoch 004:     52 / 62 loss=8.874, nll_loss=6.161, ppl=71.57, wps=1708.2, ups=0.41, wpb=4201, bsz=136, num_updates=238, lr=2.856e-06, gnorm=2.155, train_wall=5, gb_free=10.9, wall=801
2024-08-11 14:03:25 | INFO | train_inner | epoch 004:     54 / 62 loss=8.851, nll_loss=6.131, ppl=70.1, wps=1970.7, ups=0.4, wpb=4917, bsz=216, num_updates=240, lr=2.88e-06, gnorm=1.792, train_wall=5, gb_free=13.1, wall=806
2024-08-11 14:03:30 | INFO | train_inner | epoch 004:     56 / 62 loss=8.844, nll_loss=6.118, ppl=69.47, wps=1793.1, ups=0.39, wpb=4546, bsz=132, num_updates=242, lr=2.904e-06, gnorm=1.925, train_wall=5, gb_free=11.1, wall=811
2024-08-11 14:03:35 | INFO | train_inner | epoch 004:     58 / 62 loss=8.805, nll_loss=6.082, ppl=67.76, wps=1737.9, ups=0.41, wpb=4266, bsz=152, num_updates=244, lr=2.928e-06, gnorm=2.191, train_wall=5, gb_free=11.8, wall=816
2024-08-11 14:03:39 | INFO | train_inner | epoch 004:     60 / 62 loss=9.001, nll_loss=6.313, ppl=79.49, wps=1927, ups=0.42, wpb=4635.5, bsz=92, num_updates=246, lr=2.952e-06, gnorm=1.822, train_wall=5, gb_free=14.3, wall=821
2024-08-11 14:03:43 | INFO | train_inner | epoch 004:     62 / 62 loss=8.877, nll_loss=6.166, ppl=71.8, wps=2212.5, ups=0.51, wpb=4357, bsz=124, num_updates=248, lr=2.976e-06, gnorm=2.035, train_wall=4, gb_free=15.5, wall=824
2024-08-11 14:03:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9663.17578125Mb; avail=245419.08984375Mb
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000519
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9663.17578125Mb; avail=245419.08984375Mb
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004444
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9663.17578125Mb; avail=245419.08984375Mb
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004001
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009361
2024-08-11 14:03:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9663.17578125Mb; avail=245419.08984375Mb
2024-08-11 14:03:51 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.649 | nll_loss 5.75 | ppl 53.83 | wps 4765.4 | wpb 2061.8 | bsz 64.9 | num_updates 248 | best_loss 8.649
2024-08-11 14:03:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 248 updates
2024-08-11 14:03:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 14:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 14:04:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 4 @ 248 updates, score 8.649) (writing took 62.125058511737734 seconds)
2024-08-11 14:04:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-08-11 14:04:53 | INFO | train | epoch 004 | loss 9.011 | nll_loss 6.329 | ppl 80.42 | wps 1270.7 | ups 0.27 | wpb 4742.8 | bsz 150.6 | num_updates 248 | lr 2.976e-06 | gnorm 2.019 | train_wall 161 | gb_free 15.5 | wall 894
2024-08-11 14:04:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 14:04:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 14:04:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 14:04:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000742
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12858.1171875Mb; avail=242224.15234375Mb
2024-08-11 14:04:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000140
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001274
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12858.1171875Mb; avail=242224.15234375Mb
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12858.1171875Mb; avail=242224.15234375Mb
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000446
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002076
2024-08-11 14:04:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12858.1171875Mb; avail=242224.15234375Mb
2024-08-11 14:04:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 62
2024-08-11 14:04:53 | INFO | fairseq.trainer | begin training epoch 5
2024-08-11 14:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 14:04:58 | INFO | train_inner | epoch 005:      2 / 62 loss=8.805, nll_loss=6.054, ppl=66.45, wps=133.8, ups=0.03, wpb=4992, bsz=140, num_updates=250, lr=3e-06, gnorm=2.215, train_wall=5, gb_free=12.3, wall=899
2024-08-11 14:05:03 | INFO | train_inner | epoch 005:      4 / 62 loss=8.835, nll_loss=6.114, ppl=69.27, wps=2037, ups=0.4, wpb=5110, bsz=160, num_updates=252, lr=3.024e-06, gnorm=1.947, train_wall=5, gb_free=12.5, wall=904
2024-08-11 14:05:08 | INFO | train_inner | epoch 005:      6 / 62 loss=8.826, nll_loss=6.097, ppl=68.46, wps=1711.9, ups=0.41, wpb=4182.5, bsz=156, num_updates=254, lr=3.048e-06, gnorm=2.315, train_wall=5, gb_free=11.8, wall=909
2024-08-11 14:05:13 | INFO | train_inner | epoch 005:      8 / 62 loss=8.751, nll_loss=5.993, ppl=63.68, wps=1970.3, ups=0.39, wpb=5101.5, bsz=172, num_updates=256, lr=3.072e-06, gnorm=2.235, train_wall=5, gb_free=10.9, wall=914
2024-08-11 14:05:18 | INFO | train_inner | epoch 005:     10 / 62 loss=8.833, nll_loss=6.106, ppl=68.87, wps=1939.4, ups=0.42, wpb=4652, bsz=136, num_updates=258, lr=3.096e-06, gnorm=1.871, train_wall=5, gb_free=14.4, wall=919
2024-08-11 14:05:23 | INFO | train_inner | epoch 005:     12 / 62 loss=8.854, nll_loss=6.142, ppl=70.63, wps=1922.8, ups=0.41, wpb=4636.5, bsz=168, num_updates=260, lr=3.12e-06, gnorm=1.868, train_wall=5, gb_free=12.2, wall=924
2024-08-11 14:05:27 | INFO | train_inner | epoch 005:     14 / 62 loss=8.869, nll_loss=6.154, ppl=71.2, wps=1678.9, ups=0.42, wpb=4003, bsz=100, num_updates=262, lr=3.144e-06, gnorm=2.095, train_wall=5, gb_free=11.5, wall=929
2024-08-11 14:05:32 | INFO | train_inner | epoch 005:     16 / 62 loss=8.903, nll_loss=6.197, ppl=73.35, wps=1872.2, ups=0.4, wpb=4669.5, bsz=168, num_updates=264, lr=3.168e-06, gnorm=1.728, train_wall=5, gb_free=10.8, wall=934
2024-08-11 14:05:38 | INFO | train_inner | epoch 005:     18 / 62 loss=8.833, nll_loss=6.118, ppl=69.48, wps=2326.7, ups=0.39, wpb=5954, bsz=244, num_updates=266, lr=3.192e-06, gnorm=1.863, train_wall=5, gb_free=13.5, wall=939
2024-08-11 14:05:43 | INFO | train_inner | epoch 005:     20 / 62 loss=8.678, nll_loss=5.911, ppl=60.16, wps=2014.8, ups=0.4, wpb=5059.5, bsz=160, num_updates=268, lr=3.216e-06, gnorm=1.789, train_wall=5, gb_free=13.7, wall=944
2024-08-11 14:05:48 | INFO | train_inner | epoch 005:     22 / 62 loss=8.878, nll_loss=6.163, ppl=71.65, wps=2058.6, ups=0.4, wpb=5151, bsz=152, num_updates=270, lr=3.24e-06, gnorm=1.742, train_wall=5, gb_free=12.9, wall=949
2024-08-11 14:05:52 | INFO | train_inner | epoch 005:     24 / 62 loss=8.566, nll_loss=5.761, ppl=54.23, wps=1473.5, ups=0.42, wpb=3533, bsz=84, num_updates=272, lr=3.264e-06, gnorm=2.33, train_wall=5, gb_free=14.2, wall=953
2024-08-11 14:05:58 | INFO | train_inner | epoch 005:     26 / 62 loss=8.731, nll_loss=5.974, ppl=62.85, wps=2306.7, ups=0.37, wpb=6188, bsz=260, num_updates=274, lr=3.288e-06, gnorm=1.62, train_wall=5, gb_free=12.1, wall=959
2024-08-11 14:06:03 | INFO | train_inner | epoch 005:     28 / 62 loss=8.757, nll_loss=6.006, ppl=64.29, wps=2154.1, ups=0.4, wpb=5427, bsz=172, num_updates=276, lr=3.312e-06, gnorm=1.646, train_wall=5, gb_free=13.7, wall=964
2024-08-11 14:06:08 | INFO | train_inner | epoch 005:     30 / 62 loss=8.451, nll_loss=5.617, ppl=49.07, wps=1523.8, ups=0.41, wpb=3696, bsz=76, num_updates=278, lr=3.336e-06, gnorm=2.321, train_wall=5, gb_free=12.3, wall=969
2024-08-11 14:06:13 | INFO | train_inner | epoch 005:     32 / 62 loss=8.613, nll_loss=5.832, ppl=56.97, wps=1697.9, ups=0.4, wpb=4250.5, bsz=116, num_updates=280, lr=3.36e-06, gnorm=2.105, train_wall=5, gb_free=13, wall=974
2024-08-11 14:06:17 | INFO | train_inner | epoch 005:     34 / 62 loss=8.705, nll_loss=5.945, ppl=61.61, wps=2078.7, ups=0.43, wpb=4786.5, bsz=136, num_updates=282, lr=3.384e-06, gnorm=1.984, train_wall=5, gb_free=12.9, wall=978
2024-08-11 14:06:22 | INFO | train_inner | epoch 005:     36 / 62 loss=8.604, nll_loss=5.815, ppl=56.31, wps=1844.9, ups=0.41, wpb=4535.5, bsz=148, num_updates=284, lr=3.408e-06, gnorm=1.78, train_wall=5, gb_free=13.4, wall=983
2024-08-11 14:06:27 | INFO | train_inner | epoch 005:     38 / 62 loss=8.731, nll_loss=5.965, ppl=62.48, wps=1944.2, ups=0.41, wpb=4702.5, bsz=92, num_updates=286, lr=3.432e-06, gnorm=1.857, train_wall=5, gb_free=14, wall=988
2024-08-11 14:06:32 | INFO | train_inner | epoch 005:     40 / 62 loss=8.733, nll_loss=5.982, ppl=63.2, wps=1845.4, ups=0.41, wpb=4509.5, bsz=128, num_updates=288, lr=3.456e-06, gnorm=2.197, train_wall=5, gb_free=10.5, wall=993
2024-08-11 14:06:37 | INFO | train_inner | epoch 005:     42 / 62 loss=8.71, nll_loss=5.95, ppl=61.82, wps=1831.2, ups=0.41, wpb=4436, bsz=144, num_updates=290, lr=3.48e-06, gnorm=1.816, train_wall=5, gb_free=12.9, wall=998
2024-08-11 14:06:42 | INFO | train_inner | epoch 005:     44 / 62 loss=8.677, nll_loss=5.903, ppl=59.85, wps=1905.5, ups=0.41, wpb=4647, bsz=120, num_updates=292, lr=3.504e-06, gnorm=2.11, train_wall=5, gb_free=11.1, wall=1003
2024-08-11 14:06:47 | INFO | train_inner | epoch 005:     46 / 62 loss=8.634, nll_loss=5.858, ppl=57.99, wps=2065, ups=0.4, wpb=5116, bsz=180, num_updates=294, lr=3.528e-06, gnorm=1.688, train_wall=5, gb_free=12.2, wall=1008
2024-08-11 14:06:52 | INFO | train_inner | epoch 005:     48 / 62 loss=8.672, nll_loss=5.898, ppl=59.61, wps=2162.3, ups=0.39, wpb=5480.5, bsz=156, num_updates=296, lr=3.552e-06, gnorm=1.684, train_wall=5, gb_free=12.9, wall=1013
2024-08-11 14:06:56 | INFO | train_inner | epoch 005:     50 / 62 loss=8.43, nll_loss=5.595, ppl=48.34, wps=1699.4, ups=0.43, wpb=3946.5, bsz=176, num_updates=298, lr=3.576e-06, gnorm=2.097, train_wall=5, gb_free=12.9, wall=1017
2024-08-11 14:07:01 | INFO | train_inner | epoch 005:     52 / 62 loss=8.44, nll_loss=5.607, ppl=48.74, wps=1745, ups=0.39, wpb=4517.5, bsz=136, num_updates=300, lr=3.6e-06, gnorm=2.01, train_wall=5, gb_free=12.6, wall=1023
2024-08-11 14:07:07 | INFO | train_inner | epoch 005:     54 / 62 loss=8.612, nll_loss=5.818, ppl=56.41, wps=2227.1, ups=0.38, wpb=5920.5, bsz=188, num_updates=302, lr=3.624e-06, gnorm=1.628, train_wall=5, gb_free=12.8, wall=1028
2024-08-11 14:07:11 | INFO | train_inner | epoch 005:     56 / 62 loss=8.54, nll_loss=5.737, ppl=53.34, wps=1885.2, ups=0.48, wpb=3910, bsz=136.5, num_updates=304, lr=3.648e-06, gnorm=2.534, train_wall=4, gb_free=11.4, wall=1032
2024-08-11 14:07:16 | INFO | train_inner | epoch 005:     58 / 62 loss=8.551, nll_loss=5.744, ppl=53.59, wps=1956.5, ups=0.4, wpb=4878, bsz=120, num_updates=306, lr=3.672e-06, gnorm=1.855, train_wall=5, gb_free=11.9, wall=1037
2024-08-11 14:07:21 | INFO | train_inner | epoch 005:     60 / 62 loss=8.573, nll_loss=5.786, ppl=55.19, wps=2012.2, ups=0.38, wpb=5294, bsz=248, num_updates=308, lr=3.696e-06, gnorm=1.81, train_wall=5, gb_free=12.6, wall=1042
2024-08-11 14:07:25 | INFO | train_inner | epoch 005:     62 / 62 loss=8.523, nll_loss=5.709, ppl=52.31, wps=1907.9, ups=0.51, wpb=3741.5, bsz=96, num_updates=310, lr=3.72e-06, gnorm=1.893, train_wall=4, gb_free=15.6, wall=1046
2024-08-11 14:07:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12902.58203125Mb; avail=242179.64453125Mb
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12903.07421875Mb; avail=242179.15234375Mb
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004505
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12903.07421875Mb; avail=242179.15234375Mb
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004048
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009519
2024-08-11 14:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12903.07421875Mb; avail=242179.15234375Mb
2024-08-11 14:07:33 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.35 | nll_loss 5.37 | ppl 41.35 | wps 4764.3 | wpb 2061.8 | bsz 64.9 | num_updates 310 | best_loss 8.35
2024-08-11 14:07:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 310 updates
2024-08-11 14:07:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 14:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 14:08:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 5 @ 310 updates, score 8.35) (writing took 67.40617837291211 seconds)
2024-08-11 14:08:40 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-08-11 14:08:40 | INFO | train | epoch 005 | loss 8.694 | nll_loss 5.93 | ppl 60.95 | wps 1295.4 | ups 0.27 | wpb 4742.8 | bsz 150.6 | num_updates 310 | lr 3.72e-06 | gnorm 1.956 | train_wall 152 | gb_free 15.6 | wall 1121
2024-08-11 14:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 14:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 14:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 14:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000747
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=11127.5Mb; avail=243954.7578125Mb
2024-08-11 14:08:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000193
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001284
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11127.5Mb; avail=243954.7578125Mb
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11127.5Mb; avail=243954.7578125Mb
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000439
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002082
2024-08-11 14:08:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11127.5Mb; avail=243954.7578125Mb
2024-08-11 14:08:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 62
2024-08-11 14:08:40 | INFO | fairseq.trainer | begin training epoch 6
2024-08-11 14:08:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 14:08:45 | INFO | train_inner | epoch 006:      2 / 62 loss=8.511, nll_loss=5.707, ppl=52.25, wps=126.6, ups=0.02, wpb=5071, bsz=180, num_updates=312, lr=3.744e-06, gnorm=1.662, train_wall=5, gb_free=13.7, wall=1126
2024-08-11 14:08:50 | INFO | train_inner | epoch 006:      4 / 62 loss=8.502, nll_loss=5.704, ppl=52.13, wps=2478.3, ups=0.41, wpb=6100, bsz=288, num_updates=314, lr=3.768e-06, gnorm=1.73, train_wall=5, gb_free=14, wall=1131
2024-08-11 14:08:55 | INFO | train_inner | epoch 006:      6 / 62 loss=8.573, nll_loss=5.787, ppl=55.2, wps=2227.8, ups=0.4, wpb=5534, bsz=164, num_updates=316, lr=3.792e-06, gnorm=1.727, train_wall=5, gb_free=12.2, wall=1136
2024-08-11 14:09:00 | INFO | train_inner | epoch 006:      8 / 62 loss=8.575, nll_loss=5.78, ppl=54.97, wps=1817.1, ups=0.42, wpb=4298.5, bsz=136, num_updates=318, lr=3.816e-06, gnorm=1.959, train_wall=5, gb_free=14.5, wall=1141
2024-08-11 14:09:05 | INFO | train_inner | epoch 006:     10 / 62 loss=8.492, nll_loss=5.682, ppl=51.33, wps=2102, ups=0.4, wpb=5208, bsz=188, num_updates=320, lr=3.84e-06, gnorm=1.84, train_wall=5, gb_free=12.7, wall=1146
2024-08-11 14:09:10 | INFO | train_inner | epoch 006:     12 / 62 loss=8.433, nll_loss=5.571, ppl=47.53, wps=1466.8, ups=0.41, wpb=3598, bsz=68, num_updates=322, lr=3.864e-06, gnorm=2.454, train_wall=5, gb_free=12.6, wall=1151
2024-08-11 14:09:14 | INFO | train_inner | epoch 006:     14 / 62 loss=8.557, nll_loss=5.747, ppl=53.71, wps=2044.1, ups=0.48, wpb=4293.5, bsz=112.5, num_updates=324, lr=3.888e-06, gnorm=1.937, train_wall=4, gb_free=12.1, wall=1155
2024-08-11 14:09:19 | INFO | train_inner | epoch 006:     16 / 62 loss=8.255, nll_loss=5.366, ppl=41.23, wps=1709.7, ups=0.39, wpb=4418, bsz=144, num_updates=326, lr=3.912e-06, gnorm=2.334, train_wall=5, gb_free=11.9, wall=1160
2024-08-11 14:09:24 | INFO | train_inner | epoch 006:     18 / 62 loss=8.634, nll_loss=5.858, ppl=58, wps=2154, ups=0.39, wpb=5567, bsz=160, num_updates=328, lr=3.936e-06, gnorm=1.769, train_wall=5, gb_free=11.8, wall=1165
2024-08-11 14:09:29 | INFO | train_inner | epoch 006:     20 / 62 loss=8.42, nll_loss=5.578, ppl=47.78, wps=1648.2, ups=0.41, wpb=4002.5, bsz=84, num_updates=330, lr=3.96e-06, gnorm=2.306, train_wall=5, gb_free=13.5, wall=1170
2024-08-11 14:09:34 | INFO | train_inner | epoch 006:     22 / 62 loss=8.546, nll_loss=5.741, ppl=53.49, wps=1880.3, ups=0.4, wpb=4727, bsz=116, num_updates=332, lr=3.984e-06, gnorm=1.886, train_wall=5, gb_free=11.8, wall=1175
2024-08-11 14:09:39 | INFO | train_inner | epoch 006:     24 / 62 loss=8.502, nll_loss=5.686, ppl=51.49, wps=1861.3, ups=0.41, wpb=4574.5, bsz=96, num_updates=334, lr=4.008e-06, gnorm=1.949, train_wall=5, gb_free=12.5, wall=1180
2024-08-11 14:09:44 | INFO | train_inner | epoch 006:     26 / 62 loss=8.523, nll_loss=5.702, ppl=52.05, wps=2162.4, ups=0.41, wpb=5317, bsz=152, num_updates=336, lr=4.032e-06, gnorm=1.79, train_wall=5, gb_free=13.5, wall=1185
2024-08-11 14:09:49 | INFO | train_inner | epoch 006:     28 / 62 loss=8.492, nll_loss=5.664, ppl=50.71, wps=1878, ups=0.4, wpb=4718.5, bsz=152, num_updates=338, lr=4.056e-06, gnorm=1.997, train_wall=5, gb_free=11.7, wall=1190
2024-08-11 14:09:54 | INFO | train_inner | epoch 006:     30 / 62 loss=8.55, nll_loss=5.743, ppl=53.56, wps=1807.8, ups=0.4, wpb=4552.5, bsz=100, num_updates=340, lr=4.08e-06, gnorm=1.937, train_wall=5, gb_free=11.6, wall=1195
2024-08-11 14:09:59 | INFO | train_inner | epoch 006:     32 / 62 loss=8.537, nll_loss=5.729, ppl=53.04, wps=1927.6, ups=0.41, wpb=4756, bsz=116, num_updates=342, lr=4.104e-06, gnorm=1.803, train_wall=5, gb_free=13, wall=1200
2024-08-11 14:10:04 | INFO | train_inner | epoch 006:     34 / 62 loss=8.344, nll_loss=5.486, ppl=44.83, wps=2001.4, ups=0.38, wpb=5289.5, bsz=176, num_updates=344, lr=4.128e-06, gnorm=1.62, train_wall=5, gb_free=11.2, wall=1205
2024-08-11 14:10:09 | INFO | train_inner | epoch 006:     36 / 62 loss=8.528, nll_loss=5.717, ppl=52.6, wps=1866.4, ups=0.4, wpb=4620.5, bsz=132, num_updates=346, lr=4.152e-06, gnorm=1.867, train_wall=5, gb_free=11.2, wall=1210
2024-08-11 14:10:14 | INFO | train_inner | epoch 006:     38 / 62 loss=8.369, nll_loss=5.527, ppl=46.1, wps=1875.3, ups=0.4, wpb=4680, bsz=176, num_updates=348, lr=4.176e-06, gnorm=1.932, train_wall=5, gb_free=12.5, wall=1215
2024-08-11 14:10:19 | INFO | train_inner | epoch 006:     40 / 62 loss=8.241, nll_loss=5.363, ppl=41.17, wps=1937.5, ups=0.41, wpb=4770, bsz=200, num_updates=350, lr=4.2e-06, gnorm=1.705, train_wall=5, gb_free=12.8, wall=1220
2024-08-11 14:10:24 | INFO | train_inner | epoch 006:     42 / 62 loss=8.333, nll_loss=5.484, ppl=44.76, wps=1885.7, ups=0.42, wpb=4458.5, bsz=188, num_updates=352, lr=4.224e-06, gnorm=1.87, train_wall=5, gb_free=14, wall=1225
2024-08-11 14:10:29 | INFO | train_inner | epoch 006:     44 / 62 loss=8.331, nll_loss=5.463, ppl=44.12, wps=1818.7, ups=0.41, wpb=4402, bsz=116, num_updates=354, lr=4.248e-06, gnorm=1.987, train_wall=5, gb_free=12.4, wall=1230
2024-08-11 14:10:33 | INFO | train_inner | epoch 006:     46 / 62 loss=8.301, nll_loss=5.438, ppl=43.36, wps=1536, ups=0.42, wpb=3628, bsz=172, num_updates=356, lr=4.272e-06, gnorm=2.07, train_wall=5, gb_free=12.1, wall=1235
2024-08-11 14:10:38 | INFO | train_inner | epoch 006:     48 / 62 loss=8.302, nll_loss=5.447, ppl=43.63, wps=2202.3, ups=0.4, wpb=5546, bsz=212, num_updates=358, lr=4.296e-06, gnorm=1.623, train_wall=5, gb_free=11.2, wall=1240
2024-08-11 14:10:43 | INFO | train_inner | epoch 006:     50 / 62 loss=8.387, nll_loss=5.545, ppl=46.7, wps=2070.8, ups=0.4, wpb=5237.5, bsz=164, num_updates=360, lr=4.32e-06, gnorm=1.726, train_wall=5, gb_free=14, wall=1245
2024-08-11 14:10:48 | INFO | train_inner | epoch 006:     52 / 62 loss=8.405, nll_loss=5.56, ppl=47.19, wps=2068.2, ups=0.4, wpb=5148.5, bsz=140, num_updates=362, lr=4.344e-06, gnorm=1.84, train_wall=5, gb_free=12.3, wall=1250
2024-08-11 14:10:53 | INFO | train_inner | epoch 006:     54 / 62 loss=8.401, nll_loss=5.551, ppl=46.89, wps=1670.2, ups=0.42, wpb=3943, bsz=80, num_updates=364, lr=4.368e-06, gnorm=2.357, train_wall=5, gb_free=13, wall=1254
2024-08-11 14:10:58 | INFO | train_inner | epoch 006:     56 / 62 loss=8.426, nll_loss=5.589, ppl=48.14, wps=1682.2, ups=0.45, wpb=3776.5, bsz=100, num_updates=366, lr=4.392e-06, gnorm=2.024, train_wall=4, gb_free=12.7, wall=1259
2024-08-11 14:11:03 | INFO | train_inner | epoch 006:     58 / 62 loss=8.31, nll_loss=5.443, ppl=43.5, wps=1969.8, ups=0.38, wpb=5164.5, bsz=196, num_updates=368, lr=4.416e-06, gnorm=2.051, train_wall=5, gb_free=13, wall=1264
2024-08-11 14:11:08 | INFO | train_inner | epoch 006:     60 / 62 loss=8.354, nll_loss=5.498, ppl=45.18, wps=2369.2, ups=0.39, wpb=6095, bsz=216, num_updates=370, lr=4.44e-06, gnorm=1.612, train_wall=5, gb_free=12.7, wall=1269
2024-08-11 16:08:47 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-08-11 16:08:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-08-11 16:08:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-08-11 16:08:47 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-08-11 16:08:55 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-08-11 16:08:55 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-08-11 16:08:55 | INFO | fairseq_cli.train | model: TransformerModel
2024-08-11 16:08:55 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-08-11 16:08:55 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-08-11 16:08:55 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-08-11 16:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-08-11 16:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14337.0625Mb; avail=240745.21484375Mb
2024-08-11 16:08:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-08-11 16:08:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:mr-hi': 1}
2024-08-11 16:08:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-08-11 16:08:55 | INFO | fairseq.data.data_utils | loaded 1,168 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.mr-hi.mr
2024-08-11 16:08:55 | INFO | fairseq.data.data_utils | loaded 1,168 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.mr-hi.hi
2024-08-11 16:08:55 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin valid mr-hi 1168 examples
2024-08-11 16:54:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-08-11 16:54:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-08-11 16:54:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-08-11 16:54:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-08-11 16:54:52 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-08-11 16:54:52 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-08-11 16:54:52 | INFO | fairseq_cli.train | model: TransformerModel
2024-08-11 16:54:52 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-08-11 16:54:52 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-08-11 16:54:52 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-08-11 16:54:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-08-11 16:54:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18389.1171875Mb; avail=236693.125Mb
2024-08-11 16:54:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-08-11 16:54:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:mr-hi': 1}
2024-08-11 16:54:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-08-11 16:54:52 | INFO | fairseq.data.data_utils | loaded 1,168 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.mr-hi.mr
2024-08-11 16:54:52 | INFO | fairseq.data.data_utils | loaded 1,168 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.mr-hi.hi
2024-08-11 16:54:52 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin valid mr-hi 1168 examples
2024-08-11 16:54:53 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-08-11 16:54:53 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-08-11 16:54:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-08-11 16:54:53 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-08-11 16:54:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-08-11 16:54:53 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-08-11 16:54:53 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-08-11 16:54:53 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/1.2B_last_checkpoint.pt: optimizer, lr scheduler, meters, dataloader will be reset
2024-08-11 16:54:53 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/1.2B_last_checkpoint.pt
2024-08-11 16:54:59 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-08-11 16:54:59 | INFO | fairseq.trainer | Loaded checkpoint /home/krish/content/1.2B_last_checkpoint.pt (epoch 81 @ 0 updates)
2024-08-11 16:55:00 | INFO | fairseq.trainer | loading train data for epoch 1
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14767.59765625Mb; avail=240306.6015625Mb
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:mr-hi': 1}
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-08-11 16:55:00 | INFO | fairseq.data.data_utils | loaded 9,337 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/train.mr-hi.mr
2024-08-11 16:55:00 | INFO | fairseq.data.data_utils | loaded 9,337 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/train.mr-hi.hi
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin train mr-hi 9337 examples
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:mr-hi', 9337)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:mr-hi', 1.0)]
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000549
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14767.59375Mb; avail=240306.6015625Mb
2024-08-11 16:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000098
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001131
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14767.59375Mb; avail=240306.6015625Mb
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000039
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14767.59375Mb; avail=240306.6015625Mb
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000808
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002272
2024-08-11 16:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14767.59375Mb; avail=240306.6015625Mb
2024-08-11 16:55:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 16:55:00 | INFO | fairseq.trainer | begin training epoch 1
2024-08-11 16:55:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 16:55:05 | INFO | train_inner | epoch 001:      2 / 63 loss=9.139, nll_loss=6.372, ppl=82.81, wps=1711.2, ups=0.39, wpb=4087, bsz=176, num_updates=2, lr=2.4e-08, gnorm=5.219, train_wall=5, gb_free=12.5, wall=13
2024-08-11 16:55:10 | INFO | train_inner | epoch 001:      4 / 63 loss=8.948, nll_loss=6.128, ppl=69.96, wps=1629.4, ups=0.39, wpb=4184, bsz=240, num_updates=4, lr=4.8e-08, gnorm=5.161, train_wall=5, gb_free=11, wall=18
2024-08-11 16:55:15 | INFO | train_inner | epoch 001:      6 / 63 loss=9.395, nll_loss=6.688, ppl=103.09, wps=1530.4, ups=0.41, wpb=3758, bsz=124, num_updates=6, lr=7.2e-08, gnorm=4.427, train_wall=5, gb_free=13.3, wall=23
2024-08-11 16:55:20 | INFO | train_inner | epoch 001:      8 / 63 loss=9.511, nll_loss=6.836, ppl=114.24, wps=1129.2, ups=0.45, wpb=2493.5, bsz=52, num_updates=8, lr=9.6e-08, gnorm=4.826, train_wall=4, gb_free=11.2, wall=27
2024-08-11 16:55:25 | INFO | train_inner | epoch 001:     10 / 63 loss=9.152, nll_loss=6.381, ppl=83.36, wps=1402.5, ups=0.39, wpb=3635, bsz=176, num_updates=10, lr=1.2e-07, gnorm=4.949, train_wall=5, gb_free=14.5, wall=32
2024-08-11 16:55:30 | INFO | train_inner | epoch 001:     12 / 63 loss=9.115, nll_loss=6.346, ppl=81.35, wps=1392.1, ups=0.39, wpb=3589.5, bsz=112, num_updates=12, lr=1.44e-07, gnorm=4.554, train_wall=5, gb_free=11.8, wall=37
2024-08-11 16:55:35 | INFO | train_inner | epoch 001:     14 / 63 loss=9.28, nll_loss=6.555, ppl=94.02, wps=1410.2, ups=0.4, wpb=3509, bsz=100, num_updates=14, lr=1.68e-07, gnorm=4.676, train_wall=5, gb_free=13.3, wall=42
2024-08-11 16:55:40 | INFO | train_inner | epoch 001:     16 / 63 loss=9.046, nll_loss=6.248, ppl=75.99, wps=1475.5, ups=0.43, wpb=3398, bsz=176, num_updates=16, lr=1.92e-07, gnorm=4.932, train_wall=5, gb_free=15.8, wall=47
2024-08-11 16:55:45 | INFO | train_inner | epoch 001:     18 / 63 loss=8.839, nll_loss=5.991, ppl=63.6, wps=1391.7, ups=0.37, wpb=3749.5, bsz=232, num_updates=18, lr=2.16e-07, gnorm=5.616, train_wall=5, gb_free=12.3, wall=52
2024-08-11 16:55:50 | INFO | train_inner | epoch 001:     20 / 63 loss=9.393, nll_loss=6.697, ppl=103.72, wps=1380.8, ups=0.42, wpb=3289, bsz=168, num_updates=20, lr=2.4e-07, gnorm=5.066, train_wall=5, gb_free=12.7, wall=57
2024-08-11 16:55:55 | INFO | train_inner | epoch 001:     22 / 63 loss=9.146, nll_loss=6.383, ppl=83.47, wps=1465.1, ups=0.35, wpb=4129.5, bsz=168, num_updates=22, lr=2.64e-07, gnorm=4.183, train_wall=6, gb_free=10.7, wall=63
2024-08-11 16:56:01 | INFO | train_inner | epoch 001:     24 / 63 loss=9.559, nll_loss=6.903, ppl=119.68, wps=1047.6, ups=0.37, wpb=2831.5, bsz=80, num_updates=24, lr=2.88e-07, gnorm=4.088, train_wall=5, gb_free=11.7, wall=68
2024-08-11 16:56:06 | INFO | train_inner | epoch 001:     26 / 63 loss=9.256, nll_loss=6.528, ppl=92.27, wps=1181.2, ups=0.41, wpb=2880, bsz=156, num_updates=26, lr=3.12e-07, gnorm=5.062, train_wall=5, gb_free=12.1, wall=73
2024-08-11 16:56:11 | INFO | train_inner | epoch 001:     28 / 63 loss=9.266, nll_loss=6.554, ppl=93.95, wps=1163.8, ups=0.41, wpb=2834, bsz=72, num_updates=28, lr=3.36e-07, gnorm=4.097, train_wall=5, gb_free=13.4, wall=78
2024-08-11 16:56:16 | INFO | train_inner | epoch 001:     30 / 63 loss=9.215, nll_loss=6.479, ppl=89.19, wps=1231.7, ups=0.38, wpb=3259.5, bsz=132, num_updates=30, lr=3.6e-07, gnorm=4.366, train_wall=5, gb_free=10.7, wall=83
2024-08-11 16:56:21 | INFO | train_inner | epoch 001:     32 / 63 loss=8.858, nll_loss=6.031, ppl=65.4, wps=1424.1, ups=0.36, wpb=3953, bsz=180, num_updates=32, lr=3.84e-07, gnorm=4.72, train_wall=6, gb_free=13.5, wall=89
2024-08-11 16:56:26 | INFO | train_inner | epoch 001:     34 / 63 loss=9.341, nll_loss=6.637, ppl=99.53, wps=1195.5, ups=0.39, wpb=3035.5, bsz=100, num_updates=34, lr=4.08e-07, gnorm=4.29, train_wall=5, gb_free=12.6, wall=94
2024-08-11 16:56:32 | INFO | train_inner | epoch 001:     36 / 63 loss=8.674, nll_loss=5.805, ppl=55.91, wps=1616, ups=0.37, wpb=4325, bsz=248, num_updates=36, lr=4.32e-07, gnorm=4.605, train_wall=5, gb_free=12.7, wall=99
2024-08-11 16:56:37 | INFO | train_inner | epoch 001:     38 / 63 loss=9.072, nll_loss=6.305, ppl=79.05, wps=1232.1, ups=0.38, wpb=3271, bsz=128, num_updates=38, lr=4.56e-07, gnorm=3.976, train_wall=5, gb_free=9.7, wall=104
2024-08-11 16:56:41 | INFO | train_inner | epoch 001:     40 / 63 loss=8.776, nll_loss=5.931, ppl=61.02, wps=1305.5, ups=0.46, wpb=2852, bsz=188, num_updates=40, lr=4.8e-07, gnorm=4.352, train_wall=4, gb_free=20.3, wall=109
2024-08-11 16:56:46 | INFO | train_inner | epoch 001:     42 / 63 loss=9.045, nll_loss=6.281, ppl=77.75, wps=1549, ups=0.49, wpb=3173.5, bsz=112.5, num_updates=42, lr=5.04e-07, gnorm=4.624, train_wall=4, gb_free=16.7, wall=113
2024-08-11 16:56:50 | INFO | train_inner | epoch 001:     44 / 63 loss=8.692, nll_loss=5.83, ppl=56.87, wps=1318.9, ups=0.44, wpb=3008.5, bsz=140, num_updates=44, lr=5.28e-07, gnorm=4.293, train_wall=5, gb_free=11.9, wall=117
2024-08-11 16:56:55 | INFO | train_inner | epoch 001:     46 / 63 loss=9.36, nll_loss=6.686, ppl=102.96, wps=1107.4, ups=0.41, wpb=2708, bsz=96, num_updates=46, lr=5.52e-07, gnorm=4.445, train_wall=5, gb_free=16.2, wall=122
2024-08-11 16:57:00 | INFO | train_inner | epoch 001:     48 / 63 loss=8.567, nll_loss=5.684, ppl=51.43, wps=1505, ups=0.41, wpb=3705, bsz=148, num_updates=48, lr=5.76e-07, gnorm=3.663, train_wall=5, gb_free=13.1, wall=127
2024-08-11 16:57:06 | INFO | train_inner | epoch 001:     50 / 63 loss=8.718, nll_loss=5.876, ppl=58.73, wps=1320, ups=0.35, wpb=3766, bsz=180, num_updates=50, lr=6e-07, gnorm=3.608, train_wall=6, gb_free=12.8, wall=133
2024-08-11 16:57:11 | INFO | train_inner | epoch 001:     52 / 63 loss=8.677, nll_loss=5.831, ppl=56.92, wps=1537.8, ups=0.39, wpb=3920, bsz=208, num_updates=52, lr=6.24e-07, gnorm=3.473, train_wall=5, gb_free=12.2, wall=138
2024-08-11 16:57:16 | INFO | train_inner | epoch 001:     54 / 63 loss=9.151, nll_loss=6.429, ppl=86.16, wps=1122.6, ups=0.38, wpb=2956.5, bsz=100, num_updates=54, lr=6.48e-07, gnorm=3.795, train_wall=5, gb_free=10.3, wall=143
2024-08-11 16:57:22 | INFO | train_inner | epoch 001:     56 / 63 loss=8.813, nll_loss=5.999, ppl=63.96, wps=1370.5, ups=0.36, wpb=3815.5, bsz=200, num_updates=56, lr=6.72e-07, gnorm=3.687, train_wall=6, gb_free=11.5, wall=149
2024-08-11 16:57:27 | INFO | train_inner | epoch 001:     58 / 63 loss=8.839, nll_loss=6.039, ppl=65.78, wps=1319.3, ups=0.38, wpb=3447.5, bsz=144, num_updates=58, lr=6.96e-07, gnorm=3.585, train_wall=5, gb_free=13.1, wall=154
2024-08-11 16:57:32 | INFO | train_inner | epoch 001:     60 / 63 loss=8.985, nll_loss=6.221, ppl=74.6, wps=1285.3, ups=0.38, wpb=3376, bsz=160, num_updates=60, lr=7.2e-07, gnorm=3.255, train_wall=5, gb_free=12.8, wall=159
2024-08-11 16:57:37 | INFO | train_inner | epoch 001:     62 / 63 loss=8.699, nll_loss=5.868, ppl=58.4, wps=1286.7, ups=0.37, wpb=3441, bsz=156, num_updates=62, lr=7.44e-07, gnorm=3.124, train_wall=5, gb_free=10.3, wall=165
2024-08-11 16:57:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15323.5625Mb; avail=239742.6953125Mb
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000534
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15323.5625Mb; avail=239742.6953125Mb
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004903
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15323.5625Mb; avail=239742.6953125Mb
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004191
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009960
2024-08-11 16:57:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15323.5625Mb; avail=239742.6953125Mb
2024-08-11 16:57:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.304 | nll_loss 5.21 | ppl 37 | wps 3063.4 | wpb 1463.4 | bsz 64.9 | num_updates 63
2024-08-11 16:57:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 63 updates
2024-08-11 16:57:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 16:58:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 16:58:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 1 @ 63 updates, score 8.304) (writing took 48.62405239511281 seconds)
2024-08-11 16:58:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-08-11 16:58:36 | INFO | train | epoch 001 | loss 9.037 | nll_loss 6.26 | ppl 76.66 | wps 987.9 | ups 0.29 | wpb 3400.7 | bsz 148.2 | num_updates 63 | lr 7.56e-07 | gnorm 4.383 | train_wall 158 | gb_free 18.1 | wall 223
2024-08-11 16:58:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 16:58:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 16:58:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 16:58:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000834
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19302.66796875Mb; avail=235763.57421875Mb
2024-08-11 16:58:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000138
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001211
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19302.66796875Mb; avail=235763.57421875Mb
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000041
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19302.66796875Mb; avail=235763.57421875Mb
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000430
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001970
2024-08-11 16:58:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19302.66796875Mb; avail=235763.57421875Mb
2024-08-11 16:58:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 16:58:36 | INFO | fairseq.trainer | begin training epoch 2
2024-08-11 16:58:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 16:58:38 | INFO | train_inner | epoch 002:      1 / 63 loss=9.168, nll_loss=6.455, ppl=87.72, wps=71.9, ups=0.03, wpb=2177.5, bsz=100, num_updates=64, lr=7.68e-07, gnorm=4.932, train_wall=3, gb_free=14.5, wall=225
2024-08-11 16:58:49 | INFO | train_inner | epoch 002:      3 / 63 loss=8.413, nll_loss=5.519, ppl=45.86, wps=853, ups=0.19, wpb=4491.5, bsz=228, num_updates=66, lr=7.92e-07, gnorm=3.064, train_wall=11, gb_free=13.3, wall=236
2024-08-11 16:58:54 | INFO | train_inner | epoch 002:      5 / 63 loss=8.949, nll_loss=6.193, ppl=73.17, wps=1276.2, ups=0.4, wpb=3176, bsz=108, num_updates=68, lr=8.16e-07, gnorm=3.191, train_wall=5, gb_free=12.8, wall=241
2024-08-11 16:58:58 | INFO | train_inner | epoch 002:      7 / 63 loss=9.734, nll_loss=7.173, ppl=144.28, wps=1179.5, ups=0.49, wpb=2410.5, bsz=76.5, num_updates=70, lr=8.4e-07, gnorm=5.77, train_wall=4, gb_free=11.8, wall=245
2024-08-11 16:59:03 | INFO | train_inner | epoch 002:      9 / 63 loss=8.778, nll_loss=5.98, ppl=63.1, wps=1435.2, ups=0.38, wpb=3747.5, bsz=132, num_updates=72, lr=8.64e-07, gnorm=2.987, train_wall=5, gb_free=11.4, wall=250
2024-08-11 16:59:08 | INFO | train_inner | epoch 002:     11 / 63 loss=8.631, nll_loss=5.8, ppl=55.72, wps=1546.2, ups=0.38, wpb=4095, bsz=208, num_updates=74, lr=8.88e-07, gnorm=2.796, train_wall=5, gb_free=12.6, wall=255
2024-08-11 16:59:13 | INFO | train_inner | epoch 002:     13 / 63 loss=8.883, nll_loss=6.113, ppl=69.22, wps=1263.6, ups=0.39, wpb=3202.5, bsz=96, num_updates=76, lr=9.12e-07, gnorm=3.241, train_wall=5, gb_free=12.9, wall=261
2024-08-11 16:59:18 | INFO | train_inner | epoch 002:     15 / 63 loss=9.217, nll_loss=6.536, ppl=92.8, wps=1314.4, ups=0.4, wpb=3324.5, bsz=124, num_updates=78, lr=9.36e-07, gnorm=3.7, train_wall=5, gb_free=12.8, wall=266
2024-08-11 16:59:24 | INFO | train_inner | epoch 002:     17 / 63 loss=8.49, nll_loss=5.626, ppl=49.38, wps=1421.9, ups=0.37, wpb=3849.5, bsz=220, num_updates=80, lr=9.6e-07, gnorm=2.855, train_wall=5, gb_free=11.1, wall=271
2024-08-11 16:59:29 | INFO | train_inner | epoch 002:     19 / 63 loss=9.099, nll_loss=6.394, ppl=84.07, wps=1192.6, ups=0.39, wpb=3089, bsz=124, num_updates=82, lr=9.84e-07, gnorm=3.389, train_wall=5, gb_free=10.8, wall=276
2024-08-11 16:59:34 | INFO | train_inner | epoch 002:     21 / 63 loss=8.676, nll_loss=5.865, ppl=58.27, wps=1291.5, ups=0.41, wpb=3188, bsz=168, num_updates=84, lr=1.008e-06, gnorm=3.254, train_wall=5, gb_free=15.6, wall=281
2024-08-11 16:59:39 | INFO | train_inner | epoch 002:     23 / 63 loss=8.659, nll_loss=5.849, ppl=57.63, wps=1358.9, ups=0.37, wpb=3654.5, bsz=152, num_updates=86, lr=1.032e-06, gnorm=2.907, train_wall=5, gb_free=11.2, wall=286
2024-08-11 16:59:44 | INFO | train_inner | epoch 002:     25 / 63 loss=8.845, nll_loss=6.09, ppl=68.13, wps=1148.6, ups=0.4, wpb=2864, bsz=80, num_updates=88, lr=1.056e-06, gnorm=3.25, train_wall=5, gb_free=10.9, wall=291
2024-08-11 16:59:50 | INFO | train_inner | epoch 002:     27 / 63 loss=8.63, nll_loss=5.798, ppl=55.63, wps=1200.6, ups=0.37, wpb=3244.5, bsz=228, num_updates=90, lr=1.08e-06, gnorm=3.067, train_wall=5, gb_free=11.5, wall=297
2024-08-11 16:59:55 | INFO | train_inner | epoch 002:     29 / 63 loss=8.549, nll_loss=5.713, ppl=52.44, wps=1375.8, ups=0.38, wpb=3644.5, bsz=148, num_updates=92, lr=1.104e-06, gnorm=2.632, train_wall=5, gb_free=10.9, wall=302
2024-08-11 17:00:00 | INFO | train_inner | epoch 002:     31 / 63 loss=8.539, nll_loss=5.705, ppl=52.15, wps=1511.3, ups=0.39, wpb=3921, bsz=160, num_updates=94, lr=1.128e-06, gnorm=2.732, train_wall=5, gb_free=9.7, wall=307
2024-08-11 17:00:05 | INFO | train_inner | epoch 002:     33 / 63 loss=8.526, nll_loss=5.68, ppl=51.25, wps=1280.6, ups=0.4, wpb=3212.5, bsz=184, num_updates=96, lr=1.152e-06, gnorm=2.85, train_wall=5, gb_free=13.3, wall=312
2024-08-11 17:00:10 | INFO | train_inner | epoch 002:     35 / 63 loss=8.909, nll_loss=6.176, ppl=72.3, wps=1268.6, ups=0.41, wpb=3107.5, bsz=76, num_updates=98, lr=1.176e-06, gnorm=3.424, train_wall=5, gb_free=13.2, wall=317
2024-08-11 17:00:15 | INFO | train_inner | epoch 002:     37 / 63 loss=8.876, nll_loss=6.131, ppl=70.07, wps=1174.3, ups=0.41, wpb=2853, bsz=128, num_updates=100, lr=1.2e-06, gnorm=3.228, train_wall=5, gb_free=14.6, wall=322
2024-08-11 17:00:19 | INFO | train_inner | epoch 002:     39 / 63 loss=8.483, nll_loss=5.64, ppl=49.88, wps=1414.1, ups=0.44, wpb=3178, bsz=104, num_updates=102, lr=1.224e-06, gnorm=2.885, train_wall=4, gb_free=16.5, wall=327
2024-08-11 17:00:24 | INFO | train_inner | epoch 002:     41 / 63 loss=8.488, nll_loss=5.65, ppl=50.22, wps=1490.4, ups=0.43, wpb=3484.5, bsz=152, num_updates=104, lr=1.248e-06, gnorm=2.812, train_wall=5, gb_free=12.1, wall=331
2024-08-11 17:00:29 | INFO | train_inner | epoch 002:     43 / 63 loss=8.468, nll_loss=5.621, ppl=49.23, wps=1252.7, ups=0.38, wpb=3291, bsz=184, num_updates=106, lr=1.272e-06, gnorm=2.692, train_wall=5, gb_free=11.7, wall=337
2024-08-11 17:00:34 | INFO | train_inner | epoch 002:     45 / 63 loss=8.518, nll_loss=5.683, ppl=51.37, wps=1430.3, ups=0.41, wpb=3511.5, bsz=136, num_updates=108, lr=1.296e-06, gnorm=2.676, train_wall=5, gb_free=13.2, wall=341
2024-08-11 17:00:40 | INFO | train_inner | epoch 002:     47 / 63 loss=8.547, nll_loss=5.718, ppl=52.65, wps=1402.1, ups=0.37, wpb=3752, bsz=176, num_updates=110, lr=1.32e-06, gnorm=2.575, train_wall=5, gb_free=10.8, wall=347
2024-08-11 17:00:44 | INFO | train_inner | epoch 002:     49 / 63 loss=8.811, nll_loss=6.05, ppl=66.27, wps=1345.1, ups=0.41, wpb=3280.5, bsz=104, num_updates=112, lr=1.344e-06, gnorm=2.903, train_wall=5, gb_free=12.2, wall=352
2024-08-11 17:00:50 | INFO | train_inner | epoch 002:     51 / 63 loss=8.663, nll_loss=5.857, ppl=57.96, wps=1171.1, ups=0.37, wpb=3151, bsz=96, num_updates=114, lr=1.368e-06, gnorm=2.926, train_wall=5, gb_free=12.4, wall=357
2024-08-11 17:00:55 | INFO | train_inner | epoch 002:     53 / 63 loss=8.481, nll_loss=5.645, ppl=50.04, wps=1461.1, ups=0.38, wpb=3853.5, bsz=204, num_updates=116, lr=1.392e-06, gnorm=2.534, train_wall=5, gb_free=13.8, wall=362
2024-08-11 17:01:00 | INFO | train_inner | epoch 002:     55 / 63 loss=8.347, nll_loss=5.472, ppl=44.37, wps=1281.5, ups=0.37, wpb=3476.5, bsz=180, num_updates=118, lr=1.416e-06, gnorm=2.63, train_wall=5, gb_free=11.9, wall=368
2024-08-11 17:01:06 | INFO | train_inner | epoch 002:     57 / 63 loss=8.482, nll_loss=5.646, ppl=50.08, wps=1515, ups=0.38, wpb=4017, bsz=192, num_updates=120, lr=1.44e-06, gnorm=2.383, train_wall=5, gb_free=12.6, wall=373
2024-08-11 17:01:11 | INFO | train_inner | epoch 002:     59 / 63 loss=8.613, nll_loss=5.816, ppl=56.32, wps=1164, ups=0.4, wpb=2888, bsz=148, num_updates=122, lr=1.464e-06, gnorm=2.951, train_wall=5, gb_free=16.1, wall=378
2024-08-11 17:01:16 | INFO | train_inner | epoch 002:     61 / 63 loss=8.548, nll_loss=5.724, ppl=52.86, wps=1385.8, ups=0.36, wpb=3806.5, bsz=128, num_updates=124, lr=1.488e-06, gnorm=2.57, train_wall=5, gb_free=11.5, wall=384
2024-08-11 17:01:20 | INFO | train_inner | epoch 002:     63 / 63 loss=8.392, nll_loss=5.531, ppl=46.22, wps=1404.7, ups=0.48, wpb=2920, bsz=140, num_updates=126, lr=1.512e-06, gnorm=2.678, train_wall=4, gb_free=16.3, wall=388
2024-08-11 17:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16530.80078125Mb; avail=238535.44140625Mb
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000532
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16530.80078125Mb; avail=238535.44140625Mb
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004695
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16530.80078125Mb; avail=238535.44140625Mb
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003994
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009566
2024-08-11 17:01:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16530.80078125Mb; avail=238535.44140625Mb
2024-08-11 17:01:29 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.049 | nll_loss 4.933 | ppl 30.54 | wps 3061.3 | wpb 1463.4 | bsz 64.9 | num_updates 126 | best_loss 8.049
2024-08-11 17:01:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 126 updates
2024-08-11 17:01:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:02:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 2 @ 126 updates, score 8.049) (writing took 61.1195171168074 seconds)
2024-08-11 17:02:30 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-08-11 17:02:30 | INFO | train | epoch 002 | loss 8.666 | nll_loss 5.859 | ppl 58.04 | wps 914.5 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 126 | lr 1.512e-06 | gnorm 3.02 | train_wall 164 | gb_free 16.3 | wall 457
2024-08-11 17:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000861
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20377.18359375Mb; avail=234689.17578125Mb
2024-08-11 17:02:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000144
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001305
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20377.18359375Mb; avail=234689.17578125Mb
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20377.18359375Mb; avail=234689.17578125Mb
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000447
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002089
2024-08-11 17:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20377.18359375Mb; avail=234689.17578125Mb
2024-08-11 17:02:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:02:30 | INFO | fairseq.trainer | begin training epoch 3
2024-08-11 17:02:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:02:40 | INFO | train_inner | epoch 003:      2 / 63 loss=8.721, nll_loss=5.946, ppl=61.64, wps=84.3, ups=0.03, wpb=3367, bsz=136, num_updates=128, lr=1.536e-06, gnorm=2.787, train_wall=10, gb_free=9.1, wall=468
2024-08-11 17:02:45 | INFO | train_inner | epoch 003:      4 / 63 loss=8.41, nll_loss=5.553, ppl=46.96, wps=1442.9, ups=0.38, wpb=3775, bsz=168, num_updates=130, lr=1.56e-06, gnorm=2.381, train_wall=5, gb_free=11.9, wall=473
2024-08-11 17:02:50 | INFO | train_inner | epoch 003:      6 / 63 loss=8.496, nll_loss=5.651, ppl=50.26, wps=1409.3, ups=0.41, wpb=3435, bsz=196, num_updates=132, lr=1.584e-06, gnorm=2.5, train_wall=5, gb_free=12.4, wall=478
2024-08-11 17:02:55 | INFO | train_inner | epoch 003:      8 / 63 loss=8.706, nll_loss=5.932, ppl=61.04, wps=1318.1, ups=0.42, wpb=3164, bsz=128, num_updates=134, lr=1.608e-06, gnorm=2.873, train_wall=5, gb_free=13.7, wall=482
2024-08-11 17:03:00 | INFO | train_inner | epoch 003:     10 / 63 loss=8.56, nll_loss=5.735, ppl=53.26, wps=1276.6, ups=0.39, wpb=3288, bsz=196, num_updates=136, lr=1.632e-06, gnorm=2.806, train_wall=5, gb_free=12.5, wall=488
2024-08-11 17:03:05 | INFO | train_inner | epoch 003:     12 / 63 loss=8.357, nll_loss=5.498, ppl=45.18, wps=1576.3, ups=0.41, wpb=3842, bsz=160, num_updates=138, lr=1.656e-06, gnorm=2.49, train_wall=5, gb_free=12.6, wall=492
2024-08-11 17:03:10 | INFO | train_inner | epoch 003:     14 / 63 loss=8.693, nll_loss=5.903, ppl=59.83, wps=1347.8, ups=0.4, wpb=3379, bsz=136, num_updates=140, lr=1.68e-06, gnorm=2.854, train_wall=5, gb_free=14.5, wall=498
2024-08-11 17:03:15 | INFO | train_inner | epoch 003:     16 / 63 loss=8.357, nll_loss=5.484, ppl=44.76, wps=1319.9, ups=0.41, wpb=3247.5, bsz=156, num_updates=142, lr=1.704e-06, gnorm=2.686, train_wall=5, gb_free=13.4, wall=502
2024-08-11 17:03:19 | INFO | train_inner | epoch 003:     18 / 63 loss=8.831, nll_loss=6.078, ppl=67.55, wps=1387.3, ups=0.51, wpb=2708.5, bsz=60.5, num_updates=144, lr=1.728e-06, gnorm=3.405, train_wall=4, gb_free=17, wall=506
2024-08-11 17:03:25 | INFO | train_inner | epoch 003:     20 / 63 loss=8.704, nll_loss=5.927, ppl=60.83, wps=1203.8, ups=0.36, wpb=3316.5, bsz=116, num_updates=146, lr=1.752e-06, gnorm=2.776, train_wall=6, gb_free=9.8, wall=512
2024-08-11 17:03:29 | INFO | train_inner | epoch 003:     22 / 63 loss=8.501, nll_loss=5.672, ppl=51, wps=1191.4, ups=0.4, wpb=2963, bsz=108, num_updates=148, lr=1.776e-06, gnorm=2.705, train_wall=5, gb_free=12.2, wall=517
2024-08-11 17:03:35 | INFO | train_inner | epoch 003:     24 / 63 loss=8.3, nll_loss=5.419, ppl=42.77, wps=1253.8, ups=0.36, wpb=3450.5, bsz=180, num_updates=150, lr=1.8e-06, gnorm=2.635, train_wall=5, gb_free=13.3, wall=522
2024-08-11 17:03:40 | INFO | train_inner | epoch 003:     26 / 63 loss=8.392, nll_loss=5.537, ppl=46.44, wps=1402.2, ups=0.37, wpb=3769, bsz=188, num_updates=152, lr=1.824e-06, gnorm=2.421, train_wall=5, gb_free=10.6, wall=528
2024-08-11 17:03:46 | INFO | train_inner | epoch 003:     28 / 63 loss=8.257, nll_loss=5.372, ppl=41.41, wps=1413.2, ups=0.37, wpb=3825, bsz=204, num_updates=154, lr=1.848e-06, gnorm=2.273, train_wall=5, gb_free=10.9, wall=533
2024-08-11 17:03:51 | INFO | train_inner | epoch 003:     30 / 63 loss=8.515, nll_loss=5.688, ppl=51.55, wps=1441.4, ups=0.4, wpb=3565, bsz=136, num_updates=156, lr=1.872e-06, gnorm=2.6, train_wall=5, gb_free=16, wall=538
2024-08-11 17:03:56 | INFO | train_inner | epoch 003:     32 / 63 loss=8.636, nll_loss=5.837, ppl=57.15, wps=1260, ups=0.38, wpb=3297.5, bsz=108, num_updates=158, lr=1.896e-06, gnorm=2.862, train_wall=5, gb_free=13.5, wall=543
2024-08-11 17:04:01 | INFO | train_inner | epoch 003:     34 / 63 loss=8.368, nll_loss=5.508, ppl=45.51, wps=1187.9, ups=0.37, wpb=3171, bsz=168, num_updates=160, lr=1.92e-06, gnorm=2.753, train_wall=5, gb_free=11.9, wall=549
2024-08-11 17:04:06 | INFO | train_inner | epoch 003:     36 / 63 loss=8.439, nll_loss=5.597, ppl=48.41, wps=1210, ups=0.39, wpb=3070.5, bsz=164, num_updates=162, lr=1.944e-06, gnorm=2.883, train_wall=5, gb_free=13.8, wall=554
2024-08-11 17:04:12 | INFO | train_inner | epoch 003:     38 / 63 loss=8.496, nll_loss=5.66, ppl=50.57, wps=1402.4, ups=0.38, wpb=3724.5, bsz=104, num_updates=164, lr=1.968e-06, gnorm=2.542, train_wall=5, gb_free=10.9, wall=559
2024-08-11 17:04:17 | INFO | train_inner | epoch 003:     40 / 63 loss=8.499, nll_loss=5.66, ppl=50.55, wps=1193, ups=0.37, wpb=3182, bsz=120, num_updates=166, lr=1.992e-06, gnorm=2.683, train_wall=5, gb_free=11.6, wall=564
2024-08-11 17:04:22 | INFO | train_inner | epoch 003:     42 / 63 loss=8.323, nll_loss=5.455, ppl=43.87, wps=1553.2, ups=0.41, wpb=3795.5, bsz=136, num_updates=168, lr=2.016e-06, gnorm=2.538, train_wall=5, gb_free=12.8, wall=569
2024-08-11 17:04:27 | INFO | train_inner | epoch 003:     44 / 63 loss=8.565, nll_loss=5.752, ppl=53.9, wps=1196.8, ups=0.41, wpb=2890, bsz=80, num_updates=170, lr=2.04e-06, gnorm=2.729, train_wall=5, gb_free=14.7, wall=574
2024-08-11 17:04:32 | INFO | train_inner | epoch 003:     46 / 63 loss=8.308, nll_loss=5.425, ppl=42.97, wps=1378.4, ups=0.38, wpb=3599, bsz=148, num_updates=172, lr=2.064e-06, gnorm=2.373, train_wall=5, gb_free=10.5, wall=579
2024-08-11 17:04:37 | INFO | train_inner | epoch 003:     48 / 63 loss=8.14, nll_loss=5.229, ppl=37.5, wps=1640.1, ups=0.4, wpb=4068.5, bsz=196, num_updates=174, lr=2.088e-06, gnorm=2.469, train_wall=5, gb_free=13.3, wall=584
2024-08-11 17:04:42 | INFO | train_inner | epoch 003:     50 / 63 loss=8.082, nll_loss=5.144, ppl=35.35, wps=1542.5, ups=0.37, wpb=4143.5, bsz=196, num_updates=176, lr=2.112e-06, gnorm=2.215, train_wall=5, gb_free=12.7, wall=590
2024-08-11 17:04:47 | INFO | train_inner | epoch 003:     52 / 63 loss=8.286, nll_loss=5.406, ppl=42.41, wps=1405.5, ups=0.4, wpb=3557.5, bsz=156, num_updates=178, lr=2.136e-06, gnorm=2.478, train_wall=5, gb_free=13, wall=595
2024-08-11 17:04:53 | INFO | train_inner | epoch 003:     54 / 63 loss=8.497, nll_loss=5.668, ppl=50.84, wps=1164.2, ups=0.39, wpb=3002, bsz=120, num_updates=180, lr=2.16e-06, gnorm=2.702, train_wall=5, gb_free=10.7, wall=600
2024-08-11 17:04:58 | INFO | train_inner | epoch 003:     56 / 63 loss=8.486, nll_loss=5.653, ppl=50.32, wps=1319.8, ups=0.39, wpb=3413, bsz=156, num_updates=182, lr=2.184e-06, gnorm=2.454, train_wall=5, gb_free=11.7, wall=605
2024-08-11 17:05:03 | INFO | train_inner | epoch 003:     58 / 63 loss=8.383, nll_loss=5.524, ppl=46.02, wps=1174.2, ups=0.37, wpb=3135, bsz=212, num_updates=184, lr=2.208e-06, gnorm=2.524, train_wall=5, gb_free=11, wall=610
2024-08-11 17:05:08 | INFO | train_inner | epoch 003:     60 / 63 loss=8.428, nll_loss=5.576, ppl=47.71, wps=1247.3, ups=0.41, wpb=3062, bsz=132, num_updates=186, lr=2.232e-06, gnorm=2.716, train_wall=5, gb_free=13, wall=615
2024-08-11 17:05:13 | INFO | train_inner | epoch 003:     62 / 63 loss=8.23, nll_loss=5.335, ppl=40.36, wps=1507, ups=0.39, wpb=3913.5, bsz=160, num_updates=188, lr=2.256e-06, gnorm=2.284, train_wall=5, gb_free=10.6, wall=620
2024-08-11 17:05:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16424.21875Mb; avail=238642.02734375Mb
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000525
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16424.21875Mb; avail=238642.02734375Mb
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004611
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16424.21875Mb; avail=238642.02734375Mb
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003989
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009472
2024-08-11 17:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16424.21484375Mb; avail=238642.02734375Mb
2024-08-11 17:05:23 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.88 | nll_loss 4.72 | ppl 26.35 | wps 3062.9 | wpb 1463.4 | bsz 64.9 | num_updates 189 | best_loss 7.88
2024-08-11 17:05:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 189 updates
2024-08-11 17:05:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:06:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:06:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 3 @ 189 updates, score 7.88) (writing took 60.65730258170515 seconds)
2024-08-11 17:06:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-08-11 17:06:24 | INFO | train | epoch 003 | loss 8.436 | nll_loss 5.589 | ppl 48.13 | wps 916.9 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 189 | lr 2.268e-06 | gnorm 2.636 | train_wall 164 | gb_free 16.6 | wall 691
2024-08-11 17:06:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:06:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:06:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:06:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000837
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20221.20703125Mb; avail=234844.99609375Mb
2024-08-11 17:06:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000143
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001286
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20221.20703125Mb; avail=234844.99609375Mb
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000059
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20221.20703125Mb; avail=234844.99609375Mb
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000474
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002208
2024-08-11 17:06:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20221.20703125Mb; avail=234844.99609375Mb
2024-08-11 17:06:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:06:24 | INFO | fairseq.trainer | begin training epoch 4
2024-08-11 17:06:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:06:26 | INFO | train_inner | epoch 004:      1 / 63 loss=8.486, nll_loss=5.656, ppl=50.43, wps=68.9, ups=0.03, wpb=2517.5, bsz=72, num_updates=190, lr=2.28e-06, gnorm=3.251, train_wall=4, gb_free=10.9, wall=694
2024-08-11 17:06:31 | INFO | train_inner | epoch 004:      3 / 63 loss=8.46, nll_loss=5.619, ppl=49.13, wps=1319.4, ups=0.41, wpb=3221, bsz=124, num_updates=192, lr=2.304e-06, gnorm=2.531, train_wall=5, gb_free=13.9, wall=698
2024-08-11 17:06:36 | INFO | train_inner | epoch 004:      5 / 63 loss=8.24, nll_loss=5.364, ppl=41.18, wps=1488.2, ups=0.39, wpb=3797, bsz=236, num_updates=194, lr=2.328e-06, gnorm=2.417, train_wall=5, gb_free=13.7, wall=703
2024-08-11 17:06:41 | INFO | train_inner | epoch 004:      7 / 63 loss=8.38, nll_loss=5.522, ppl=45.95, wps=1190, ups=0.4, wpb=3006, bsz=148, num_updates=196, lr=2.352e-06, gnorm=2.488, train_wall=5, gb_free=13.8, wall=709
2024-08-11 17:06:47 | INFO | train_inner | epoch 004:      9 / 63 loss=8.286, nll_loss=5.404, ppl=42.34, wps=1441.4, ups=0.37, wpb=3911.5, bsz=168, num_updates=198, lr=2.376e-06, gnorm=2.178, train_wall=5, gb_free=12.1, wall=714
2024-08-11 17:06:52 | INFO | train_inner | epoch 004:     11 / 63 loss=8.293, nll_loss=5.414, ppl=42.63, wps=1330.4, ups=0.37, wpb=3604, bsz=184, num_updates=200, lr=2.4e-06, gnorm=2.281, train_wall=5, gb_free=11.2, wall=719
2024-08-11 17:06:57 | INFO | train_inner | epoch 004:     13 / 63 loss=8.257, nll_loss=5.37, ppl=41.34, wps=1309.3, ups=0.42, wpb=3126, bsz=132, num_updates=202, lr=2.424e-06, gnorm=2.63, train_wall=5, gb_free=12, wall=724
2024-08-11 17:07:02 | INFO | train_inner | epoch 004:     15 / 63 loss=8.721, nll_loss=5.941, ppl=61.42, wps=1133.4, ups=0.38, wpb=3013, bsz=100, num_updates=204, lr=2.448e-06, gnorm=2.632, train_wall=5, gb_free=10.7, wall=729
2024-08-11 17:07:07 | INFO | train_inner | epoch 004:     17 / 63 loss=8.303, nll_loss=5.423, ppl=42.89, wps=1463.3, ups=0.43, wpb=3387.5, bsz=96, num_updates=206, lr=2.472e-06, gnorm=2.5, train_wall=5, gb_free=12.6, wall=734
2024-08-11 17:07:12 | INFO | train_inner | epoch 004:     19 / 63 loss=8.245, nll_loss=5.354, ppl=40.91, wps=1411.4, ups=0.37, wpb=3787.5, bsz=168, num_updates=208, lr=2.496e-06, gnorm=2.21, train_wall=5, gb_free=12.5, wall=739
2024-08-11 17:07:17 | INFO | train_inner | epoch 004:     21 / 63 loss=8.105, nll_loss=5.178, ppl=36.2, wps=1489.2, ups=0.39, wpb=3813, bsz=184, num_updates=210, lr=2.52e-06, gnorm=2.32, train_wall=5, gb_free=15.2, wall=745
2024-08-11 17:07:22 | INFO | train_inner | epoch 004:     23 / 63 loss=8.095, nll_loss=5.164, ppl=35.86, wps=1447.4, ups=0.39, wpb=3703, bsz=192, num_updates=212, lr=2.544e-06, gnorm=2.216, train_wall=5, gb_free=15.1, wall=750
2024-08-11 17:07:28 | INFO | train_inner | epoch 004:     25 / 63 loss=8.578, nll_loss=5.774, ppl=54.7, wps=1284.2, ups=0.38, wpb=3405.5, bsz=72, num_updates=214, lr=2.568e-06, gnorm=2.563, train_wall=5, gb_free=13.6, wall=755
2024-08-11 17:07:33 | INFO | train_inner | epoch 004:     27 / 63 loss=8.173, nll_loss=5.26, ppl=38.32, wps=1321.4, ups=0.37, wpb=3529.5, bsz=160, num_updates=216, lr=2.592e-06, gnorm=2.494, train_wall=5, gb_free=11.5, wall=760
2024-08-11 17:07:38 | INFO | train_inner | epoch 004:     29 / 63 loss=8.227, nll_loss=5.333, ppl=40.32, wps=1329.8, ups=0.39, wpb=3408.5, bsz=180, num_updates=218, lr=2.616e-06, gnorm=2.453, train_wall=5, gb_free=12.5, wall=765
2024-08-11 17:07:43 | INFO | train_inner | epoch 004:     31 / 63 loss=8.498, nll_loss=5.668, ppl=50.86, wps=1226.9, ups=0.39, wpb=3160.5, bsz=116, num_updates=220, lr=2.64e-06, gnorm=2.472, train_wall=5, gb_free=11.5, wall=771
2024-08-11 17:07:48 | INFO | train_inner | epoch 004:     33 / 63 loss=8.432, nll_loss=5.592, ppl=48.24, wps=1383.5, ups=0.47, wpb=2955, bsz=116, num_updates=222, lr=2.664e-06, gnorm=2.77, train_wall=4, gb_free=16.8, wall=775
2024-08-11 17:07:52 | INFO | train_inner | epoch 004:     35 / 63 loss=8.186, nll_loss=5.261, ppl=38.34, wps=1658.7, ups=0.48, wpb=3445.5, bsz=132.5, num_updates=224, lr=2.688e-06, gnorm=2.581, train_wall=4, gb_free=10.9, wall=779
2024-08-11 17:07:57 | INFO | train_inner | epoch 004:     37 / 63 loss=8.295, nll_loss=5.408, ppl=42.47, wps=1146.2, ups=0.39, wpb=2912, bsz=108, num_updates=226, lr=2.712e-06, gnorm=2.682, train_wall=5, gb_free=12, wall=784
2024-08-11 17:08:02 | INFO | train_inner | epoch 004:     39 / 63 loss=8.162, nll_loss=5.247, ppl=37.97, wps=1286.4, ups=0.38, wpb=3396, bsz=172, num_updates=228, lr=2.736e-06, gnorm=2.365, train_wall=5, gb_free=11.9, wall=789
2024-08-11 17:08:07 | INFO | train_inner | epoch 004:     41 / 63 loss=8.273, nll_loss=5.382, ppl=41.71, wps=1251.7, ups=0.38, wpb=3334, bsz=132, num_updates=230, lr=2.76e-06, gnorm=2.422, train_wall=5, gb_free=11.7, wall=795
2024-08-11 17:08:13 | INFO | train_inner | epoch 004:     43 / 63 loss=8.169, nll_loss=5.252, ppl=38.11, wps=1403.2, ups=0.39, wpb=3576.5, bsz=144, num_updates=232, lr=2.784e-06, gnorm=2.333, train_wall=5, gb_free=13.4, wall=800
2024-08-11 17:08:18 | INFO | train_inner | epoch 004:     45 / 63 loss=7.89, nll_loss=4.907, ppl=30, wps=1360.8, ups=0.37, wpb=3677, bsz=232, num_updates=234, lr=2.808e-06, gnorm=2.397, train_wall=5, gb_free=12.4, wall=805
2024-08-11 17:08:23 | INFO | train_inner | epoch 004:     47 / 63 loss=8.045, nll_loss=5.101, ppl=34.32, wps=1448.8, ups=0.37, wpb=3956, bsz=192, num_updates=236, lr=2.832e-06, gnorm=2.248, train_wall=5, gb_free=11.1, wall=811
2024-08-11 17:08:28 | INFO | train_inner | epoch 004:     49 / 63 loss=8.391, nll_loss=5.531, ppl=46.23, wps=1208.6, ups=0.4, wpb=2990.5, bsz=104, num_updates=238, lr=2.856e-06, gnorm=2.768, train_wall=5, gb_free=12.8, wall=816
2024-08-11 17:08:34 | INFO | train_inner | epoch 004:     51 / 63 loss=8.153, nll_loss=5.234, ppl=37.63, wps=1436.5, ups=0.38, wpb=3781, bsz=176, num_updates=240, lr=2.88e-06, gnorm=2.229, train_wall=5, gb_free=13.3, wall=821
2024-08-11 17:08:39 | INFO | train_inner | epoch 004:     53 / 63 loss=8.316, nll_loss=5.439, ppl=43.37, wps=1137.8, ups=0.4, wpb=2862, bsz=100, num_updates=242, lr=2.904e-06, gnorm=2.548, train_wall=5, gb_free=11.2, wall=826
2024-08-11 17:08:44 | INFO | train_inner | epoch 004:     55 / 63 loss=7.973, nll_loss=5.024, ppl=32.54, wps=1344.9, ups=0.4, wpb=3389.5, bsz=244, num_updates=244, lr=2.928e-06, gnorm=2.397, train_wall=5, gb_free=11.8, wall=831
2024-08-11 17:08:49 | INFO | train_inner | epoch 004:     57 / 63 loss=7.953, nll_loss=4.982, ppl=31.61, wps=1488.4, ups=0.36, wpb=4089.5, bsz=196, num_updates=246, lr=2.952e-06, gnorm=2.256, train_wall=5, gb_free=11.8, wall=836
2024-08-11 17:08:54 | INFO | train_inner | epoch 004:     59 / 63 loss=8.165, nll_loss=5.254, ppl=38.16, wps=1390.5, ups=0.41, wpb=3391.5, bsz=120, num_updates=248, lr=2.976e-06, gnorm=2.441, train_wall=5, gb_free=12.6, wall=841
2024-08-11 17:08:59 | INFO | train_inner | epoch 004:     61 / 63 loss=8.308, nll_loss=5.404, ppl=42.35, wps=1301.3, ups=0.4, wpb=3258, bsz=76, num_updates=250, lr=3e-06, gnorm=2.604, train_wall=5, gb_free=11.2, wall=846
2024-08-11 17:09:03 | INFO | train_inner | epoch 004:     63 / 63 loss=8.014, nll_loss=5.064, ppl=33.46, wps=1564.6, ups=0.58, wpb=2719, bsz=136, num_updates=252, lr=3.024e-06, gnorm=2.933, train_wall=3, gb_free=19.5, wall=850
2024-08-11 17:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20206.42578125Mb; avail=234859.78515625Mb
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000629
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20206.42578125Mb; avail=234859.78515625Mb
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004702
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20206.42578125Mb; avail=234859.78515625Mb
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004032
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009702
2024-08-11 17:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20206.42578125Mb; avail=234859.78515625Mb
2024-08-11 17:09:11 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.725 | nll_loss 4.508 | ppl 22.76 | wps 3066.4 | wpb 1463.4 | bsz 64.9 | num_updates 252 | best_loss 7.725
2024-08-11 17:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 252 updates
2024-08-11 17:09:11 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:09:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:10:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 4 @ 252 updates, score 7.725) (writing took 60.589637972880155 seconds)
2024-08-11 17:10:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-08-11 17:10:12 | INFO | train | epoch 004 | loss 8.242 | nll_loss 5.348 | ppl 40.73 | wps 940.1 | ups 0.28 | wpb 3400.7 | bsz 148.2 | num_updates 252 | lr 3.024e-06 | gnorm 2.475 | train_wall 159 | gb_free 19.5 | wall 919
2024-08-11 17:10:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:10:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:10:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:10:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000867
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26925.44140625Mb; avail=228140.73046875Mb
2024-08-11 17:10:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000138
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001249
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26925.44140625Mb; avail=228140.73046875Mb
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26925.44140625Mb; avail=228140.73046875Mb
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000455
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002057
2024-08-11 17:10:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26925.44140625Mb; avail=228140.73046875Mb
2024-08-11 17:10:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:10:12 | INFO | fairseq.trainer | begin training epoch 5
2024-08-11 17:10:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:10:17 | INFO | train_inner | epoch 005:      2 / 63 loss=7.887, nll_loss=4.906, ppl=29.98, wps=112.2, ups=0.03, wpb=4173.5, bsz=236, num_updates=254, lr=3.048e-06, gnorm=1.953, train_wall=5, gb_free=13.6, wall=924
2024-08-11 17:10:22 | INFO | train_inner | epoch 005:      4 / 63 loss=8.11, nll_loss=5.165, ppl=35.87, wps=1329.1, ups=0.38, wpb=3473.5, bsz=136, num_updates=256, lr=3.072e-06, gnorm=2.238, train_wall=5, gb_free=12.4, wall=929
2024-08-11 17:10:27 | INFO | train_inner | epoch 005:      6 / 63 loss=8.415, nll_loss=5.565, ppl=47.35, wps=1230, ups=0.41, wpb=3035, bsz=136, num_updates=258, lr=3.096e-06, gnorm=2.49, train_wall=5, gb_free=12.9, wall=934
2024-08-11 17:10:33 | INFO | train_inner | epoch 005:      8 / 63 loss=7.96, nll_loss=4.991, ppl=31.8, wps=1482.2, ups=0.37, wpb=4028.5, bsz=224, num_updates=260, lr=3.12e-06, gnorm=2.22, train_wall=5, gb_free=11.7, wall=940
2024-08-11 17:10:48 | INFO | train_inner | epoch 005:     10 / 63 loss=8.234, nll_loss=5.338, ppl=40.44, wps=425.8, ups=0.13, wpb=3202.5, bsz=88, num_updates=262, lr=3.144e-06, gnorm=2.555, train_wall=15, gb_free=15.1, wall=955
2024-08-11 17:10:53 | INFO | train_inner | epoch 005:     12 / 63 loss=8.21, nll_loss=5.301, ppl=39.43, wps=1325.9, ups=0.4, wpb=3325, bsz=108, num_updates=264, lr=3.168e-06, gnorm=2.504, train_wall=5, gb_free=10.3, wall=960
2024-08-11 17:10:57 | INFO | train_inner | epoch 005:     14 / 63 loss=8.182, nll_loss=5.276, ppl=38.75, wps=1464.2, ups=0.44, wpb=3352, bsz=116, num_updates=266, lr=3.192e-06, gnorm=2.516, train_wall=5, gb_free=12.9, wall=964
2024-08-11 17:11:02 | INFO | train_inner | epoch 005:     16 / 63 loss=7.971, nll_loss=5.007, ppl=32.15, wps=1297, ups=0.45, wpb=2873.5, bsz=132, num_updates=268, lr=3.216e-06, gnorm=2.541, train_wall=4, gb_free=17.9, wall=969
2024-08-11 17:11:07 | INFO | train_inner | epoch 005:     18 / 63 loss=8.182, nll_loss=5.273, ppl=38.68, wps=1476.9, ups=0.38, wpb=3842, bsz=172, num_updates=270, lr=3.24e-06, gnorm=2.286, train_wall=5, gb_free=11.6, wall=974
2024-08-11 17:11:12 | INFO | train_inner | epoch 005:     20 / 63 loss=7.975, nll_loss=5.01, ppl=32.23, wps=1407.9, ups=0.39, wpb=3611.5, bsz=204, num_updates=272, lr=3.264e-06, gnorm=2.104, train_wall=5, gb_free=14.2, wall=979
2024-08-11 17:11:17 | INFO | train_inner | epoch 005:     22 / 63 loss=7.867, nll_loss=4.869, ppl=29.23, wps=1495.3, ups=0.39, wpb=3829.5, bsz=180, num_updates=274, lr=3.288e-06, gnorm=2.493, train_wall=5, gb_free=12.8, wall=984
2024-08-11 17:11:22 | INFO | train_inner | epoch 005:     24 / 63 loss=8.073, nll_loss=5.116, ppl=34.68, wps=1458.2, ups=0.41, wpb=3536, bsz=128, num_updates=276, lr=3.312e-06, gnorm=2.505, train_wall=5, gb_free=12.9, wall=989
2024-08-11 17:11:27 | INFO | train_inner | epoch 005:     26 / 63 loss=8.068, nll_loss=5.117, ppl=34.71, wps=1332.9, ups=0.38, wpb=3547.5, bsz=144, num_updates=278, lr=3.336e-06, gnorm=2.232, train_wall=5, gb_free=11.4, wall=995
2024-08-11 17:11:32 | INFO | train_inner | epoch 005:     28 / 63 loss=8.048, nll_loss=5.101, ppl=34.33, wps=1400.2, ups=0.43, wpb=3288, bsz=144, num_updates=280, lr=3.36e-06, gnorm=2.454, train_wall=5, gb_free=12.9, wall=999
2024-08-11 17:11:37 | INFO | train_inner | epoch 005:     30 / 63 loss=7.941, nll_loss=4.971, ppl=31.36, wps=1420.7, ups=0.39, wpb=3612.5, bsz=192, num_updates=282, lr=3.384e-06, gnorm=2.212, train_wall=5, gb_free=13.2, wall=1004
2024-08-11 17:11:42 | INFO | train_inner | epoch 005:     32 / 63 loss=7.981, nll_loss=5.005, ppl=32.1, wps=1401.1, ups=0.4, wpb=3542.5, bsz=132, num_updates=284, lr=3.408e-06, gnorm=2.332, train_wall=5, gb_free=14.5, wall=1009
2024-08-11 17:11:47 | INFO | train_inner | epoch 005:     34 / 63 loss=8.192, nll_loss=5.28, ppl=38.85, wps=1265.3, ups=0.38, wpb=3304, bsz=124, num_updates=286, lr=3.432e-06, gnorm=2.354, train_wall=5, gb_free=11.6, wall=1015
2024-08-11 17:11:53 | INFO | train_inner | epoch 005:     36 / 63 loss=8.109, nll_loss=5.168, ppl=35.95, wps=1356.1, ups=0.37, wpb=3699, bsz=136, num_updates=288, lr=3.456e-06, gnorm=2.224, train_wall=5, gb_free=11.9, wall=1020
2024-08-11 17:11:58 | INFO | train_inner | epoch 005:     38 / 63 loss=7.791, nll_loss=4.77, ppl=27.29, wps=1453.5, ups=0.37, wpb=3979.5, bsz=200, num_updates=290, lr=3.48e-06, gnorm=2.066, train_wall=5, gb_free=15.2, wall=1026
2024-08-11 17:12:03 | INFO | train_inner | epoch 005:     40 / 63 loss=8.215, nll_loss=5.307, ppl=39.58, wps=1347, ups=0.42, wpb=3183.5, bsz=84, num_updates=292, lr=3.504e-06, gnorm=2.506, train_wall=5, gb_free=15, wall=1030
2024-08-11 17:12:08 | INFO | train_inner | epoch 005:     42 / 63 loss=8.015, nll_loss=5.06, ppl=33.36, wps=1544.1, ups=0.4, wpb=3852.5, bsz=168, num_updates=294, lr=3.528e-06, gnorm=2.201, train_wall=5, gb_free=13.1, wall=1035
2024-08-11 17:12:13 | INFO | train_inner | epoch 005:     44 / 63 loss=8.061, nll_loss=5.104, ppl=34.4, wps=1324.8, ups=0.37, wpb=3615, bsz=120, num_updates=296, lr=3.552e-06, gnorm=2.56, train_wall=5, gb_free=10.5, wall=1041
2024-08-11 17:12:19 | INFO | train_inner | epoch 005:     46 / 63 loss=7.879, nll_loss=4.894, ppl=29.73, wps=1224.3, ups=0.39, wpb=3175, bsz=184, num_updates=298, lr=3.576e-06, gnorm=2.635, train_wall=5, gb_free=11.6, wall=1046
2024-08-11 17:12:24 | INFO | train_inner | epoch 005:     48 / 63 loss=8.168, nll_loss=5.238, ppl=37.75, wps=1196, ups=0.39, wpb=3047, bsz=112, num_updates=300, lr=3.6e-06, gnorm=2.434, train_wall=5, gb_free=12.8, wall=1051
2024-08-11 17:12:29 | INFO | train_inner | epoch 005:     50 / 63 loss=8.074, nll_loss=5.115, ppl=34.66, wps=1258.4, ups=0.38, wpb=3314, bsz=120, num_updates=302, lr=3.624e-06, gnorm=2.273, train_wall=5, gb_free=11.9, wall=1056
2024-08-11 17:12:34 | INFO | train_inner | epoch 005:     52 / 63 loss=8.032, nll_loss=5.079, ppl=33.8, wps=1257.1, ups=0.4, wpb=3138.5, bsz=164, num_updates=304, lr=3.648e-06, gnorm=2.582, train_wall=5, gb_free=14.8, wall=1061
2024-08-11 17:12:39 | INFO | train_inner | epoch 005:     54 / 63 loss=7.957, nll_loss=4.985, ppl=31.67, wps=1357.4, ups=0.38, wpb=3536.5, bsz=184, num_updates=306, lr=3.672e-06, gnorm=2.147, train_wall=5, gb_free=11.9, wall=1066
2024-08-11 17:12:43 | INFO | train_inner | epoch 005:     56 / 63 loss=8.094, nll_loss=5.161, ppl=35.77, wps=1200.9, ups=0.47, wpb=2570, bsz=120.5, num_updates=308, lr=3.696e-06, gnorm=2.701, train_wall=4, gb_free=12.3, wall=1071
2024-08-11 17:12:49 | INFO | train_inner | epoch 005:     58 / 63 loss=8.142, nll_loss=5.214, ppl=37.12, wps=1109.4, ups=0.38, wpb=2902, bsz=140, num_updates=310, lr=3.72e-06, gnorm=2.356, train_wall=5, gb_free=13.4, wall=1076
2024-08-11 17:12:54 | INFO | train_inner | epoch 005:     60 / 63 loss=8.029, nll_loss=5.067, ppl=33.53, wps=1083, ups=0.37, wpb=2914.5, bsz=112, num_updates=312, lr=3.744e-06, gnorm=2.526, train_wall=5, gb_free=11.2, wall=1081
2024-08-11 17:12:59 | INFO | train_inner | epoch 005:     62 / 63 loss=7.94, nll_loss=4.977, ppl=31.49, wps=1295.7, ups=0.38, wpb=3376.5, bsz=168, num_updates=314, lr=3.768e-06, gnorm=2.479, train_wall=5, gb_free=11.2, wall=1087
2024-08-11 17:13:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19191.109375Mb; avail=235875.08203125Mb
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000525
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19191.109375Mb; avail=235875.08203125Mb
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004716
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19191.109375Mb; avail=235875.08203125Mb
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004003
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009570
2024-08-11 17:13:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19191.109375Mb; avail=235875.08203125Mb
2024-08-11 17:13:09 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.572 | nll_loss 4.313 | ppl 19.88 | wps 3065.7 | wpb 1463.4 | bsz 64.9 | num_updates 315 | best_loss 7.572
2024-08-11 17:13:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 315 updates
2024-08-11 17:13:09 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:13:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:14:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 5 @ 315 updates, score 7.572) (writing took 61.210374013055116 seconds)
2024-08-11 17:14:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-08-11 17:14:10 | INFO | train | epoch 005 | loss 8.05 | nll_loss 5.1 | ppl 34.31 | wps 896.9 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 315 | lr 3.78e-06 | gnorm 2.387 | train_wall 169 | gb_free 15.2 | wall 1158
2024-08-11 17:14:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:14:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:14:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:14:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000753
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38109.78515625Mb; avail=216955.9140625Mb
2024-08-11 17:14:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000141
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001265
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38113.23046875Mb; avail=216952.9609375Mb
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38113.72265625Mb; avail=216952.46875Mb
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000448
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002074
2024-08-11 17:14:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38115.19921875Mb; avail=216950.9921875Mb
2024-08-11 17:14:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:14:10 | INFO | fairseq.trainer | begin training epoch 6
2024-08-11 17:14:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:14:13 | INFO | train_inner | epoch 006:      1 / 63 loss=7.811, nll_loss=4.801, ppl=27.88, wps=85.9, ups=0.03, wpb=3170, bsz=172, num_updates=316, lr=3.792e-06, gnorm=2.544, train_wall=4, gb_free=13.3, wall=1160
2024-08-11 17:14:18 | INFO | train_inner | epoch 006:      3 / 63 loss=7.918, nll_loss=4.94, ppl=30.69, wps=1373.2, ups=0.42, wpb=3245.5, bsz=156, num_updates=318, lr=3.816e-06, gnorm=2.451, train_wall=5, gb_free=12.8, wall=1165
2024-08-11 17:14:23 | INFO | train_inner | epoch 006:      5 / 63 loss=7.977, nll_loss=5.028, ppl=32.63, wps=1279.7, ups=0.4, wpb=3231, bsz=200, num_updates=320, lr=3.84e-06, gnorm=2.235, train_wall=5, gb_free=13.6, wall=1170
2024-08-11 17:14:28 | INFO | train_inner | epoch 006:      7 / 63 loss=8.007, nll_loss=5.033, ppl=32.73, wps=1442, ups=0.38, wpb=3782, bsz=144, num_updates=322, lr=3.864e-06, gnorm=2.215, train_wall=5, gb_free=13.2, wall=1175
2024-08-11 17:14:33 | INFO | train_inner | epoch 006:      9 / 63 loss=7.974, nll_loss=4.998, ppl=31.97, wps=1421.2, ups=0.4, wpb=3575, bsz=144, num_updates=324, lr=3.888e-06, gnorm=2.244, train_wall=5, gb_free=13.8, wall=1180
2024-08-11 17:14:43 | INFO | train_inner | epoch 006:     11 / 63 loss=7.738, nll_loss=4.708, ppl=26.14, wps=762.4, ups=0.19, wpb=3924.5, bsz=196, num_updates=326, lr=3.912e-06, gnorm=2.275, train_wall=10, gb_free=10.4, wall=1191
2024-08-11 17:14:47 | INFO | train_inner | epoch 006:     13 / 63 loss=8.066, nll_loss=5.114, ppl=34.62, wps=1543.6, ups=0.56, wpb=2743, bsz=84.5, num_updates=328, lr=3.936e-06, gnorm=2.822, train_wall=4, gb_free=10.5, wall=1194
2024-08-11 17:14:52 | INFO | train_inner | epoch 006:     15 / 63 loss=7.915, nll_loss=4.922, ppl=30.31, wps=1379.4, ups=0.39, wpb=3531, bsz=148, num_updates=330, lr=3.96e-06, gnorm=2.329, train_wall=5, gb_free=10.9, wall=1199
2024-08-11 17:14:58 | INFO | train_inner | epoch 006:     17 / 63 loss=8.029, nll_loss=5.066, ppl=33.51, wps=1256.7, ups=0.37, wpb=3410.5, bsz=92, num_updates=332, lr=3.984e-06, gnorm=2.506, train_wall=5, gb_free=10.5, wall=1205
2024-08-11 17:15:02 | INFO | train_inner | epoch 006:     19 / 63 loss=8.017, nll_loss=5.049, ppl=33.1, wps=1345.1, ups=0.4, wpb=3346, bsz=100, num_updates=334, lr=4.008e-06, gnorm=2.495, train_wall=5, gb_free=10.9, wall=1210
2024-08-11 17:15:08 | INFO | train_inner | epoch 006:     21 / 63 loss=7.976, nll_loss=4.993, ppl=31.85, wps=1322.4, ups=0.38, wpb=3471.5, bsz=128, num_updates=336, lr=4.032e-06, gnorm=2.352, train_wall=5, gb_free=11.1, wall=1215
2024-08-11 17:15:12 | INFO | train_inner | epoch 006:     23 / 63 loss=8.058, nll_loss=5.096, ppl=34.2, wps=1529.7, ups=0.51, wpb=3028.5, bsz=84, num_updates=338, lr=4.056e-06, gnorm=2.983, train_wall=4, gb_free=19.6, wall=1219
2024-08-11 17:15:17 | INFO | train_inner | epoch 006:     25 / 63 loss=8.025, nll_loss=5.06, ppl=33.36, wps=1363.6, ups=0.39, wpb=3488.5, bsz=116, num_updates=340, lr=4.08e-06, gnorm=2.394, train_wall=5, gb_free=10.9, wall=1224
2024-08-11 17:15:22 | INFO | train_inner | epoch 006:     27 / 63 loss=7.838, nll_loss=4.825, ppl=28.34, wps=1374.3, ups=0.39, wpb=3507.5, bsz=132, num_updates=342, lr=4.104e-06, gnorm=2.401, train_wall=5, gb_free=11.9, wall=1229
2024-08-11 17:15:27 | INFO | train_inner | epoch 006:     29 / 63 loss=7.782, nll_loss=4.76, ppl=27.09, wps=1311, ups=0.38, wpb=3479.5, bsz=164, num_updates=344, lr=4.128e-06, gnorm=2.297, train_wall=5, gb_free=12, wall=1235
2024-08-11 17:15:33 | INFO | train_inner | epoch 006:     31 / 63 loss=7.938, nll_loss=4.944, ppl=30.78, wps=1437.4, ups=0.37, wpb=3841.5, bsz=132, num_updates=346, lr=4.152e-06, gnorm=2.342, train_wall=5, gb_free=13.1, wall=1240
2024-08-11 17:15:38 | INFO | train_inner | epoch 006:     33 / 63 loss=7.739, nll_loss=4.71, ppl=26.18, wps=1361.8, ups=0.39, wpb=3533, bsz=164, num_updates=348, lr=4.176e-06, gnorm=2.327, train_wall=5, gb_free=12.5, wall=1245
2024-08-11 17:15:43 | INFO | train_inner | epoch 006:     35 / 63 loss=8.084, nll_loss=5.139, ppl=35.23, wps=1282.6, ups=0.39, wpb=3250.5, bsz=100, num_updates=350, lr=4.2e-06, gnorm=2.62, train_wall=5, gb_free=14.5, wall=1250
2024-08-11 17:15:48 | INFO | train_inner | epoch 006:     37 / 63 loss=7.787, nll_loss=4.782, ppl=27.51, wps=1442.5, ups=0.41, wpb=3552, bsz=204, num_updates=352, lr=4.224e-06, gnorm=2.257, train_wall=5, gb_free=12.4, wall=1255
2024-08-11 17:15:53 | INFO | train_inner | epoch 006:     39 / 63 loss=7.688, nll_loss=4.646, ppl=25.05, wps=1292.9, ups=0.38, wpb=3366, bsz=204, num_updates=354, lr=4.248e-06, gnorm=2.225, train_wall=5, gb_free=11.9, wall=1260
2024-08-11 17:15:59 | INFO | train_inner | epoch 006:     41 / 63 loss=7.816, nll_loss=4.803, ppl=27.92, wps=1253, ups=0.36, wpb=3519, bsz=164, num_updates=356, lr=4.272e-06, gnorm=2.244, train_wall=6, gb_free=11.5, wall=1266
2024-08-11 17:16:04 | INFO | train_inner | epoch 006:     43 / 63 loss=7.918, nll_loss=4.94, ppl=30.69, wps=1181.7, ups=0.4, wpb=2989, bsz=160, num_updates=358, lr=4.296e-06, gnorm=2.495, train_wall=5, gb_free=10.3, wall=1271
2024-08-11 17:16:09 | INFO | train_inner | epoch 006:     45 / 63 loss=7.564, nll_loss=4.493, ppl=22.51, wps=1341.9, ups=0.38, wpb=3490.5, bsz=212, num_updates=360, lr=4.32e-06, gnorm=2.245, train_wall=5, gb_free=11, wall=1276
2024-08-11 17:16:14 | INFO | train_inner | epoch 006:     47 / 63 loss=7.895, nll_loss=4.902, ppl=29.9, wps=1354.3, ups=0.37, wpb=3674.5, bsz=164, num_updates=362, lr=4.344e-06, gnorm=2.091, train_wall=5, gb_free=11.1, wall=1282
2024-08-11 17:16:20 | INFO | train_inner | epoch 006:     49 / 63 loss=7.721, nll_loss=4.692, ppl=25.85, wps=1271.6, ups=0.37, wpb=3426, bsz=176, num_updates=364, lr=4.368e-06, gnorm=2.33, train_wall=5, gb_free=11.5, wall=1287
2024-08-11 17:16:25 | INFO | train_inner | epoch 006:     51 / 63 loss=7.751, nll_loss=4.713, ppl=26.23, wps=1372.7, ups=0.39, wpb=3546, bsz=124, num_updates=366, lr=4.392e-06, gnorm=2.399, train_wall=5, gb_free=12.6, wall=1292
2024-08-11 17:16:30 | INFO | train_inner | epoch 006:     53 / 63 loss=7.878, nll_loss=4.88, ppl=29.45, wps=1362.9, ups=0.38, wpb=3586, bsz=148, num_updates=368, lr=4.416e-06, gnorm=2.727, train_wall=5, gb_free=13.5, wall=1297
2024-08-11 17:16:35 | INFO | train_inner | epoch 006:     55 / 63 loss=7.941, nll_loss=4.943, ppl=30.75, wps=1201.7, ups=0.4, wpb=3002, bsz=96, num_updates=370, lr=4.44e-06, gnorm=2.472, train_wall=5, gb_free=12.4, wall=1302
2024-08-11 17:16:40 | INFO | train_inner | epoch 006:     57 / 63 loss=7.811, nll_loss=4.801, ppl=27.88, wps=1302.4, ups=0.39, wpb=3327.5, bsz=156, num_updates=372, lr=4.464e-06, gnorm=2.425, train_wall=5, gb_free=13.3, wall=1308
2024-08-11 17:16:45 | INFO | train_inner | epoch 006:     59 / 63 loss=7.602, nll_loss=4.54, ppl=23.26, wps=1488.7, ups=0.4, wpb=3732, bsz=200, num_updates=374, lr=4.488e-06, gnorm=2.347, train_wall=5, gb_free=13.1, wall=1313
2024-08-11 17:16:51 | INFO | train_inner | epoch 006:     61 / 63 loss=7.972, nll_loss=5.002, ppl=32.04, wps=1196.1, ups=0.37, wpb=3267.5, bsz=132, num_updates=376, lr=4.512e-06, gnorm=2.388, train_wall=5, gb_free=9.5, wall=1318
2024-08-11 17:16:55 | INFO | train_inner | epoch 006:     63 / 63 loss=7.614, nll_loss=4.55, ppl=23.42, wps=1158.6, ups=0.5, wpb=2326.5, bsz=132, num_updates=378, lr=4.536e-06, gnorm=2.555, train_wall=4, gb_free=19, wall=1322
2024-08-11 17:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19235.3984375Mb; avail=235830.84375Mb
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000523
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19235.3984375Mb; avail=235830.84375Mb
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004715
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19235.3984375Mb; avail=235830.84375Mb
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003979
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009551
2024-08-11 17:16:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19235.3984375Mb; avail=235830.84375Mb
2024-08-11 17:17:03 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.416 | nll_loss 4.091 | ppl 17.05 | wps 3059.3 | wpb 1463.4 | bsz 64.9 | num_updates 378 | best_loss 7.416
2024-08-11 17:17:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 378 updates
2024-08-11 17:17:03 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:17:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:18:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 6 @ 378 updates, score 7.416) (writing took 60.74486920097843 seconds)
2024-08-11 17:18:04 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-08-11 17:18:04 | INFO | train | epoch 006 | loss 7.869 | nll_loss 4.869 | ppl 29.22 | wps 917.7 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 378 | lr 4.536e-06 | gnorm 2.397 | train_wall 164 | gb_free 19 | wall 1391
2024-08-11 17:18:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:18:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:18:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:18:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000809
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43742.34375Mb; avail=211323.8828125Mb
2024-08-11 17:18:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000196
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001310
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43742.8359375Mb; avail=211323.390625Mb
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000056
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43742.8359375Mb; avail=211323.8828125Mb
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000450
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002119
2024-08-11 17:18:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43742.8359375Mb; avail=211323.390625Mb
2024-08-11 17:18:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:18:04 | INFO | fairseq.trainer | begin training epoch 7
2024-08-11 17:18:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:18:09 | INFO | train_inner | epoch 007:      2 / 63 loss=7.796, nll_loss=4.76, ppl=27.1, wps=75.9, ups=0.03, wpb=2816, bsz=84, num_updates=380, lr=4.56e-06, gnorm=2.707, train_wall=5, gb_free=12.5, wall=1396
2024-08-11 17:18:19 | INFO | train_inner | epoch 007:      4 / 63 loss=7.757, nll_loss=4.729, ppl=26.52, wps=719.2, ups=0.2, wpb=3677.5, bsz=180, num_updates=382, lr=4.584e-06, gnorm=2.294, train_wall=10, gb_free=11.8, wall=1406
2024-08-11 17:18:24 | INFO | train_inner | epoch 007:      6 / 63 loss=7.579, nll_loss=4.5, ppl=22.63, wps=1463, ups=0.39, wpb=3717, bsz=192, num_updates=384, lr=4.608e-06, gnorm=2.222, train_wall=5, gb_free=12, wall=1412
2024-08-11 17:18:28 | INFO | train_inner | epoch 007:      8 / 63 loss=7.843, nll_loss=4.817, ppl=28.19, wps=1316.4, ups=0.48, wpb=2719.5, bsz=52.5, num_updates=386, lr=4.632e-06, gnorm=2.855, train_wall=4, gb_free=11.9, wall=1416
2024-08-11 17:18:34 | INFO | train_inner | epoch 007:     10 / 63 loss=7.709, nll_loss=4.664, ppl=25.35, wps=1503.9, ups=0.38, wpb=3970, bsz=164, num_updates=388, lr=4.656e-06, gnorm=2.222, train_wall=5, gb_free=11.8, wall=1421
2024-08-11 17:18:39 | INFO | train_inner | epoch 007:     12 / 63 loss=7.75, nll_loss=4.719, ppl=26.34, wps=1217.9, ups=0.4, wpb=3074.5, bsz=128, num_updates=390, lr=4.68e-06, gnorm=2.545, train_wall=5, gb_free=10.3, wall=1426
2024-08-11 17:18:44 | INFO | train_inner | epoch 007:     14 / 63 loss=7.916, nll_loss=4.925, ppl=30.37, wps=1173.6, ups=0.4, wpb=2939, bsz=112, num_updates=392, lr=4.704e-06, gnorm=2.53, train_wall=5, gb_free=10.8, wall=1431
2024-08-11 17:18:49 | INFO | train_inner | epoch 007:     16 / 63 loss=7.829, nll_loss=4.824, ppl=28.33, wps=1329.5, ups=0.38, wpb=3456.5, bsz=144, num_updates=394, lr=4.728e-06, gnorm=2.29, train_wall=5, gb_free=13.1, wall=1436
2024-08-11 17:18:54 | INFO | train_inner | epoch 007:     18 / 63 loss=7.591, nll_loss=4.521, ppl=22.96, wps=1415, ups=0.4, wpb=3497, bsz=172, num_updates=396, lr=4.752e-06, gnorm=2.217, train_wall=5, gb_free=11.1, wall=1441
2024-08-11 17:18:59 | INFO | train_inner | epoch 007:     20 / 63 loss=7.875, nll_loss=4.88, ppl=29.45, wps=1164.8, ups=0.38, wpb=3052, bsz=132, num_updates=398, lr=4.776e-06, gnorm=2.635, train_wall=5, gb_free=9.6, wall=1446
2024-08-11 17:19:04 | INFO | train_inner | epoch 007:     22 / 63 loss=7.487, nll_loss=4.393, ppl=21, wps=1442.6, ups=0.38, wpb=3815.5, bsz=204, num_updates=400, lr=4.8e-06, gnorm=2.583, train_wall=5, gb_free=12.6, wall=1452
2024-08-11 17:19:09 | INFO | train_inner | epoch 007:     24 / 63 loss=8.156, nll_loss=5.227, ppl=37.45, wps=1163.6, ups=0.43, wpb=2707.5, bsz=76, num_updates=402, lr=4.824e-06, gnorm=2.828, train_wall=5, gb_free=12.2, wall=1456
2024-08-11 17:19:14 | INFO | train_inner | epoch 007:     26 / 63 loss=7.657, nll_loss=4.598, ppl=24.21, wps=1417.8, ups=0.37, wpb=3781, bsz=144, num_updates=404, lr=4.848e-06, gnorm=2.25, train_wall=5, gb_free=12.5, wall=1462
2024-08-11 17:19:19 | INFO | train_inner | epoch 007:     28 / 63 loss=7.978, nll_loss=5.012, ppl=32.27, wps=1308.3, ups=0.4, wpb=3232, bsz=124, num_updates=406, lr=4.872e-06, gnorm=2.381, train_wall=5, gb_free=11.1, wall=1467
2024-08-11 17:19:24 | INFO | train_inner | epoch 007:     30 / 63 loss=7.647, nll_loss=4.595, ppl=24.17, wps=1596.3, ups=0.38, wpb=4170.5, bsz=208, num_updates=408, lr=4.896e-06, gnorm=2.134, train_wall=5, gb_free=11.2, wall=1472
2024-08-11 17:19:30 | INFO | train_inner | epoch 007:     32 / 63 loss=7.565, nll_loss=4.486, ppl=22.41, wps=1307.9, ups=0.38, wpb=3477.5, bsz=200, num_updates=410, lr=4.92e-06, gnorm=2.181, train_wall=5, gb_free=11.1, wall=1477
2024-08-11 17:19:35 | INFO | train_inner | epoch 007:     34 / 63 loss=7.701, nll_loss=4.665, ppl=25.38, wps=1378.7, ups=0.4, wpb=3459.5, bsz=164, num_updates=412, lr=4.944e-06, gnorm=2.474, train_wall=5, gb_free=12, wall=1482
2024-08-11 17:19:40 | INFO | train_inner | epoch 007:     36 / 63 loss=7.68, nll_loss=4.618, ppl=24.55, wps=1248.4, ups=0.39, wpb=3169.5, bsz=152, num_updates=414, lr=4.968e-06, gnorm=2.365, train_wall=5, gb_free=14.2, wall=1487
2024-08-11 17:19:45 | INFO | train_inner | epoch 007:     38 / 63 loss=7.613, nll_loss=4.542, ppl=23.29, wps=1316.3, ups=0.41, wpb=3248, bsz=136, num_updates=416, lr=4.992e-06, gnorm=2.323, train_wall=5, gb_free=10.2, wall=1492
2024-08-11 17:19:50 | INFO | train_inner | epoch 007:     40 / 63 loss=7.569, nll_loss=4.5, ppl=22.62, wps=1645.5, ups=0.41, wpb=4036, bsz=192, num_updates=418, lr=5.016e-06, gnorm=2.194, train_wall=5, gb_free=16.6, wall=1497
2024-08-11 17:19:55 | INFO | train_inner | epoch 007:     42 / 63 loss=7.69, nll_loss=4.643, ppl=24.99, wps=1102.3, ups=0.4, wpb=2771.5, bsz=132, num_updates=420, lr=5.04e-06, gnorm=2.655, train_wall=5, gb_free=13.6, wall=1502
2024-08-11 17:20:00 | INFO | train_inner | epoch 007:     44 / 63 loss=7.633, nll_loss=4.561, ppl=23.6, wps=1385.5, ups=0.38, wpb=3687.5, bsz=148, num_updates=422, lr=5.064e-06, gnorm=2.267, train_wall=5, gb_free=12.7, wall=1507
2024-08-11 17:20:06 | INFO | train_inner | epoch 007:     46 / 63 loss=7.548, nll_loss=4.466, ppl=22.1, wps=1641.2, ups=0.36, wpb=4559.5, bsz=220, num_updates=424, lr=5.088e-06, gnorm=2.027, train_wall=6, gb_free=12.2, wall=1513
2024-08-11 17:20:11 | INFO | train_inner | epoch 007:     48 / 63 loss=7.705, nll_loss=4.668, ppl=25.43, wps=1381.2, ups=0.36, wpb=3814.5, bsz=172, num_updates=426, lr=5.112e-06, gnorm=2.457, train_wall=6, gb_free=12.4, wall=1519
2024-08-11 17:20:16 | INFO | train_inner | epoch 007:     50 / 63 loss=7.561, nll_loss=4.495, ppl=22.55, wps=1284.8, ups=0.39, wpb=3262.5, bsz=176, num_updates=428, lr=5.136e-06, gnorm=2.336, train_wall=5, gb_free=13.1, wall=1524
2024-08-11 17:20:21 | INFO | train_inner | epoch 007:     52 / 63 loss=7.66, nll_loss=4.611, ppl=24.44, wps=1245.1, ups=0.43, wpb=2882.5, bsz=144, num_updates=430, lr=5.16e-06, gnorm=2.557, train_wall=5, gb_free=12.5, wall=1528
2024-08-11 17:20:26 | INFO | train_inner | epoch 007:     54 / 63 loss=7.687, nll_loss=4.632, ppl=24.79, wps=1391.2, ups=0.4, wpb=3508.5, bsz=108, num_updates=432, lr=5.184e-06, gnorm=2.293, train_wall=5, gb_free=13.1, wall=1533
2024-08-11 17:20:31 | INFO | train_inner | epoch 007:     56 / 63 loss=7.53, nll_loss=4.445, ppl=21.78, wps=1202.2, ups=0.37, wpb=3242.5, bsz=188, num_updates=434, lr=5.208e-06, gnorm=2.168, train_wall=5, gb_free=11.7, wall=1539
2024-08-11 17:20:37 | INFO | train_inner | epoch 007:     58 / 63 loss=7.636, nll_loss=4.565, ppl=23.67, wps=1234.5, ups=0.38, wpb=3229, bsz=132, num_updates=436, lr=5.232e-06, gnorm=2.352, train_wall=5, gb_free=14.3, wall=1544
2024-08-11 17:20:42 | INFO | train_inner | epoch 007:     60 / 63 loss=7.665, nll_loss=4.593, ppl=24.13, wps=1338.1, ups=0.37, wpb=3664.5, bsz=116, num_updates=438, lr=5.256e-06, gnorm=2.304, train_wall=5, gb_free=9.7, wall=1549
2024-08-11 17:20:47 | INFO | train_inner | epoch 007:     62 / 63 loss=7.583, nll_loss=4.504, ppl=22.69, wps=1459.1, ups=0.37, wpb=3894, bsz=156, num_updates=440, lr=5.28e-06, gnorm=2.245, train_wall=5, gb_free=12.9, wall=1555
2024-08-11 17:20:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20413.9375Mb; avail=234652.296875Mb
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000558
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20413.9375Mb; avail=234652.296875Mb
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004707
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20413.9375Mb; avail=234652.296875Mb
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003997
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009629
2024-08-11 17:20:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20413.9375Mb; avail=234652.296875Mb
2024-08-11 17:20:57 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.253 | nll_loss 3.879 | ppl 14.72 | wps 3058 | wpb 1463.4 | bsz 64.9 | num_updates 441 | best_loss 7.253
2024-08-11 17:20:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 441 updates
2024-08-11 17:20:57 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 7 @ 441 updates, score 7.253) (writing took 61.90717739518732 seconds)
2024-08-11 17:21:59 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-08-11 17:21:59 | INFO | train | epoch 007 | loss 7.688 | nll_loss 4.639 | ppl 24.92 | wps 911.3 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 441 | lr 5.292e-06 | gnorm 2.41 | train_wall 164 | gb_free 17.9 | wall 1626
2024-08-11 17:21:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:21:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:21:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:21:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.002581
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41266.1171875Mb; avail=213800.60546875Mb
2024-08-11 17:21:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000147
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001793
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41266.1171875Mb; avail=213800.11328125Mb
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000051
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41266.1171875Mb; avail=213800.11328125Mb
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000433
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002610
2024-08-11 17:21:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41266.1171875Mb; avail=213800.11328125Mb
2024-08-11 17:21:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:21:59 | INFO | fairseq.trainer | begin training epoch 8
2024-08-11 17:21:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:22:02 | INFO | train_inner | epoch 008:      1 / 63 loss=7.883, nll_loss=4.874, ppl=29.33, wps=51.9, ups=0.03, wpb=1924, bsz=68, num_updates=442, lr=5.304e-06, gnorm=3.328, train_wall=4, gb_free=10.8, wall=1629
2024-08-11 17:22:06 | INFO | train_inner | epoch 008:      3 / 63 loss=7.67, nll_loss=4.601, ppl=24.27, wps=1400.8, ups=0.4, wpb=3467.5, bsz=100, num_updates=444, lr=5.328e-06, gnorm=2.372, train_wall=5, gb_free=10.5, wall=1634
2024-08-11 17:22:11 | INFO | train_inner | epoch 008:      5 / 63 loss=7.587, nll_loss=4.513, ppl=22.84, wps=1399.2, ups=0.44, wpb=3203, bsz=128, num_updates=446, lr=5.352e-06, gnorm=2.407, train_wall=5, gb_free=12.7, wall=1638
2024-08-11 17:22:16 | INFO | train_inner | epoch 008:      7 / 63 loss=7.425, nll_loss=4.313, ppl=19.88, wps=1497.9, ups=0.4, wpb=3702.5, bsz=224, num_updates=448, lr=5.376e-06, gnorm=2.075, train_wall=5, gb_free=11.2, wall=1643
2024-08-11 17:22:21 | INFO | train_inner | epoch 008:      9 / 63 loss=7.419, nll_loss=4.294, ppl=19.62, wps=1088.5, ups=0.39, wpb=2805, bsz=104, num_updates=450, lr=5.4e-06, gnorm=2.532, train_wall=5, gb_free=11.1, wall=1648
2024-08-11 17:22:26 | INFO | train_inner | epoch 008:     11 / 63 loss=7.681, nll_loss=4.58, ppl=23.92, wps=1438.5, ups=0.41, wpb=3496.5, bsz=112, num_updates=452, lr=5.424e-06, gnorm=2.454, train_wall=5, gb_free=11.3, wall=1653
2024-08-11 17:22:31 | INFO | train_inner | epoch 008:     13 / 63 loss=7.611, nll_loss=4.542, ppl=23.29, wps=1433.2, ups=0.44, wpb=3247.5, bsz=104, num_updates=454, lr=5.448e-06, gnorm=2.589, train_wall=5, gb_free=12.2, wall=1658
2024-08-11 17:22:35 | INFO | train_inner | epoch 008:     15 / 63 loss=7.679, nll_loss=4.637, ppl=24.89, wps=1294.3, ups=0.41, wpb=3164.5, bsz=128, num_updates=456, lr=5.472e-06, gnorm=2.516, train_wall=5, gb_free=14.8, wall=1663
2024-08-11 17:22:40 | INFO | train_inner | epoch 008:     17 / 63 loss=7.432, nll_loss=4.313, ppl=19.88, wps=1500.4, ups=0.4, wpb=3798, bsz=220, num_updates=458, lr=5.496e-06, gnorm=2.282, train_wall=5, gb_free=11.9, wall=1668
2024-08-11 17:22:46 | INFO | train_inner | epoch 008:     19 / 63 loss=7.632, nll_loss=4.576, ppl=23.86, wps=1270.2, ups=0.39, wpb=3247.5, bsz=136, num_updates=460, lr=5.52e-06, gnorm=2.469, train_wall=5, gb_free=12.2, wall=1673
2024-08-11 17:22:51 | INFO | train_inner | epoch 008:     21 / 63 loss=7.548, nll_loss=4.46, ppl=22.01, wps=1509.3, ups=0.4, wpb=3765, bsz=160, num_updates=462, lr=5.544e-06, gnorm=2.394, train_wall=5, gb_free=13.9, wall=1678
2024-08-11 17:22:56 | INFO | train_inner | epoch 008:     23 / 63 loss=7.474, nll_loss=4.362, ppl=20.57, wps=1274.8, ups=0.39, wpb=3310, bsz=160, num_updates=464, lr=5.568e-06, gnorm=2.438, train_wall=5, gb_free=12.1, wall=1683
2024-08-11 17:23:01 | INFO | train_inner | epoch 008:     25 / 63 loss=7.556, nll_loss=4.475, ppl=22.24, wps=1382, ups=0.36, wpb=3805.5, bsz=172, num_updates=466, lr=5.592e-06, gnorm=2.17, train_wall=5, gb_free=12.4, wall=1689
2024-08-11 17:23:07 | INFO | train_inner | epoch 008:     27 / 63 loss=7.494, nll_loss=4.39, ppl=20.96, wps=1158.6, ups=0.38, wpb=3069, bsz=132, num_updates=468, lr=5.616e-06, gnorm=2.502, train_wall=5, gb_free=11.9, wall=1694
2024-08-11 17:23:11 | INFO | train_inner | epoch 008:     29 / 63 loss=7.634, nll_loss=4.56, ppl=23.58, wps=1168.3, ups=0.43, wpb=2709, bsz=84, num_updates=470, lr=5.64e-06, gnorm=2.696, train_wall=5, gb_free=11.1, wall=1699
2024-08-11 17:23:17 | INFO | train_inner | epoch 008:     31 / 63 loss=7.461, nll_loss=4.356, ppl=20.48, wps=1502, ups=0.37, wpb=4015.5, bsz=224, num_updates=472, lr=5.664e-06, gnorm=2.113, train_wall=5, gb_free=12.8, wall=1704
2024-08-11 17:23:22 | INFO | train_inner | epoch 008:     33 / 63 loss=7.44, nll_loss=4.331, ppl=20.12, wps=1355.5, ups=0.38, wpb=3534.5, bsz=192, num_updates=474, lr=5.688e-06, gnorm=2.17, train_wall=5, gb_free=12.2, wall=1709
2024-08-11 17:23:27 | INFO | train_inner | epoch 008:     35 / 63 loss=7.72, nll_loss=4.669, ppl=25.43, wps=1207.6, ups=0.38, wpb=3180.5, bsz=76, num_updates=476, lr=5.712e-06, gnorm=2.546, train_wall=5, gb_free=12.4, wall=1714
2024-08-11 17:23:32 | INFO | train_inner | epoch 008:     37 / 63 loss=7.355, nll_loss=4.238, ppl=18.87, wps=1685.7, ups=0.37, wpb=4536.5, bsz=244, num_updates=478, lr=5.736e-06, gnorm=1.998, train_wall=5, gb_free=12.8, wall=1720
2024-08-11 17:23:37 | INFO | train_inner | epoch 008:     39 / 63 loss=7.712, nll_loss=4.665, ppl=25.37, wps=1180.3, ups=0.41, wpb=2911.5, bsz=100, num_updates=480, lr=5.76e-06, gnorm=2.481, train_wall=5, gb_free=11.5, wall=1725
2024-08-11 17:23:42 | INFO | train_inner | epoch 008:     41 / 63 loss=7.538, nll_loss=4.451, ppl=21.88, wps=1555.9, ups=0.45, wpb=3470, bsz=136, num_updates=482, lr=5.784e-06, gnorm=2.513, train_wall=4, gb_free=13.3, wall=1729
2024-08-11 17:23:47 | INFO | train_inner | epoch 008:     43 / 63 loss=7.254, nll_loss=4.092, ppl=17.05, wps=1465, ups=0.38, wpb=3906, bsz=228, num_updates=484, lr=5.808e-06, gnorm=2.02, train_wall=5, gb_free=11.6, wall=1735
2024-08-11 17:23:52 | INFO | train_inner | epoch 008:     45 / 63 loss=7.4, nll_loss=4.273, ppl=19.33, wps=1513.9, ups=0.38, wpb=3982.5, bsz=204, num_updates=486, lr=5.832e-06, gnorm=2.102, train_wall=5, gb_free=16.1, wall=1740
2024-08-11 17:23:58 | INFO | train_inner | epoch 008:     47 / 63 loss=7.563, nll_loss=4.464, ppl=22.07, wps=1210.3, ups=0.36, wpb=3374, bsz=104, num_updates=488, lr=5.856e-06, gnorm=2.356, train_wall=6, gb_free=9.8, wall=1745
2024-08-11 17:24:03 | INFO | train_inner | epoch 008:     49 / 63 loss=7.791, nll_loss=4.76, ppl=27.09, wps=1278.7, ups=0.37, wpb=3482.5, bsz=108, num_updates=490, lr=5.88e-06, gnorm=2.382, train_wall=5, gb_free=10.3, wall=1751
2024-08-11 17:24:09 | INFO | train_inner | epoch 008:     51 / 63 loss=7.524, nll_loss=4.431, ppl=21.57, wps=1275.1, ups=0.38, wpb=3383, bsz=164, num_updates=492, lr=5.904e-06, gnorm=2.401, train_wall=5, gb_free=11, wall=1756
2024-08-11 17:24:14 | INFO | train_inner | epoch 008:     53 / 63 loss=7.467, nll_loss=4.353, ppl=20.43, wps=1201.2, ups=0.41, wpb=2960, bsz=128, num_updates=494, lr=5.928e-06, gnorm=2.363, train_wall=5, gb_free=12.3, wall=1761
2024-08-11 17:24:19 | INFO | train_inner | epoch 008:     55 / 63 loss=7.37, nll_loss=4.234, ppl=18.82, wps=1395.1, ups=0.37, wpb=3797, bsz=164, num_updates=496, lr=5.952e-06, gnorm=2.148, train_wall=5, gb_free=13.9, wall=1766
2024-08-11 17:24:25 | INFO | train_inner | epoch 008:     57 / 63 loss=7.462, nll_loss=4.354, ppl=20.45, wps=1266.8, ups=0.37, wpb=3452.5, bsz=140, num_updates=498, lr=5.976e-06, gnorm=2.232, train_wall=5, gb_free=12.4, wall=1772
2024-08-11 17:24:30 | INFO | train_inner | epoch 008:     59 / 63 loss=7.528, nll_loss=4.433, ppl=21.6, wps=1259.3, ups=0.39, wpb=3256, bsz=88, num_updates=500, lr=6e-06, gnorm=2.387, train_wall=5, gb_free=11.1, wall=1777
2024-08-11 17:24:35 | INFO | train_inner | epoch 008:     61 / 63 loss=7.394, nll_loss=4.285, ppl=19.5, wps=1218.6, ups=0.35, wpb=3457.5, bsz=196, num_updates=502, lr=6.024e-06, gnorm=2.077, train_wall=6, gb_free=10.2, wall=1783
2024-08-11 17:24:39 | INFO | train_inner | epoch 008:     63 / 63 loss=7.253, nll_loss=4.104, ppl=17.19, wps=1346.4, ups=0.59, wpb=2297.5, bsz=156.5, num_updates=504, lr=6.048e-06, gnorm=2.724, train_wall=3, gb_free=15.5, wall=1786
2024-08-11 17:24:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26079.875Mb; avail=228986.328125Mb
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000625
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26079.875Mb; avail=228986.328125Mb
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004687
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26079.875Mb; avail=228986.328125Mb
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004016
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009692
2024-08-11 17:24:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26080.36328125Mb; avail=228985.8359375Mb
2024-08-11 17:24:47 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.096 | nll_loss 3.691 | ppl 12.91 | wps 3057.3 | wpb 1463.4 | bsz 64.9 | num_updates 504 | best_loss 7.096
2024-08-11 17:24:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 504 updates
2024-08-11 17:24:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:25:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 8 @ 504 updates, score 7.096) (writing took 61.16435978608206 seconds)
2024-08-11 17:25:48 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-08-11 17:25:48 | INFO | train | epoch 008 | loss 7.519 | nll_loss 4.423 | ppl 21.46 | wps 933.6 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 504 | lr 6.048e-06 | gnorm 2.356 | train_wall 160 | gb_free 15.5 | wall 1856
2024-08-11 17:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000876
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46797.53515625Mb; avail=208268.62890625Mb
2024-08-11 17:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000191
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001401
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46798.02734375Mb; avail=208268.13671875Mb
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000052
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46798.02734375Mb; avail=208268.13671875Mb
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000477
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002282
2024-08-11 17:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46798.02734375Mb; avail=208268.13671875Mb
2024-08-11 17:25:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:25:48 | INFO | fairseq.trainer | begin training epoch 9
2024-08-11 17:25:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:25:54 | INFO | train_inner | epoch 009:      2 / 63 loss=7.33, nll_loss=4.18, ppl=18.13, wps=93.6, ups=0.03, wpb=3502, bsz=132, num_updates=506, lr=6.072e-06, gnorm=2.369, train_wall=5, gb_free=11.8, wall=1861
2024-08-11 17:25:59 | INFO | train_inner | epoch 009:      4 / 63 loss=7.337, nll_loss=4.21, ppl=18.51, wps=1315, ups=0.41, wpb=3211.5, bsz=180, num_updates=508, lr=6.096e-06, gnorm=2.419, train_wall=5, gb_free=12, wall=1866
2024-08-11 17:26:03 | INFO | train_inner | epoch 009:      6 / 63 loss=7.682, nll_loss=4.625, ppl=24.67, wps=1428.3, ups=0.49, wpb=2904, bsz=88.5, num_updates=510, lr=6.12e-06, gnorm=3.252, train_wall=4, gb_free=15, wall=1870
2024-08-11 17:26:08 | INFO | train_inner | epoch 009:      8 / 63 loss=7.279, nll_loss=4.115, ppl=17.33, wps=1596, ups=0.39, wpb=4051.5, bsz=192, num_updates=512, lr=6.144e-06, gnorm=2.246, train_wall=5, gb_free=15.6, wall=1875
2024-08-11 17:26:12 | INFO | train_inner | epoch 009:     10 / 63 loss=7.447, nll_loss=4.318, ppl=19.94, wps=1355.6, ups=0.43, wpb=3162.5, bsz=116, num_updates=514, lr=6.168e-06, gnorm=2.55, train_wall=5, gb_free=14.4, wall=1880
2024-08-11 17:26:17 | INFO | train_inner | epoch 009:     12 / 63 loss=7.348, nll_loss=4.198, ppl=18.36, wps=1591.2, ups=0.39, wpb=4048, bsz=184, num_updates=516, lr=6.192e-06, gnorm=2.232, train_wall=5, gb_free=12, wall=1885
2024-08-11 17:26:23 | INFO | train_inner | epoch 009:     14 / 63 loss=7.273, nll_loss=4.099, ppl=17.14, wps=1405, ups=0.38, wpb=3700.5, bsz=148, num_updates=518, lr=6.216e-06, gnorm=2.272, train_wall=5, gb_free=12.4, wall=1890
2024-08-11 17:26:28 | INFO | train_inner | epoch 009:     16 / 63 loss=7.297, nll_loss=4.133, ppl=17.55, wps=1499.5, ups=0.38, wpb=3976.5, bsz=164, num_updates=520, lr=6.24e-06, gnorm=2.148, train_wall=5, gb_free=12.1, wall=1895
2024-08-11 17:26:44 | INFO | train_inner | epoch 009:     18 / 63 loss=7.559, nll_loss=4.449, ppl=21.84, wps=392.8, ups=0.13, wpb=3044.5, bsz=64, num_updates=522, lr=6.264e-06, gnorm=2.47, train_wall=15, gb_free=9.2, wall=1911
2024-08-11 17:26:49 | INFO | train_inner | epoch 009:     20 / 63 loss=7.434, nll_loss=4.323, ppl=20.01, wps=1186.8, ups=0.4, wpb=2999, bsz=152, num_updates=524, lr=6.288e-06, gnorm=2.667, train_wall=5, gb_free=12.7, wall=1916
2024-08-11 17:26:54 | INFO | train_inner | epoch 009:     22 / 63 loss=7.295, nll_loss=4.135, ppl=17.57, wps=1381.2, ups=0.38, wpb=3680.5, bsz=180, num_updates=526, lr=6.312e-06, gnorm=2.19, train_wall=5, gb_free=10.5, wall=1921
2024-08-11 17:26:59 | INFO | train_inner | epoch 009:     24 / 63 loss=7.352, nll_loss=4.219, ppl=18.63, wps=1283, ups=0.41, wpb=3098.5, bsz=144, num_updates=528, lr=6.336e-06, gnorm=2.417, train_wall=5, gb_free=13, wall=1926
2024-08-11 17:27:04 | INFO | train_inner | epoch 009:     26 / 63 loss=7.486, nll_loss=4.377, ppl=20.78, wps=1098.9, ups=0.39, wpb=2818, bsz=76, num_updates=530, lr=6.36e-06, gnorm=2.668, train_wall=5, gb_free=11.4, wall=1931
2024-08-11 17:27:08 | INFO | train_inner | epoch 009:     28 / 63 loss=7.385, nll_loss=4.243, ppl=18.94, wps=1423.3, ups=0.44, wpb=3212.5, bsz=100, num_updates=532, lr=6.384e-06, gnorm=2.307, train_wall=5, gb_free=15.5, wall=1936
2024-08-11 17:27:13 | INFO | train_inner | epoch 009:     30 / 63 loss=7.703, nll_loss=4.649, ppl=25.08, wps=1343, ups=0.4, wpb=3320.5, bsz=80, num_updates=534, lr=6.408e-06, gnorm=2.325, train_wall=5, gb_free=12, wall=1941
2024-08-11 17:27:19 | INFO | train_inner | epoch 009:     32 / 63 loss=7.183, nll_loss=4.016, ppl=16.18, wps=1664.7, ups=0.36, wpb=4591, bsz=316, num_updates=536, lr=6.432e-06, gnorm=2.003, train_wall=6, gb_free=12.2, wall=1946
2024-08-11 17:27:24 | INFO | train_inner | epoch 009:     34 / 63 loss=7.303, nll_loss=4.151, ppl=17.76, wps=1578.3, ups=0.39, wpb=4068.5, bsz=200, num_updates=538, lr=6.456e-06, gnorm=2.095, train_wall=5, gb_free=11.9, wall=1951
2024-08-11 17:27:29 | INFO | train_inner | epoch 009:     36 / 63 loss=7.32, nll_loss=4.19, ppl=18.25, wps=1276.7, ups=0.38, wpb=3348, bsz=196, num_updates=540, lr=6.48e-06, gnorm=2.269, train_wall=5, gb_free=10.8, wall=1957
2024-08-11 17:27:35 | INFO | train_inner | epoch 009:     38 / 63 loss=7.376, nll_loss=4.238, ppl=18.87, wps=1196.1, ups=0.37, wpb=3238, bsz=132, num_updates=542, lr=6.504e-06, gnorm=2.279, train_wall=5, gb_free=11.3, wall=1962
2024-08-11 17:27:40 | INFO | train_inner | epoch 009:     40 / 63 loss=7.552, nll_loss=4.462, ppl=22.04, wps=1182.6, ups=0.39, wpb=3036, bsz=104, num_updates=544, lr=6.528e-06, gnorm=2.414, train_wall=5, gb_free=12.1, wall=1967
2024-08-11 17:27:45 | INFO | train_inner | epoch 009:     42 / 63 loss=7.412, nll_loss=4.288, ppl=19.54, wps=1042.4, ups=0.38, wpb=2717, bsz=128, num_updates=546, lr=6.552e-06, gnorm=2.292, train_wall=5, gb_free=11.8, wall=1972
2024-08-11 17:27:50 | INFO | train_inner | epoch 009:     44 / 63 loss=7.188, nll_loss=4.009, ppl=16.1, wps=1270, ups=0.39, wpb=3225, bsz=148, num_updates=548, lr=6.576e-06, gnorm=2.2, train_wall=5, gb_free=12.6, wall=1977
2024-08-11 17:27:55 | INFO | train_inner | epoch 009:     46 / 63 loss=7.239, nll_loss=4.073, ppl=16.83, wps=1355.6, ups=0.4, wpb=3409, bsz=164, num_updates=550, lr=6.6e-06, gnorm=2.314, train_wall=5, gb_free=14.7, wall=1982
2024-08-11 17:28:00 | INFO | train_inner | epoch 009:     48 / 63 loss=7.536, nll_loss=4.435, ppl=21.63, wps=1265, ups=0.39, wpb=3241.5, bsz=76, num_updates=552, lr=6.624e-06, gnorm=2.584, train_wall=5, gb_free=11.7, wall=1988
2024-08-11 17:28:06 | INFO | train_inner | epoch 009:     50 / 63 loss=7.224, nll_loss=4.056, ppl=16.63, wps=1418.2, ups=0.36, wpb=3908.5, bsz=196, num_updates=554, lr=6.648e-06, gnorm=2.072, train_wall=6, gb_free=11.6, wall=1993
2024-08-11 17:28:11 | INFO | train_inner | epoch 009:     52 / 63 loss=7.284, nll_loss=4.123, ppl=17.42, wps=1322.7, ups=0.38, wpb=3512.5, bsz=132, num_updates=556, lr=6.672e-06, gnorm=2.487, train_wall=5, gb_free=10.4, wall=1998
2024-08-11 17:28:16 | INFO | train_inner | epoch 009:     54 / 63 loss=7.157, nll_loss=3.983, ppl=15.81, wps=1196, ups=0.38, wpb=3151.5, bsz=228, num_updates=558, lr=6.696e-06, gnorm=2.359, train_wall=5, gb_free=10.2, wall=2004
2024-08-11 17:28:22 | INFO | train_inner | epoch 009:     56 / 63 loss=7.449, nll_loss=4.331, ppl=20.12, wps=1359.5, ups=0.38, wpb=3554, bsz=124, num_updates=560, lr=6.72e-06, gnorm=2.286, train_wall=5, gb_free=12.7, wall=2009
2024-08-11 17:28:27 | INFO | train_inner | epoch 009:     58 / 63 loss=7.358, nll_loss=4.222, ppl=18.66, wps=1322.5, ups=0.4, wpb=3306, bsz=140, num_updates=562, lr=6.744e-06, gnorm=2.482, train_wall=5, gb_free=11.9, wall=2014
2024-08-11 17:28:31 | INFO | train_inner | epoch 009:     60 / 63 loss=7.263, nll_loss=4.1, ppl=17.15, wps=1437.9, ups=0.41, wpb=3542, bsz=164, num_updates=564, lr=6.768e-06, gnorm=2.301, train_wall=5, gb_free=13.4, wall=2019
2024-08-11 17:28:37 | INFO | train_inner | epoch 009:     62 / 63 loss=7.185, nll_loss=4.009, ppl=16.1, wps=1399.7, ups=0.36, wpb=3897, bsz=200, num_updates=566, lr=6.792e-06, gnorm=2.14, train_wall=6, gb_free=11.6, wall=2024
2024-08-11 17:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16194.08984375Mb; avail=238872.11328125Mb
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000529
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16194.08984375Mb; avail=238872.11328125Mb
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004695
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16194.08984375Mb; avail=238872.11328125Mb
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004059
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009620
2024-08-11 17:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16194.08984375Mb; avail=238872.11328125Mb
2024-08-11 17:28:47 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.99 | nll_loss 3.543 | ppl 11.66 | wps 3058.4 | wpb 1463.4 | bsz 64.9 | num_updates 567 | best_loss 6.99
2024-08-11 17:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 567 updates
2024-08-11 17:28:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:29:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:29:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 9 @ 567 updates, score 6.99) (writing took 61.16980180097744 seconds)
2024-08-11 17:29:48 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-08-11 17:29:48 | INFO | train | epoch 009 | loss 7.354 | nll_loss 4.214 | ppl 18.56 | wps 894.3 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 567 | lr 6.804e-06 | gnorm 2.38 | train_wall 170 | gb_free 17.1 | wall 2095
2024-08-11 17:29:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:29:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:29:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:29:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000782
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25279.015625Mb; avail=229786.6953125Mb
2024-08-11 17:29:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000139
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001242
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25277.046875Mb; avail=229788.6640625Mb
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25278.5234375Mb; avail=229787.6796875Mb
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000443
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002022
2024-08-11 17:29:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25281.96875Mb; avail=229783.7421875Mb
2024-08-11 17:29:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:29:48 | INFO | fairseq.trainer | begin training epoch 10
2024-08-11 17:29:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:29:51 | INFO | train_inner | epoch 010:      1 / 63 loss=7.368, nll_loss=4.213, ppl=18.55, wps=59.1, ups=0.03, wpb=2177, bsz=60, num_updates=568, lr=6.816e-06, gnorm=3.037, train_wall=4, gb_free=10.7, wall=2098
2024-08-11 17:29:56 | INFO | train_inner | epoch 010:      3 / 63 loss=7.429, nll_loss=4.301, ppl=19.71, wps=1536.9, ups=0.41, wpb=3791, bsz=160, num_updates=570, lr=6.84e-06, gnorm=2.265, train_wall=5, gb_free=12.4, wall=2103
2024-08-11 17:30:01 | INFO | train_inner | epoch 010:      5 / 63 loss=7.292, nll_loss=4.137, ppl=17.6, wps=1485, ups=0.41, wpb=3634, bsz=168, num_updates=572, lr=6.864e-06, gnorm=2.27, train_wall=5, gb_free=13.4, wall=2108
2024-08-11 17:30:05 | INFO | train_inner | epoch 010:      7 / 63 loss=7.225, nll_loss=4.061, ppl=16.69, wps=1548.5, ups=0.4, wpb=3823.5, bsz=196, num_updates=574, lr=6.888e-06, gnorm=2.122, train_wall=5, gb_free=11.3, wall=2113
2024-08-11 17:30:10 | INFO | train_inner | epoch 010:      9 / 63 loss=7.33, nll_loss=4.197, ppl=18.35, wps=1213.9, ups=0.4, wpb=3037.5, bsz=160, num_updates=576, lr=6.912e-06, gnorm=2.339, train_wall=5, gb_free=12.1, wall=2118
2024-08-11 17:30:16 | INFO | train_inner | epoch 010:     11 / 63 loss=7.229, nll_loss=4.06, ppl=16.68, wps=1408, ups=0.39, wpb=3632, bsz=140, num_updates=578, lr=6.936e-06, gnorm=2.413, train_wall=5, gb_free=12.8, wall=2123
2024-08-11 17:30:26 | INFO | train_inner | epoch 010:     13 / 63 loss=7.391, nll_loss=4.257, ppl=19.12, wps=727.6, ups=0.19, wpb=3745.5, bsz=120, num_updates=580, lr=6.96e-06, gnorm=2.18, train_wall=10, gb_free=11.8, wall=2133
2024-08-11 17:30:31 | INFO | train_inner | epoch 010:     15 / 63 loss=7.235, nll_loss=4.068, ppl=16.78, wps=1408.2, ups=0.43, wpb=3278, bsz=128, num_updates=582, lr=6.984e-06, gnorm=2.326, train_wall=5, gb_free=15.8, wall=2138
2024-08-11 17:30:36 | INFO | train_inner | epoch 010:     17 / 63 loss=7.188, nll_loss=4.008, ppl=16.09, wps=1258.3, ups=0.38, wpb=3290.5, bsz=168, num_updates=584, lr=7.008e-06, gnorm=2.38, train_wall=5, gb_free=11.5, wall=2143
2024-08-11 17:30:41 | INFO | train_inner | epoch 010:     19 / 63 loss=7.203, nll_loss=4.008, ppl=16.08, wps=1337.9, ups=0.39, wpb=3399, bsz=96, num_updates=586, lr=7.032e-06, gnorm=2.31, train_wall=5, gb_free=10.7, wall=2148
2024-08-11 17:30:46 | INFO | train_inner | epoch 010:     21 / 63 loss=7.111, nll_loss=3.92, ppl=15.14, wps=1454.9, ups=0.36, wpb=3996, bsz=252, num_updates=588, lr=7.056e-06, gnorm=2.01, train_wall=5, gb_free=10.9, wall=2154
2024-08-11 17:30:51 | INFO | train_inner | epoch 010:     23 / 63 loss=7.431, nll_loss=4.302, ppl=19.72, wps=1332.2, ups=0.42, wpb=3163.5, bsz=104, num_updates=590, lr=7.08e-06, gnorm=2.44, train_wall=5, gb_free=10.6, wall=2158
2024-08-11 17:30:55 | INFO | train_inner | epoch 010:     25 / 63 loss=7.383, nll_loss=4.233, ppl=18.8, wps=1322.4, ups=0.47, wpb=2789, bsz=112.5, num_updates=592, lr=7.104e-06, gnorm=2.744, train_wall=4, gb_free=12, wall=2163
2024-08-11 17:31:01 | INFO | train_inner | epoch 010:     27 / 63 loss=7.384, nll_loss=4.243, ppl=18.94, wps=1132.2, ups=0.38, wpb=2986, bsz=100, num_updates=594, lr=7.128e-06, gnorm=2.464, train_wall=5, gb_free=9.9, wall=2168
2024-08-11 17:31:06 | INFO | train_inner | epoch 010:     29 / 63 loss=7.216, nll_loss=4.042, ppl=16.47, wps=1285.5, ups=0.4, wpb=3254.5, bsz=160, num_updates=596, lr=7.152e-06, gnorm=2.304, train_wall=5, gb_free=12.4, wall=2173
2024-08-11 17:31:10 | INFO | train_inner | epoch 010:     31 / 63 loss=7.317, nll_loss=4.161, ppl=17.89, wps=1191.6, ups=0.42, wpb=2833.5, bsz=104, num_updates=598, lr=7.176e-06, gnorm=2.483, train_wall=5, gb_free=11.3, wall=2178
2024-08-11 17:31:15 | INFO | train_inner | epoch 010:     33 / 63 loss=7.196, nll_loss=4.013, ppl=16.14, wps=1194.1, ups=0.4, wpb=2953.5, bsz=148, num_updates=600, lr=7.2e-06, gnorm=2.526, train_wall=5, gb_free=15.3, wall=2183
2024-08-11 17:31:21 | INFO | train_inner | epoch 010:     35 / 63 loss=7.157, nll_loss=3.965, ppl=15.62, wps=1220, ups=0.37, wpb=3325, bsz=148, num_updates=602, lr=7.224e-06, gnorm=2.334, train_wall=5, gb_free=9.6, wall=2188
2024-08-11 17:31:26 | INFO | train_inner | epoch 010:     37 / 63 loss=7.085, nll_loss=3.889, ppl=14.81, wps=1563.4, ups=0.36, wpb=4394, bsz=252, num_updates=604, lr=7.248e-06, gnorm=1.905, train_wall=6, gb_free=11.5, wall=2194
2024-08-11 17:31:32 | INFO | train_inner | epoch 010:     39 / 63 loss=7.093, nll_loss=3.88, ppl=14.72, wps=1571.3, ups=0.39, wpb=4002, bsz=152, num_updates=606, lr=7.272e-06, gnorm=2.076, train_wall=5, gb_free=11.8, wall=2199
2024-08-11 17:31:37 | INFO | train_inner | epoch 010:     41 / 63 loss=7.159, nll_loss=3.974, ppl=15.72, wps=1220, ups=0.39, wpb=3160, bsz=140, num_updates=608, lr=7.296e-06, gnorm=2.324, train_wall=5, gb_free=11.2, wall=2204
2024-08-11 17:31:42 | INFO | train_inner | epoch 010:     43 / 63 loss=7.139, nll_loss=3.944, ppl=15.39, wps=1487.7, ups=0.36, wpb=4106, bsz=176, num_updates=610, lr=7.32e-06, gnorm=2.255, train_wall=6, gb_free=13.9, wall=2210
2024-08-11 17:31:47 | INFO | train_inner | epoch 010:     45 / 63 loss=7.388, nll_loss=4.238, ppl=18.87, wps=1225.9, ups=0.4, wpb=3038, bsz=100, num_updates=612, lr=7.344e-06, gnorm=2.361, train_wall=5, gb_free=11.4, wall=2215
2024-08-11 17:31:53 | INFO | train_inner | epoch 010:     47 / 63 loss=7.32, nll_loss=4.176, ppl=18.08, wps=1237.2, ups=0.36, wpb=3461, bsz=152, num_updates=614, lr=7.368e-06, gnorm=2.243, train_wall=6, gb_free=9.4, wall=2220
2024-08-11 17:31:58 | INFO | train_inner | epoch 010:     49 / 63 loss=7.031, nll_loss=3.806, ppl=13.99, wps=1378.3, ups=0.39, wpb=3515, bsz=200, num_updates=616, lr=7.392e-06, gnorm=2.228, train_wall=5, gb_free=12.1, wall=2225
2024-08-11 17:32:03 | INFO | train_inner | epoch 010:     51 / 63 loss=7.198, nll_loss=4.008, ppl=16.09, wps=1407.2, ups=0.4, wpb=3553, bsz=148, num_updates=618, lr=7.416e-06, gnorm=2.247, train_wall=5, gb_free=14, wall=2230
2024-08-11 17:32:08 | INFO | train_inner | epoch 010:     53 / 63 loss=7.114, nll_loss=3.906, ppl=14.99, wps=1414.7, ups=0.37, wpb=3776, bsz=164, num_updates=620, lr=7.44e-06, gnorm=2.186, train_wall=5, gb_free=11.8, wall=2236
2024-08-11 17:32:13 | INFO | train_inner | epoch 010:     55 / 63 loss=7.189, nll_loss=3.996, ppl=15.96, wps=1453.3, ups=0.39, wpb=3765.5, bsz=140, num_updates=622, lr=7.464e-06, gnorm=2.359, train_wall=5, gb_free=14.2, wall=2241
2024-08-11 17:32:18 | INFO | train_inner | epoch 010:     57 / 63 loss=7.234, nll_loss=4.061, ppl=16.69, wps=1079.7, ups=0.43, wpb=2497, bsz=96, num_updates=624, lr=7.488e-06, gnorm=2.611, train_wall=5, gb_free=12.7, wall=2245
2024-08-11 17:32:23 | INFO | train_inner | epoch 010:     59 / 63 loss=7.326, nll_loss=4.17, ppl=18, wps=1049.4, ups=0.4, wpb=2639.5, bsz=84, num_updates=626, lr=7.512e-06, gnorm=2.579, train_wall=5, gb_free=12.1, wall=2250
2024-08-11 17:32:29 | INFO | train_inner | epoch 010:     61 / 63 loss=7.026, nll_loss=3.798, ppl=13.91, wps=1514.9, ups=0.37, wpb=4123.5, bsz=204, num_updates=628, lr=7.536e-06, gnorm=2.1, train_wall=5, gb_free=12.1, wall=2256
2024-08-11 17:32:32 | INFO | train_inner | epoch 010:     63 / 63 loss=7.085, nll_loss=3.891, ppl=14.84, wps=1346.9, ups=0.51, wpb=2629, bsz=156, num_updates=630, lr=7.56e-06, gnorm=2.345, train_wall=4, gb_free=16.9, wall=2260
2024-08-11 17:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14834.73828125Mb; avail=240231.5078125Mb
2024-08-11 17:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000527
2024-08-11 17:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14834.73828125Mb; avail=240231.5078125Mb
2024-08-11 17:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004760
2024-08-11 17:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14834.73828125Mb; avail=240231.5078125Mb
2024-08-11 17:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003986
2024-08-11 17:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009625
2024-08-11 17:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14834.73828125Mb; avail=240231.5078125Mb
2024-08-11 17:32:41 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.878 | nll_loss 3.407 | ppl 10.61 | wps 3055.2 | wpb 1463.4 | bsz 64.9 | num_updates 630 | best_loss 6.878
2024-08-11 17:32:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 630 updates
2024-08-11 17:32:41 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:33:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:33:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 10 @ 630 updates, score 6.878) (writing took 58.98984703980386 seconds)
2024-08-11 17:33:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-08-11 17:33:40 | INFO | train | epoch 010 | loss 7.224 | nll_loss 4.048 | ppl 16.55 | wps 923.8 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 630 | lr 7.56e-06 | gnorm 2.314 | train_wall 164 | gb_free 16.9 | wall 2327
2024-08-11 17:33:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:33:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:33:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:33:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000866
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24955.18359375Mb; avail=230099.046875Mb
2024-08-11 17:33:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000140
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001345
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24955.18359375Mb; avail=230099.046875Mb
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000056
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24954.69140625Mb; avail=230099.046875Mb
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000534
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002302
2024-08-11 17:33:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24955.18359375Mb; avail=230099.046875Mb
2024-08-11 17:33:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:33:40 | INFO | fairseq.trainer | begin training epoch 11
2024-08-11 17:33:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:33:44 | INFO | train_inner | epoch 011:      2 / 63 loss=7.371, nll_loss=4.23, ppl=18.76, wps=68, ups=0.03, wpb=2423.5, bsz=56.5, num_updates=632, lr=7.584e-06, gnorm=3.004, train_wall=4, gb_free=14, wall=2331
2024-08-11 17:33:49 | INFO | train_inner | epoch 011:      4 / 63 loss=7, nll_loss=3.76, ppl=13.55, wps=1490, ups=0.4, wpb=3700, bsz=140, num_updates=634, lr=7.608e-06, gnorm=2.179, train_wall=5, gb_free=11.1, wall=2336
2024-08-11 17:33:54 | INFO | train_inner | epoch 011:      6 / 63 loss=7.053, nll_loss=3.839, ppl=14.31, wps=1480, ups=0.39, wpb=3835.5, bsz=204, num_updates=636, lr=7.632e-06, gnorm=2.141, train_wall=5, gb_free=12.4, wall=2341
2024-08-11 17:33:59 | INFO | train_inner | epoch 011:      8 / 63 loss=7.124, nll_loss=3.919, ppl=15.13, wps=1349.5, ups=0.42, wpb=3236, bsz=132, num_updates=638, lr=7.656e-06, gnorm=2.335, train_wall=5, gb_free=11.9, wall=2346
2024-08-11 17:34:09 | INFO | train_inner | epoch 011:     10 / 63 loss=7.413, nll_loss=4.276, ppl=19.37, wps=651.5, ups=0.19, wpb=3372, bsz=72, num_updates=640, lr=7.68e-06, gnorm=2.452, train_wall=10, gb_free=10.6, wall=2356
2024-08-11 17:34:14 | INFO | train_inner | epoch 011:     12 / 63 loss=7.299, nll_loss=4.159, ppl=17.86, wps=1582.6, ups=0.4, wpb=4006.5, bsz=204, num_updates=642, lr=7.704e-06, gnorm=2.279, train_wall=5, gb_free=13, wall=2361
2024-08-11 17:34:19 | INFO | train_inner | epoch 011:     14 / 63 loss=7.052, nll_loss=3.84, ppl=14.32, wps=1473.6, ups=0.4, wpb=3648, bsz=204, num_updates=644, lr=7.728e-06, gnorm=2.089, train_wall=5, gb_free=11.3, wall=2366
2024-08-11 17:34:24 | INFO | train_inner | epoch 011:     16 / 63 loss=7.182, nll_loss=3.984, ppl=15.82, wps=1418.3, ups=0.4, wpb=3571.5, bsz=144, num_updates=646, lr=7.752e-06, gnorm=2.352, train_wall=5, gb_free=11.5, wall=2371
2024-08-11 17:34:29 | INFO | train_inner | epoch 011:     18 / 63 loss=6.846, nll_loss=3.576, ppl=11.93, wps=1449.4, ups=0.41, wpb=3499, bsz=236, num_updates=648, lr=7.776e-06, gnorm=2.049, train_wall=5, gb_free=16.8, wall=2376
2024-08-11 17:34:34 | INFO | train_inner | epoch 011:     20 / 63 loss=7.225, nll_loss=4.033, ppl=16.38, wps=1432.3, ups=0.41, wpb=3481.5, bsz=96, num_updates=650, lr=7.8e-06, gnorm=2.212, train_wall=5, gb_free=9.5, wall=2381
2024-08-11 17:34:39 | INFO | train_inner | epoch 011:     22 / 63 loss=7.205, nll_loss=4.016, ppl=16.18, wps=1394.6, ups=0.39, wpb=3548.5, bsz=124, num_updates=652, lr=7.824e-06, gnorm=2.094, train_wall=5, gb_free=11.9, wall=2386
2024-08-11 17:34:44 | INFO | train_inner | epoch 011:     24 / 63 loss=7.103, nll_loss=3.914, ppl=15.07, wps=1406.4, ups=0.39, wpb=3570.5, bsz=184, num_updates=654, lr=7.848e-06, gnorm=2.137, train_wall=5, gb_free=13.2, wall=2391
2024-08-11 17:34:49 | INFO | train_inner | epoch 011:     26 / 63 loss=6.987, nll_loss=3.751, ppl=13.46, wps=1535.5, ups=0.38, wpb=4047, bsz=172, num_updates=656, lr=7.872e-06, gnorm=2.112, train_wall=5, gb_free=12, wall=2397
2024-08-11 17:34:54 | INFO | train_inner | epoch 011:     28 / 63 loss=7.062, nll_loss=3.85, ppl=14.42, wps=1490, ups=0.39, wpb=3835.5, bsz=164, num_updates=658, lr=7.896e-06, gnorm=2.116, train_wall=5, gb_free=13.4, wall=2402
2024-08-11 17:35:00 | INFO | train_inner | epoch 011:     30 / 63 loss=7.049, nll_loss=3.819, ppl=14.12, wps=1221.8, ups=0.38, wpb=3242, bsz=104, num_updates=660, lr=7.92e-06, gnorm=2.427, train_wall=5, gb_free=11.7, wall=2407
2024-08-11 17:35:05 | INFO | train_inner | epoch 011:     32 / 63 loss=7.143, nll_loss=3.929, ppl=15.23, wps=1211.3, ups=0.37, wpb=3287, bsz=128, num_updates=662, lr=7.944e-06, gnorm=2.264, train_wall=5, gb_free=11.4, wall=2412
2024-08-11 17:35:11 | INFO | train_inner | epoch 011:     34 / 63 loss=6.941, nll_loss=3.699, ppl=12.99, wps=1364.1, ups=0.37, wpb=3651, bsz=228, num_updates=664, lr=7.968e-06, gnorm=2.04, train_wall=5, gb_free=10.9, wall=2418
2024-08-11 17:35:16 | INFO | train_inner | epoch 011:     36 / 63 loss=6.992, nll_loss=3.753, ppl=13.48, wps=1298.9, ups=0.39, wpb=3322.5, bsz=188, num_updates=666, lr=7.992e-06, gnorm=2.177, train_wall=5, gb_free=11.1, wall=2423
2024-08-11 17:35:21 | INFO | train_inner | epoch 011:     38 / 63 loss=7.177, nll_loss=3.994, ppl=15.93, wps=1395.6, ups=0.35, wpb=3976.5, bsz=156, num_updates=668, lr=8.016e-06, gnorm=2.264, train_wall=6, gb_free=10, wall=2429
2024-08-11 17:35:26 | INFO | train_inner | epoch 011:     40 / 63 loss=7.151, nll_loss=3.962, ppl=15.59, wps=1289.7, ups=0.39, wpb=3320, bsz=156, num_updates=670, lr=8.04e-06, gnorm=2.218, train_wall=5, gb_free=11.9, wall=2434
2024-08-11 17:35:32 | INFO | train_inner | epoch 011:     42 / 63 loss=7.047, nll_loss=3.826, ppl=14.18, wps=1254.8, ups=0.37, wpb=3392.5, bsz=152, num_updates=672, lr=8.064e-06, gnorm=2.067, train_wall=5, gb_free=10.3, wall=2439
2024-08-11 17:35:37 | INFO | train_inner | epoch 011:     44 / 63 loss=6.969, nll_loss=3.733, ppl=13.29, wps=1400.7, ups=0.38, wpb=3656, bsz=160, num_updates=674, lr=8.088e-06, gnorm=2.079, train_wall=5, gb_free=15.2, wall=2444
2024-08-11 17:35:42 | INFO | train_inner | epoch 011:     46 / 63 loss=7.183, nll_loss=3.99, ppl=15.89, wps=1274.1, ups=0.39, wpb=3288.5, bsz=132, num_updates=676, lr=8.112e-06, gnorm=2.283, train_wall=5, gb_free=12.3, wall=2450
2024-08-11 17:35:47 | INFO | train_inner | epoch 011:     48 / 63 loss=6.978, nll_loss=3.75, ppl=13.46, wps=1320.4, ups=0.38, wpb=3449.5, bsz=204, num_updates=678, lr=8.136e-06, gnorm=2.232, train_wall=5, gb_free=12.3, wall=2455
2024-08-11 17:35:53 | INFO | train_inner | epoch 011:     50 / 63 loss=7.215, nll_loss=4.03, ppl=16.33, wps=1205.8, ups=0.39, wpb=3121, bsz=96, num_updates=680, lr=8.16e-06, gnorm=2.316, train_wall=5, gb_free=11.5, wall=2460
2024-08-11 17:35:58 | INFO | train_inner | epoch 011:     52 / 63 loss=6.998, nll_loss=3.775, ppl=13.69, wps=1350.7, ups=0.39, wpb=3496.5, bsz=208, num_updates=682, lr=8.184e-06, gnorm=2.116, train_wall=5, gb_free=12.1, wall=2465
2024-08-11 17:36:03 | INFO | train_inner | epoch 011:     54 / 63 loss=6.942, nll_loss=3.701, ppl=13.01, wps=1315, ups=0.38, wpb=3442.5, bsz=176, num_updates=684, lr=8.208e-06, gnorm=2.269, train_wall=5, gb_free=10.5, wall=2470
2024-08-11 17:36:08 | INFO | train_inner | epoch 011:     56 / 63 loss=7.091, nll_loss=3.867, ppl=14.59, wps=1300.9, ups=0.37, wpb=3509, bsz=80, num_updates=686, lr=8.232e-06, gnorm=2.291, train_wall=5, gb_free=12.1, wall=2476
2024-08-11 17:36:13 | INFO | train_inner | epoch 011:     58 / 63 loss=7.206, nll_loss=4.035, ppl=16.4, wps=1370.1, ups=0.43, wpb=3175.5, bsz=132, num_updates=688, lr=8.256e-06, gnorm=2.443, train_wall=5, gb_free=10.6, wall=2480
2024-08-11 17:36:18 | INFO | train_inner | epoch 011:     60 / 63 loss=7.121, nll_loss=3.918, ppl=15.12, wps=1015.6, ups=0.38, wpb=2647.5, bsz=84, num_updates=690, lr=8.28e-06, gnorm=2.554, train_wall=5, gb_free=13.8, wall=2486
2024-08-11 17:36:23 | INFO | train_inner | epoch 011:     62 / 63 loss=6.898, nll_loss=3.629, ppl=12.37, wps=1078.4, ups=0.41, wpb=2659, bsz=136, num_updates=692, lr=8.304e-06, gnorm=2.537, train_wall=5, gb_free=11.5, wall=2491
2024-08-11 17:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15879.61328125Mb; avail=239174.63671875Mb
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000543
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15879.61328125Mb; avail=239174.63671875Mb
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004736
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15879.61328125Mb; avail=239174.63671875Mb
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004084
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009727
2024-08-11 17:36:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15879.61328125Mb; avail=239174.63671875Mb
2024-08-11 17:36:33 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.788 | nll_loss 3.283 | ppl 9.73 | wps 3058.6 | wpb 1463.4 | bsz 64.9 | num_updates 693 | best_loss 6.788
2024-08-11 17:36:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 693 updates
2024-08-11 17:36:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:37:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:37:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 11 @ 693 updates, score 6.788) (writing took 58.808912965934724 seconds)
2024-08-11 17:37:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-08-11 17:37:32 | INFO | train | epoch 011 | loss 7.095 | nll_loss 3.886 | ppl 14.79 | wps 924.7 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 693 | lr 8.316e-06 | gnorm 2.283 | train_wall 164 | gb_free 20.2 | wall 2559
2024-08-11 17:37:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:37:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:37:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:37:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000776
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26348.51953125Mb; avail=228718.1953125Mb
2024-08-11 17:37:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000191
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001305
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26348.02734375Mb; avail=228717.703125Mb
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000056
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26348.02734375Mb; avail=228718.1953125Mb
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000468
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002122
2024-08-11 17:37:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26348.02734375Mb; avail=228718.1953125Mb
2024-08-11 17:37:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:37:32 | INFO | fairseq.trainer | begin training epoch 12
2024-08-11 17:37:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:37:34 | INFO | train_inner | epoch 012:      1 / 63 loss=6.902, nll_loss=3.633, ppl=12.41, wps=72.3, ups=0.03, wpb=2569, bsz=104, num_updates=694, lr=8.328e-06, gnorm=2.782, train_wall=4, gb_free=11.8, wall=2562
2024-08-11 17:37:39 | INFO | train_inner | epoch 012:      3 / 63 loss=7.016, nll_loss=3.778, ppl=13.72, wps=1556, ups=0.4, wpb=3889.5, bsz=160, num_updates=696, lr=8.352e-06, gnorm=2.137, train_wall=5, gb_free=12.7, wall=2567
2024-08-11 17:37:43 | INFO | train_inner | epoch 012:      5 / 63 loss=7.129, nll_loss=3.919, ppl=15.13, wps=1239.1, ups=0.52, wpb=2386.5, bsz=88.5, num_updates=698, lr=8.376e-06, gnorm=2.61, train_wall=4, gb_free=11.6, wall=2571
2024-08-11 17:37:53 | INFO | train_inner | epoch 012:      7 / 63 loss=7.101, nll_loss=3.881, ppl=14.73, wps=638.5, ups=0.21, wpb=2995.5, bsz=80, num_updates=700, lr=8.4e-06, gnorm=2.396, train_wall=9, gb_free=13.4, wall=2580
2024-08-11 17:37:58 | INFO | train_inner | epoch 012:      9 / 63 loss=7.014, nll_loss=3.782, ppl=13.76, wps=1275, ups=0.38, wpb=3332.5, bsz=140, num_updates=702, lr=8.424e-06, gnorm=2.278, train_wall=5, gb_free=12.7, wall=2585
2024-08-11 17:38:03 | INFO | train_inner | epoch 012:     11 / 63 loss=7.166, nll_loss=3.965, ppl=15.62, wps=1336.7, ups=0.41, wpb=3227, bsz=88, num_updates=704, lr=8.448e-06, gnorm=2.589, train_wall=5, gb_free=12.8, wall=2590
2024-08-11 17:38:08 | INFO | train_inner | epoch 012:     13 / 63 loss=6.935, nll_loss=3.681, ppl=12.82, wps=1268.8, ups=0.4, wpb=3141.5, bsz=148, num_updates=706, lr=8.472e-06, gnorm=2.176, train_wall=5, gb_free=12, wall=2595
2024-08-11 17:38:12 | INFO | train_inner | epoch 012:     15 / 63 loss=6.875, nll_loss=3.606, ppl=12.17, wps=1251.1, ups=0.43, wpb=2885, bsz=160, num_updates=708, lr=8.496e-06, gnorm=2.283, train_wall=5, gb_free=13.4, wall=2600
2024-08-11 17:38:18 | INFO | train_inner | epoch 012:     17 / 63 loss=6.888, nll_loss=3.636, ppl=12.43, wps=1434.2, ups=0.36, wpb=3954, bsz=216, num_updates=710, lr=8.52e-06, gnorm=2.002, train_wall=6, gb_free=10.8, wall=2605
2024-08-11 17:38:23 | INFO | train_inner | epoch 012:     19 / 63 loss=7.102, nll_loss=3.886, ppl=14.78, wps=1425.5, ups=0.41, wpb=3469, bsz=116, num_updates=712, lr=8.544e-06, gnorm=2.353, train_wall=5, gb_free=15.2, wall=2610
2024-08-11 17:38:28 | INFO | train_inner | epoch 012:     21 / 63 loss=7.064, nll_loss=3.831, ppl=14.23, wps=1244.7, ups=0.36, wpb=3415.5, bsz=92, num_updates=714, lr=8.568e-06, gnorm=2.5, train_wall=5, gb_free=11.9, wall=2615
2024-08-11 17:38:33 | INFO | train_inner | epoch 012:     23 / 63 loss=6.909, nll_loss=3.65, ppl=12.55, wps=1386.5, ups=0.4, wpb=3460.5, bsz=156, num_updates=716, lr=8.592e-06, gnorm=2.295, train_wall=5, gb_free=13.2, wall=2620
2024-08-11 17:38:38 | INFO | train_inner | epoch 012:     25 / 63 loss=6.849, nll_loss=3.591, ppl=12.05, wps=1216.8, ups=0.4, wpb=3027, bsz=184, num_updates=718, lr=8.616e-06, gnorm=2.235, train_wall=5, gb_free=15.9, wall=2625
2024-08-11 17:38:44 | INFO | train_inner | epoch 012:     27 / 63 loss=6.888, nll_loss=3.624, ppl=12.33, wps=1436.2, ups=0.37, wpb=3934, bsz=172, num_updates=720, lr=8.64e-06, gnorm=2.191, train_wall=5, gb_free=10, wall=2631
2024-08-11 17:38:49 | INFO | train_inner | epoch 012:     29 / 63 loss=6.907, nll_loss=3.652, ppl=12.57, wps=1302, ups=0.37, wpb=3527.5, bsz=176, num_updates=722, lr=8.664e-06, gnorm=2.209, train_wall=5, gb_free=12.7, wall=2636
2024-08-11 17:38:54 | INFO | train_inner | epoch 012:     31 / 63 loss=7.067, nll_loss=3.85, ppl=14.42, wps=1397.3, ups=0.41, wpb=3437, bsz=144, num_updates=724, lr=8.688e-06, gnorm=2.394, train_wall=5, gb_free=12.6, wall=2641
2024-08-11 17:38:59 | INFO | train_inner | epoch 012:     33 / 63 loss=6.935, nll_loss=3.678, ppl=12.8, wps=1230.7, ups=0.39, wpb=3164.5, bsz=164, num_updates=726, lr=8.712e-06, gnorm=2.215, train_wall=5, gb_free=11.6, wall=2646
2024-08-11 17:39:04 | INFO | train_inner | epoch 012:     35 / 63 loss=6.807, nll_loss=3.531, ppl=11.56, wps=1280.9, ups=0.39, wpb=3283, bsz=208, num_updates=728, lr=8.736e-06, gnorm=2.139, train_wall=5, gb_free=12, wall=2651
2024-08-11 17:39:09 | INFO | train_inner | epoch 012:     37 / 63 loss=7.02, nll_loss=3.805, ppl=13.98, wps=1350, ups=0.41, wpb=3290.5, bsz=176, num_updates=730, lr=8.76e-06, gnorm=2.178, train_wall=5, gb_free=9.3, wall=2656
2024-08-11 17:39:14 | INFO | train_inner | epoch 012:     39 / 63 loss=6.801, nll_loss=3.519, ppl=11.46, wps=1262.3, ups=0.38, wpb=3364, bsz=184, num_updates=732, lr=8.784e-06, gnorm=2.235, train_wall=5, gb_free=10.5, wall=2662
2024-08-11 17:39:19 | INFO | train_inner | epoch 012:     41 / 63 loss=6.96, nll_loss=3.703, ppl=13.02, wps=1301.2, ups=0.39, wpb=3334, bsz=128, num_updates=734, lr=8.808e-06, gnorm=2.109, train_wall=5, gb_free=11.9, wall=2667
2024-08-11 17:39:25 | INFO | train_inner | epoch 012:     43 / 63 loss=7.041, nll_loss=3.81, ppl=14.03, wps=1235.2, ups=0.38, wpb=3229, bsz=148, num_updates=736, lr=8.832e-06, gnorm=2.243, train_wall=5, gb_free=10.8, wall=2672
2024-08-11 17:39:30 | INFO | train_inner | epoch 012:     45 / 63 loss=6.985, nll_loss=3.748, ppl=13.43, wps=1558.8, ups=0.37, wpb=4181, bsz=212, num_updates=738, lr=8.856e-06, gnorm=1.927, train_wall=5, gb_free=11.4, wall=2677
2024-08-11 17:39:35 | INFO | train_inner | epoch 012:     47 / 63 loss=7.054, nll_loss=3.829, ppl=14.21, wps=1332.6, ups=0.37, wpb=3587, bsz=116, num_updates=740, lr=8.88e-06, gnorm=2.345, train_wall=5, gb_free=12.9, wall=2683
2024-08-11 17:39:41 | INFO | train_inner | epoch 012:     49 / 63 loss=7.02, nll_loss=3.787, ppl=13.8, wps=1441.8, ups=0.36, wpb=4001.5, bsz=156, num_updates=742, lr=8.904e-06, gnorm=2.29, train_wall=6, gb_free=11.6, wall=2688
2024-08-11 17:39:46 | INFO | train_inner | epoch 012:     51 / 63 loss=6.998, nll_loss=3.753, ppl=13.49, wps=1546.9, ups=0.38, wpb=4044, bsz=176, num_updates=744, lr=8.928e-06, gnorm=1.989, train_wall=5, gb_free=10.2, wall=2694
2024-08-11 17:39:51 | INFO | train_inner | epoch 012:     53 / 63 loss=7.114, nll_loss=3.904, ppl=14.97, wps=1100.6, ups=0.39, wpb=2815.5, bsz=92, num_updates=746, lr=8.952e-06, gnorm=2.468, train_wall=5, gb_free=11.7, wall=2699
2024-08-11 17:39:57 | INFO | train_inner | epoch 012:     55 / 63 loss=6.908, nll_loss=3.646, ppl=12.52, wps=1279.4, ups=0.36, wpb=3534.5, bsz=164, num_updates=748, lr=8.976e-06, gnorm=2.237, train_wall=6, gb_free=10.6, wall=2704
2024-08-11 17:40:02 | INFO | train_inner | epoch 012:     57 / 63 loss=7.064, nll_loss=3.845, ppl=14.37, wps=1390.3, ups=0.39, wpb=3600.5, bsz=144, num_updates=750, lr=9e-06, gnorm=2.15, train_wall=5, gb_free=12.8, wall=2709
2024-08-11 17:40:07 | INFO | train_inner | epoch 012:     59 / 63 loss=6.972, nll_loss=3.708, ppl=13.07, wps=1346.3, ups=0.38, wpb=3563, bsz=84, num_updates=752, lr=9.024e-06, gnorm=2.153, train_wall=5, gb_free=15.3, wall=2715
2024-08-11 17:40:12 | INFO | train_inner | epoch 012:     61 / 63 loss=6.853, nll_loss=3.601, ppl=12.13, wps=1261.6, ups=0.41, wpb=3102.5, bsz=172, num_updates=754, lr=9.048e-06, gnorm=2.397, train_wall=5, gb_free=12.9, wall=2720
2024-08-11 17:40:16 | INFO | train_inner | epoch 012:     63 / 63 loss=6.815, nll_loss=3.539, ppl=11.62, wps=1585.3, ups=0.51, wpb=3087, bsz=136, num_updates=756, lr=9.072e-06, gnorm=2.4, train_wall=4, gb_free=18.6, wall=2723
2024-08-11 17:40:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16179.6796875Mb; avail=238886.56640625Mb
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000522
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16179.6796875Mb; avail=238886.56640625Mb
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004681
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16179.6796875Mb; avail=238886.56640625Mb
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004045
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009585
2024-08-11 17:40:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16179.6796875Mb; avail=238886.56640625Mb
2024-08-11 17:40:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.708 | nll_loss 3.197 | ppl 9.17 | wps 3058.9 | wpb 1463.4 | bsz 64.9 | num_updates 756 | best_loss 6.708
2024-08-11 17:40:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 756 updates
2024-08-11 17:40:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:41:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 12 @ 756 updates, score 6.708) (writing took 58.48077584197745 seconds)
2024-08-11 17:41:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-08-11 17:41:23 | INFO | train | epoch 012 | loss 6.972 | nll_loss 3.728 | ppl 13.25 | wps 925.7 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 756 | lr 9.072e-06 | gnorm 2.258 | train_wall 164 | gb_free 18.6 | wall 2790
2024-08-11 17:41:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:41:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:41:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:41:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001237
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28636.01953125Mb; avail=226430.16796875Mb
2024-08-11 17:41:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000200
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001339
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28636.01953125Mb; avail=226430.16796875Mb
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28635.52734375Mb; avail=226430.66015625Mb
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000470
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002168
2024-08-11 17:41:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28636.01953125Mb; avail=226430.16796875Mb
2024-08-11 17:41:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:41:23 | INFO | fairseq.trainer | begin training epoch 13
2024-08-11 17:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:41:33 | INFO | train_inner | epoch 013:      2 / 63 loss=6.968, nll_loss=3.729, ppl=13.26, wps=82.3, ups=0.03, wpb=3183.5, bsz=132, num_updates=758, lr=9.096e-06, gnorm=2.209, train_wall=10, gb_free=12.3, wall=2801
2024-08-11 17:41:39 | INFO | train_inner | epoch 013:      4 / 63 loss=6.898, nll_loss=3.643, ppl=12.49, wps=1431.6, ups=0.4, wpb=3620.5, bsz=164, num_updates=760, lr=9.12e-06, gnorm=2.165, train_wall=5, gb_free=15.9, wall=2806
2024-08-11 17:41:44 | INFO | train_inner | epoch 013:      6 / 63 loss=6.742, nll_loss=3.433, ppl=10.8, wps=1248.4, ups=0.4, wpb=3104, bsz=152, num_updates=762, lr=9.144e-06, gnorm=2.228, train_wall=5, gb_free=12.4, wall=2811
2024-08-11 17:41:48 | INFO | train_inner | epoch 013:      8 / 63 loss=6.981, nll_loss=3.732, ppl=13.28, wps=1251.3, ups=0.41, wpb=3077.5, bsz=136, num_updates=764, lr=9.168e-06, gnorm=2.255, train_wall=5, gb_free=9.9, wall=2816
2024-08-11 17:41:54 | INFO | train_inner | epoch 013:     10 / 63 loss=7.133, nll_loss=3.943, ppl=15.38, wps=1530.1, ups=0.38, wpb=4038, bsz=180, num_updates=766, lr=9.192e-06, gnorm=2.479, train_wall=5, gb_free=10.1, wall=2821
2024-08-11 17:41:59 | INFO | train_inner | epoch 013:     12 / 63 loss=7.019, nll_loss=3.789, ppl=13.82, wps=1395.8, ups=0.39, wpb=3538.5, bsz=172, num_updates=768, lr=9.216e-06, gnorm=2.24, train_wall=5, gb_free=9.9, wall=2826
2024-08-11 17:42:04 | INFO | train_inner | epoch 013:     14 / 63 loss=6.85, nll_loss=3.586, ppl=12.01, wps=1275.9, ups=0.41, wpb=3115, bsz=152, num_updates=770, lr=9.24e-06, gnorm=2.28, train_wall=5, gb_free=13.2, wall=2831
2024-08-11 17:42:09 | INFO | train_inner | epoch 013:     16 / 63 loss=6.696, nll_loss=3.383, ppl=10.43, wps=1391.9, ups=0.38, wpb=3626, bsz=200, num_updates=772, lr=9.264e-06, gnorm=2.038, train_wall=5, gb_free=12.8, wall=2836
2024-08-11 17:42:14 | INFO | train_inner | epoch 013:     18 / 63 loss=6.729, nll_loss=3.429, ppl=10.77, wps=1470.3, ups=0.37, wpb=4020.5, bsz=236, num_updates=774, lr=9.288e-06, gnorm=1.964, train_wall=5, gb_free=10.8, wall=2842
2024-08-11 17:42:19 | INFO | train_inner | epoch 013:     20 / 63 loss=6.9, nll_loss=3.628, ppl=12.36, wps=1311.1, ups=0.42, wpb=3113, bsz=124, num_updates=776, lr=9.312e-06, gnorm=2.376, train_wall=5, gb_free=14, wall=2846
2024-08-11 17:42:24 | INFO | train_inner | epoch 013:     22 / 63 loss=6.929, nll_loss=3.66, ppl=12.64, wps=1340, ups=0.41, wpb=3297.5, bsz=136, num_updates=778, lr=9.336e-06, gnorm=2.21, train_wall=5, gb_free=11.2, wall=2851
2024-08-11 17:42:29 | INFO | train_inner | epoch 013:     24 / 63 loss=7.013, nll_loss=3.764, ppl=13.59, wps=1408.4, ups=0.4, wpb=3561.5, bsz=124, num_updates=780, lr=9.36e-06, gnorm=2.195, train_wall=5, gb_free=12.3, wall=2856
2024-08-11 17:42:34 | INFO | train_inner | epoch 013:     26 / 63 loss=6.876, nll_loss=3.597, ppl=12.1, wps=1418.4, ups=0.45, wpb=3187, bsz=100, num_updates=782, lr=9.384e-06, gnorm=2.336, train_wall=4, gb_free=15.3, wall=2861
2024-08-11 17:42:38 | INFO | train_inner | epoch 013:     28 / 63 loss=6.856, nll_loss=3.578, ppl=11.95, wps=1283.9, ups=0.45, wpb=2828, bsz=148.5, num_updates=784, lr=9.408e-06, gnorm=2.503, train_wall=4, gb_free=15.1, wall=2865
2024-08-11 17:42:43 | INFO | train_inner | epoch 013:     30 / 63 loss=6.985, nll_loss=3.75, ppl=13.45, wps=1301.3, ups=0.43, wpb=3061.5, bsz=96, num_updates=786, lr=9.432e-06, gnorm=2.421, train_wall=5, gb_free=11.4, wall=2870
2024-08-11 17:42:48 | INFO | train_inner | epoch 013:     32 / 63 loss=6.883, nll_loss=3.62, ppl=12.29, wps=1357.7, ups=0.38, wpb=3545, bsz=128, num_updates=788, lr=9.456e-06, gnorm=2.284, train_wall=5, gb_free=12.6, wall=2875
2024-08-11 17:42:53 | INFO | train_inner | epoch 013:     34 / 63 loss=6.856, nll_loss=3.564, ppl=11.83, wps=1187.7, ups=0.37, wpb=3206, bsz=88, num_updates=790, lr=9.48e-06, gnorm=2.394, train_wall=5, gb_free=10.6, wall=2881
2024-08-11 17:42:59 | INFO | train_inner | epoch 013:     36 / 63 loss=6.943, nll_loss=3.677, ppl=12.79, wps=1224.7, ups=0.37, wpb=3328.5, bsz=132, num_updates=792, lr=9.504e-06, gnorm=2.262, train_wall=5, gb_free=12.8, wall=2886
2024-08-11 17:43:03 | INFO | train_inner | epoch 013:     38 / 63 loss=7.043, nll_loss=3.807, ppl=13.99, wps=1375, ups=0.42, wpb=3241.5, bsz=92, num_updates=794, lr=9.528e-06, gnorm=2.581, train_wall=5, gb_free=13.7, wall=2891
2024-08-11 17:43:08 | INFO | train_inner | epoch 013:     40 / 63 loss=6.716, nll_loss=3.425, ppl=10.74, wps=1120.3, ups=0.4, wpb=2782.5, bsz=168, num_updates=796, lr=9.552e-06, gnorm=2.367, train_wall=5, gb_free=11.8, wall=2896
2024-08-11 17:43:14 | INFO | train_inner | epoch 013:     42 / 63 loss=6.779, nll_loss=3.489, ppl=11.23, wps=1362.2, ups=0.37, wpb=3700, bsz=152, num_updates=798, lr=9.576e-06, gnorm=2.008, train_wall=5, gb_free=11.8, wall=2901
2024-08-11 17:43:19 | INFO | train_inner | epoch 013:     44 / 63 loss=6.769, nll_loss=3.485, ppl=11.2, wps=1469.9, ups=0.36, wpb=4037.5, bsz=204, num_updates=800, lr=9.6e-06, gnorm=1.993, train_wall=5, gb_free=11.1, wall=2907
2024-08-11 17:43:24 | INFO | train_inner | epoch 013:     46 / 63 loss=6.744, nll_loss=3.444, ppl=10.88, wps=1119.6, ups=0.39, wpb=2873, bsz=148, num_updates=802, lr=9.624e-06, gnorm=2.421, train_wall=5, gb_free=14.3, wall=2912
2024-08-11 17:43:30 | INFO | train_inner | epoch 013:     48 / 63 loss=6.889, nll_loss=3.618, ppl=12.28, wps=1539, ups=0.36, wpb=4261, bsz=220, num_updates=804, lr=9.648e-06, gnorm=2.036, train_wall=6, gb_free=10.5, wall=2917
2024-08-11 17:43:35 | INFO | train_inner | epoch 013:     50 / 63 loss=6.948, nll_loss=3.68, ppl=12.82, wps=1311.1, ups=0.39, wpb=3391, bsz=132, num_updates=806, lr=9.672e-06, gnorm=2.353, train_wall=5, gb_free=12, wall=2923
2024-08-11 17:43:40 | INFO | train_inner | epoch 013:     52 / 63 loss=6.831, nll_loss=3.543, ppl=11.66, wps=1534.5, ups=0.38, wpb=4053, bsz=168, num_updates=808, lr=9.696e-06, gnorm=2.106, train_wall=5, gb_free=12.8, wall=2928
2024-08-11 17:43:45 | INFO | train_inner | epoch 013:     54 / 63 loss=6.892, nll_loss=3.617, ppl=12.27, wps=1115.5, ups=0.41, wpb=2751.5, bsz=80, num_updates=810, lr=9.72e-06, gnorm=2.62, train_wall=5, gb_free=13.1, wall=2933
2024-08-11 17:43:50 | INFO | train_inner | epoch 013:     56 / 63 loss=6.992, nll_loss=3.766, ppl=13.6, wps=1410.9, ups=0.4, wpb=3552, bsz=144, num_updates=812, lr=9.744e-06, gnorm=2.118, train_wall=5, gb_free=12.3, wall=2938
2024-08-11 17:43:56 | INFO | train_inner | epoch 013:     58 / 63 loss=6.795, nll_loss=3.499, ppl=11.31, wps=1488.8, ups=0.36, wpb=4168, bsz=136, num_updates=814, lr=9.768e-06, gnorm=2.009, train_wall=6, gb_free=12.1, wall=2943
2024-08-11 17:44:01 | INFO | train_inner | epoch 013:     60 / 63 loss=6.728, nll_loss=3.432, ppl=10.79, wps=1240.5, ups=0.4, wpb=3137, bsz=148, num_updates=816, lr=9.792e-06, gnorm=2.265, train_wall=5, gb_free=13.1, wall=2948
2024-08-11 17:44:07 | INFO | train_inner | epoch 013:     62 / 63 loss=6.757, nll_loss=3.464, ppl=11.03, wps=1471.9, ups=0.37, wpb=4027.5, bsz=232, num_updates=818, lr=9.816e-06, gnorm=1.928, train_wall=5, gb_free=13.3, wall=2954
2024-08-11 17:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24909.0859375Mb; avail=230157.1171875Mb
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000540
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24909.0859375Mb; avail=230157.1171875Mb
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004732
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24909.0859375Mb; avail=230157.1171875Mb
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004070
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009682
2024-08-11 17:44:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24909.08203125Mb; avail=230157.1171875Mb
2024-08-11 17:44:16 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.649 | nll_loss 3.106 | ppl 8.61 | wps 3048.5 | wpb 1463.4 | bsz 64.9 | num_updates 819 | best_loss 6.649
2024-08-11 17:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 819 updates
2024-08-11 17:44:16 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:44:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:45:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 13 @ 819 updates, score 6.649) (writing took 66.16140648117289 seconds)
2024-08-11 17:45:23 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-08-11 17:45:23 | INFO | train | epoch 013 | loss 6.875 | nll_loss 3.605 | ppl 12.17 | wps 894.3 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 819 | lr 9.828e-06 | gnorm 2.263 | train_wall 165 | gb_free 15.8 | wall 3030
2024-08-11 17:45:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:45:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:45:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:45:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.002592
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36255.578125Mb; avail=218810.5859375Mb
2024-08-11 17:45:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000183
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001269
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36255.578125Mb; avail=218810.5859375Mb
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36255.578125Mb; avail=218810.5859375Mb
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000437
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002041
2024-08-11 17:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36255.578125Mb; avail=218810.5859375Mb
2024-08-11 17:45:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:45:23 | INFO | fairseq.trainer | begin training epoch 14
2024-08-11 17:45:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:45:25 | INFO | train_inner | epoch 014:      1 / 63 loss=6.762, nll_loss=3.466, ppl=11.05, wps=62.7, ups=0.03, wpb=2464, bsz=144, num_updates=820, lr=9.84e-06, gnorm=2.641, train_wall=4, gb_free=15.1, wall=3032
2024-08-11 17:45:35 | INFO | train_inner | epoch 014:      3 / 63 loss=6.8, nll_loss=3.489, ppl=11.22, wps=737.9, ups=0.2, wpb=3715, bsz=112, num_updates=822, lr=9.864e-06, gnorm=2.361, train_wall=10, gb_free=12.5, wall=3043
2024-08-11 17:45:40 | INFO | train_inner | epoch 014:      5 / 63 loss=6.612, nll_loss=3.276, ppl=9.69, wps=1378.7, ups=0.39, wpb=3507, bsz=200, num_updates=824, lr=9.888e-06, gnorm=2.097, train_wall=5, gb_free=13.6, wall=3048
2024-08-11 17:45:45 | INFO | train_inner | epoch 014:      7 / 63 loss=6.788, nll_loss=3.496, ppl=11.29, wps=1067.6, ups=0.4, wpb=2677.5, bsz=116, num_updates=826, lr=9.912e-06, gnorm=2.403, train_wall=5, gb_free=10.5, wall=3053
2024-08-11 17:45:50 | INFO | train_inner | epoch 014:      9 / 63 loss=6.775, nll_loss=3.472, ppl=11.09, wps=1533.5, ups=0.4, wpb=3876, bsz=140, num_updates=828, lr=9.936e-06, gnorm=2.077, train_wall=5, gb_free=12.8, wall=3058
2024-08-11 17:45:55 | INFO | train_inner | epoch 014:     11 / 63 loss=6.828, nll_loss=3.553, ppl=11.74, wps=1019.6, ups=0.41, wpb=2458, bsz=116, num_updates=830, lr=9.96e-06, gnorm=2.753, train_wall=5, gb_free=11.6, wall=3063
2024-08-11 17:46:00 | INFO | train_inner | epoch 014:     13 / 63 loss=6.968, nll_loss=3.72, ppl=13.17, wps=1228.5, ups=0.41, wpb=3009.5, bsz=68, num_updates=832, lr=9.984e-06, gnorm=2.36, train_wall=5, gb_free=12.3, wall=3067
2024-08-11 17:46:05 | INFO | train_inner | epoch 014:     15 / 63 loss=6.784, nll_loss=3.492, ppl=11.25, wps=1391.5, ups=0.39, wpb=3537, bsz=140, num_updates=834, lr=1.0008e-05, gnorm=2.133, train_wall=5, gb_free=12.8, wall=3073
2024-08-11 17:46:10 | INFO | train_inner | epoch 014:     17 / 63 loss=6.716, nll_loss=3.416, ppl=10.67, wps=1444.2, ups=0.4, wpb=3647.5, bsz=208, num_updates=836, lr=1.0032e-05, gnorm=2.173, train_wall=5, gb_free=10.5, wall=3078
2024-08-11 17:46:15 | INFO | train_inner | epoch 014:     19 / 63 loss=6.748, nll_loss=3.45, ppl=10.93, wps=1423.7, ups=0.42, wpb=3427, bsz=184, num_updates=838, lr=1.0056e-05, gnorm=2.235, train_wall=5, gb_free=15.3, wall=3082
2024-08-11 17:46:20 | INFO | train_inner | epoch 014:     21 / 63 loss=6.888, nll_loss=3.622, ppl=12.31, wps=1195.6, ups=0.42, wpb=2881, bsz=120, num_updates=840, lr=1.008e-05, gnorm=2.442, train_wall=5, gb_free=12.7, wall=3087
2024-08-11 17:46:25 | INFO | train_inner | epoch 014:     23 / 63 loss=6.78, nll_loss=3.465, ppl=11.04, wps=1189, ups=0.37, wpb=3210, bsz=124, num_updates=842, lr=1.0104e-05, gnorm=2.4, train_wall=5, gb_free=12.7, wall=3093
2024-08-11 17:46:31 | INFO | train_inner | epoch 014:     25 / 63 loss=6.741, nll_loss=3.422, ppl=10.72, wps=1314.7, ups=0.35, wpb=3709, bsz=180, num_updates=844, lr=1.0128e-05, gnorm=2.013, train_wall=6, gb_free=11.2, wall=3098
2024-08-11 17:46:36 | INFO | train_inner | epoch 014:     27 / 63 loss=6.825, nll_loss=3.532, ppl=11.57, wps=1392.1, ups=0.39, wpb=3612.5, bsz=144, num_updates=846, lr=1.0152e-05, gnorm=2.418, train_wall=5, gb_free=13, wall=3103
2024-08-11 17:46:41 | INFO | train_inner | epoch 014:     29 / 63 loss=6.574, nll_loss=3.233, ppl=9.4, wps=1361.9, ups=0.41, wpb=3339, bsz=224, num_updates=848, lr=1.0176e-05, gnorm=2.1, train_wall=5, gb_free=16.4, wall=3108
2024-08-11 17:46:46 | INFO | train_inner | epoch 014:     31 / 63 loss=6.766, nll_loss=3.464, ppl=11.03, wps=1401.7, ups=0.37, wpb=3746, bsz=128, num_updates=850, lr=1.02e-05, gnorm=2.121, train_wall=5, gb_free=10.8, wall=3114
2024-08-11 17:46:51 | INFO | train_inner | epoch 014:     33 / 63 loss=7.015, nll_loss=3.795, ppl=13.88, wps=1449.4, ups=0.41, wpb=3546, bsz=124, num_updates=852, lr=1.0224e-05, gnorm=2.192, train_wall=5, gb_free=11.4, wall=3119
2024-08-11 17:46:57 | INFO | train_inner | epoch 014:     35 / 63 loss=6.884, nll_loss=3.625, ppl=12.34, wps=1246.5, ups=0.37, wpb=3340, bsz=108, num_updates=854, lr=1.0248e-05, gnorm=2.16, train_wall=5, gb_free=13.4, wall=3124
2024-08-11 17:47:01 | INFO | train_inner | epoch 014:     37 / 63 loss=6.982, nll_loss=3.743, ppl=13.39, wps=1102.5, ups=0.45, wpb=2457, bsz=84.5, num_updates=856, lr=1.0272e-05, gnorm=2.713, train_wall=4, gb_free=13.9, wall=3128
2024-08-11 17:47:06 | INFO | train_inner | epoch 014:     39 / 63 loss=6.921, nll_loss=3.657, ppl=12.62, wps=1404.1, ups=0.44, wpb=3195, bsz=124, num_updates=858, lr=1.0296e-05, gnorm=2.351, train_wall=5, gb_free=20.4, wall=3133
2024-08-11 17:47:11 | INFO | train_inner | epoch 014:     41 / 63 loss=6.669, nll_loss=3.328, ppl=10.04, wps=1403, ups=0.37, wpb=3837, bsz=148, num_updates=860, lr=1.032e-05, gnorm=2.004, train_wall=5, gb_free=11.7, wall=3138
2024-08-11 17:47:17 | INFO | train_inner | epoch 014:     43 / 63 loss=6.705, nll_loss=3.38, ppl=10.41, wps=1372.8, ups=0.37, wpb=3718.5, bsz=176, num_updates=862, lr=1.0344e-05, gnorm=2.219, train_wall=5, gb_free=10.9, wall=3144
2024-08-11 17:47:22 | INFO | train_inner | epoch 014:     45 / 63 loss=6.845, nll_loss=3.539, ppl=11.63, wps=1133.3, ups=0.37, wpb=3064, bsz=80, num_updates=864, lr=1.0368e-05, gnorm=2.427, train_wall=5, gb_free=10.7, wall=3149
2024-08-11 17:47:27 | INFO | train_inner | epoch 014:     47 / 63 loss=6.702, nll_loss=3.385, ppl=10.44, wps=1478.2, ups=0.37, wpb=3981.5, bsz=156, num_updates=866, lr=1.0392e-05, gnorm=2.025, train_wall=5, gb_free=12.4, wall=3155
2024-08-11 17:47:32 | INFO | train_inner | epoch 014:     49 / 63 loss=6.666, nll_loss=3.362, ppl=10.28, wps=1448.7, ups=0.39, wpb=3692, bsz=200, num_updates=868, lr=1.0416e-05, gnorm=2.096, train_wall=5, gb_free=15, wall=3160
2024-08-11 17:47:38 | INFO | train_inner | epoch 014:     51 / 63 loss=6.815, nll_loss=3.54, ppl=11.63, wps=1151.1, ups=0.38, wpb=3069, bsz=128, num_updates=870, lr=1.044e-05, gnorm=2.191, train_wall=5, gb_free=10.4, wall=3165
2024-08-11 17:47:43 | INFO | train_inner | epoch 014:     53 / 63 loss=6.761, nll_loss=3.474, ppl=11.11, wps=1450.5, ups=0.35, wpb=4114.5, bsz=180, num_updates=872, lr=1.0464e-05, gnorm=1.968, train_wall=6, gb_free=10.3, wall=3171
2024-08-11 17:47:48 | INFO | train_inner | epoch 014:     55 / 63 loss=6.746, nll_loss=3.439, ppl=10.85, wps=1397, ups=0.42, wpb=3327.5, bsz=116, num_updates=874, lr=1.0488e-05, gnorm=2.232, train_wall=5, gb_free=16.4, wall=3176
2024-08-11 17:47:54 | INFO | train_inner | epoch 014:     57 / 63 loss=6.643, nll_loss=3.314, ppl=9.95, wps=1516.8, ups=0.38, wpb=4042.5, bsz=204, num_updates=876, lr=1.0512e-05, gnorm=1.884, train_wall=5, gb_free=12, wall=3181
2024-08-11 17:47:59 | INFO | train_inner | epoch 014:     59 / 63 loss=6.747, nll_loss=3.436, ppl=10.83, wps=1333.2, ups=0.37, wpb=3576.5, bsz=176, num_updates=878, lr=1.0536e-05, gnorm=2.179, train_wall=5, gb_free=12.1, wall=3186
2024-08-11 17:48:04 | INFO | train_inner | epoch 014:     61 / 63 loss=6.639, nll_loss=3.303, ppl=9.87, wps=1289.4, ups=0.39, wpb=3276, bsz=184, num_updates=880, lr=1.056e-05, gnorm=2.169, train_wall=5, gb_free=11.2, wall=3191
2024-08-11 17:48:08 | INFO | train_inner | epoch 014:     63 / 63 loss=6.527, nll_loss=3.165, ppl=8.97, wps=1330.7, ups=0.48, wpb=2764, bsz=176, num_updates=882, lr=1.0584e-05, gnorm=2.309, train_wall=4, gb_free=15.8, wall=3195
2024-08-11 17:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16995.328125Mb; avail=238070.75Mb
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000525
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16995.328125Mb; avail=238070.75Mb
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004658
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16995.40234375Mb; avail=238070.75Mb
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004028
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009546
2024-08-11 17:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16995.40234375Mb; avail=238070.75Mb
2024-08-11 17:48:17 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.577 | nll_loss 3.026 | ppl 8.14 | wps 3058.3 | wpb 1463.4 | bsz 64.9 | num_updates 882 | best_loss 6.577
2024-08-11 17:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 882 updates
2024-08-11 17:48:17 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:48:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:49:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 14 @ 882 updates, score 6.577) (writing took 66.68986280960962 seconds)
2024-08-11 17:49:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-08-11 17:49:23 | INFO | train | epoch 014 | loss 6.764 | nll_loss 3.463 | ppl 11.03 | wps 890.4 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 882 | lr 1.0584e-05 | gnorm 2.228 | train_wall 165 | gb_free 15.8 | wall 3271
2024-08-11 17:49:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:49:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:49:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:49:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000775
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25970.40625Mb; avail=229095.75Mb
2024-08-11 17:49:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000179
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001270
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25970.40625Mb; avail=229095.75Mb
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25970.40625Mb; avail=229095.75Mb
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000455
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002067
2024-08-11 17:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25970.40625Mb; avail=229095.75Mb
2024-08-11 17:49:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:49:23 | INFO | fairseq.trainer | begin training epoch 15
2024-08-11 17:49:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:49:28 | INFO | train_inner | epoch 015:      2 / 63 loss=6.666, nll_loss=3.355, ppl=10.23, wps=73.3, ups=0.03, wpb=2919, bsz=148, num_updates=884, lr=1.0608e-05, gnorm=2.29, train_wall=4, gb_free=12.5, wall=3275
2024-08-11 17:49:33 | INFO | train_inner | epoch 015:      4 / 63 loss=6.648, nll_loss=3.328, ppl=10.04, wps=1236.3, ups=0.42, wpb=2968, bsz=148, num_updates=886, lr=1.0632e-05, gnorm=2.359, train_wall=5, gb_free=11.1, wall=3280
2024-08-11 17:49:43 | INFO | train_inner | epoch 015:      6 / 63 loss=6.504, nll_loss=3.152, ppl=8.89, wps=712.8, ups=0.19, wpb=3736, bsz=232, num_updates=888, lr=1.0656e-05, gnorm=2.04, train_wall=10, gb_free=11.9, wall=3290
2024-08-11 17:49:48 | INFO | train_inner | epoch 015:      8 / 63 loss=6.68, nll_loss=3.349, ppl=10.19, wps=1398.9, ups=0.4, wpb=3492, bsz=124, num_updates=890, lr=1.068e-05, gnorm=2.397, train_wall=5, gb_free=11.1, wall=3295
2024-08-11 17:49:53 | INFO | train_inner | epoch 015:     10 / 63 loss=6.657, nll_loss=3.34, ppl=10.12, wps=1504.8, ups=0.39, wpb=3890, bsz=212, num_updates=892, lr=1.0704e-05, gnorm=1.946, train_wall=5, gb_free=12, wall=3301
2024-08-11 17:49:58 | INFO | train_inner | epoch 015:     12 / 63 loss=6.76, nll_loss=3.461, ppl=11.01, wps=1366.5, ups=0.41, wpb=3319, bsz=124, num_updates=894, lr=1.0728e-05, gnorm=2.219, train_wall=5, gb_free=15.1, wall=3305
2024-08-11 17:50:03 | INFO | train_inner | epoch 015:     14 / 63 loss=6.611, nll_loss=3.261, ppl=9.59, wps=1150.9, ups=0.37, wpb=3082.5, bsz=104, num_updates=896, lr=1.0752e-05, gnorm=2.329, train_wall=5, gb_free=10.2, wall=3311
2024-08-11 17:50:09 | INFO | train_inner | epoch 015:     16 / 63 loss=6.73, nll_loss=3.409, ppl=10.62, wps=1332.3, ups=0.38, wpb=3493, bsz=116, num_updates=898, lr=1.0776e-05, gnorm=2.446, train_wall=5, gb_free=12.7, wall=3316
2024-08-11 17:50:14 | INFO | train_inner | epoch 015:     18 / 63 loss=6.796, nll_loss=3.494, ppl=11.26, wps=1238.5, ups=0.39, wpb=3151.5, bsz=124, num_updates=900, lr=1.08e-05, gnorm=2.174, train_wall=5, gb_free=10.7, wall=3321
2024-08-11 17:50:19 | INFO | train_inner | epoch 015:     20 / 63 loss=6.666, nll_loss=3.324, ppl=10.01, wps=1476.2, ups=0.39, wpb=3794, bsz=164, num_updates=902, lr=1.0824e-05, gnorm=2.006, train_wall=5, gb_free=11.9, wall=3326
2024-08-11 17:50:24 | INFO | train_inner | epoch 015:     22 / 63 loss=6.682, nll_loss=3.369, ppl=10.33, wps=1521.1, ups=0.42, wpb=3655, bsz=188, num_updates=904, lr=1.0848e-05, gnorm=2.193, train_wall=5, gb_free=12.5, wall=3331
2024-08-11 17:50:29 | INFO | train_inner | epoch 015:     24 / 63 loss=6.658, nll_loss=3.326, ppl=10.03, wps=1552.1, ups=0.36, wpb=4273, bsz=196, num_updates=906, lr=1.0872e-05, gnorm=1.889, train_wall=5, gb_free=13.3, wall=3337
2024-08-11 17:50:35 | INFO | train_inner | epoch 015:     26 / 63 loss=6.375, nll_loss=2.987, ppl=7.93, wps=1216.5, ups=0.35, wpb=3430, bsz=228, num_updates=908, lr=1.0896e-05, gnorm=1.995, train_wall=6, gb_free=11.2, wall=3342
2024-08-11 17:50:40 | INFO | train_inner | epoch 015:     28 / 63 loss=6.744, nll_loss=3.44, ppl=10.86, wps=1488.2, ups=0.39, wpb=3775.5, bsz=144, num_updates=910, lr=1.092e-05, gnorm=2.001, train_wall=5, gb_free=11.3, wall=3347
2024-08-11 17:50:45 | INFO | train_inner | epoch 015:     30 / 63 loss=6.625, nll_loss=3.292, ppl=9.79, wps=1280.5, ups=0.4, wpb=3199.5, bsz=136, num_updates=912, lr=1.0944e-05, gnorm=2.278, train_wall=5, gb_free=13.1, wall=3352
2024-08-11 17:50:50 | INFO | train_inner | epoch 015:     32 / 63 loss=6.818, nll_loss=3.538, ppl=11.62, wps=1226.5, ups=0.42, wpb=2917, bsz=128, num_updates=914, lr=1.0968e-05, gnorm=2.293, train_wall=5, gb_free=12, wall=3357
2024-08-11 17:50:55 | INFO | train_inner | epoch 015:     34 / 63 loss=6.616, nll_loss=3.275, ppl=9.68, wps=1169.4, ups=0.38, wpb=3100.5, bsz=120, num_updates=916, lr=1.0992e-05, gnorm=2.27, train_wall=5, gb_free=12.7, wall=3362
2024-08-11 17:51:00 | INFO | train_inner | epoch 015:     36 / 63 loss=6.688, nll_loss=3.357, ppl=10.25, wps=1415.1, ups=0.4, wpb=3520, bsz=128, num_updates=918, lr=1.1016e-05, gnorm=2.17, train_wall=5, gb_free=13.5, wall=3367
2024-08-11 17:51:05 | INFO | train_inner | epoch 015:     38 / 63 loss=6.769, nll_loss=3.459, ppl=11, wps=1337.2, ups=0.38, wpb=3556.5, bsz=160, num_updates=920, lr=1.104e-05, gnorm=2.464, train_wall=5, gb_free=13, wall=3373
2024-08-11 17:51:10 | INFO | train_inner | epoch 015:     40 / 63 loss=6.886, nll_loss=3.602, ppl=12.14, wps=1313.5, ups=0.4, wpb=3298.5, bsz=76, num_updates=922, lr=1.1064e-05, gnorm=2.246, train_wall=5, gb_free=12.3, wall=3378
2024-08-11 17:51:15 | INFO | train_inner | epoch 015:     42 / 63 loss=6.776, nll_loss=3.469, ppl=11.07, wps=1501.1, ups=0.44, wpb=3450.5, bsz=104, num_updates=924, lr=1.1088e-05, gnorm=2.303, train_wall=5, gb_free=15.2, wall=3382
2024-08-11 17:51:20 | INFO | train_inner | epoch 015:     44 / 63 loss=6.762, nll_loss=3.469, ppl=11.07, wps=1068.2, ups=0.38, wpb=2784, bsz=132, num_updates=926, lr=1.1112e-05, gnorm=2.441, train_wall=5, gb_free=12.3, wall=3387
2024-08-11 17:51:25 | INFO | train_inner | epoch 015:     46 / 63 loss=6.686, nll_loss=3.376, ppl=10.38, wps=1037.8, ups=0.38, wpb=2739.5, bsz=108, num_updates=928, lr=1.1136e-05, gnorm=2.345, train_wall=5, gb_free=11.7, wall=3393
2024-08-11 17:51:31 | INFO | train_inner | epoch 015:     48 / 63 loss=6.715, nll_loss=3.423, ppl=10.73, wps=1062.9, ups=0.38, wpb=2807.5, bsz=140, num_updates=930, lr=1.116e-05, gnorm=2.449, train_wall=5, gb_free=10.6, wall=3398
2024-08-11 17:51:36 | INFO | train_inner | epoch 015:     50 / 63 loss=6.732, nll_loss=3.442, ppl=10.87, wps=1529, ups=0.36, wpb=4288, bsz=204, num_updates=932, lr=1.1184e-05, gnorm=2.151, train_wall=6, gb_free=10.9, wall=3404
2024-08-11 17:51:41 | INFO | train_inner | epoch 015:     52 / 63 loss=6.783, nll_loss=3.469, ppl=11.07, wps=1240.2, ups=0.39, wpb=3192, bsz=84, num_updates=934, lr=1.1208e-05, gnorm=2.349, train_wall=5, gb_free=11.4, wall=3409
2024-08-11 17:51:47 | INFO | train_inner | epoch 015:     54 / 63 loss=6.646, nll_loss=3.297, ppl=9.83, wps=1392.4, ups=0.38, wpb=3696, bsz=120, num_updates=936, lr=1.1232e-05, gnorm=2.053, train_wall=5, gb_free=10.7, wall=3414
2024-08-11 17:51:51 | INFO | train_inner | epoch 015:     56 / 63 loss=6.783, nll_loss=3.469, ppl=11.07, wps=1507.7, ups=0.46, wpb=3246, bsz=116.5, num_updates=938, lr=1.1256e-05, gnorm=2.427, train_wall=4, gb_free=10.9, wall=3418
2024-08-11 17:51:56 | INFO | train_inner | epoch 015:     58 / 63 loss=6.586, nll_loss=3.235, ppl=9.42, wps=1337, ups=0.37, wpb=3629.5, bsz=204, num_updates=940, lr=1.128e-05, gnorm=2.276, train_wall=5, gb_free=13.4, wall=3424
2024-08-11 17:52:02 | INFO | train_inner | epoch 015:     60 / 63 loss=6.505, nll_loss=3.147, ppl=8.86, wps=1449.7, ups=0.36, wpb=3988, bsz=260, num_updates=942, lr=1.1304e-05, gnorm=1.893, train_wall=5, gb_free=11.5, wall=3429
2024-08-11 17:52:07 | INFO | train_inner | epoch 015:     62 / 63 loss=6.584, nll_loss=3.228, ppl=9.37, wps=1350.1, ups=0.36, wpb=3714, bsz=152, num_updates=944, lr=1.1328e-05, gnorm=2.1, train_wall=5, gb_free=12.5, wall=3435
2024-08-11 17:52:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15064.05078125Mb; avail=240002.1484375Mb
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000541
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15064.54296875Mb; avail=240001.65625Mb
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004756
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15071.43359375Mb; avail=239994.2734375Mb
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004024
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009678
2024-08-11 17:52:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15077.83203125Mb; avail=239988.3671875Mb
2024-08-11 17:52:17 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.523 | nll_loss 2.971 | ppl 7.84 | wps 3052.1 | wpb 1463.4 | bsz 64.9 | num_updates 945 | best_loss 6.523
2024-08-11 17:52:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 945 updates
2024-08-11 17:52:17 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 15 @ 945 updates, score 6.523) (writing took 67.83196787303314 seconds)
2024-08-11 17:53:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-08-11 17:53:25 | INFO | train | epoch 015 | loss 6.678 | nll_loss 3.355 | ppl 10.23 | wps 885.2 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 945 | lr 1.134e-05 | gnorm 2.225 | train_wall 165 | gb_free 17.7 | wall 3513
2024-08-11 17:53:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:53:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:53:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:53:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000804
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23492.26171875Mb; avail=231573.9375Mb
2024-08-11 17:53:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000181
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001361
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23492.26171875Mb; avail=231573.9375Mb
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23492.26171875Mb; avail=231573.9375Mb
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000461
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002202
2024-08-11 17:53:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23492.26171875Mb; avail=231573.9375Mb
2024-08-11 17:53:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:53:25 | INFO | fairseq.trainer | begin training epoch 16
2024-08-11 17:53:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:53:28 | INFO | train_inner | epoch 016:      1 / 63 loss=6.578, nll_loss=3.233, ppl=9.4, wps=70.6, ups=0.02, wpb=2832, bsz=112, num_updates=946, lr=1.1352e-05, gnorm=2.493, train_wall=4, gb_free=12.6, wall=3515
2024-08-11 17:53:33 | INFO | train_inner | epoch 016:      3 / 63 loss=6.714, nll_loss=3.404, ppl=10.59, wps=1367, ups=0.4, wpb=3412, bsz=108, num_updates=948, lr=1.1376e-05, gnorm=2.337, train_wall=5, gb_free=11.2, wall=3520
2024-08-11 17:53:38 | INFO | train_inner | epoch 016:      5 / 63 loss=6.669, nll_loss=3.359, ppl=10.26, wps=1487.7, ups=0.4, wpb=3698, bsz=176, num_updates=950, lr=1.14e-05, gnorm=1.955, train_wall=5, gb_free=15.5, wall=3525
2024-08-11 17:53:43 | INFO | train_inner | epoch 016:      7 / 63 loss=6.619, nll_loss=3.297, ppl=9.83, wps=1347.4, ups=0.4, wpb=3397.5, bsz=148, num_updates=952, lr=1.1424e-05, gnorm=2.136, train_wall=5, gb_free=11.9, wall=3530
2024-08-11 17:53:53 | INFO | train_inner | epoch 016:      9 / 63 loss=6.483, nll_loss=3.106, ppl=8.61, wps=721.8, ups=0.2, wpb=3589.5, bsz=212, num_updates=954, lr=1.1448e-05, gnorm=1.952, train_wall=10, gb_free=10.7, wall=3540
2024-08-11 17:53:58 | INFO | train_inner | epoch 016:     11 / 63 loss=6.756, nll_loss=3.432, ppl=10.8, wps=1448.6, ups=0.4, wpb=3605, bsz=132, num_updates=956, lr=1.1472e-05, gnorm=2.175, train_wall=5, gb_free=12.4, wall=3545
2024-08-11 17:54:03 | INFO | train_inner | epoch 016:     13 / 63 loss=6.581, nll_loss=3.218, ppl=9.3, wps=1034.8, ups=0.38, wpb=2700.5, bsz=140, num_updates=958, lr=1.1496e-05, gnorm=2.301, train_wall=5, gb_free=12.9, wall=3550
2024-08-11 17:54:08 | INFO | train_inner | epoch 016:     15 / 63 loss=6.616, nll_loss=3.253, ppl=9.53, wps=1452.9, ups=0.42, wpb=3482, bsz=120, num_updates=960, lr=1.152e-05, gnorm=2.104, train_wall=5, gb_free=13.7, wall=3555
2024-08-11 17:54:13 | INFO | train_inner | epoch 016:     17 / 63 loss=6.64, nll_loss=3.294, ppl=9.81, wps=1414.6, ups=0.37, wpb=3847, bsz=136, num_updates=962, lr=1.1544e-05, gnorm=2.028, train_wall=5, gb_free=10.4, wall=3560
2024-08-11 17:54:18 | INFO | train_inner | epoch 016:     19 / 63 loss=6.709, nll_loss=3.376, ppl=10.38, wps=1433.8, ups=0.4, wpb=3587, bsz=104, num_updates=964, lr=1.1568e-05, gnorm=2.112, train_wall=5, gb_free=12, wall=3565
2024-08-11 17:54:23 | INFO | train_inner | epoch 016:     21 / 63 loss=6.612, nll_loss=3.29, ppl=9.78, wps=1261.4, ups=0.4, wpb=3137, bsz=164, num_updates=966, lr=1.1592e-05, gnorm=2.138, train_wall=5, gb_free=13.5, wall=3570
2024-08-11 17:54:28 | INFO | train_inner | epoch 016:     23 / 63 loss=6.384, nll_loss=3.003, ppl=8.02, wps=1489, ups=0.38, wpb=3966.5, bsz=236, num_updates=968, lr=1.1616e-05, gnorm=1.973, train_wall=5, gb_free=13.1, wall=3576
2024-08-11 17:54:33 | INFO | train_inner | epoch 016:     25 / 63 loss=6.513, nll_loss=3.154, ppl=8.9, wps=1546.7, ups=0.4, wpb=3881.5, bsz=192, num_updates=970, lr=1.164e-05, gnorm=1.911, train_wall=5, gb_free=12.9, wall=3581
2024-08-11 17:54:38 | INFO | train_inner | epoch 016:     27 / 63 loss=6.653, nll_loss=3.319, ppl=9.98, wps=1414, ups=0.41, wpb=3489, bsz=116, num_updates=972, lr=1.1664e-05, gnorm=2.082, train_wall=5, gb_free=13.4, wall=3586
2024-08-11 17:54:43 | INFO | train_inner | epoch 016:     29 / 63 loss=6.605, nll_loss=3.25, ppl=9.52, wps=1342.4, ups=0.42, wpb=3161.5, bsz=148, num_updates=974, lr=1.1688e-05, gnorm=2.219, train_wall=5, gb_free=12.3, wall=3590
2024-08-11 17:54:48 | INFO | train_inner | epoch 016:     31 / 63 loss=6.507, nll_loss=3.118, ppl=8.68, wps=1337.2, ups=0.4, wpb=3316.5, bsz=156, num_updates=976, lr=1.1712e-05, gnorm=2.047, train_wall=5, gb_free=11.5, wall=3595
2024-08-11 17:54:54 | INFO | train_inner | epoch 016:     33 / 63 loss=6.454, nll_loss=3.07, ppl=8.4, wps=1249.1, ups=0.35, wpb=3550.5, bsz=216, num_updates=978, lr=1.1736e-05, gnorm=1.898, train_wall=6, gb_free=11, wall=3601
2024-08-11 17:54:59 | INFO | train_inner | epoch 016:     35 / 63 loss=6.543, nll_loss=3.172, ppl=9.01, wps=1494.7, ups=0.37, wpb=4060.5, bsz=152, num_updates=980, lr=1.176e-05, gnorm=1.867, train_wall=5, gb_free=11.8, wall=3606
2024-08-11 17:55:04 | INFO | train_inner | epoch 016:     37 / 63 loss=6.348, nll_loss=2.948, ppl=7.72, wps=1196, ups=0.38, wpb=3115.5, bsz=164, num_updates=982, lr=1.1784e-05, gnorm=2.022, train_wall=5, gb_free=11, wall=3612
2024-08-11 17:55:10 | INFO | train_inner | epoch 016:     39 / 63 loss=6.533, nll_loss=3.198, ppl=9.18, wps=1436.7, ups=0.35, wpb=4108, bsz=224, num_updates=984, lr=1.1808e-05, gnorm=1.853, train_wall=6, gb_free=12, wall=3617
2024-08-11 17:55:15 | INFO | train_inner | epoch 016:     41 / 63 loss=6.698, nll_loss=3.402, ppl=10.57, wps=1435.5, ups=0.39, wpb=3693.5, bsz=188, num_updates=986, lr=1.1832e-05, gnorm=2.071, train_wall=5, gb_free=13, wall=3623
2024-08-11 17:55:20 | INFO | train_inner | epoch 016:     43 / 63 loss=6.705, nll_loss=3.379, ppl=10.41, wps=1067.3, ups=0.38, wpb=2777, bsz=84, num_updates=988, lr=1.1856e-05, gnorm=2.38, train_wall=5, gb_free=10.5, wall=3628
2024-08-11 17:55:26 | INFO | train_inner | epoch 016:     45 / 63 loss=6.544, nll_loss=3.179, ppl=9.06, wps=1500.2, ups=0.39, wpb=3841, bsz=188, num_updates=990, lr=1.188e-05, gnorm=1.895, train_wall=5, gb_free=11.1, wall=3633
2024-08-11 17:55:31 | INFO | train_inner | epoch 016:     47 / 63 loss=6.359, nll_loss=2.953, ppl=7.74, wps=1208.9, ups=0.37, wpb=3259, bsz=208, num_updates=992, lr=1.1904e-05, gnorm=1.96, train_wall=5, gb_free=12.6, wall=3638
2024-08-11 17:55:36 | INFO | train_inner | epoch 016:     49 / 63 loss=6.917, nll_loss=3.638, ppl=12.45, wps=1272.1, ups=0.4, wpb=3212, bsz=76, num_updates=994, lr=1.1928e-05, gnorm=2.429, train_wall=5, gb_free=11.9, wall=3643
2024-08-11 17:55:40 | INFO | train_inner | epoch 016:     51 / 63 loss=6.883, nll_loss=3.594, ppl=12.08, wps=1465.9, ups=0.51, wpb=2854.5, bsz=76.5, num_updates=996, lr=1.1952e-05, gnorm=2.575, train_wall=4, gb_free=11.7, wall=3647
2024-08-11 17:55:45 | INFO | train_inner | epoch 016:     53 / 63 loss=6.469, nll_loss=3.091, ppl=8.52, wps=1198.7, ups=0.38, wpb=3133, bsz=144, num_updates=998, lr=1.1976e-05, gnorm=2.147, train_wall=5, gb_free=13.6, wall=3652
2024-08-11 17:55:50 | INFO | train_inner | epoch 016:     55 / 63 loss=6.616, nll_loss=3.274, ppl=9.67, wps=1156.6, ups=0.4, wpb=2912.5, bsz=72, num_updates=1000, lr=1.2e-05, gnorm=2.377, train_wall=5, gb_free=12.6, wall=3657
2024-08-11 17:55:56 | INFO | train_inner | epoch 016:     57 / 63 loss=6.44, nll_loss=3.066, ppl=8.38, wps=1272.6, ups=0.37, wpb=3436.5, bsz=156, num_updates=1002, lr=1.2024e-05, gnorm=1.993, train_wall=5, gb_free=12.9, wall=3663
2024-08-11 17:56:01 | INFO | train_inner | epoch 016:     59 / 63 loss=6.561, nll_loss=3.211, ppl=9.26, wps=1195.6, ups=0.37, wpb=3270, bsz=164, num_updates=1004, lr=1.2048e-05, gnorm=2.074, train_wall=5, gb_free=12.8, wall=3668
2024-08-11 17:56:06 | INFO | train_inner | epoch 016:     61 / 63 loss=6.714, nll_loss=3.396, ppl=10.53, wps=1280.8, ups=0.39, wpb=3277, bsz=116, num_updates=1006, lr=1.2072e-05, gnorm=2.311, train_wall=5, gb_free=12.9, wall=3673
2024-08-11 17:56:10 | INFO | train_inner | epoch 016:     63 / 63 loss=6.474, nll_loss=3.088, ppl=8.51, wps=1308.4, ups=0.52, wpb=2535.5, bsz=84, num_updates=1008, lr=1.2096e-05, gnorm=2.597, train_wall=4, gb_free=16.6, wall=3677
2024-08-11 17:56:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31430.265625Mb; avail=223635.9140625Mb
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000526
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31430.7578125Mb; avail=223635.421875Mb
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004742
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31430.9921875Mb; avail=223635.1875Mb
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004085
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009680
2024-08-11 17:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31431.484375Mb; avail=223635.1875Mb
2024-08-11 17:56:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.493 | nll_loss 2.897 | ppl 7.45 | wps 3055.2 | wpb 1463.4 | bsz 64.9 | num_updates 1008 | best_loss 6.493
2024-08-11 17:56:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1008 updates
2024-08-11 17:56:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:57:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 17:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 16 @ 1008 updates, score 6.493) (writing took 66.82324046408758 seconds)
2024-08-11 17:57:25 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-08-11 17:57:25 | INFO | train | epoch 016 | loss 6.588 | nll_loss 3.24 | ppl 9.45 | wps 892.6 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 1008 | lr 1.2096e-05 | gnorm 2.13 | train_wall 164 | gb_free 16.6 | wall 3753
2024-08-11 17:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 17:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 17:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 17:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000745
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23487.8125Mb; avail=231578.38671875Mb
2024-08-11 17:57:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000211
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001349
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23487.8125Mb; avail=231578.38671875Mb
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23487.8125Mb; avail=231578.38671875Mb
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000459
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002155
2024-08-11 17:57:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23487.8125Mb; avail=231578.38671875Mb
2024-08-11 17:57:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 17:57:25 | INFO | fairseq.trainer | begin training epoch 17
2024-08-11 17:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 17:57:31 | INFO | train_inner | epoch 017:      2 / 63 loss=6.5, nll_loss=3.124, ppl=8.72, wps=107.2, ups=0.02, wpb=4315.5, bsz=216, num_updates=1010, lr=1.212e-05, gnorm=1.877, train_wall=5, gb_free=14.1, wall=3758
2024-08-11 17:57:35 | INFO | train_inner | epoch 017:      4 / 63 loss=6.579, nll_loss=3.229, ppl=9.37, wps=1573.7, ups=0.46, wpb=3437, bsz=180.5, num_updates=1012, lr=1.2144e-05, gnorm=2.378, train_wall=4, gb_free=11.1, wall=3762
2024-08-11 17:57:40 | INFO | train_inner | epoch 017:      6 / 63 loss=6.583, nll_loss=3.215, ppl=9.29, wps=1160.2, ups=0.4, wpb=2896, bsz=72, num_updates=1014, lr=1.2168e-05, gnorm=2.397, train_wall=5, gb_free=14.1, wall=3767
2024-08-11 17:57:44 | INFO | train_inner | epoch 017:      8 / 63 loss=6.828, nll_loss=3.531, ppl=11.56, wps=1308.6, ups=0.44, wpb=2956, bsz=72, num_updates=1016, lr=1.2192e-05, gnorm=2.45, train_wall=5, gb_free=13.7, wall=3772
2024-08-11 17:57:49 | INFO | train_inner | epoch 017:     10 / 63 loss=6.442, nll_loss=3.045, ppl=8.26, wps=1498.6, ups=0.43, wpb=3526, bsz=132, num_updates=1018, lr=1.2216e-05, gnorm=2.086, train_wall=5, gb_free=15.1, wall=3776
2024-08-11 17:57:54 | INFO | train_inner | epoch 017:     12 / 63 loss=6.613, nll_loss=3.293, ppl=9.8, wps=1238.7, ups=0.39, wpb=3171, bsz=132, num_updates=1020, lr=1.224e-05, gnorm=2.208, train_wall=5, gb_free=12.8, wall=3782
2024-08-11 17:58:00 | INFO | train_inner | epoch 017:     14 / 63 loss=6.447, nll_loss=3.067, ppl=8.38, wps=1415.5, ups=0.36, wpb=3920.5, bsz=200, num_updates=1022, lr=1.2264e-05, gnorm=1.99, train_wall=6, gb_free=10.7, wall=3787
2024-08-11 17:58:05 | INFO | train_inner | epoch 017:     16 / 63 loss=6.472, nll_loss=3.084, ppl=8.48, wps=1288.7, ups=0.41, wpb=3112, bsz=136, num_updates=1024, lr=1.2288e-05, gnorm=2.181, train_wall=5, gb_free=11.2, wall=3792
2024-08-11 17:58:10 | INFO | train_inner | epoch 017:     18 / 63 loss=6.484, nll_loss=3.091, ppl=8.52, wps=1344.4, ups=0.38, wpb=3495, bsz=132, num_updates=1026, lr=1.2312e-05, gnorm=2.09, train_wall=5, gb_free=10.9, wall=3797
2024-08-11 17:58:20 | INFO | train_inner | epoch 017:     20 / 63 loss=6.469, nll_loss=3.079, ppl=8.45, wps=738.9, ups=0.19, wpb=3816, bsz=152, num_updates=1028, lr=1.2336e-05, gnorm=2.031, train_wall=10, gb_free=10.7, wall=3807
2024-08-11 17:58:25 | INFO | train_inner | epoch 017:     22 / 63 loss=6.385, nll_loss=2.992, ppl=7.96, wps=1408.3, ups=0.38, wpb=3702, bsz=228, num_updates=1030, lr=1.236e-05, gnorm=1.957, train_wall=5, gb_free=13.3, wall=3813
2024-08-11 17:58:30 | INFO | train_inner | epoch 017:     24 / 63 loss=6.353, nll_loss=2.938, ppl=7.67, wps=1176.7, ups=0.4, wpb=2948, bsz=144, num_updates=1032, lr=1.2384e-05, gnorm=2.251, train_wall=5, gb_free=11.7, wall=3818
2024-08-11 17:58:36 | INFO | train_inner | epoch 017:     26 / 63 loss=6.438, nll_loss=3.052, ppl=8.3, wps=1484.3, ups=0.38, wpb=3938, bsz=216, num_updates=1034, lr=1.2408e-05, gnorm=1.957, train_wall=5, gb_free=13.8, wall=3823
2024-08-11 17:58:41 | INFO | train_inner | epoch 017:     28 / 63 loss=6.601, nll_loss=3.244, ppl=9.48, wps=1257.5, ups=0.38, wpb=3304, bsz=112, num_updates=1036, lr=1.2432e-05, gnorm=2.226, train_wall=5, gb_free=11.2, wall=3828
2024-08-11 17:58:46 | INFO | train_inner | epoch 017:     30 / 63 loss=6.469, nll_loss=3.099, ppl=8.57, wps=1139.8, ups=0.38, wpb=2980.5, bsz=144, num_updates=1038, lr=1.2456e-05, gnorm=2.175, train_wall=5, gb_free=12.3, wall=3834
2024-08-11 17:58:51 | INFO | train_inner | epoch 017:     32 / 63 loss=6.643, nll_loss=3.324, ppl=10.01, wps=1377, ups=0.39, wpb=3570, bsz=144, num_updates=1040, lr=1.248e-05, gnorm=2.041, train_wall=5, gb_free=11.3, wall=3839
2024-08-11 17:58:56 | INFO | train_inner | epoch 017:     34 / 63 loss=6.51, nll_loss=3.132, ppl=8.77, wps=1283.4, ups=0.4, wpb=3204, bsz=132, num_updates=1042, lr=1.2504e-05, gnorm=2.082, train_wall=5, gb_free=13.9, wall=3844
2024-08-11 17:59:02 | INFO | train_inner | epoch 017:     36 / 63 loss=6.375, nll_loss=2.951, ppl=7.73, wps=1174.5, ups=0.37, wpb=3159, bsz=180, num_updates=1044, lr=1.2528e-05, gnorm=2.056, train_wall=5, gb_free=12.8, wall=3849
2024-08-11 17:59:07 | INFO | train_inner | epoch 017:     38 / 63 loss=6.567, nll_loss=3.199, ppl=9.18, wps=1295.4, ups=0.39, wpb=3356, bsz=140, num_updates=1046, lr=1.2552e-05, gnorm=2.107, train_wall=5, gb_free=11.6, wall=3854
2024-08-11 17:59:12 | INFO | train_inner | epoch 017:     40 / 63 loss=6.481, nll_loss=3.098, ppl=8.56, wps=1306.3, ups=0.39, wpb=3318, bsz=140, num_updates=1048, lr=1.2576e-05, gnorm=1.956, train_wall=5, gb_free=12.3, wall=3859
2024-08-11 17:59:17 | INFO | train_inner | epoch 017:     42 / 63 loss=6.592, nll_loss=3.266, ppl=9.62, wps=1580.6, ups=0.38, wpb=4124.5, bsz=184, num_updates=1050, lr=1.26e-05, gnorm=1.927, train_wall=5, gb_free=11.6, wall=3865
2024-08-11 17:59:22 | INFO | train_inner | epoch 017:     44 / 63 loss=6.557, nll_loss=3.198, ppl=9.18, wps=1200.3, ups=0.4, wpb=2991, bsz=72, num_updates=1052, lr=1.2624e-05, gnorm=2.409, train_wall=5, gb_free=14.6, wall=3870
2024-08-11 17:59:27 | INFO | train_inner | epoch 017:     46 / 63 loss=6.567, nll_loss=3.205, ppl=9.22, wps=1232.6, ups=0.41, wpb=3027.5, bsz=80, num_updates=1054, lr=1.2648e-05, gnorm=2.137, train_wall=5, gb_free=12, wall=3874
2024-08-11 17:59:33 | INFO | train_inner | epoch 017:     48 / 63 loss=6.648, nll_loss=3.306, ppl=9.89, wps=1241.1, ups=0.37, wpb=3380, bsz=124, num_updates=1056, lr=1.2672e-05, gnorm=2.123, train_wall=5, gb_free=11.9, wall=3880
2024-08-11 17:59:37 | INFO | train_inner | epoch 017:     50 / 63 loss=6.608, nll_loss=3.26, ppl=9.58, wps=1224.8, ups=0.46, wpb=2686, bsz=96, num_updates=1058, lr=1.2696e-05, gnorm=2.38, train_wall=4, gb_free=15.7, wall=3884
2024-08-11 17:59:43 | INFO | train_inner | epoch 017:     52 / 63 loss=6.441, nll_loss=3.036, ppl=8.2, wps=1308.6, ups=0.36, wpb=3630, bsz=128, num_updates=1060, lr=1.272e-05, gnorm=2.103, train_wall=6, gb_free=11.4, wall=3890
2024-08-11 17:59:47 | INFO | train_inner | epoch 017:     54 / 63 loss=6.65, nll_loss=3.305, ppl=9.88, wps=1368.3, ups=0.43, wpb=3203, bsz=92, num_updates=1062, lr=1.2744e-05, gnorm=2.255, train_wall=5, gb_free=15, wall=3895
2024-08-11 17:59:52 | INFO | train_inner | epoch 017:     56 / 63 loss=6.553, nll_loss=3.2, ppl=9.19, wps=1518.7, ups=0.38, wpb=3973.5, bsz=200, num_updates=1064, lr=1.2768e-05, gnorm=1.993, train_wall=5, gb_free=13, wall=3900
2024-08-11 17:59:58 | INFO | train_inner | epoch 017:     58 / 63 loss=6.311, nll_loss=2.893, ppl=7.43, wps=1270, ups=0.39, wpb=3284.5, bsz=176, num_updates=1066, lr=1.2792e-05, gnorm=2.145, train_wall=5, gb_free=11, wall=3905
2024-08-11 18:00:03 | INFO | train_inner | epoch 017:     60 / 63 loss=6.429, nll_loss=3.053, ppl=8.3, wps=1163.4, ups=0.4, wpb=2936, bsz=148, num_updates=1068, lr=1.2816e-05, gnorm=2.102, train_wall=5, gb_free=12.6, wall=3910
2024-08-11 18:00:08 | INFO | train_inner | epoch 017:     62 / 63 loss=6.285, nll_loss=2.87, ppl=7.31, wps=1597.4, ups=0.35, wpb=4600, bsz=304, num_updates=1070, lr=1.284e-05, gnorm=1.67, train_wall=6, gb_free=12.9, wall=3916
2024-08-11 18:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31428.36328125Mb; avail=223637.8828125Mb
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000535
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31428.36328125Mb; avail=223637.8828125Mb
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004687
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31428.36328125Mb; avail=223637.8828125Mb
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004014
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009565
2024-08-11 18:00:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31428.36328125Mb; avail=223637.390625Mb
2024-08-11 18:00:18 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.448 | nll_loss 2.852 | ppl 7.22 | wps 3060.2 | wpb 1463.4 | bsz 64.9 | num_updates 1071 | best_loss 6.448
2024-08-11 18:00:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1071 updates
2024-08-11 18:00:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:00:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 17 @ 1071 updates, score 6.448) (writing took 66.33871356025338 seconds)
2024-08-11 18:01:25 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-08-11 18:01:25 | INFO | train | epoch 017 | loss 6.506 | nll_loss 3.134 | ppl 8.78 | wps 895.2 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 1071 | lr 1.2852e-05 | gnorm 2.124 | train_wall 164 | gb_free 16.5 | wall 3992
2024-08-11 18:01:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:01:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:01:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:01:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000877
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16650.33984375Mb; avail=238415.90625Mb
2024-08-11 18:01:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000150
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001350
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16650.33984375Mb; avail=238415.90625Mb
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000064
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16650.33984375Mb; avail=238415.90625Mb
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000483
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002277
2024-08-11 18:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16650.33984375Mb; avail=238415.90625Mb
2024-08-11 18:01:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:01:25 | INFO | fairseq.trainer | begin training epoch 18
2024-08-11 18:01:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:01:27 | INFO | train_inner | epoch 018:      1 / 63 loss=6.357, nll_loss=2.943, ppl=7.69, wps=77.2, ups=0.03, wpb=3049, bsz=136, num_updates=1072, lr=1.2864e-05, gnorm=2.107, train_wall=4, gb_free=10.7, wall=3995
2024-08-11 18:01:33 | INFO | train_inner | epoch 018:      3 / 63 loss=6.329, nll_loss=2.917, ppl=7.55, wps=1693.3, ups=0.38, wpb=4399, bsz=272, num_updates=1074, lr=1.2888e-05, gnorm=1.81, train_wall=5, gb_free=12.2, wall=4000
2024-08-11 18:01:37 | INFO | train_inner | epoch 018:      5 / 63 loss=6.507, nll_loss=3.116, ppl=8.67, wps=1170.9, ups=0.43, wpb=2694, bsz=56, num_updates=1076, lr=1.2912e-05, gnorm=2.518, train_wall=5, gb_free=10.8, wall=4005
2024-08-11 18:01:42 | INFO | train_inner | epoch 018:      7 / 63 loss=6.471, nll_loss=3.08, ppl=8.45, wps=1422.6, ups=0.43, wpb=3315, bsz=136, num_updates=1078, lr=1.2936e-05, gnorm=1.996, train_wall=5, gb_free=16.2, wall=4009
2024-08-11 18:01:47 | INFO | train_inner | epoch 018:      9 / 63 loss=6.401, nll_loss=3.024, ppl=8.14, wps=1575.7, ups=0.38, wpb=4197.5, bsz=280, num_updates=1080, lr=1.296e-05, gnorm=1.78, train_wall=5, gb_free=11.1, wall=4014
2024-08-11 18:01:52 | INFO | train_inner | epoch 018:     11 / 63 loss=6.521, nll_loss=3.151, ppl=8.88, wps=1188.4, ups=0.4, wpb=2986, bsz=100, num_updates=1082, lr=1.2984e-05, gnorm=2.18, train_wall=5, gb_free=13.5, wall=4020
2024-08-11 18:01:57 | INFO | train_inner | epoch 018:     13 / 63 loss=6.49, nll_loss=3.094, ppl=8.54, wps=1434.5, ups=0.42, wpb=3435.5, bsz=108, num_updates=1084, lr=1.3008e-05, gnorm=1.952, train_wall=5, gb_free=15.9, wall=4024
2024-08-11 18:02:07 | INFO | train_inner | epoch 018:     15 / 63 loss=6.49, nll_loss=3.123, ppl=8.71, wps=537.4, ups=0.2, wpb=2661, bsz=104, num_updates=1086, lr=1.3032e-05, gnorm=2.486, train_wall=10, gb_free=13.5, wall=4034
2024-08-11 18:02:12 | INFO | train_inner | epoch 018:     17 / 63 loss=6.451, nll_loss=3.07, ppl=8.4, wps=1470.1, ups=0.39, wpb=3729, bsz=152, num_updates=1088, lr=1.3056e-05, gnorm=1.971, train_wall=5, gb_free=13.6, wall=4039
2024-08-11 18:02:17 | INFO | train_inner | epoch 018:     19 / 63 loss=6.682, nll_loss=3.354, ppl=10.23, wps=1344.5, ups=0.38, wpb=3520.5, bsz=80, num_updates=1090, lr=1.308e-05, gnorm=2.232, train_wall=5, gb_free=12, wall=4045
2024-08-11 18:02:22 | INFO | train_inner | epoch 018:     21 / 63 loss=6.492, nll_loss=3.083, ppl=8.47, wps=1359.2, ups=0.39, wpb=3454.5, bsz=100, num_updates=1092, lr=1.3104e-05, gnorm=2.027, train_wall=5, gb_free=12.6, wall=4050
2024-08-11 18:02:28 | INFO | train_inner | epoch 018:     23 / 63 loss=6.224, nll_loss=2.792, ppl=6.93, wps=1442.6, ups=0.37, wpb=3938.5, bsz=320, num_updates=1094, lr=1.3128e-05, gnorm=1.932, train_wall=5, gb_free=12.9, wall=4055
2024-08-11 18:02:33 | INFO | train_inner | epoch 018:     25 / 63 loss=6.408, nll_loss=3.007, ppl=8.04, wps=1561, ups=0.38, wpb=4142, bsz=184, num_updates=1096, lr=1.3152e-05, gnorm=1.998, train_wall=5, gb_free=12.8, wall=4060
2024-08-11 18:02:38 | INFO | train_inner | epoch 018:     27 / 63 loss=6.392, nll_loss=2.987, ppl=7.93, wps=1518.6, ups=0.37, wpb=4052.5, bsz=224, num_updates=1098, lr=1.3176e-05, gnorm=1.893, train_wall=5, gb_free=12.1, wall=4066
2024-08-11 18:02:43 | INFO | train_inner | epoch 018:     29 / 63 loss=6.407, nll_loss=2.988, ppl=7.93, wps=1560.7, ups=0.43, wpb=3600, bsz=124.5, num_updates=1100, lr=1.32e-05, gnorm=2.188, train_wall=5, gb_free=16.3, wall=4070
2024-08-11 18:02:48 | INFO | train_inner | epoch 018:     31 / 63 loss=6.462, nll_loss=3.079, ppl=8.45, wps=1228.5, ups=0.38, wpb=3229.5, bsz=140, num_updates=1102, lr=1.3224e-05, gnorm=2.018, train_wall=5, gb_free=11.4, wall=4076
2024-08-11 18:02:54 | INFO | train_inner | epoch 018:     33 / 63 loss=6.438, nll_loss=3.056, ppl=8.32, wps=1115.3, ups=0.36, wpb=3078.5, bsz=120, num_updates=1104, lr=1.3248e-05, gnorm=2.059, train_wall=6, gb_free=10.8, wall=4081
2024-08-11 18:02:59 | INFO | train_inner | epoch 018:     35 / 63 loss=6.392, nll_loss=3.003, ppl=8.02, wps=1120.5, ups=0.41, wpb=2709, bsz=120, num_updates=1106, lr=1.3272e-05, gnorm=2.488, train_wall=5, gb_free=13.8, wall=4086
2024-08-11 18:03:04 | INFO | train_inner | epoch 018:     37 / 63 loss=6.409, nll_loss=3.008, ppl=8.04, wps=1471.2, ups=0.4, wpb=3673.5, bsz=128, num_updates=1108, lr=1.3296e-05, gnorm=1.976, train_wall=5, gb_free=10.6, wall=4091
2024-08-11 18:03:08 | INFO | train_inner | epoch 018:     39 / 63 loss=6.469, nll_loss=3.082, ppl=8.47, wps=1375.1, ups=0.43, wpb=3162, bsz=100, num_updates=1110, lr=1.332e-05, gnorm=2.089, train_wall=5, gb_free=15.4, wall=4096
2024-08-11 18:03:14 | INFO | train_inner | epoch 018:     41 / 63 loss=6.232, nll_loss=2.794, ppl=6.93, wps=1392.2, ups=0.36, wpb=3881, bsz=208, num_updates=1112, lr=1.3344e-05, gnorm=2.073, train_wall=6, gb_free=12.4, wall=4101
2024-08-11 18:03:19 | INFO | train_inner | epoch 018:     43 / 63 loss=6.505, nll_loss=3.129, ppl=8.75, wps=1263.6, ups=0.42, wpb=3027.5, bsz=124, num_updates=1114, lr=1.3368e-05, gnorm=2.233, train_wall=5, gb_free=12.4, wall=4106
2024-08-11 18:03:24 | INFO | train_inner | epoch 018:     45 / 63 loss=6.515, nll_loss=3.136, ppl=8.79, wps=1419.4, ups=0.36, wpb=3904.5, bsz=168, num_updates=1116, lr=1.3392e-05, gnorm=2.093, train_wall=5, gb_free=9.6, wall=4111
2024-08-11 18:03:29 | INFO | train_inner | epoch 018:     47 / 63 loss=6.225, nll_loss=2.779, ppl=6.86, wps=1080.6, ups=0.38, wpb=2860.5, bsz=164, num_updates=1118, lr=1.3416e-05, gnorm=2.07, train_wall=5, gb_free=11.6, wall=4117
2024-08-11 18:03:34 | INFO | train_inner | epoch 018:     49 / 63 loss=6.536, nll_loss=3.169, ppl=8.99, wps=1251.4, ups=0.41, wpb=3084.5, bsz=136, num_updates=1120, lr=1.344e-05, gnorm=2.248, train_wall=5, gb_free=12.8, wall=4122
2024-08-11 18:03:39 | INFO | train_inner | epoch 018:     51 / 63 loss=6.452, nll_loss=3.075, ppl=8.42, wps=1518.2, ups=0.39, wpb=3909.5, bsz=156, num_updates=1122, lr=1.3464e-05, gnorm=1.899, train_wall=5, gb_free=12.1, wall=4127
2024-08-11 18:03:45 | INFO | train_inner | epoch 018:     53 / 63 loss=6.305, nll_loss=2.886, ppl=7.39, wps=1174.3, ups=0.38, wpb=3064.5, bsz=144, num_updates=1124, lr=1.3488e-05, gnorm=2.074, train_wall=5, gb_free=12.5, wall=4132
2024-08-11 18:03:49 | INFO | train_inner | epoch 018:     55 / 63 loss=6.639, nll_loss=3.303, ppl=9.87, wps=1290.1, ups=0.45, wpb=2855.5, bsz=104, num_updates=1126, lr=1.3512e-05, gnorm=2.685, train_wall=4, gb_free=17.8, wall=4136
2024-08-11 18:03:54 | INFO | train_inner | epoch 018:     57 / 63 loss=6.288, nll_loss=2.837, ppl=7.15, wps=1105.9, ups=0.42, wpb=2658.5, bsz=124, num_updates=1128, lr=1.3536e-05, gnorm=2.204, train_wall=5, gb_free=12.4, wall=4141
2024-08-11 18:03:59 | INFO | train_inner | epoch 018:     59 / 63 loss=6.507, nll_loss=3.127, ppl=8.74, wps=1294, ups=0.41, wpb=3147.5, bsz=92, num_updates=1130, lr=1.356e-05, gnorm=2.085, train_wall=5, gb_free=14.2, wall=4146
2024-08-11 18:04:04 | INFO | train_inner | epoch 018:     61 / 63 loss=6.454, nll_loss=3.066, ppl=8.37, wps=1495.4, ups=0.38, wpb=3891, bsz=156, num_updates=1132, lr=1.3584e-05, gnorm=1.938, train_wall=5, gb_free=13.2, wall=4151
2024-08-11 18:04:08 | INFO | train_inner | epoch 018:     63 / 63 loss=6.489, nll_loss=3.131, ppl=8.76, wps=1453.3, ups=0.49, wpb=2972.5, bsz=168, num_updates=1134, lr=1.3608e-05, gnorm=2.254, train_wall=4, gb_free=16, wall=4155
2024-08-11 18:04:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21094.39453125Mb; avail=233971.80859375Mb
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000553
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21094.39453125Mb; avail=233971.80859375Mb
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004694
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21094.39453125Mb; avail=233971.80859375Mb
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004060
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009671
2024-08-11 18:04:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21094.39453125Mb; avail=233971.80859375Mb
2024-08-11 18:04:16 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.41 | nll_loss 2.802 | ppl 6.97 | wps 3061.3 | wpb 1463.4 | bsz 64.9 | num_updates 1134 | best_loss 6.41
2024-08-11 18:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1134 updates
2024-08-11 18:04:16 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:05:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 18 @ 1134 updates, score 6.41) (writing took 62.67574311187491 seconds)
2024-08-11 18:05:19 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-08-11 18:05:19 | INFO | train | epoch 018 | loss 6.433 | nll_loss 3.041 | ppl 8.23 | wps 913.5 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1134 | lr 1.3608e-05 | gnorm 2.108 | train_wall 163 | gb_free 16 | wall 4226
2024-08-11 18:05:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:05:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:05:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:05:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000814
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18143.8203125Mb; avail=236922.42578125Mb
2024-08-11 18:05:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000157
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001306
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18143.8203125Mb; avail=236922.42578125Mb
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000047
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18143.8203125Mb; avail=236922.42578125Mb
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000463
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002126
2024-08-11 18:05:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18143.8203125Mb; avail=236922.42578125Mb
2024-08-11 18:05:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:05:19 | INFO | fairseq.trainer | begin training epoch 19
2024-08-11 18:05:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:05:24 | INFO | train_inner | epoch 019:      2 / 63 loss=6.387, nll_loss=2.987, ppl=7.93, wps=106, ups=0.03, wpb=4040.5, bsz=172, num_updates=1136, lr=1.3632e-05, gnorm=1.803, train_wall=5, gb_free=11.7, wall=4232
2024-08-11 18:05:29 | INFO | train_inner | epoch 019:      4 / 63 loss=6.234, nll_loss=2.793, ppl=6.93, wps=1252.8, ups=0.42, wpb=3011.5, bsz=168, num_updates=1138, lr=1.3656e-05, gnorm=2.134, train_wall=5, gb_free=13.6, wall=4236
2024-08-11 18:05:33 | INFO | train_inner | epoch 019:      6 / 63 loss=6.527, nll_loss=3.15, ppl=8.87, wps=1132, ups=0.52, wpb=2179, bsz=60.5, num_updates=1140, lr=1.368e-05, gnorm=2.79, train_wall=4, gb_free=13.2, wall=4240
2024-08-11 18:05:38 | INFO | train_inner | epoch 019:      8 / 63 loss=6.432, nll_loss=3.044, ppl=8.25, wps=1485.6, ups=0.39, wpb=3785.5, bsz=180, num_updates=1142, lr=1.3704e-05, gnorm=1.942, train_wall=5, gb_free=13.8, wall=4245
2024-08-11 18:05:43 | INFO | train_inner | epoch 019:     10 / 63 loss=6.448, nll_loss=3.059, ppl=8.33, wps=1290.4, ups=0.41, wpb=3171.5, bsz=120, num_updates=1144, lr=1.3728e-05, gnorm=2.133, train_wall=5, gb_free=13.2, wall=4250
2024-08-11 18:05:48 | INFO | train_inner | epoch 019:     12 / 63 loss=6.435, nll_loss=3.045, ppl=8.26, wps=1497.5, ups=0.37, wpb=4047.5, bsz=164, num_updates=1146, lr=1.3752e-05, gnorm=2.034, train_wall=5, gb_free=12, wall=4256
2024-08-11 18:05:53 | INFO | train_inner | epoch 019:     14 / 63 loss=6.211, nll_loss=2.772, ppl=6.83, wps=1460, ups=0.4, wpb=3677.5, bsz=228, num_updates=1148, lr=1.3776e-05, gnorm=1.952, train_wall=5, gb_free=10.6, wall=4261
2024-08-11 18:05:58 | INFO | train_inner | epoch 019:     16 / 63 loss=6.318, nll_loss=2.89, ppl=7.41, wps=1395.7, ups=0.4, wpb=3449.5, bsz=144, num_updates=1150, lr=1.38e-05, gnorm=2.107, train_wall=5, gb_free=15.4, wall=4266
2024-08-11 18:06:04 | INFO | train_inner | epoch 019:     18 / 63 loss=6.383, nll_loss=2.967, ppl=7.82, wps=1321.9, ups=0.37, wpb=3607, bsz=116, num_updates=1152, lr=1.3824e-05, gnorm=2.026, train_wall=5, gb_free=13, wall=4271
2024-08-11 18:06:14 | INFO | train_inner | epoch 019:     20 / 63 loss=6.522, nll_loss=3.149, ppl=8.87, wps=786.3, ups=0.2, wpb=3847.5, bsz=152, num_updates=1154, lr=1.3848e-05, gnorm=2.225, train_wall=10, gb_free=16.2, wall=4281
2024-08-11 18:06:19 | INFO | train_inner | epoch 019:     22 / 63 loss=6.42, nll_loss=3.026, ppl=8.14, wps=1538.7, ups=0.39, wpb=3915, bsz=184, num_updates=1156, lr=1.3872e-05, gnorm=2.1, train_wall=5, gb_free=12.6, wall=4286
2024-08-11 18:06:24 | INFO | train_inner | epoch 019:     24 / 63 loss=6.42, nll_loss=3.02, ppl=8.11, wps=1486.7, ups=0.4, wpb=3719, bsz=148, num_updates=1158, lr=1.3896e-05, gnorm=1.906, train_wall=5, gb_free=11.8, wall=4291
2024-08-11 18:06:29 | INFO | train_inner | epoch 019:     26 / 63 loss=6.288, nll_loss=2.876, ppl=7.34, wps=1123.4, ups=0.38, wpb=2931.5, bsz=164, num_updates=1160, lr=1.392e-05, gnorm=2.041, train_wall=5, gb_free=11.5, wall=4296
2024-08-11 18:06:34 | INFO | train_inner | epoch 019:     28 / 63 loss=6.365, nll_loss=2.954, ppl=7.75, wps=1365.7, ups=0.36, wpb=3775.5, bsz=172, num_updates=1162, lr=1.3944e-05, gnorm=1.819, train_wall=6, gb_free=10.8, wall=4302
2024-08-11 18:06:40 | INFO | train_inner | epoch 019:     30 / 63 loss=6.304, nll_loss=2.89, ppl=7.41, wps=1331.2, ups=0.36, wpb=3715.5, bsz=228, num_updates=1164, lr=1.3968e-05, gnorm=1.98, train_wall=6, gb_free=12.6, wall=4307
2024-08-11 18:06:45 | INFO | train_inner | epoch 019:     32 / 63 loss=6.571, nll_loss=3.195, ppl=9.16, wps=1157.5, ups=0.38, wpb=3084, bsz=84, num_updates=1166, lr=1.3992e-05, gnorm=2.23, train_wall=5, gb_free=12.1, wall=4313
2024-08-11 18:06:50 | INFO | train_inner | epoch 019:     34 / 63 loss=6.442, nll_loss=3.047, ppl=8.27, wps=1152.6, ups=0.42, wpb=2721, bsz=88, num_updates=1168, lr=1.4016e-05, gnorm=2.333, train_wall=5, gb_free=12, wall=4317
2024-08-11 18:06:55 | INFO | train_inner | epoch 019:     36 / 63 loss=6.399, nll_loss=2.998, ppl=7.99, wps=1567.4, ups=0.4, wpb=3872, bsz=160, num_updates=1170, lr=1.404e-05, gnorm=1.979, train_wall=5, gb_free=12.4, wall=4322
2024-08-11 18:07:00 | INFO | train_inner | epoch 019:     38 / 63 loss=6.424, nll_loss=3.008, ppl=8.04, wps=1276.9, ups=0.43, wpb=2963, bsz=84, num_updates=1172, lr=1.4064e-05, gnorm=2.248, train_wall=5, gb_free=10.7, wall=4327
2024-08-11 18:07:05 | INFO | train_inner | epoch 019:     40 / 63 loss=6.322, nll_loss=2.899, ppl=7.46, wps=1294.9, ups=0.37, wpb=3540, bsz=148, num_updates=1174, lr=1.4088e-05, gnorm=1.976, train_wall=5, gb_free=12.4, wall=4332
2024-08-11 18:07:10 | INFO | train_inner | epoch 019:     42 / 63 loss=6.28, nll_loss=2.843, ppl=7.17, wps=1455, ups=0.38, wpb=3811.5, bsz=184, num_updates=1176, lr=1.4112e-05, gnorm=1.873, train_wall=5, gb_free=12.9, wall=4338
2024-08-11 18:07:15 | INFO | train_inner | epoch 019:     44 / 63 loss=6.381, nll_loss=2.97, ppl=7.83, wps=1195.8, ups=0.43, wpb=2796, bsz=80, num_updates=1178, lr=1.4136e-05, gnorm=2.24, train_wall=5, gb_free=16.2, wall=4342
2024-08-11 18:07:20 | INFO | train_inner | epoch 019:     46 / 63 loss=6.191, nll_loss=2.735, ppl=6.66, wps=1081.7, ups=0.43, wpb=2510.5, bsz=88, num_updates=1180, lr=1.416e-05, gnorm=2.171, train_wall=5, gb_free=11.8, wall=4347
2024-08-11 18:07:25 | INFO | train_inner | epoch 019:     48 / 63 loss=6.354, nll_loss=2.943, ppl=7.69, wps=1309.3, ups=0.37, wpb=3544.5, bsz=188, num_updates=1182, lr=1.4184e-05, gnorm=2.047, train_wall=5, gb_free=12.2, wall=4352
2024-08-11 18:07:30 | INFO | train_inner | epoch 019:     50 / 63 loss=6.333, nll_loss=2.901, ppl=7.47, wps=1274.3, ups=0.38, wpb=3397.5, bsz=116, num_updates=1184, lr=1.4208e-05, gnorm=2.066, train_wall=5, gb_free=12.4, wall=4358
2024-08-11 18:07:36 | INFO | train_inner | epoch 019:     52 / 63 loss=6.109, nll_loss=2.634, ppl=6.21, wps=1481.4, ups=0.37, wpb=4045, bsz=308, num_updates=1186, lr=1.4232e-05, gnorm=1.777, train_wall=5, gb_free=12.4, wall=4363
2024-08-11 18:07:41 | INFO | train_inner | epoch 019:     54 / 63 loss=6.236, nll_loss=2.77, ppl=6.82, wps=1261.9, ups=0.4, wpb=3172.5, bsz=108, num_updates=1188, lr=1.4256e-05, gnorm=2.023, train_wall=5, gb_free=10.1, wall=4368
2024-08-11 18:07:46 | INFO | train_inner | epoch 019:     56 / 63 loss=6.29, nll_loss=2.858, ppl=7.25, wps=1515.4, ups=0.38, wpb=4016, bsz=180, num_updates=1190, lr=1.428e-05, gnorm=1.844, train_wall=5, gb_free=12.4, wall=4374
2024-08-11 18:07:51 | INFO | train_inner | epoch 019:     58 / 63 loss=6.41, nll_loss=3.03, ppl=8.17, wps=1464.9, ups=0.39, wpb=3753, bsz=188, num_updates=1192, lr=1.4304e-05, gnorm=1.926, train_wall=5, gb_free=11.4, wall=4379
2024-08-11 18:07:56 | INFO | train_inner | epoch 019:     60 / 63 loss=6.457, nll_loss=3.081, ppl=8.46, wps=1326.6, ups=0.42, wpb=3138, bsz=144, num_updates=1194, lr=1.4328e-05, gnorm=2.024, train_wall=5, gb_free=11.6, wall=4383
2024-08-11 18:08:01 | INFO | train_inner | epoch 019:     62 / 63 loss=6.368, nll_loss=2.948, ppl=7.72, wps=1243.5, ups=0.38, wpb=3276, bsz=92, num_updates=1196, lr=1.4352e-05, gnorm=2.162, train_wall=5, gb_free=10.7, wall=4389
2024-08-11 18:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22534.85546875Mb; avail=232531.34765625Mb
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000532
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22534.85546875Mb; avail=232531.34765625Mb
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004686
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22534.85546875Mb; avail=232531.34765625Mb
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004042
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009586
2024-08-11 18:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22534.85546875Mb; avail=232531.34765625Mb
2024-08-11 18:08:11 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.364 | nll_loss 2.761 | ppl 6.78 | wps 3062.9 | wpb 1463.4 | bsz 64.9 | num_updates 1197 | best_loss 6.364
2024-08-11 18:08:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1197 updates
2024-08-11 18:08:11 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 19 @ 1197 updates, score 6.364) (writing took 66.02048554411158 seconds)
2024-08-11 18:09:17 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-08-11 18:09:17 | INFO | train | epoch 019 | loss 6.361 | nll_loss 2.949 | ppl 7.72 | wps 900.4 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 1197 | lr 1.4364e-05 | gnorm 2.076 | train_wall 163 | gb_free 16.7 | wall 4464
2024-08-11 18:09:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:09:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:09:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:09:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000789
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19616.43359375Mb; avail=235449.80078125Mb
2024-08-11 18:09:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000192
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001272
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19616.43359375Mb; avail=235449.80078125Mb
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000047
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19616.43359375Mb; avail=235449.80078125Mb
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000456
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002082
2024-08-11 18:09:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19616.43359375Mb; avail=235449.80078125Mb
2024-08-11 18:09:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:09:17 | INFO | fairseq.trainer | begin training epoch 20
2024-08-11 18:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:09:20 | INFO | train_inner | epoch 020:      1 / 63 loss=6.261, nll_loss=2.835, ppl=7.14, wps=72, ups=0.03, wpb=2829, bsz=128, num_updates=1198, lr=1.4376e-05, gnorm=2.396, train_wall=4, gb_free=11.9, wall=4467
2024-08-11 18:09:25 | INFO | train_inner | epoch 020:      3 / 63 loss=6.323, nll_loss=2.9, ppl=7.46, wps=1301.6, ups=0.38, wpb=3405.5, bsz=100, num_updates=1200, lr=1.44e-05, gnorm=2.073, train_wall=5, gb_free=10.2, wall=4472
2024-08-11 18:09:30 | INFO | train_inner | epoch 020:      5 / 63 loss=6.029, nll_loss=2.536, ppl=5.8, wps=1195.9, ups=0.4, wpb=2992, bsz=160, num_updates=1202, lr=1.4424e-05, gnorm=1.905, train_wall=5, gb_free=11.9, wall=4477
2024-08-11 18:09:35 | INFO | train_inner | epoch 020:      7 / 63 loss=6.216, nll_loss=2.775, ppl=6.84, wps=1306.9, ups=0.42, wpb=3137.5, bsz=148, num_updates=1204, lr=1.4448e-05, gnorm=2.11, train_wall=5, gb_free=14.8, wall=4482
2024-08-11 18:09:40 | INFO | train_inner | epoch 020:      9 / 63 loss=6.273, nll_loss=2.849, ppl=7.21, wps=1495, ups=0.41, wpb=3654, bsz=176, num_updates=1206, lr=1.4472e-05, gnorm=1.913, train_wall=5, gb_free=13, wall=4487
2024-08-11 18:09:45 | INFO | train_inner | epoch 020:     11 / 63 loss=6.348, nll_loss=2.915, ppl=7.54, wps=1541.4, ups=0.39, wpb=3983.5, bsz=168, num_updates=1208, lr=1.4496e-05, gnorm=1.954, train_wall=5, gb_free=11.4, wall=4492
2024-08-11 18:09:50 | INFO | train_inner | epoch 020:     13 / 63 loss=6.131, nll_loss=2.626, ppl=6.17, wps=1249, ups=0.37, wpb=3398, bsz=156, num_updates=1210, lr=1.452e-05, gnorm=2.146, train_wall=5, gb_free=10.9, wall=4498
2024-08-11 18:09:55 | INFO | train_inner | epoch 020:     15 / 63 loss=6.175, nll_loss=2.704, ppl=6.51, wps=1390.4, ups=0.41, wpb=3368.5, bsz=196, num_updates=1212, lr=1.4544e-05, gnorm=1.877, train_wall=5, gb_free=11.4, wall=4503
2024-08-11 18:10:01 | INFO | train_inner | epoch 020:     17 / 63 loss=6.372, nll_loss=2.965, ppl=7.81, wps=1417.7, ups=0.38, wpb=3779.5, bsz=156, num_updates=1214, lr=1.4568e-05, gnorm=1.944, train_wall=5, gb_free=12.3, wall=4508
2024-08-11 18:10:05 | INFO | train_inner | epoch 020:     19 / 63 loss=6.688, nll_loss=3.353, ppl=10.22, wps=1311.8, ups=0.48, wpb=2760, bsz=72.5, num_updates=1216, lr=1.4592e-05, gnorm=2.828, train_wall=4, gb_free=13.4, wall=4512
2024-08-11 18:10:15 | INFO | train_inner | epoch 020:     21 / 63 loss=6.341, nll_loss=2.934, ppl=7.64, wps=766.8, ups=0.2, wpb=3815.5, bsz=156, num_updates=1218, lr=1.4616e-05, gnorm=1.97, train_wall=10, gb_free=12.8, wall=4522
2024-08-11 18:10:20 | INFO | train_inner | epoch 020:     23 / 63 loss=6.285, nll_loss=2.858, ppl=7.25, wps=1620.1, ups=0.38, wpb=4240, bsz=216, num_updates=1220, lr=1.464e-05, gnorm=1.855, train_wall=5, gb_free=11.4, wall=4527
2024-08-11 18:10:25 | INFO | train_inner | epoch 020:     25 / 63 loss=6.304, nll_loss=2.854, ppl=7.23, wps=1247, ups=0.38, wpb=3297.5, bsz=120, num_updates=1222, lr=1.4664e-05, gnorm=1.976, train_wall=5, gb_free=11.9, wall=4533
2024-08-11 18:10:30 | INFO | train_inner | epoch 020:     27 / 63 loss=6.285, nll_loss=2.841, ppl=7.17, wps=1263.4, ups=0.4, wpb=3144.5, bsz=124, num_updates=1224, lr=1.4688e-05, gnorm=2.105, train_wall=5, gb_free=12.3, wall=4538
2024-08-11 18:10:36 | INFO | train_inner | epoch 020:     29 / 63 loss=6.348, nll_loss=2.927, ppl=7.61, wps=1321.3, ups=0.37, wpb=3564, bsz=132, num_updates=1226, lr=1.4712e-05, gnorm=1.834, train_wall=5, gb_free=12, wall=4543
2024-08-11 18:10:41 | INFO | train_inner | epoch 020:     31 / 63 loss=6.28, nll_loss=2.851, ppl=7.22, wps=1603.1, ups=0.39, wpb=4130, bsz=220, num_updates=1228, lr=1.4736e-05, gnorm=1.852, train_wall=5, gb_free=11.1, wall=4548
2024-08-11 18:10:46 | INFO | train_inner | epoch 020:     33 / 63 loss=6.196, nll_loss=2.769, ppl=6.82, wps=1278.8, ups=0.36, wpb=3515, bsz=204, num_updates=1230, lr=1.476e-05, gnorm=1.985, train_wall=5, gb_free=11.3, wall=4554
2024-08-11 18:10:51 | INFO | train_inner | epoch 020:     35 / 63 loss=6.242, nll_loss=2.816, ppl=7.04, wps=1064.9, ups=0.39, wpb=2734.5, bsz=124, num_updates=1232, lr=1.4784e-05, gnorm=2.223, train_wall=5, gb_free=13.3, wall=4559
2024-08-11 18:10:56 | INFO | train_inner | epoch 020:     37 / 63 loss=6.239, nll_loss=2.786, ppl=6.9, wps=1385.4, ups=0.4, wpb=3478.5, bsz=132, num_updates=1234, lr=1.4808e-05, gnorm=1.863, train_wall=5, gb_free=12, wall=4564
2024-08-11 18:11:02 | INFO | train_inner | epoch 020:     39 / 63 loss=6.38, nll_loss=2.965, ppl=7.81, wps=1314.3, ups=0.39, wpb=3383.5, bsz=136, num_updates=1236, lr=1.4832e-05, gnorm=1.981, train_wall=5, gb_free=10.7, wall=4569
2024-08-11 18:11:07 | INFO | train_inner | epoch 020:     41 / 63 loss=6.448, nll_loss=3.027, ppl=8.15, wps=1270.8, ups=0.39, wpb=3233.5, bsz=84, num_updates=1238, lr=1.4856e-05, gnorm=2.723, train_wall=5, gb_free=13.8, wall=4574
2024-08-11 18:11:12 | INFO | train_inner | epoch 020:     43 / 63 loss=6.323, nll_loss=2.895, ppl=7.44, wps=1078.7, ups=0.39, wpb=2736.5, bsz=136, num_updates=1240, lr=1.488e-05, gnorm=2.21, train_wall=5, gb_free=11.3, wall=4579
2024-08-11 18:11:17 | INFO | train_inner | epoch 020:     45 / 63 loss=6.087, nll_loss=2.601, ppl=6.07, wps=1233.9, ups=0.37, wpb=3295, bsz=140, num_updates=1242, lr=1.4904e-05, gnorm=1.89, train_wall=5, gb_free=11.5, wall=4584
2024-08-11 18:11:22 | INFO | train_inner | epoch 020:     47 / 63 loss=6.343, nll_loss=2.939, ppl=7.67, wps=1441, ups=0.39, wpb=3674, bsz=164, num_updates=1244, lr=1.4928e-05, gnorm=2.039, train_wall=5, gb_free=13.5, wall=4590
2024-08-11 18:11:27 | INFO | train_inner | epoch 020:     49 / 63 loss=6.345, nll_loss=2.932, ppl=7.63, wps=1369.6, ups=0.43, wpb=3197, bsz=112, num_updates=1246, lr=1.4952e-05, gnorm=2.153, train_wall=5, gb_free=10.9, wall=4594
2024-08-11 18:11:32 | INFO | train_inner | epoch 020:     51 / 63 loss=6.346, nll_loss=2.923, ppl=7.58, wps=1321.6, ups=0.41, wpb=3206, bsz=104, num_updates=1248, lr=1.4976e-05, gnorm=2.154, train_wall=5, gb_free=15.5, wall=4599
2024-08-11 18:11:36 | INFO | train_inner | epoch 020:     53 / 63 loss=6.487, nll_loss=3.123, ppl=8.71, wps=1054.7, ups=0.42, wpb=2494.5, bsz=84, num_updates=1250, lr=1.5e-05, gnorm=2.577, train_wall=5, gb_free=14.4, wall=4604
2024-08-11 18:11:42 | INFO | train_inner | epoch 020:     55 / 63 loss=6.193, nll_loss=2.72, ppl=6.59, wps=1570.1, ups=0.36, wpb=4376.5, bsz=196, num_updates=1252, lr=1.5024e-05, gnorm=1.687, train_wall=6, gb_free=12, wall=4609
2024-08-11 18:11:48 | INFO | train_inner | epoch 020:     57 / 63 loss=6.322, nll_loss=2.871, ppl=7.32, wps=1270.7, ups=0.37, wpb=3477, bsz=128, num_updates=1254, lr=1.5048e-05, gnorm=2.059, train_wall=5, gb_free=11.4, wall=4615
2024-08-11 18:11:53 | INFO | train_inner | epoch 020:     59 / 63 loss=6.131, nll_loss=2.664, ppl=6.34, wps=1243.5, ups=0.4, wpb=3111, bsz=212, num_updates=1256, lr=1.5072e-05, gnorm=2.001, train_wall=5, gb_free=13.2, wall=4620
2024-08-11 18:11:58 | INFO | train_inner | epoch 020:     61 / 63 loss=6.174, nll_loss=2.727, ppl=6.62, wps=1306.1, ups=0.39, wpb=3368.5, bsz=168, num_updates=1258, lr=1.5096e-05, gnorm=1.867, train_wall=5, gb_free=12.7, wall=4625
2024-08-11 18:12:02 | INFO | train_inner | epoch 020:     63 / 63 loss=6.224, nll_loss=2.793, ppl=6.93, wps=1520.2, ups=0.52, wpb=2950, bsz=144, num_updates=1260, lr=1.512e-05, gnorm=1.994, train_wall=4, gb_free=16.1, wall=4629
2024-08-11 18:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15810.27734375Mb; avail=239255.96484375Mb
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000543
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15810.27734375Mb; avail=239255.96484375Mb
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004739
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15810.27734375Mb; avail=239255.96484375Mb
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004013
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009665
2024-08-11 18:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15810.27734375Mb; avail=239255.96484375Mb
2024-08-11 18:12:10 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.343 | nll_loss 2.735 | ppl 6.66 | wps 3052.7 | wpb 1463.4 | bsz 64.9 | num_updates 1260 | best_loss 6.343
2024-08-11 18:12:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1260 updates
2024-08-11 18:12:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:12:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:13:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 20 @ 1260 updates, score 6.343) (writing took 62.230696495156735 seconds)
2024-08-11 18:13:12 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-08-11 18:13:12 | INFO | train | epoch 020 | loss 6.283 | nll_loss 2.849 | ppl 7.21 | wps 911.1 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1260 | lr 1.512e-05 | gnorm 2.053 | train_wall 164 | gb_free 16.1 | wall 4700
2024-08-11 18:13:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:13:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:13:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:13:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000736
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21435.26171875Mb; avail=233630.94921875Mb
2024-08-11 18:13:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000223
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001474
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21435.26171875Mb; avail=233630.94921875Mb
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21435.26171875Mb; avail=233630.94921875Mb
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000449
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002271
2024-08-11 18:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21435.26171875Mb; avail=233630.94921875Mb
2024-08-11 18:13:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:13:12 | INFO | fairseq.trainer | begin training epoch 21
2024-08-11 18:13:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:13:17 | INFO | train_inner | epoch 021:      2 / 63 loss=6.252, nll_loss=2.813, ppl=7.03, wps=88.7, ups=0.03, wpb=3356.5, bsz=76, num_updates=1262, lr=1.5144e-05, gnorm=2.137, train_wall=5, gb_free=10.1, wall=4705
2024-08-11 18:13:22 | INFO | train_inner | epoch 021:      4 / 63 loss=6.004, nll_loss=2.505, ppl=5.68, wps=1212.6, ups=0.4, wpb=3060, bsz=164, num_updates=1264, lr=1.5168e-05, gnorm=2.084, train_wall=5, gb_free=14.1, wall=4710
2024-08-11 18:13:27 | INFO | train_inner | epoch 021:      6 / 63 loss=6.278, nll_loss=2.838, ppl=7.15, wps=1299.1, ups=0.4, wpb=3246.5, bsz=144, num_updates=1266, lr=1.5192e-05, gnorm=1.978, train_wall=5, gb_free=15.6, wall=4715
2024-08-11 18:13:32 | INFO | train_inner | epoch 021:      8 / 63 loss=6.288, nll_loss=2.838, ppl=7.15, wps=1418.2, ups=0.41, wpb=3430, bsz=156, num_updates=1268, lr=1.5216e-05, gnorm=2.111, train_wall=5, gb_free=11.5, wall=4719
2024-08-11 18:13:37 | INFO | train_inner | epoch 021:     10 / 63 loss=6.107, nll_loss=2.605, ppl=6.09, wps=1379.9, ups=0.39, wpb=3581, bsz=156, num_updates=1270, lr=1.524e-05, gnorm=1.897, train_wall=5, gb_free=15.5, wall=4725
2024-08-11 18:13:42 | INFO | train_inner | epoch 021:     12 / 63 loss=6.385, nll_loss=2.968, ppl=7.82, wps=1276.5, ups=0.4, wpb=3210.5, bsz=76, num_updates=1272, lr=1.5264e-05, gnorm=2.147, train_wall=5, gb_free=10.2, wall=4730
2024-08-11 18:13:47 | INFO | train_inner | epoch 021:     14 / 63 loss=6.154, nll_loss=2.696, ppl=6.48, wps=1400.3, ups=0.4, wpb=3467.5, bsz=168, num_updates=1274, lr=1.5288e-05, gnorm=2.021, train_wall=5, gb_free=14.2, wall=4735
2024-08-11 18:13:53 | INFO | train_inner | epoch 021:     16 / 63 loss=6.156, nll_loss=2.694, ppl=6.47, wps=1569, ups=0.35, wpb=4442.5, bsz=224, num_updates=1276, lr=1.5312e-05, gnorm=1.653, train_wall=6, gb_free=11.3, wall=4740
2024-08-11 18:13:58 | INFO | train_inner | epoch 021:     18 / 63 loss=6.202, nll_loss=2.751, ppl=6.73, wps=1475.1, ups=0.39, wpb=3771.5, bsz=172, num_updates=1278, lr=1.5336e-05, gnorm=1.85, train_wall=5, gb_free=13.2, wall=4745
2024-08-11 18:14:03 | INFO | train_inner | epoch 021:     20 / 63 loss=6.15, nll_loss=2.67, ppl=6.36, wps=1040.4, ups=0.37, wpb=2775, bsz=112, num_updates=1280, lr=1.536e-05, gnorm=2.209, train_wall=5, gb_free=11.7, wall=4751
2024-08-11 18:14:14 | INFO | train_inner | epoch 021:     22 / 63 loss=6.221, nll_loss=2.777, ppl=6.85, wps=728.4, ups=0.19, wpb=3741.5, bsz=204, num_updates=1282, lr=1.5384e-05, gnorm=1.885, train_wall=10, gb_free=11.2, wall=4761
2024-08-11 18:14:19 | INFO | train_inner | epoch 021:     24 / 63 loss=6, nll_loss=2.494, ppl=5.63, wps=1333.7, ups=0.41, wpb=3274.5, bsz=176, num_updates=1284, lr=1.5408e-05, gnorm=1.799, train_wall=5, gb_free=13.4, wall=4766
2024-08-11 18:14:23 | INFO | train_inner | epoch 021:     26 / 63 loss=6.224, nll_loss=2.766, ppl=6.8, wps=1424.6, ups=0.48, wpb=2980, bsz=92.5, num_updates=1286, lr=1.5432e-05, gnorm=2.301, train_wall=4, gb_free=13.3, wall=4770
2024-08-11 18:14:28 | INFO | train_inner | epoch 021:     28 / 63 loss=6.351, nll_loss=2.95, ppl=7.73, wps=1527.9, ups=0.41, wpb=3688, bsz=144, num_updates=1288, lr=1.5456e-05, gnorm=1.933, train_wall=5, gb_free=12, wall=4775
2024-08-11 18:14:33 | INFO | train_inner | epoch 021:     30 / 63 loss=6.182, nll_loss=2.731, ppl=6.64, wps=1118.3, ups=0.39, wpb=2842, bsz=160, num_updates=1290, lr=1.548e-05, gnorm=2.214, train_wall=5, gb_free=13.7, wall=4780
2024-08-11 18:14:38 | INFO | train_inner | epoch 021:     32 / 63 loss=6.09, nll_loss=2.597, ppl=6.05, wps=1183.8, ups=0.38, wpb=3105, bsz=136, num_updates=1292, lr=1.5504e-05, gnorm=2.052, train_wall=5, gb_free=12.6, wall=4785
2024-08-11 18:14:43 | INFO | train_inner | epoch 021:     34 / 63 loss=6.173, nll_loss=2.703, ppl=6.51, wps=1473.7, ups=0.42, wpb=3482.5, bsz=192, num_updates=1294, lr=1.5528e-05, gnorm=1.942, train_wall=5, gb_free=17.3, wall=4790
2024-08-11 18:14:48 | INFO | train_inner | epoch 021:     36 / 63 loss=6.112, nll_loss=2.622, ppl=6.16, wps=1494.3, ups=0.38, wpb=3898, bsz=176, num_updates=1296, lr=1.5552e-05, gnorm=1.799, train_wall=5, gb_free=12.2, wall=4795
2024-08-11 18:14:53 | INFO | train_inner | epoch 021:     38 / 63 loss=6.391, nll_loss=2.981, ppl=7.9, wps=1196.4, ups=0.4, wpb=2975.5, bsz=116, num_updates=1298, lr=1.5576e-05, gnorm=2.131, train_wall=5, gb_free=13.1, wall=4800
2024-08-11 18:14:58 | INFO | train_inner | epoch 021:     40 / 63 loss=6.178, nll_loss=2.72, ppl=6.59, wps=1348, ups=0.37, wpb=3609, bsz=148, num_updates=1300, lr=1.56e-05, gnorm=1.905, train_wall=5, gb_free=11.1, wall=4806
2024-08-11 18:15:04 | INFO | train_inner | epoch 021:     42 / 63 loss=6.299, nll_loss=2.879, ppl=7.36, wps=1552, ups=0.37, wpb=4193.5, bsz=200, num_updates=1302, lr=1.5624e-05, gnorm=1.685, train_wall=5, gb_free=12.5, wall=4811
2024-08-11 18:15:09 | INFO | train_inner | epoch 021:     44 / 63 loss=6.21, nll_loss=2.74, ppl=6.68, wps=1226.2, ups=0.4, wpb=3040, bsz=84, num_updates=1304, lr=1.5648e-05, gnorm=2.024, train_wall=5, gb_free=11.7, wall=4816
2024-08-11 18:15:14 | INFO | train_inner | epoch 021:     46 / 63 loss=6.215, nll_loss=2.774, ppl=6.84, wps=1375.5, ups=0.37, wpb=3697.5, bsz=200, num_updates=1306, lr=1.5672e-05, gnorm=1.849, train_wall=5, gb_free=11.4, wall=4821
2024-08-11 18:15:19 | INFO | train_inner | epoch 021:     48 / 63 loss=6.382, nll_loss=2.955, ppl=7.75, wps=1349.4, ups=0.41, wpb=3282.5, bsz=88, num_updates=1308, lr=1.5696e-05, gnorm=2.149, train_wall=5, gb_free=11.5, wall=4826
2024-08-11 18:15:24 | INFO | train_inner | epoch 021:     50 / 63 loss=6.166, nll_loss=2.684, ppl=6.43, wps=1456.7, ups=0.37, wpb=3915, bsz=176, num_updates=1310, lr=1.572e-05, gnorm=1.778, train_wall=5, gb_free=13.3, wall=4832
2024-08-11 18:15:30 | INFO | train_inner | epoch 021:     52 / 63 loss=6.141, nll_loss=2.682, ppl=6.42, wps=1445.2, ups=0.36, wpb=4035, bsz=248, num_updates=1312, lr=1.5744e-05, gnorm=1.751, train_wall=6, gb_free=11.2, wall=4837
2024-08-11 18:15:35 | INFO | train_inner | epoch 021:     54 / 63 loss=6.346, nll_loss=2.935, ppl=7.65, wps=1164, ups=0.36, wpb=3205, bsz=104, num_updates=1314, lr=1.5768e-05, gnorm=2.129, train_wall=5, gb_free=9.2, wall=4843
2024-08-11 18:15:41 | INFO | train_inner | epoch 021:     56 / 63 loss=6.101, nll_loss=2.634, ppl=6.21, wps=1445.6, ups=0.35, wpb=4124.5, bsz=220, num_updates=1316, lr=1.5792e-05, gnorm=1.66, train_wall=6, gb_free=11.1, wall=4848
2024-08-11 18:15:45 | INFO | train_inner | epoch 021:     58 / 63 loss=6.3, nll_loss=2.877, ppl=7.35, wps=1155.3, ups=0.48, wpb=2414.5, bsz=80, num_updates=1318, lr=1.5816e-05, gnorm=2.365, train_wall=4, gb_free=16, wall=4853
2024-08-11 18:15:50 | INFO | train_inner | epoch 021:     60 / 63 loss=6.181, nll_loss=2.724, ppl=6.61, wps=1130.1, ups=0.39, wpb=2910.5, bsz=140, num_updates=1320, lr=1.584e-05, gnorm=2.114, train_wall=5, gb_free=11.9, wall=4858
2024-08-11 18:15:55 | INFO | train_inner | epoch 021:     62 / 63 loss=6.286, nll_loss=2.847, ppl=7.19, wps=1424.6, ups=0.42, wpb=3422, bsz=120, num_updates=1322, lr=1.5864e-05, gnorm=2.022, train_wall=5, gb_free=13.2, wall=4862
2024-08-11 18:15:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17596.1796875Mb; avail=237470.06640625Mb
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000527
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17596.1796875Mb; avail=237470.06640625Mb
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004750
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17596.1796875Mb; avail=237470.06640625Mb
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004021
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009627
2024-08-11 18:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17596.1796875Mb; avail=237470.06640625Mb
2024-08-11 18:16:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.333 | nll_loss 2.687 | ppl 6.44 | wps 3050.2 | wpb 1463.4 | bsz 64.9 | num_updates 1323 | best_loss 6.333
2024-08-11 18:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1323 updates
2024-08-11 18:16:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:17:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 21 @ 1323 updates, score 6.333) (writing took 62.30453161569312 seconds)
2024-08-11 18:17:07 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-08-11 18:17:07 | INFO | train | epoch 021 | loss 6.209 | nll_loss 2.754 | ppl 6.75 | wps 911.5 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1323 | lr 1.5876e-05 | gnorm 2.002 | train_wall 164 | gb_free 17.1 | wall 4935
2024-08-11 18:17:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:17:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:17:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:17:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000788
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25511.56640625Mb; avail=229554.6328125Mb
2024-08-11 18:17:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000193
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001298
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25511.56640625Mb; avail=229554.6328125Mb
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000047
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25511.56640625Mb; avail=229554.6328125Mb
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000452
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002115
2024-08-11 18:17:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25511.56640625Mb; avail=229554.6328125Mb
2024-08-11 18:17:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:17:07 | INFO | fairseq.trainer | begin training epoch 22
2024-08-11 18:17:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:17:10 | INFO | train_inner | epoch 022:      1 / 63 loss=6.277, nll_loss=2.81, ppl=7.01, wps=63.2, ups=0.03, wpb=2364, bsz=44, num_updates=1324, lr=1.5888e-05, gnorm=2.624, train_wall=4, gb_free=13, wall=4937
2024-08-11 18:17:14 | INFO | train_inner | epoch 022:      3 / 63 loss=6.042, nll_loss=2.543, ppl=5.83, wps=1283.9, ups=0.45, wpb=2864, bsz=136, num_updates=1326, lr=1.5912e-05, gnorm=2.026, train_wall=4, gb_free=14.4, wall=4942
2024-08-11 18:17:20 | INFO | train_inner | epoch 022:      5 / 63 loss=6.019, nll_loss=2.524, ppl=5.75, wps=1549.9, ups=0.39, wpb=4011.5, bsz=216, num_updates=1328, lr=1.5936e-05, gnorm=1.747, train_wall=5, gb_free=11.9, wall=4947
2024-08-11 18:17:25 | INFO | train_inner | epoch 022:      7 / 63 loss=6.146, nll_loss=2.668, ppl=6.36, wps=1413.8, ups=0.38, wpb=3714.5, bsz=136, num_updates=1330, lr=1.596e-05, gnorm=1.933, train_wall=5, gb_free=11.6, wall=4952
2024-08-11 18:17:30 | INFO | train_inner | epoch 022:      9 / 63 loss=6.147, nll_loss=2.677, ppl=6.4, wps=1205.2, ups=0.39, wpb=3120.5, bsz=136, num_updates=1332, lr=1.5984e-05, gnorm=2.124, train_wall=5, gb_free=10.4, wall=4957
2024-08-11 18:17:35 | INFO | train_inner | epoch 022:     11 / 63 loss=5.898, nll_loss=2.364, ppl=5.15, wps=1263.4, ups=0.38, wpb=3340.5, bsz=196, num_updates=1334, lr=1.6008e-05, gnorm=1.915, train_wall=5, gb_free=10.6, wall=4963
2024-08-11 18:17:41 | INFO | train_inner | epoch 022:     13 / 63 loss=6.132, nll_loss=2.642, ppl=6.24, wps=1473.6, ups=0.38, wpb=3906, bsz=136, num_updates=1336, lr=1.6032e-05, gnorm=1.886, train_wall=5, gb_free=13.5, wall=4968
2024-08-11 18:17:46 | INFO | train_inner | epoch 022:     15 / 63 loss=6.035, nll_loss=2.525, ppl=5.76, wps=1413.6, ups=0.39, wpb=3667.5, bsz=176, num_updates=1338, lr=1.6056e-05, gnorm=1.806, train_wall=5, gb_free=10.7, wall=4973
2024-08-11 18:17:51 | INFO | train_inner | epoch 022:     17 / 63 loss=6.273, nll_loss=2.823, ppl=7.08, wps=1558.8, ups=0.41, wpb=3772.5, bsz=144, num_updates=1340, lr=1.608e-05, gnorm=2.015, train_wall=5, gb_free=12.8, wall=4978
2024-08-11 18:17:56 | INFO | train_inner | epoch 022:     19 / 63 loss=6.03, nll_loss=2.526, ppl=5.76, wps=1245.8, ups=0.38, wpb=3252, bsz=152, num_updates=1342, lr=1.6104e-05, gnorm=1.806, train_wall=5, gb_free=12.7, wall=4983
2024-08-11 18:18:06 | INFO | train_inner | epoch 022:     21 / 63 loss=6.117, nll_loss=2.654, ppl=6.29, wps=624.5, ups=0.19, wpb=3267.5, bsz=124, num_updates=1344, lr=1.6128e-05, gnorm=1.993, train_wall=10, gb_free=11.9, wall=4994
2024-08-11 18:18:12 | INFO | train_inner | epoch 022:     23 / 63 loss=6.137, nll_loss=2.684, ppl=6.43, wps=1365, ups=0.37, wpb=3690, bsz=176, num_updates=1346, lr=1.6152e-05, gnorm=1.833, train_wall=5, gb_free=13.4, wall=4999
2024-08-11 18:18:17 | INFO | train_inner | epoch 022:     25 / 63 loss=6.199, nll_loss=2.744, ppl=6.7, wps=1444.3, ups=0.38, wpb=3793, bsz=136, num_updates=1348, lr=1.6176e-05, gnorm=2.049, train_wall=5, gb_free=11.4, wall=5004
2024-08-11 18:18:22 | INFO | train_inner | epoch 022:     27 / 63 loss=6.171, nll_loss=2.68, ppl=6.41, wps=1282.9, ups=0.39, wpb=3321.5, bsz=104, num_updates=1350, lr=1.62e-05, gnorm=1.913, train_wall=5, gb_free=14.9, wall=5009
2024-08-11 18:18:27 | INFO | train_inner | epoch 022:     29 / 63 loss=6.204, nll_loss=2.748, ppl=6.72, wps=1410.4, ups=0.41, wpb=3404, bsz=132, num_updates=1352, lr=1.6224e-05, gnorm=1.923, train_wall=5, gb_free=15.5, wall=5014
2024-08-11 18:18:32 | INFO | train_inner | epoch 022:     31 / 63 loss=6.15, nll_loss=2.691, ppl=6.46, wps=1380.2, ups=0.37, wpb=3681.5, bsz=204, num_updates=1354, lr=1.6248e-05, gnorm=1.777, train_wall=5, gb_free=11.8, wall=5020
2024-08-11 18:18:37 | INFO | train_inner | epoch 022:     33 / 63 loss=6.283, nll_loss=2.851, ppl=7.22, wps=1414.2, ups=0.42, wpb=3363, bsz=120, num_updates=1356, lr=1.6272e-05, gnorm=2.02, train_wall=5, gb_free=13.6, wall=5024
2024-08-11 18:18:42 | INFO | train_inner | epoch 022:     35 / 63 loss=6.14, nll_loss=2.673, ppl=6.38, wps=1461.4, ups=0.38, wpb=3825, bsz=172, num_updates=1358, lr=1.6296e-05, gnorm=1.82, train_wall=5, gb_free=10, wall=5030
2024-08-11 18:18:47 | INFO | train_inner | epoch 022:     37 / 63 loss=6.258, nll_loss=2.817, ppl=7.05, wps=1350.8, ups=0.41, wpb=3262.5, bsz=124, num_updates=1360, lr=1.632e-05, gnorm=1.996, train_wall=5, gb_free=12.8, wall=5034
2024-08-11 18:18:52 | INFO | train_inner | epoch 022:     39 / 63 loss=6.247, nll_loss=2.779, ppl=6.86, wps=1222.1, ups=0.38, wpb=3195.5, bsz=76, num_updates=1362, lr=1.6344e-05, gnorm=2.279, train_wall=5, gb_free=11.1, wall=5040
2024-08-11 18:18:58 | INFO | train_inner | epoch 022:     41 / 63 loss=6.074, nll_loss=2.583, ppl=5.99, wps=1408.8, ups=0.38, wpb=3736, bsz=232, num_updates=1364, lr=1.6368e-05, gnorm=1.758, train_wall=5, gb_free=11.5, wall=5045
2024-08-11 18:19:03 | INFO | train_inner | epoch 022:     43 / 63 loss=6.113, nll_loss=2.629, ppl=6.19, wps=1596.8, ups=0.38, wpb=4201, bsz=208, num_updates=1366, lr=1.6392e-05, gnorm=1.708, train_wall=5, gb_free=13.6, wall=5050
2024-08-11 18:19:08 | INFO | train_inner | epoch 022:     45 / 63 loss=6.152, nll_loss=2.696, ppl=6.48, wps=1132.4, ups=0.4, wpb=2803, bsz=148, num_updates=1368, lr=1.6416e-05, gnorm=2.038, train_wall=5, gb_free=15, wall=5055
2024-08-11 18:19:13 | INFO | train_inner | epoch 022:     47 / 63 loss=6.156, nll_loss=2.674, ppl=6.38, wps=1154.9, ups=0.39, wpb=2974, bsz=68, num_updates=1370, lr=1.644e-05, gnorm=2.1, train_wall=5, gb_free=11.3, wall=5060
2024-08-11 18:19:18 | INFO | train_inner | epoch 022:     49 / 63 loss=6.135, nll_loss=2.68, ppl=6.41, wps=1379.3, ups=0.39, wpb=3523.5, bsz=180, num_updates=1372, lr=1.6464e-05, gnorm=1.968, train_wall=5, gb_free=11.4, wall=5065
2024-08-11 18:19:23 | INFO | train_inner | epoch 022:     51 / 63 loss=6.29, nll_loss=2.872, ppl=7.32, wps=1331.7, ups=0.43, wpb=3076.5, bsz=120, num_updates=1374, lr=1.6488e-05, gnorm=2.152, train_wall=5, gb_free=16.5, wall=5070
2024-08-11 18:19:28 | INFO | train_inner | epoch 022:     53 / 63 loss=6.243, nll_loss=2.789, ppl=6.91, wps=1207.8, ups=0.39, wpb=3071.5, bsz=104, num_updates=1376, lr=1.6512e-05, gnorm=2.002, train_wall=5, gb_free=11, wall=5075
2024-08-11 18:19:33 | INFO | train_inner | epoch 022:     55 / 63 loss=6.167, nll_loss=2.683, ppl=6.42, wps=1055.2, ups=0.38, wpb=2770.5, bsz=76, num_updates=1378, lr=1.6536e-05, gnorm=2.112, train_wall=5, gb_free=11.4, wall=5080
2024-08-11 18:19:38 | INFO | train_inner | epoch 022:     57 / 63 loss=6.005, nll_loss=2.501, ppl=5.66, wps=1277.5, ups=0.37, wpb=3431, bsz=176, num_updates=1380, lr=1.656e-05, gnorm=1.892, train_wall=5, gb_free=12.5, wall=5086
2024-08-11 18:19:43 | INFO | train_inner | epoch 022:     59 / 63 loss=6.087, nll_loss=2.623, ppl=6.16, wps=1603.6, ups=0.4, wpb=4026, bsz=252, num_updates=1382, lr=1.6584e-05, gnorm=1.762, train_wall=5, gb_free=16.1, wall=5091
2024-08-11 18:19:48 | INFO | train_inner | epoch 022:     61 / 63 loss=6.17, nll_loss=2.704, ppl=6.52, wps=1316.6, ups=0.47, wpb=2804.5, bsz=116.5, num_updates=1384, lr=1.6608e-05, gnorm=2.316, train_wall=4, gb_free=14.4, wall=5095
2024-08-11 18:19:52 | INFO | train_inner | epoch 022:     63 / 63 loss=6.078, nll_loss=2.598, ppl=6.05, wps=1373, ups=0.48, wpb=2833, bsz=164, num_updates=1386, lr=1.6632e-05, gnorm=2.227, train_wall=4, gb_free=16, wall=5099
2024-08-11 18:19:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19813.17578125Mb; avail=235253.06640625Mb
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000542
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19813.17578125Mb; avail=235253.06640625Mb
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004707
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19813.17578125Mb; avail=235253.06640625Mb
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004025
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009597
2024-08-11 18:19:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19813.17578125Mb; avail=235253.06640625Mb
2024-08-11 18:20:00 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.304 | nll_loss 2.658 | ppl 6.31 | wps 3054.8 | wpb 1463.4 | bsz 64.9 | num_updates 1386 | best_loss 6.304
2024-08-11 18:20:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1386 updates
2024-08-11 18:20:00 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:20:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:21:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 22 @ 1386 updates, score 6.304) (writing took 63.881490025669336 seconds)
2024-08-11 18:21:04 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-08-11 18:21:04 | INFO | train | epoch 022 | loss 6.139 | nll_loss 2.666 | ppl 6.35 | wps 904.3 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1386 | lr 1.6632e-05 | gnorm 1.969 | train_wall 164 | gb_free 16 | wall 5172
2024-08-11 18:21:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:21:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:21:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:21:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000786
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45065.140625Mb; avail=210001.5234375Mb
2024-08-11 18:21:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000184
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001298
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45064.6484375Mb; avail=210001.5234375Mb
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45065.140625Mb; avail=210001.03125Mb
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000443
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002132
2024-08-11 18:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45064.6484375Mb; avail=210001.5234375Mb
2024-08-11 18:21:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:21:04 | INFO | fairseq.trainer | begin training epoch 23
2024-08-11 18:21:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:21:10 | INFO | train_inner | epoch 023:      2 / 63 loss=6.127, nll_loss=2.639, ppl=6.23, wps=107.8, ups=0.03, wpb=4199, bsz=144, num_updates=1388, lr=1.6656e-05, gnorm=1.755, train_wall=5, gb_free=13, wall=5177
2024-08-11 18:21:15 | INFO | train_inner | epoch 023:      4 / 63 loss=6.107, nll_loss=2.632, ppl=6.2, wps=1229.3, ups=0.41, wpb=3017.5, bsz=144, num_updates=1390, lr=1.668e-05, gnorm=2.001, train_wall=5, gb_free=10.7, wall=5182
2024-08-11 18:21:19 | INFO | train_inner | epoch 023:      6 / 63 loss=6.176, nll_loss=2.712, ppl=6.55, wps=1520.5, ups=0.44, wpb=3476.5, bsz=124, num_updates=1392, lr=1.6704e-05, gnorm=1.986, train_wall=5, gb_free=12.5, wall=5187
2024-08-11 18:21:24 | INFO | train_inner | epoch 023:      8 / 63 loss=6.079, nll_loss=2.594, ppl=6.04, wps=1655.1, ups=0.39, wpb=4295.5, bsz=228, num_updates=1394, lr=1.6728e-05, gnorm=1.792, train_wall=5, gb_free=13.9, wall=5192
2024-08-11 18:21:30 | INFO | train_inner | epoch 023:     10 / 63 loss=5.963, nll_loss=2.449, ppl=5.46, wps=1289.1, ups=0.38, wpb=3378.5, bsz=176, num_updates=1396, lr=1.6752e-05, gnorm=1.948, train_wall=5, gb_free=12, wall=5197
2024-08-11 18:21:35 | INFO | train_inner | epoch 023:     12 / 63 loss=6.033, nll_loss=2.507, ppl=5.68, wps=1299.3, ups=0.38, wpb=3453.5, bsz=116, num_updates=1398, lr=1.6776e-05, gnorm=1.87, train_wall=5, gb_free=10.8, wall=5202
2024-08-11 18:21:40 | INFO | train_inner | epoch 023:     14 / 63 loss=6.1, nll_loss=2.619, ppl=6.14, wps=1439.5, ups=0.39, wpb=3693.5, bsz=156, num_updates=1400, lr=1.68e-05, gnorm=1.928, train_wall=5, gb_free=13.4, wall=5207
2024-08-11 18:21:45 | INFO | train_inner | epoch 023:     16 / 63 loss=5.883, nll_loss=2.355, ppl=5.12, wps=1048.9, ups=0.39, wpb=2722, bsz=148, num_updates=1402, lr=1.6824e-05, gnorm=2.016, train_wall=5, gb_free=12.2, wall=5213
2024-08-11 18:21:56 | INFO | train_inner | epoch 023:     18 / 63 loss=6.024, nll_loss=2.516, ppl=5.72, wps=696.4, ups=0.19, wpb=3572, bsz=172, num_updates=1404, lr=1.6848e-05, gnorm=1.724, train_wall=10, gb_free=13.3, wall=5223
2024-08-11 18:22:01 | INFO | train_inner | epoch 023:     20 / 63 loss=6.026, nll_loss=2.526, ppl=5.76, wps=1239.7, ups=0.39, wpb=3187, bsz=148, num_updates=1406, lr=1.6872e-05, gnorm=1.902, train_wall=5, gb_free=11.2, wall=5228
2024-08-11 18:22:06 | INFO | train_inner | epoch 023:     22 / 63 loss=6.142, nll_loss=2.671, ppl=6.37, wps=1247.3, ups=0.41, wpb=3027, bsz=128, num_updates=1408, lr=1.6896e-05, gnorm=2.014, train_wall=5, gb_free=13.5, wall=5233
2024-08-11 18:22:11 | INFO | train_inner | epoch 023:     24 / 63 loss=6.003, nll_loss=2.491, ppl=5.62, wps=1230.6, ups=0.38, wpb=3201, bsz=104, num_updates=1410, lr=1.692e-05, gnorm=1.953, train_wall=5, gb_free=12.4, wall=5238
2024-08-11 18:22:17 | INFO | train_inner | epoch 023:     26 / 63 loss=6.047, nll_loss=2.555, ppl=5.88, wps=1427.7, ups=0.34, wpb=4260.5, bsz=224, num_updates=1412, lr=1.6944e-05, gnorm=2.057, train_wall=6, gb_free=9.8, wall=5244
2024-08-11 18:22:22 | INFO | train_inner | epoch 023:     28 / 63 loss=6.122, nll_loss=2.655, ppl=6.3, wps=1400.3, ups=0.37, wpb=3811.5, bsz=184, num_updates=1414, lr=1.6968e-05, gnorm=1.897, train_wall=5, gb_free=12.3, wall=5250
2024-08-11 18:22:27 | INFO | train_inner | epoch 023:     30 / 63 loss=6.113, nll_loss=2.613, ppl=6.12, wps=1377.1, ups=0.43, wpb=3200.5, bsz=108, num_updates=1416, lr=1.6992e-05, gnorm=1.979, train_wall=5, gb_free=15.9, wall=5254
2024-08-11 18:22:32 | INFO | train_inner | epoch 023:     32 / 63 loss=6.173, nll_loss=2.684, ppl=6.43, wps=1368.6, ups=0.39, wpb=3490.5, bsz=120, num_updates=1418, lr=1.7016e-05, gnorm=1.931, train_wall=5, gb_free=10.5, wall=5259
2024-08-11 18:22:37 | INFO | train_inner | epoch 023:     34 / 63 loss=6.001, nll_loss=2.466, ppl=5.53, wps=1088.7, ups=0.39, wpb=2762, bsz=100, num_updates=1420, lr=1.704e-05, gnorm=2.117, train_wall=5, gb_free=11.5, wall=5264
2024-08-11 18:22:42 | INFO | train_inner | epoch 023:     36 / 63 loss=6.218, nll_loss=2.764, ppl=6.79, wps=1432.3, ups=0.38, wpb=3775, bsz=136, num_updates=1422, lr=1.7064e-05, gnorm=1.987, train_wall=5, gb_free=12.4, wall=5270
2024-08-11 18:22:47 | INFO | train_inner | epoch 023:     38 / 63 loss=6.128, nll_loss=2.65, ppl=6.27, wps=1313, ups=0.43, wpb=3056.5, bsz=112, num_updates=1424, lr=1.7088e-05, gnorm=2.751, train_wall=5, gb_free=12.7, wall=5274
2024-08-11 18:22:52 | INFO | train_inner | epoch 023:     40 / 63 loss=6.065, nll_loss=2.588, ppl=6.01, wps=1122.2, ups=0.41, wpb=2725.5, bsz=112, num_updates=1426, lr=1.7112e-05, gnorm=2.091, train_wall=5, gb_free=13.1, wall=5279
2024-08-11 18:22:57 | INFO | train_inner | epoch 023:     42 / 63 loss=5.958, nll_loss=2.452, ppl=5.47, wps=1523.2, ups=0.39, wpb=3884, bsz=232, num_updates=1428, lr=1.7136e-05, gnorm=1.728, train_wall=5, gb_free=16.7, wall=5284
2024-08-11 18:23:02 | INFO | train_inner | epoch 023:     44 / 63 loss=6.05, nll_loss=2.532, ppl=5.79, wps=1319.4, ups=0.4, wpb=3324, bsz=96, num_updates=1430, lr=1.716e-05, gnorm=2.022, train_wall=5, gb_free=12.6, wall=5289
2024-08-11 18:23:07 | INFO | train_inner | epoch 023:     46 / 63 loss=6.106, nll_loss=2.601, ppl=6.07, wps=1271.5, ups=0.39, wpb=3265.5, bsz=116, num_updates=1432, lr=1.7184e-05, gnorm=1.932, train_wall=5, gb_free=11.5, wall=5294
2024-08-11 18:23:12 | INFO | train_inner | epoch 023:     48 / 63 loss=6.033, nll_loss=2.527, ppl=5.76, wps=1472.3, ups=0.38, wpb=3895.5, bsz=184, num_updates=1434, lr=1.7208e-05, gnorm=1.877, train_wall=5, gb_free=11, wall=5300
2024-08-11 18:23:17 | INFO | train_inner | epoch 023:     50 / 63 loss=6.172, nll_loss=2.715, ppl=6.56, wps=1472.9, ups=0.44, wpb=3375.5, bsz=132, num_updates=1436, lr=1.7232e-05, gnorm=2.107, train_wall=5, gb_free=15.7, wall=5304
2024-08-11 18:23:22 | INFO | train_inner | epoch 023:     52 / 63 loss=5.895, nll_loss=2.379, ppl=5.2, wps=1215.7, ups=0.37, wpb=3257, bsz=220, num_updates=1438, lr=1.7256e-05, gnorm=1.831, train_wall=5, gb_free=12.1, wall=5310
2024-08-11 18:23:28 | INFO | train_inner | epoch 023:     54 / 63 loss=5.908, nll_loss=2.368, ppl=5.16, wps=1060.2, ups=0.37, wpb=2873, bsz=128, num_updates=1440, lr=1.728e-05, gnorm=2, train_wall=5, gb_free=11.6, wall=5315
2024-08-11 18:23:32 | INFO | train_inner | epoch 023:     56 / 63 loss=6.149, nll_loss=2.669, ppl=6.36, wps=1458.7, ups=0.47, wpb=3122, bsz=124.5, num_updates=1442, lr=1.7304e-05, gnorm=2.014, train_wall=4, gb_free=14, wall=5319
2024-08-11 18:23:37 | INFO | train_inner | epoch 023:     58 / 63 loss=6.064, nll_loss=2.579, ppl=5.98, wps=1401.6, ups=0.38, wpb=3683.5, bsz=192, num_updates=1444, lr=1.7328e-05, gnorm=1.791, train_wall=5, gb_free=15, wall=5325
2024-08-11 18:23:42 | INFO | train_inner | epoch 023:     60 / 63 loss=6.066, nll_loss=2.588, ppl=6.01, wps=1310.7, ups=0.39, wpb=3331.5, bsz=160, num_updates=1446, lr=1.7352e-05, gnorm=1.989, train_wall=5, gb_free=12, wall=5330
2024-08-11 18:23:48 | INFO | train_inner | epoch 023:     62 / 63 loss=6.071, nll_loss=2.586, ppl=6, wps=1358.6, ups=0.37, wpb=3644.5, bsz=148, num_updates=1448, lr=1.7376e-05, gnorm=1.844, train_wall=5, gb_free=10.1, wall=5335
2024-08-11 18:23:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20032.03515625Mb; avail=235034.20703125Mb
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000524
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20032.03515625Mb; avail=235034.20703125Mb
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004747
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20032.03515625Mb; avail=235034.20703125Mb
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004056
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009658
2024-08-11 18:23:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20032.03515625Mb; avail=235034.20703125Mb
2024-08-11 18:23:58 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.268 | nll_loss 2.615 | ppl 6.13 | wps 3057.9 | wpb 1463.4 | bsz 64.9 | num_updates 1449 | best_loss 6.268
2024-08-11 18:23:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1449 updates
2024-08-11 18:23:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:24:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:25:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 23 @ 1449 updates, score 6.268) (writing took 64.8946021432057 seconds)
2024-08-11 18:25:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-08-11 18:25:03 | INFO | train | epoch 023 | loss 6.065 | nll_loss 2.571 | ppl 5.94 | wps 898.7 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 1449 | lr 1.7388e-05 | gnorm 1.964 | train_wall 165 | gb_free 15.9 | wall 5410
2024-08-11 18:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001157
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39558.27734375Mb; avail=215507.9609375Mb
2024-08-11 18:25:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000254
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002094
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39558.27734375Mb; avail=215507.9609375Mb
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000068
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39558.27734375Mb; avail=215507.9609375Mb
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000692
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.003338
2024-08-11 18:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39558.27734375Mb; avail=215507.9609375Mb
2024-08-11 18:25:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:25:03 | INFO | fairseq.trainer | begin training epoch 24
2024-08-11 18:25:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:25:05 | INFO | train_inner | epoch 024:      1 / 63 loss=5.767, nll_loss=2.201, ppl=4.6, wps=77.2, ups=0.03, wpb=2995.5, bsz=212, num_updates=1450, lr=1.74e-05, gnorm=1.783, train_wall=4, gb_free=13.1, wall=5413
2024-08-11 18:25:09 | INFO | train_inner | epoch 024:      3 / 63 loss=6.066, nll_loss=2.587, ppl=6.01, wps=1430.9, ups=0.49, wpb=2907, bsz=148.5, num_updates=1452, lr=1.7424e-05, gnorm=2.506, train_wall=4, gb_free=15, wall=5417
2024-08-11 18:25:14 | INFO | train_inner | epoch 024:      5 / 63 loss=6.106, nll_loss=2.617, ppl=6.13, wps=1441.9, ups=0.43, wpb=3385, bsz=120, num_updates=1454, lr=1.7448e-05, gnorm=1.953, train_wall=5, gb_free=14, wall=5421
2024-08-11 18:25:19 | INFO | train_inner | epoch 024:      7 / 63 loss=6.206, nll_loss=2.751, ppl=6.73, wps=1381.8, ups=0.41, wpb=3373, bsz=144, num_updates=1456, lr=1.7472e-05, gnorm=1.898, train_wall=5, gb_free=11.5, wall=5426
2024-08-11 18:25:24 | INFO | train_inner | epoch 024:      9 / 63 loss=5.95, nll_loss=2.418, ppl=5.34, wps=1472.8, ups=0.38, wpb=3852.5, bsz=184, num_updates=1458, lr=1.7496e-05, gnorm=1.697, train_wall=5, gb_free=12.8, wall=5432
2024-08-11 18:25:29 | INFO | train_inner | epoch 024:     11 / 63 loss=5.954, nll_loss=2.434, ppl=5.4, wps=1426.6, ups=0.41, wpb=3499.5, bsz=148, num_updates=1460, lr=1.752e-05, gnorm=1.8, train_wall=5, gb_free=11.7, wall=5436
2024-08-11 18:25:34 | INFO | train_inner | epoch 024:     13 / 63 loss=5.971, nll_loss=2.472, ppl=5.55, wps=1446.1, ups=0.4, wpb=3575, bsz=188, num_updates=1462, lr=1.7544e-05, gnorm=1.814, train_wall=5, gb_free=12.9, wall=5441
2024-08-11 18:25:39 | INFO | train_inner | epoch 024:     15 / 63 loss=5.982, nll_loss=2.465, ppl=5.52, wps=1020.1, ups=0.38, wpb=2678.5, bsz=120, num_updates=1464, lr=1.7568e-05, gnorm=2.171, train_wall=5, gb_free=12.8, wall=5447
2024-08-11 18:25:44 | INFO | train_inner | epoch 024:     17 / 63 loss=5.965, nll_loss=2.43, ppl=5.39, wps=1359.7, ups=0.41, wpb=3283.5, bsz=120, num_updates=1466, lr=1.7592e-05, gnorm=1.907, train_wall=5, gb_free=12.5, wall=5451
2024-08-11 18:25:55 | INFO | train_inner | epoch 024:     19 / 63 loss=5.992, nll_loss=2.486, ppl=5.6, wps=720.4, ups=0.19, wpb=3847, bsz=224, num_updates=1468, lr=1.7616e-05, gnorm=1.806, train_wall=11, gb_free=14.9, wall=5462
2024-08-11 18:26:00 | INFO | train_inner | epoch 024:     21 / 63 loss=6.013, nll_loss=2.508, ppl=5.69, wps=1277.1, ups=0.41, wpb=3150, bsz=120, num_updates=1470, lr=1.764e-05, gnorm=1.874, train_wall=5, gb_free=13.3, wall=5467
2024-08-11 18:26:05 | INFO | train_inner | epoch 024:     23 / 63 loss=5.959, nll_loss=2.455, ppl=5.48, wps=1557.3, ups=0.36, wpb=4374, bsz=236, num_updates=1472, lr=1.7664e-05, gnorm=1.6, train_wall=6, gb_free=12.3, wall=5473
2024-08-11 18:26:11 | INFO | train_inner | epoch 024:     25 / 63 loss=6.011, nll_loss=2.508, ppl=5.69, wps=1508, ups=0.37, wpb=4048, bsz=172, num_updates=1474, lr=1.7688e-05, gnorm=1.747, train_wall=5, gb_free=11.1, wall=5478
2024-08-11 18:26:16 | INFO | train_inner | epoch 024:     27 / 63 loss=5.889, nll_loss=2.333, ppl=5.04, wps=1177.7, ups=0.4, wpb=2928.5, bsz=108, num_updates=1476, lr=1.7712e-05, gnorm=1.982, train_wall=5, gb_free=12.7, wall=5483
2024-08-11 18:26:21 | INFO | train_inner | epoch 024:     29 / 63 loss=6.13, nll_loss=2.625, ppl=6.17, wps=1327.3, ups=0.39, wpb=3439.5, bsz=76, num_updates=1478, lr=1.7736e-05, gnorm=2.084, train_wall=5, gb_free=11.5, wall=5488
2024-08-11 18:26:26 | INFO | train_inner | epoch 024:     31 / 63 loss=6.025, nll_loss=2.502, ppl=5.67, wps=1472, ups=0.4, wpb=3713, bsz=136, num_updates=1480, lr=1.776e-05, gnorm=1.838, train_wall=5, gb_free=12.6, wall=5493
2024-08-11 18:26:31 | INFO | train_inner | epoch 024:     33 / 63 loss=6.034, nll_loss=2.53, ppl=5.77, wps=1170.2, ups=0.37, wpb=3201.5, bsz=108, num_updates=1482, lr=1.7784e-05, gnorm=2.072, train_wall=5, gb_free=10.7, wall=5499
2024-08-11 18:26:37 | INFO | train_inner | epoch 024:     35 / 63 loss=6.011, nll_loss=2.498, ppl=5.65, wps=1360.1, ups=0.37, wpb=3714, bsz=172, num_updates=1484, lr=1.7808e-05, gnorm=1.828, train_wall=5, gb_free=11.7, wall=5504
2024-08-11 18:26:42 | INFO | train_inner | epoch 024:     37 / 63 loss=5.972, nll_loss=2.452, ppl=5.47, wps=1424.7, ups=0.36, wpb=3978.5, bsz=200, num_updates=1486, lr=1.7832e-05, gnorm=1.652, train_wall=6, gb_free=11, wall=5510
2024-08-11 18:26:47 | INFO | train_inner | epoch 024:     39 / 63 loss=5.963, nll_loss=2.421, ppl=5.36, wps=1135.2, ups=0.45, wpb=2539, bsz=64, num_updates=1488, lr=1.7856e-05, gnorm=2.115, train_wall=4, gb_free=15.4, wall=5514
2024-08-11 18:26:52 | INFO | train_inner | epoch 024:     41 / 63 loss=5.938, nll_loss=2.424, ppl=5.37, wps=1345.4, ups=0.37, wpb=3624.5, bsz=212, num_updates=1490, lr=1.788e-05, gnorm=1.834, train_wall=5, gb_free=14.9, wall=5520
2024-08-11 18:26:58 | INFO | train_inner | epoch 024:     43 / 63 loss=5.967, nll_loss=2.459, ppl=5.5, wps=1460.6, ups=0.39, wpb=3763.5, bsz=160, num_updates=1492, lr=1.7904e-05, gnorm=1.806, train_wall=5, gb_free=12.3, wall=5525
2024-08-11 18:27:03 | INFO | train_inner | epoch 024:     45 / 63 loss=5.94, nll_loss=2.433, ppl=5.4, wps=1351, ups=0.38, wpb=3556.5, bsz=188, num_updates=1494, lr=1.7928e-05, gnorm=1.793, train_wall=5, gb_free=11.6, wall=5530
2024-08-11 18:27:08 | INFO | train_inner | epoch 024:     47 / 63 loss=6.092, nll_loss=2.607, ppl=6.09, wps=1241.5, ups=0.42, wpb=2976, bsz=104, num_updates=1496, lr=1.7952e-05, gnorm=2.143, train_wall=5, gb_free=12.9, wall=5535
2024-08-11 18:27:12 | INFO | train_inner | epoch 024:     49 / 63 loss=6.022, nll_loss=2.498, ppl=5.65, wps=1226.9, ups=0.42, wpb=2951.5, bsz=76, num_updates=1498, lr=1.7976e-05, gnorm=1.947, train_wall=5, gb_free=11.5, wall=5540
2024-08-11 18:27:18 | INFO | train_inner | epoch 024:     51 / 63 loss=5.983, nll_loss=2.457, ppl=5.49, wps=1522.7, ups=0.36, wpb=4232, bsz=220, num_updates=1500, lr=1.8e-05, gnorm=1.657, train_wall=6, gb_free=12, wall=5545
2024-08-11 18:27:23 | INFO | train_inner | epoch 024:     53 / 63 loss=6.102, nll_loss=2.6, ppl=6.06, wps=1407.5, ups=0.39, wpb=3615.5, bsz=120, num_updates=1502, lr=1.8024e-05, gnorm=1.911, train_wall=5, gb_free=12.9, wall=5550
2024-08-11 18:27:28 | INFO | train_inner | epoch 024:     55 / 63 loss=6.17, nll_loss=2.699, ppl=6.49, wps=1390.8, ups=0.37, wpb=3719.5, bsz=96, num_updates=1504, lr=1.8048e-05, gnorm=1.954, train_wall=5, gb_free=11.2, wall=5556
2024-08-11 18:27:34 | INFO | train_inner | epoch 024:     57 / 63 loss=5.929, nll_loss=2.397, ppl=5.27, wps=1176.3, ups=0.37, wpb=3193.5, bsz=140, num_updates=1506, lr=1.8072e-05, gnorm=1.889, train_wall=5, gb_free=12.3, wall=5561
2024-08-11 18:27:39 | INFO | train_inner | epoch 024:     59 / 63 loss=5.97, nll_loss=2.457, ppl=5.49, wps=1253.1, ups=0.4, wpb=3120, bsz=176, num_updates=1508, lr=1.8096e-05, gnorm=1.861, train_wall=5, gb_free=11.2, wall=5566
2024-08-11 18:27:43 | INFO | train_inner | epoch 024:     61 / 63 loss=6.069, nll_loss=2.564, ppl=5.91, wps=1285.2, ups=0.43, wpb=2993.5, bsz=84, num_updates=1510, lr=1.812e-05, gnorm=2.123, train_wall=5, gb_free=10.2, wall=5571
2024-08-11 18:27:48 | INFO | train_inner | epoch 024:     63 / 63 loss=5.846, nll_loss=2.304, ppl=4.94, wps=1021, ups=0.5, wpb=2054.5, bsz=144, num_updates=1512, lr=1.8144e-05, gnorm=2.389, train_wall=4, gb_free=18, wall=5575
2024-08-11 18:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15419.20703125Mb; avail=239646.99609375Mb
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000517
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15419.20703125Mb; avail=239646.99609375Mb
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004691
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15419.20703125Mb; avail=239646.99609375Mb
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004042
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009580
2024-08-11 18:27:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15419.20703125Mb; avail=239646.99609375Mb
2024-08-11 18:27:56 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.254 | nll_loss 2.604 | ppl 6.08 | wps 3059.2 | wpb 1463.4 | bsz 64.9 | num_updates 1512 | best_loss 6.254
2024-08-11 18:27:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1512 updates
2024-08-11 18:27:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:28:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 24 @ 1512 updates, score 6.254) (writing took 67.25214058998972 seconds)
2024-08-11 18:29:03 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-08-11 18:29:03 | INFO | train | epoch 024 | loss 6.003 | nll_loss 2.492 | ppl 5.63 | wps 890.5 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 1512 | lr 1.8144e-05 | gnorm 1.918 | train_wall 165 | gb_free 18 | wall 5651
2024-08-11 18:29:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:29:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:29:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:29:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000812
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25747.40625Mb; avail=229318.7890625Mb
2024-08-11 18:29:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000185
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001399
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25747.40625Mb; avail=229318.7890625Mb
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25747.40625Mb; avail=229318.7890625Mb
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000450
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002202
2024-08-11 18:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25747.40625Mb; avail=229318.7890625Mb
2024-08-11 18:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:29:03 | INFO | fairseq.trainer | begin training epoch 25
2024-08-11 18:29:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:29:08 | INFO | train_inner | epoch 025:      2 / 63 loss=5.842, nll_loss=2.287, ppl=4.88, wps=88.2, ups=0.02, wpb=3566.5, bsz=152, num_updates=1514, lr=1.8168e-05, gnorm=1.892, train_wall=5, gb_free=12.4, wall=5656
2024-08-11 18:29:13 | INFO | train_inner | epoch 025:      4 / 63 loss=5.939, nll_loss=2.4, ppl=5.28, wps=1380, ups=0.42, wpb=3273, bsz=84, num_updates=1516, lr=1.8192e-05, gnorm=1.921, train_wall=5, gb_free=11.3, wall=5660
2024-08-11 18:29:18 | INFO | train_inner | epoch 025:      6 / 63 loss=5.989, nll_loss=2.482, ppl=5.59, wps=1516.3, ups=0.41, wpb=3679.5, bsz=208, num_updates=1518, lr=1.8216e-05, gnorm=1.762, train_wall=5, gb_free=15, wall=5665
2024-08-11 18:29:23 | INFO | train_inner | epoch 025:      8 / 63 loss=5.807, nll_loss=2.239, ppl=4.72, wps=1376.5, ups=0.37, wpb=3688.5, bsz=192, num_updates=1520, lr=1.824e-05, gnorm=1.76, train_wall=5, gb_free=11.2, wall=5671
2024-08-11 18:29:28 | INFO | train_inner | epoch 025:     10 / 63 loss=5.729, nll_loss=2.157, ppl=4.46, wps=1244.8, ups=0.4, wpb=3147, bsz=192, num_updates=1522, lr=1.8264e-05, gnorm=1.739, train_wall=5, gb_free=11.9, wall=5676
2024-08-11 18:29:34 | INFO | train_inner | epoch 025:     12 / 63 loss=6.089, nll_loss=2.586, ppl=6, wps=1201.1, ups=0.39, wpb=3087, bsz=80, num_updates=1524, lr=1.8288e-05, gnorm=2.099, train_wall=5, gb_free=11.4, wall=5681
2024-08-11 18:29:38 | INFO | train_inner | epoch 025:     14 / 63 loss=6.153, nll_loss=2.667, ppl=6.35, wps=1261.9, ups=0.42, wpb=2970.5, bsz=64, num_updates=1526, lr=1.8312e-05, gnorm=2.222, train_wall=5, gb_free=14.4, wall=5686
2024-08-11 18:29:43 | INFO | train_inner | epoch 025:     16 / 63 loss=5.889, nll_loss=2.352, ppl=5.11, wps=1197.9, ups=0.39, wpb=3034.5, bsz=92, num_updates=1528, lr=1.8336e-05, gnorm=1.891, train_wall=5, gb_free=11.1, wall=5691
2024-08-11 18:29:49 | INFO | train_inner | epoch 025:     18 / 63 loss=6.04, nll_loss=2.55, ppl=5.86, wps=1595.7, ups=0.37, wpb=4317, bsz=168, num_updates=1530, lr=1.836e-05, gnorm=1.713, train_wall=5, gb_free=11, wall=5696
2024-08-11 18:29:59 | INFO | train_inner | epoch 025:     20 / 63 loss=5.933, nll_loss=2.402, ppl=5.28, wps=732.5, ups=0.2, wpb=3629.5, bsz=156, num_updates=1532, lr=1.8384e-05, gnorm=1.892, train_wall=10, gb_free=13.1, wall=5706
2024-08-11 18:30:04 | INFO | train_inner | epoch 025:     22 / 63 loss=5.847, nll_loss=2.303, ppl=4.93, wps=1092.2, ups=0.38, wpb=2866, bsz=168, num_updates=1534, lr=1.8408e-05, gnorm=2.177, train_wall=5, gb_free=11.1, wall=5711
2024-08-11 18:30:10 | INFO | train_inner | epoch 025:     24 / 63 loss=5.796, nll_loss=2.235, ppl=4.71, wps=1462.8, ups=0.34, wpb=4261, bsz=276, num_updates=1536, lr=1.8432e-05, gnorm=1.517, train_wall=6, gb_free=10.9, wall=5717
2024-08-11 18:30:15 | INFO | train_inner | epoch 025:     26 / 63 loss=5.972, nll_loss=2.434, ppl=5.4, wps=1463.2, ups=0.41, wpb=3590, bsz=132, num_updates=1538, lr=1.8456e-05, gnorm=1.853, train_wall=5, gb_free=11.2, wall=5722
2024-08-11 18:30:19 | INFO | train_inner | epoch 025:     28 / 63 loss=6.085, nll_loss=2.595, ppl=6.04, wps=1410.4, ups=0.43, wpb=3273, bsz=92, num_updates=1540, lr=1.848e-05, gnorm=1.993, train_wall=5, gb_free=11.2, wall=5727
2024-08-11 18:30:24 | INFO | train_inner | epoch 025:     30 / 63 loss=6.02, nll_loss=2.513, ppl=5.71, wps=1403.6, ups=0.42, wpb=3329.5, bsz=148, num_updates=1542, lr=1.8504e-05, gnorm=1.942, train_wall=5, gb_free=13, wall=5731
2024-08-11 18:30:29 | INFO | train_inner | epoch 025:     32 / 63 loss=5.978, nll_loss=2.467, ppl=5.53, wps=1231.6, ups=0.39, wpb=3172, bsz=156, num_updates=1544, lr=1.8528e-05, gnorm=1.898, train_wall=5, gb_free=11.9, wall=5736
2024-08-11 18:30:34 | INFO | train_inner | epoch 025:     34 / 63 loss=5.999, nll_loss=2.482, ppl=5.59, wps=1217.8, ups=0.37, wpb=3254.5, bsz=120, num_updates=1546, lr=1.8552e-05, gnorm=1.917, train_wall=5, gb_free=10.3, wall=5742
2024-08-11 18:30:39 | INFO | train_inner | epoch 025:     36 / 63 loss=5.882, nll_loss=2.34, ppl=5.06, wps=1253.9, ups=0.42, wpb=3020.5, bsz=124, num_updates=1548, lr=1.8576e-05, gnorm=1.999, train_wall=5, gb_free=11.8, wall=5747
2024-08-11 18:30:44 | INFO | train_inner | epoch 025:     38 / 63 loss=6.044, nll_loss=2.543, ppl=5.83, wps=1316.2, ups=0.39, wpb=3368.5, bsz=104, num_updates=1550, lr=1.86e-05, gnorm=1.935, train_wall=5, gb_free=11.8, wall=5752
2024-08-11 18:30:50 | INFO | train_inner | epoch 025:     40 / 63 loss=6.024, nll_loss=2.524, ppl=5.75, wps=1465.5, ups=0.39, wpb=3758, bsz=180, num_updates=1552, lr=1.8624e-05, gnorm=1.908, train_wall=5, gb_free=13.2, wall=5757
2024-08-11 18:30:55 | INFO | train_inner | epoch 025:     42 / 63 loss=5.84, nll_loss=2.261, ppl=4.79, wps=1136, ups=0.38, wpb=2971, bsz=112, num_updates=1554, lr=1.8648e-05, gnorm=1.92, train_wall=5, gb_free=11.8, wall=5762
2024-08-11 18:31:00 | INFO | train_inner | epoch 025:     44 / 63 loss=5.884, nll_loss=2.336, ppl=5.05, wps=1592.8, ups=0.36, wpb=4415.5, bsz=244, num_updates=1556, lr=1.8672e-05, gnorm=1.589, train_wall=6, gb_free=11.1, wall=5768
2024-08-11 18:31:06 | INFO | train_inner | epoch 025:     46 / 63 loss=5.854, nll_loss=2.31, ppl=4.96, wps=1450.1, ups=0.37, wpb=3892.5, bsz=204, num_updates=1558, lr=1.8696e-05, gnorm=1.737, train_wall=5, gb_free=11.7, wall=5773
2024-08-11 18:31:11 | INFO | train_inner | epoch 025:     48 / 63 loss=6.011, nll_loss=2.516, ppl=5.72, wps=1477.5, ups=0.38, wpb=3925, bsz=168, num_updates=1560, lr=1.872e-05, gnorm=1.733, train_wall=5, gb_free=12.6, wall=5778
2024-08-11 18:31:16 | INFO | train_inner | epoch 025:     50 / 63 loss=5.933, nll_loss=2.427, ppl=5.38, wps=1341.1, ups=0.39, wpb=3456.5, bsz=184, num_updates=1562, lr=1.8744e-05, gnorm=1.729, train_wall=5, gb_free=12.1, wall=5783
2024-08-11 18:31:20 | INFO | train_inner | epoch 025:     52 / 63 loss=6.003, nll_loss=2.492, ppl=5.63, wps=1175.2, ups=0.47, wpb=2487.5, bsz=116.5, num_updates=1564, lr=1.8768e-05, gnorm=2.7, train_wall=4, gb_free=13.2, wall=5788
2024-08-11 18:31:26 | INFO | train_inner | epoch 025:     54 / 63 loss=5.948, nll_loss=2.424, ppl=5.37, wps=1222, ups=0.36, wpb=3387.5, bsz=144, num_updates=1566, lr=1.8792e-05, gnorm=1.931, train_wall=6, gb_free=10.8, wall=5793
2024-08-11 18:31:31 | INFO | train_inner | epoch 025:     56 / 63 loss=5.853, nll_loss=2.309, ppl=4.96, wps=1474, ups=0.37, wpb=3973, bsz=232, num_updates=1568, lr=1.8816e-05, gnorm=1.734, train_wall=5, gb_free=10.6, wall=5799
2024-08-11 18:31:37 | INFO | train_inner | epoch 025:     58 / 63 loss=5.908, nll_loss=2.348, ppl=5.09, wps=1193.4, ups=0.38, wpb=3148.5, bsz=100, num_updates=1570, lr=1.884e-05, gnorm=2.036, train_wall=5, gb_free=12, wall=5804
2024-08-11 18:31:42 | INFO | train_inner | epoch 025:     60 / 63 loss=5.949, nll_loss=2.414, ppl=5.33, wps=1303.8, ups=0.4, wpb=3261, bsz=116, num_updates=1572, lr=1.8864e-05, gnorm=2.004, train_wall=5, gb_free=13.4, wall=5809
2024-08-11 18:31:46 | INFO | train_inner | epoch 025:     62 / 63 loss=6.029, nll_loss=2.54, ppl=5.82, wps=1330.3, ups=0.42, wpb=3176.5, bsz=144, num_updates=1574, lr=1.8888e-05, gnorm=1.999, train_wall=5, gb_free=15.1, wall=5814
2024-08-11 18:31:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32668.05078125Mb; avail=222398.6015625Mb
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000543
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32667.55859375Mb; avail=222398.6015625Mb
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004693
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32668.05078125Mb; avail=222398.109375Mb
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004054
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009616
2024-08-11 18:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32667.55859375Mb; avail=222398.6015625Mb
2024-08-11 18:31:56 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.246 | nll_loss 2.585 | ppl 6 | wps 3056.1 | wpb 1463.4 | bsz 64.9 | num_updates 1575 | best_loss 6.246
2024-08-11 18:31:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1575 updates
2024-08-11 18:31:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:32:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:33:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 25 @ 1575 updates, score 6.246) (writing took 66.09619556600228 seconds)
2024-08-11 18:33:02 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-08-11 18:33:02 | INFO | train | epoch 025 | loss 5.942 | nll_loss 2.414 | ppl 5.33 | wps 895.7 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 1575 | lr 1.89e-05 | gnorm 1.923 | train_wall 164 | gb_free 14.9 | wall 5890
2024-08-11 18:33:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:33:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:33:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:33:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000788
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28682.83984375Mb; avail=226383.359375Mb
2024-08-11 18:33:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000178
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001308
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28682.83984375Mb; avail=226383.359375Mb
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28682.83984375Mb; avail=226383.359375Mb
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000462
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002153
2024-08-11 18:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28682.83984375Mb; avail=226383.359375Mb
2024-08-11 18:33:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:33:02 | INFO | fairseq.trainer | begin training epoch 26
2024-08-11 18:33:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:33:05 | INFO | train_inner | epoch 026:      1 / 63 loss=5.83, nll_loss=2.269, ppl=4.82, wps=62.1, ups=0.03, wpb=2434, bsz=72, num_updates=1576, lr=1.8912e-05, gnorm=2.469, train_wall=4, gb_free=14.4, wall=5892
2024-08-11 18:33:10 | INFO | train_inner | epoch 026:      3 / 63 loss=5.968, nll_loss=2.43, ppl=5.39, wps=1512.7, ups=0.4, wpb=3758, bsz=132, num_updates=1578, lr=1.8936e-05, gnorm=1.854, train_wall=5, gb_free=9.9, wall=5897
2024-08-11 18:33:15 | INFO | train_inner | epoch 026:      5 / 63 loss=5.678, nll_loss=2.085, ppl=4.24, wps=1259, ups=0.4, wpb=3177, bsz=224, num_updates=1580, lr=1.896e-05, gnorm=1.864, train_wall=5, gb_free=11, wall=5902
2024-08-11 18:33:19 | INFO | train_inner | epoch 026:      7 / 63 loss=5.831, nll_loss=2.255, ppl=4.77, wps=1441.9, ups=0.44, wpb=3279.5, bsz=96, num_updates=1582, lr=1.8984e-05, gnorm=1.842, train_wall=5, gb_free=15.2, wall=5907
2024-08-11 18:33:24 | INFO | train_inner | epoch 026:      9 / 63 loss=5.757, nll_loss=2.18, ppl=4.53, wps=1217.3, ups=0.4, wpb=3017.5, bsz=152, num_updates=1584, lr=1.9008e-05, gnorm=1.908, train_wall=5, gb_free=13.1, wall=5912
2024-08-11 18:33:30 | INFO | train_inner | epoch 026:     11 / 63 loss=5.859, nll_loss=2.33, ppl=5.03, wps=1313.7, ups=0.38, wpb=3496.5, bsz=180, num_updates=1586, lr=1.9032e-05, gnorm=1.804, train_wall=5, gb_free=12.9, wall=5917
2024-08-11 18:33:34 | INFO | train_inner | epoch 026:     13 / 63 loss=5.967, nll_loss=2.426, ppl=5.38, wps=1338.7, ups=0.47, wpb=2863, bsz=76, num_updates=1588, lr=1.9056e-05, gnorm=2.129, train_wall=4, gb_free=15.7, wall=5921
2024-08-11 18:33:39 | INFO | train_inner | epoch 026:     15 / 63 loss=5.866, nll_loss=2.319, ppl=4.99, wps=1176.2, ups=0.38, wpb=3072, bsz=152, num_updates=1590, lr=1.908e-05, gnorm=1.908, train_wall=5, gb_free=11.5, wall=5927
2024-08-11 18:33:49 | INFO | train_inner | epoch 026:     17 / 63 loss=5.843, nll_loss=2.281, ppl=4.86, wps=657.4, ups=0.2, wpb=3278, bsz=172, num_updates=1592, lr=1.9104e-05, gnorm=1.876, train_wall=10, gb_free=13.6, wall=5936
2024-08-11 18:33:54 | INFO | train_inner | epoch 026:     19 / 63 loss=5.672, nll_loss=2.081, ppl=4.23, wps=1260.4, ups=0.38, wpb=3352, bsz=240, num_updates=1594, lr=1.9128e-05, gnorm=1.726, train_wall=5, gb_free=11.5, wall=5942
2024-08-11 18:33:59 | INFO | train_inner | epoch 026:     21 / 63 loss=5.954, nll_loss=2.413, ppl=5.33, wps=1369.2, ups=0.41, wpb=3362.5, bsz=76, num_updates=1596, lr=1.9152e-05, gnorm=1.947, train_wall=5, gb_free=12.6, wall=5947
2024-08-11 18:34:04 | INFO | train_inner | epoch 026:     23 / 63 loss=5.974, nll_loss=2.465, ppl=5.52, wps=1499.8, ups=0.46, wpb=3249, bsz=136.5, num_updates=1598, lr=1.9176e-05, gnorm=1.924, train_wall=4, gb_free=13.1, wall=5951
2024-08-11 18:34:09 | INFO | train_inner | epoch 026:     25 / 63 loss=6.003, nll_loss=2.492, ppl=5.63, wps=1410.2, ups=0.38, wpb=3678.5, bsz=140, num_updates=1600, lr=1.92e-05, gnorm=1.786, train_wall=5, gb_free=13.4, wall=5956
2024-08-11 18:34:14 | INFO | train_inner | epoch 026:     27 / 63 loss=5.98, nll_loss=2.465, ppl=5.52, wps=1297.8, ups=0.38, wpb=3409, bsz=164, num_updates=1602, lr=1.9224e-05, gnorm=1.928, train_wall=5, gb_free=11.8, wall=5962
2024-08-11 18:34:20 | INFO | train_inner | epoch 026:     29 / 63 loss=5.898, nll_loss=2.35, ppl=5.1, wps=1521.6, ups=0.36, wpb=4185, bsz=196, num_updates=1604, lr=1.9248e-05, gnorm=1.626, train_wall=5, gb_free=10.5, wall=5967
2024-08-11 18:34:25 | INFO | train_inner | epoch 026:     31 / 63 loss=5.941, nll_loss=2.396, ppl=5.26, wps=1354.2, ups=0.41, wpb=3339, bsz=112, num_updates=1606, lr=1.9272e-05, gnorm=1.86, train_wall=5, gb_free=14.2, wall=5972
2024-08-11 18:34:30 | INFO | train_inner | epoch 026:     33 / 63 loss=5.981, nll_loss=2.473, ppl=5.55, wps=1499.1, ups=0.35, wpb=4252.5, bsz=212, num_updates=1608, lr=1.9296e-05, gnorm=1.626, train_wall=6, gb_free=11.7, wall=5978
2024-08-11 18:34:35 | INFO | train_inner | epoch 026:     35 / 63 loss=5.915, nll_loss=2.364, ppl=5.15, wps=1277.1, ups=0.41, wpb=3135, bsz=96, num_updates=1610, lr=1.932e-05, gnorm=1.886, train_wall=5, gb_free=12.7, wall=5983
2024-08-11 18:34:41 | INFO | train_inner | epoch 026:     37 / 63 loss=5.851, nll_loss=2.297, ppl=4.92, wps=1083.9, ups=0.38, wpb=2878.5, bsz=108, num_updates=1612, lr=1.9344e-05, gnorm=2.157, train_wall=5, gb_free=13.4, wall=5988
2024-08-11 18:34:46 | INFO | train_inner | epoch 026:     39 / 63 loss=5.821, nll_loss=2.278, ppl=4.85, wps=1302.1, ups=0.37, wpb=3481.5, bsz=192, num_updates=1614, lr=1.9368e-05, gnorm=1.729, train_wall=5, gb_free=12.2, wall=5993
2024-08-11 18:34:51 | INFO | train_inner | epoch 026:     41 / 63 loss=5.892, nll_loss=2.364, ppl=5.15, wps=1409.2, ups=0.39, wpb=3654.5, bsz=172, num_updates=1616, lr=1.9392e-05, gnorm=1.712, train_wall=5, gb_free=10.7, wall=5998
2024-08-11 18:34:57 | INFO | train_inner | epoch 026:     43 / 63 loss=5.873, nll_loss=2.313, ppl=4.97, wps=1450.8, ups=0.36, wpb=4002, bsz=156, num_updates=1618, lr=1.9416e-05, gnorm=1.671, train_wall=6, gb_free=11.7, wall=6004
2024-08-11 18:35:02 | INFO | train_inner | epoch 026:     45 / 63 loss=5.753, nll_loss=2.165, ppl=4.48, wps=1323.4, ups=0.37, wpb=3568.5, bsz=160, num_updates=1620, lr=1.944e-05, gnorm=1.771, train_wall=5, gb_free=11.8, wall=6009
2024-08-11 18:35:07 | INFO | train_inner | epoch 026:     47 / 63 loss=5.857, nll_loss=2.313, ppl=4.97, wps=1434, ups=0.38, wpb=3801, bsz=236, num_updates=1622, lr=1.9464e-05, gnorm=1.722, train_wall=5, gb_free=11.3, wall=6015
2024-08-11 18:35:13 | INFO | train_inner | epoch 026:     49 / 63 loss=6.044, nll_loss=2.529, ppl=5.77, wps=1166.5, ups=0.36, wpb=3218, bsz=88, num_updates=1624, lr=1.9488e-05, gnorm=2.013, train_wall=6, gb_free=11.4, wall=6020
2024-08-11 18:35:18 | INFO | train_inner | epoch 026:     51 / 63 loss=5.804, nll_loss=2.242, ppl=4.73, wps=1410.6, ups=0.41, wpb=3450, bsz=144, num_updates=1626, lr=1.9512e-05, gnorm=1.763, train_wall=5, gb_free=12.5, wall=6025
2024-08-11 18:35:23 | INFO | train_inner | epoch 026:     53 / 63 loss=5.86, nll_loss=2.313, ppl=4.97, wps=1157.3, ups=0.38, wpb=3078, bsz=120, num_updates=1628, lr=1.9536e-05, gnorm=1.849, train_wall=5, gb_free=12.8, wall=6030
2024-08-11 18:35:29 | INFO | train_inner | epoch 026:     55 / 63 loss=5.803, nll_loss=2.226, ppl=4.68, wps=1499.7, ups=0.36, wpb=4152, bsz=164, num_updates=1630, lr=1.956e-05, gnorm=1.517, train_wall=6, gb_free=12.1, wall=6036
2024-08-11 18:35:33 | INFO | train_inner | epoch 026:     57 / 63 loss=5.969, nll_loss=2.453, ppl=5.48, wps=1243.5, ups=0.4, wpb=3088, bsz=132, num_updates=1632, lr=1.9584e-05, gnorm=2.147, train_wall=5, gb_free=10.8, wall=6041
2024-08-11 18:35:38 | INFO | train_inner | epoch 026:     59 / 63 loss=5.881, nll_loss=2.319, ppl=4.99, wps=1279.8, ups=0.41, wpb=3122, bsz=120, num_updates=1634, lr=1.9608e-05, gnorm=1.929, train_wall=5, gb_free=12.8, wall=6046
2024-08-11 18:35:44 | INFO | train_inner | epoch 026:     61 / 63 loss=5.816, nll_loss=2.229, ppl=4.69, wps=1169.1, ups=0.37, wpb=3133.5, bsz=100, num_updates=1636, lr=1.9632e-05, gnorm=1.893, train_wall=5, gb_free=12.9, wall=6051
2024-08-11 18:35:48 | INFO | train_inner | epoch 026:     63 / 63 loss=5.806, nll_loss=2.264, ppl=4.8, wps=1435.9, ups=0.5, wpb=2898, bsz=164, num_updates=1638, lr=1.9656e-05, gnorm=2.234, train_wall=4, gb_free=16.4, wall=6055
2024-08-11 18:35:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25401.6953125Mb; avail=229664.5078125Mb
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000525
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25401.6953125Mb; avail=229664.5078125Mb
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004708
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25401.6953125Mb; avail=229664.5078125Mb
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004019
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009583
2024-08-11 18:35:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25401.6953125Mb; avail=229664.5078125Mb
2024-08-11 18:35:56 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.22 | nll_loss 2.563 | ppl 5.91 | wps 3054.2 | wpb 1463.4 | bsz 64.9 | num_updates 1638 | best_loss 6.22
2024-08-11 18:35:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1638 updates
2024-08-11 18:35:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:36:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 26 @ 1638 updates, score 6.22) (writing took 62.11119431722909 seconds)
2024-08-11 18:36:58 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-08-11 18:36:58 | INFO | train | epoch 026 | loss 5.875 | nll_loss 2.327 | ppl 5.02 | wps 908.1 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1638 | lr 1.9656e-05 | gnorm 1.861 | train_wall 165 | gb_free 16.4 | wall 6126
2024-08-11 18:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000800
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31512.63671875Mb; avail=223553.56640625Mb
2024-08-11 18:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000142
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001293
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31512.63671875Mb; avail=223553.56640625Mb
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31512.63671875Mb; avail=223553.56640625Mb
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000458
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002107
2024-08-11 18:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31512.63671875Mb; avail=223553.56640625Mb
2024-08-11 18:36:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:36:58 | INFO | fairseq.trainer | begin training epoch 27
2024-08-11 18:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:37:04 | INFO | train_inner | epoch 027:      2 / 63 loss=5.729, nll_loss=2.169, ppl=4.5, wps=106.2, ups=0.03, wpb=4026.5, bsz=252, num_updates=1640, lr=1.968e-05, gnorm=1.775, train_wall=5, gb_free=12.2, wall=6131
2024-08-11 18:37:09 | INFO | train_inner | epoch 027:      4 / 63 loss=5.692, nll_loss=2.122, ppl=4.35, wps=1217.5, ups=0.39, wpb=3109, bsz=188, num_updates=1642, lr=1.9704e-05, gnorm=1.795, train_wall=5, gb_free=12.3, wall=6136
2024-08-11 18:37:14 | INFO | train_inner | epoch 027:      6 / 63 loss=5.791, nll_loss=2.211, ppl=4.63, wps=1255.2, ups=0.38, wpb=3320.5, bsz=124, num_updates=1644, lr=1.9728e-05, gnorm=2.025, train_wall=5, gb_free=12, wall=6141
2024-08-11 18:37:18 | INFO | train_inner | epoch 027:      8 / 63 loss=5.93, nll_loss=2.371, ppl=5.17, wps=1614.6, ups=0.45, wpb=3591.5, bsz=140, num_updates=1646, lr=1.9752e-05, gnorm=1.949, train_wall=4, gb_free=15.7, wall=6146
2024-08-11 18:37:24 | INFO | train_inner | epoch 027:     10 / 63 loss=5.803, nll_loss=2.219, ppl=4.66, wps=1445.9, ups=0.38, wpb=3778, bsz=184, num_updates=1648, lr=1.9776e-05, gnorm=1.862, train_wall=5, gb_free=12, wall=6151
2024-08-11 18:37:29 | INFO | train_inner | epoch 027:     12 / 63 loss=5.812, nll_loss=2.237, ppl=4.71, wps=1166.4, ups=0.39, wpb=3023.5, bsz=100, num_updates=1650, lr=1.98e-05, gnorm=2.099, train_wall=5, gb_free=11, wall=6156
2024-08-11 18:37:34 | INFO | train_inner | epoch 027:     14 / 63 loss=5.921, nll_loss=2.427, ppl=5.38, wps=1471.4, ups=0.37, wpb=3936, bsz=184, num_updates=1652, lr=1.9824e-05, gnorm=1.733, train_wall=5, gb_free=12.1, wall=6162
2024-08-11 18:37:39 | INFO | train_inner | epoch 027:     16 / 63 loss=5.882, nll_loss=2.353, ppl=5.11, wps=1291.4, ups=0.4, wpb=3235.5, bsz=116, num_updates=1654, lr=1.9848e-05, gnorm=2.093, train_wall=5, gb_free=13.3, wall=6167
2024-08-11 18:37:45 | INFO | train_inner | epoch 027:     18 / 63 loss=5.739, nll_loss=2.139, ppl=4.4, wps=1224, ups=0.37, wpb=3329, bsz=136, num_updates=1656, lr=1.9872e-05, gnorm=1.747, train_wall=5, gb_free=11.5, wall=6172
2024-08-11 18:37:50 | INFO | train_inner | epoch 027:     20 / 63 loss=6.015, nll_loss=2.483, ppl=5.59, wps=1221.3, ups=0.38, wpb=3252.5, bsz=100, num_updates=1658, lr=1.9896e-05, gnorm=2.016, train_wall=5, gb_free=11.8, wall=6177
2024-08-11 18:38:00 | INFO | train_inner | epoch 027:     22 / 63 loss=5.878, nll_loss=2.312, ppl=4.97, wps=709.7, ups=0.19, wpb=3649, bsz=196, num_updates=1660, lr=1.992e-05, gnorm=1.76, train_wall=10, gb_free=12.4, wall=6188
2024-08-11 18:38:06 | INFO | train_inner | epoch 027:     24 / 63 loss=5.89, nll_loss=2.352, ppl=5.11, wps=1590.9, ups=0.37, wpb=4329.5, bsz=180, num_updates=1662, lr=1.9944e-05, gnorm=1.707, train_wall=5, gb_free=12, wall=6193
2024-08-11 18:38:11 | INFO | train_inner | epoch 027:     26 / 63 loss=5.83, nll_loss=2.292, ppl=4.9, wps=1396.1, ups=0.41, wpb=3398, bsz=200, num_updates=1664, lr=1.9968e-05, gnorm=1.771, train_wall=5, gb_free=15.4, wall=6198
2024-08-11 18:38:16 | INFO | train_inner | epoch 027:     28 / 63 loss=5.813, nll_loss=2.249, ppl=4.75, wps=1291.7, ups=0.38, wpb=3409, bsz=128, num_updates=1666, lr=1.9992e-05, gnorm=1.808, train_wall=5, gb_free=13.4, wall=6203
2024-08-11 18:38:21 | INFO | train_inner | epoch 027:     30 / 63 loss=5.869, nll_loss=2.33, ppl=5.03, wps=1160.8, ups=0.41, wpb=2846, bsz=92, num_updates=1668, lr=2.0016e-05, gnorm=2.005, train_wall=5, gb_free=13.3, wall=6208
2024-08-11 18:38:26 | INFO | train_inner | epoch 027:     32 / 63 loss=5.774, nll_loss=2.183, ppl=4.54, wps=1182.5, ups=0.4, wpb=2968, bsz=112, num_updates=1670, lr=2.004e-05, gnorm=2.036, train_wall=5, gb_free=12.8, wall=6213
2024-08-11 18:38:30 | INFO | train_inner | epoch 027:     34 / 63 loss=5.866, nll_loss=2.293, ppl=4.9, wps=1246.5, ups=0.48, wpb=2573.5, bsz=108.5, num_updates=1672, lr=2.0064e-05, gnorm=2.323, train_wall=4, gb_free=10.4, wall=6217
2024-08-11 18:38:35 | INFO | train_inner | epoch 027:     36 / 63 loss=5.739, nll_loss=2.175, ppl=4.52, wps=1305.4, ups=0.42, wpb=3118.5, bsz=144, num_updates=1674, lr=2.0088e-05, gnorm=1.897, train_wall=5, gb_free=16.6, wall=6222
2024-08-11 18:38:40 | INFO | train_inner | epoch 027:     38 / 63 loss=5.803, nll_loss=2.255, ppl=4.77, wps=1595.4, ups=0.36, wpb=4483.5, bsz=240, num_updates=1676, lr=2.0112e-05, gnorm=1.731, train_wall=6, gb_free=11.8, wall=6228
2024-08-11 18:38:46 | INFO | train_inner | epoch 027:     40 / 63 loss=5.778, nll_loss=2.209, ppl=4.62, wps=1481.2, ups=0.37, wpb=4044, bsz=184, num_updates=1678, lr=2.0136e-05, gnorm=1.663, train_wall=5, gb_free=11.1, wall=6233
2024-08-11 18:38:51 | INFO | train_inner | epoch 027:     42 / 63 loss=5.697, nll_loss=2.103, ppl=4.3, wps=1318.4, ups=0.38, wpb=3482, bsz=176, num_updates=1680, lr=2.016e-05, gnorm=1.937, train_wall=5, gb_free=12.6, wall=6238
2024-08-11 18:38:56 | INFO | train_inner | epoch 027:     44 / 63 loss=5.798, nll_loss=2.228, ppl=4.68, wps=1330.4, ups=0.4, wpb=3312.5, bsz=148, num_updates=1682, lr=2.0184e-05, gnorm=1.838, train_wall=5, gb_free=9.7, wall=6243
2024-08-11 18:39:01 | INFO | train_inner | epoch 027:     46 / 63 loss=5.936, nll_loss=2.37, ppl=5.17, wps=1295.4, ups=0.4, wpb=3259.5, bsz=64, num_updates=1684, lr=2.0208e-05, gnorm=2.135, train_wall=5, gb_free=15.3, wall=6248
2024-08-11 18:39:06 | INFO | train_inner | epoch 027:     48 / 63 loss=6.011, nll_loss=2.487, ppl=5.61, wps=1397.6, ups=0.4, wpb=3519.5, bsz=88, num_updates=1686, lr=2.0232e-05, gnorm=1.984, train_wall=5, gb_free=13.9, wall=6253
2024-08-11 18:39:12 | INFO | train_inner | epoch 027:     50 / 63 loss=5.868, nll_loss=2.321, ppl=5, wps=1145, ups=0.37, wpb=3082.5, bsz=156, num_updates=1688, lr=2.0256e-05, gnorm=1.815, train_wall=5, gb_free=12.7, wall=6259
2024-08-11 18:39:16 | INFO | train_inner | epoch 027:     52 / 63 loss=5.8, nll_loss=2.236, ppl=4.71, wps=1342.6, ups=0.41, wpb=3255, bsz=148, num_updates=1690, lr=2.028e-05, gnorm=1.762, train_wall=5, gb_free=11.5, wall=6264
2024-08-11 18:39:21 | INFO | train_inner | epoch 027:     54 / 63 loss=5.81, nll_loss=2.26, ppl=4.79, wps=1352.7, ups=0.45, wpb=2996, bsz=164, num_updates=1692, lr=2.0304e-05, gnorm=2.05, train_wall=4, gb_free=11.3, wall=6268
2024-08-11 18:39:26 | INFO | train_inner | epoch 027:     56 / 63 loss=5.895, nll_loss=2.35, ppl=5.1, wps=1444.2, ups=0.4, wpb=3635.5, bsz=132, num_updates=1694, lr=2.0328e-05, gnorm=1.73, train_wall=5, gb_free=14.1, wall=6273
2024-08-11 18:39:31 | INFO | train_inner | epoch 027:     58 / 63 loss=5.674, nll_loss=2.076, ppl=4.22, wps=1340.6, ups=0.38, wpb=3572, bsz=200, num_updates=1696, lr=2.0352e-05, gnorm=1.64, train_wall=5, gb_free=11.8, wall=6278
2024-08-11 18:39:36 | INFO | train_inner | epoch 027:     60 / 63 loss=5.849, nll_loss=2.286, ppl=4.88, wps=1374, ups=0.39, wpb=3568, bsz=112, num_updates=1698, lr=2.0376e-05, gnorm=1.811, train_wall=5, gb_free=11.5, wall=6284
2024-08-11 18:39:41 | INFO | train_inner | epoch 027:     62 / 63 loss=5.854, nll_loss=2.293, ppl=4.9, wps=1341.1, ups=0.4, wpb=3388, bsz=124, num_updates=1700, lr=2.04e-05, gnorm=1.857, train_wall=5, gb_free=16, wall=6289
2024-08-11 18:39:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27177.9609375Mb; avail=227888.2421875Mb
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000542
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27177.9609375Mb; avail=227888.2421875Mb
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004699
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27177.9609375Mb; avail=227888.2421875Mb
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003986
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009555
2024-08-11 18:39:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27177.9609375Mb; avail=227888.2421875Mb
2024-08-11 18:39:51 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.224 | nll_loss 2.546 | ppl 5.84 | wps 3060.9 | wpb 1463.4 | bsz 64.9 | num_updates 1701 | best_loss 6.22
2024-08-11 18:39:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1701 updates
2024-08-11 18:39:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 18:40:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 18:40:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 27 @ 1701 updates, score 6.224) (writing took 37.92888015182689 seconds)
2024-08-11 18:40:29 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-08-11 18:40:29 | INFO | train | epoch 027 | loss 5.829 | nll_loss 2.269 | ppl 4.82 | wps 1016.2 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 1701 | lr 2.0412e-05 | gnorm 1.893 | train_wall 164 | gb_free 16.4 | wall 6336
2024-08-11 18:40:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:40:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:40:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:40:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000751
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32208.28125Mb; avail=222857.921875Mb
2024-08-11 18:40:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000182
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001255
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32208.28125Mb; avail=222857.921875Mb
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000047
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32208.28125Mb; avail=222857.921875Mb
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000435
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002034
2024-08-11 18:40:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32208.28125Mb; avail=222857.921875Mb
2024-08-11 18:40:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:40:29 | INFO | fairseq.trainer | begin training epoch 28
2024-08-11 18:40:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:40:32 | INFO | train_inner | epoch 028:      1 / 63 loss=5.719, nll_loss=2.133, ppl=4.38, wps=106, ups=0.04, wpb=2670.5, bsz=136, num_updates=1702, lr=2.0424e-05, gnorm=2.112, train_wall=4, gb_free=12.1, wall=6339
2024-08-11 18:40:37 | INFO | train_inner | epoch 028:      3 / 63 loss=5.799, nll_loss=2.231, ppl=4.69, wps=1448.9, ups=0.39, wpb=3734, bsz=188, num_updates=1704, lr=2.0448e-05, gnorm=1.736, train_wall=5, gb_free=12.4, wall=6344
2024-08-11 18:40:41 | INFO | train_inner | epoch 028:      5 / 63 loss=5.76, nll_loss=2.174, ppl=4.51, wps=1657.1, ups=0.46, wpb=3628, bsz=160, num_updates=1706, lr=2.0472e-05, gnorm=1.897, train_wall=4, gb_free=13.4, wall=6349
2024-08-11 18:40:46 | INFO | train_inner | epoch 028:      7 / 63 loss=5.739, nll_loss=2.152, ppl=4.44, wps=1480.3, ups=0.39, wpb=3781.5, bsz=160, num_updates=1708, lr=2.0496e-05, gnorm=1.744, train_wall=5, gb_free=13.9, wall=6354
2024-08-11 18:40:51 | INFO | train_inner | epoch 028:      9 / 63 loss=5.727, nll_loss=2.125, ppl=4.36, wps=1406.2, ups=0.43, wpb=3292.5, bsz=120, num_updates=1710, lr=2.052e-05, gnorm=1.823, train_wall=5, gb_free=12.9, wall=6358
2024-08-11 18:40:56 | INFO | train_inner | epoch 028:     11 / 63 loss=5.854, nll_loss=2.307, ppl=4.95, wps=1399.4, ups=0.38, wpb=3711.5, bsz=168, num_updates=1712, lr=2.0544e-05, gnorm=1.707, train_wall=5, gb_free=10.9, wall=6364
2024-08-11 18:41:02 | INFO | train_inner | epoch 028:     13 / 63 loss=5.654, nll_loss=2.058, ppl=4.16, wps=1118.5, ups=0.39, wpb=2860, bsz=168, num_updates=1714, lr=2.0568e-05, gnorm=1.761, train_wall=5, gb_free=11.2, wall=6369
2024-08-11 18:41:07 | INFO | train_inner | epoch 028:     15 / 63 loss=5.958, nll_loss=2.437, ppl=5.42, wps=1237.1, ups=0.39, wpb=3208.5, bsz=132, num_updates=1716, lr=2.0592e-05, gnorm=1.949, train_wall=5, gb_free=12.1, wall=6374
2024-08-11 18:41:12 | INFO | train_inner | epoch 028:     17 / 63 loss=5.818, nll_loss=2.244, ppl=4.74, wps=1369.6, ups=0.4, wpb=3411, bsz=100, num_updates=1718, lr=2.0616e-05, gnorm=1.87, train_wall=5, gb_free=12.2, wall=6379
2024-08-11 18:41:17 | INFO | train_inner | epoch 028:     19 / 63 loss=5.715, nll_loss=2.128, ppl=4.37, wps=1330.6, ups=0.37, wpb=3565.5, bsz=192, num_updates=1720, lr=2.064e-05, gnorm=1.668, train_wall=5, gb_free=11.6, wall=6384
2024-08-11 18:41:22 | INFO | train_inner | epoch 028:     21 / 63 loss=5.735, nll_loss=2.16, ppl=4.47, wps=1389.9, ups=0.38, wpb=3629, bsz=200, num_updates=1722, lr=2.0664e-05, gnorm=1.746, train_wall=5, gb_free=15.6, wall=6390
2024-08-11 18:41:26 | INFO | train_inner | epoch 028:     23 / 63 loss=6.035, nll_loss=2.529, ppl=5.77, wps=1354.1, ups=0.51, wpb=2647, bsz=56.5, num_updates=1724, lr=2.0688e-05, gnorm=2.779, train_wall=4, gb_free=19.3, wall=6394
2024-08-11 18:41:36 | INFO | train_inner | epoch 028:     25 / 63 loss=5.78, nll_loss=2.186, ppl=4.55, wps=627.1, ups=0.19, wpb=3217, bsz=100, num_updates=1726, lr=2.0712e-05, gnorm=1.959, train_wall=10, gb_free=9.9, wall=6404
2024-08-11 18:41:42 | INFO | train_inner | epoch 028:     27 / 63 loss=5.723, nll_loss=2.112, ppl=4.32, wps=1328, ups=0.37, wpb=3604, bsz=124, num_updates=1728, lr=2.0736e-05, gnorm=1.727, train_wall=5, gb_free=11.8, wall=6409
2024-08-11 18:41:47 | INFO | train_inner | epoch 028:     29 / 63 loss=5.637, nll_loss=2.029, ppl=4.08, wps=1165.6, ups=0.39, wpb=2964, bsz=168, num_updates=1730, lr=2.076e-05, gnorm=1.942, train_wall=5, gb_free=12.7, wall=6414
2024-08-11 18:41:52 | INFO | train_inner | epoch 028:     31 / 63 loss=5.708, nll_loss=2.107, ppl=4.31, wps=1141.6, ups=0.38, wpb=2999, bsz=100, num_updates=1732, lr=2.0784e-05, gnorm=1.832, train_wall=5, gb_free=11.3, wall=6420
2024-08-11 18:41:58 | INFO | train_inner | epoch 028:     33 / 63 loss=5.807, nll_loss=2.232, ppl=4.7, wps=1349.4, ups=0.37, wpb=3652, bsz=104, num_updates=1734, lr=2.0808e-05, gnorm=1.803, train_wall=5, gb_free=10.4, wall=6425
2024-08-11 18:42:02 | INFO | train_inner | epoch 028:     35 / 63 loss=5.77, nll_loss=2.191, ppl=4.57, wps=1129.5, ups=0.42, wpb=2698.5, bsz=112, num_updates=1736, lr=2.0832e-05, gnorm=2.137, train_wall=5, gb_free=13.2, wall=6430
2024-08-11 18:42:08 | INFO | train_inner | epoch 028:     37 / 63 loss=5.781, nll_loss=2.209, ppl=4.62, wps=1397, ups=0.38, wpb=3653.5, bsz=148, num_updates=1738, lr=2.0856e-05, gnorm=1.703, train_wall=5, gb_free=13.2, wall=6435
2024-08-11 18:42:13 | INFO | train_inner | epoch 028:     39 / 63 loss=5.679, nll_loss=2.105, ppl=4.3, wps=1283.3, ups=0.36, wpb=3579, bsz=240, num_updates=1740, lr=2.088e-05, gnorm=1.718, train_wall=6, gb_free=11.1, wall=6441
2024-08-11 18:42:19 | INFO | train_inner | epoch 028:     41 / 63 loss=5.784, nll_loss=2.215, ppl=4.64, wps=1283, ups=0.37, wpb=3447.5, bsz=152, num_updates=1742, lr=2.0904e-05, gnorm=1.676, train_wall=5, gb_free=13.6, wall=6446
2024-08-11 18:42:24 | INFO | train_inner | epoch 028:     43 / 63 loss=5.83, nll_loss=2.271, ppl=4.83, wps=1423.6, ups=0.4, wpb=3577.5, bsz=164, num_updates=1744, lr=2.0928e-05, gnorm=1.721, train_wall=5, gb_free=12, wall=6451
2024-08-11 18:42:28 | INFO | train_inner | epoch 028:     45 / 63 loss=5.772, nll_loss=2.194, ppl=4.58, wps=1424.7, ups=0.41, wpb=3444.5, bsz=144, num_updates=1746, lr=2.0952e-05, gnorm=1.79, train_wall=5, gb_free=13.3, wall=6456
2024-08-11 18:42:34 | INFO | train_inner | epoch 028:     47 / 63 loss=5.742, nll_loss=2.141, ppl=4.41, wps=1368.5, ups=0.37, wpb=3671.5, bsz=124, num_updates=1748, lr=2.0976e-05, gnorm=1.652, train_wall=5, gb_free=10.3, wall=6461
2024-08-11 18:42:39 | INFO | train_inner | epoch 028:     49 / 63 loss=5.648, nll_loss=2.028, ppl=4.08, wps=1185.5, ups=0.35, wpb=3353, bsz=156, num_updates=1750, lr=2.1e-05, gnorm=1.886, train_wall=6, gb_free=10.6, wall=6467
2024-08-11 18:42:45 | INFO | train_inner | epoch 028:     51 / 63 loss=5.582, nll_loss=1.968, ppl=3.91, wps=1169.6, ups=0.37, wpb=3120, bsz=212, num_updates=1752, lr=2.1024e-05, gnorm=1.8, train_wall=5, gb_free=11.6, wall=6472
2024-08-11 18:42:50 | INFO | train_inner | epoch 028:     53 / 63 loss=5.767, nll_loss=2.214, ppl=4.64, wps=1411.8, ups=0.39, wpb=3634.5, bsz=196, num_updates=1754, lr=2.1048e-05, gnorm=1.76, train_wall=5, gb_free=13.9, wall=6477
2024-08-11 18:42:55 | INFO | train_inner | epoch 028:     55 / 63 loss=5.714, nll_loss=2.127, ppl=4.37, wps=1477.9, ups=0.38, wpb=3856, bsz=184, num_updates=1756, lr=2.1072e-05, gnorm=1.65, train_wall=5, gb_free=12.2, wall=6483
2024-08-11 18:43:00 | INFO | train_inner | epoch 028:     57 / 63 loss=5.77, nll_loss=2.2, ppl=4.59, wps=1178.2, ups=0.42, wpb=2835, bsz=124, num_updates=1758, lr=2.1096e-05, gnorm=1.927, train_wall=5, gb_free=13.9, wall=6487
2024-08-11 18:43:05 | INFO | train_inner | epoch 028:     59 / 63 loss=5.848, nll_loss=2.29, ppl=4.89, wps=1531.2, ups=0.37, wpb=4169.5, bsz=192, num_updates=1760, lr=2.112e-05, gnorm=1.633, train_wall=5, gb_free=11.3, wall=6493
2024-08-11 18:43:11 | INFO | train_inner | epoch 028:     61 / 63 loss=5.798, nll_loss=2.22, ppl=4.66, wps=1432.7, ups=0.37, wpb=3858, bsz=136, num_updates=1762, lr=2.1144e-05, gnorm=2.181, train_wall=5, gb_free=11.1, wall=6498
2024-08-11 18:43:15 | INFO | train_inner | epoch 028:     63 / 63 loss=5.762, nll_loss=2.186, ppl=4.55, wps=1187.9, ups=0.52, wpb=2279.5, bsz=56, num_updates=1764, lr=2.1168e-05, gnorm=2.634, train_wall=4, gb_free=16.3, wall=6502
2024-08-11 18:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25596.2109375Mb; avail=229469.9921875Mb
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000526
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25596.2109375Mb; avail=229469.9921875Mb
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004861
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25596.2109375Mb; avail=229469.9921875Mb
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004043
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009765
2024-08-11 18:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25596.2109375Mb; avail=229469.9921875Mb
2024-08-11 18:43:23 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.197 | nll_loss 2.519 | ppl 5.73 | wps 3056.7 | wpb 1463.4 | bsz 64.9 | num_updates 1764 | best_loss 6.197
2024-08-11 18:43:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1764 updates
2024-08-11 18:43:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:44:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:44:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 28 @ 1764 updates, score 6.197) (writing took 62.185733447782695 seconds)
2024-08-11 18:44:25 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-08-11 18:44:25 | INFO | train | epoch 028 | loss 5.765 | nll_loss 2.186 | ppl 4.55 | wps 907.3 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1764 | lr 2.1168e-05 | gnorm 1.862 | train_wall 165 | gb_free 16.3 | wall 6573
2024-08-11 18:44:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:44:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:44:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:44:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001673
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31076.484375Mb; avail=223989.71484375Mb
2024-08-11 18:44:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000359
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.002316
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31076.484375Mb; avail=223989.71484375Mb
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31076.484375Mb; avail=223989.71484375Mb
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000439
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.003081
2024-08-11 18:44:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31076.484375Mb; avail=223989.71484375Mb
2024-08-11 18:44:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:44:25 | INFO | fairseq.trainer | begin training epoch 29
2024-08-11 18:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:44:31 | INFO | train_inner | epoch 029:      2 / 63 loss=5.618, nll_loss=2.004, ppl=4.01, wps=108.6, ups=0.03, wpb=4137.5, bsz=236, num_updates=1766, lr=2.1192e-05, gnorm=1.411, train_wall=5, gb_free=12.3, wall=6578
2024-08-11 18:44:36 | INFO | train_inner | epoch 029:      4 / 63 loss=5.613, nll_loss=1.978, ppl=3.94, wps=1338, ups=0.39, wpb=3441, bsz=120, num_updates=1768, lr=2.1216e-05, gnorm=1.807, train_wall=5, gb_free=10.3, wall=6583
2024-08-11 18:44:41 | INFO | train_inner | epoch 029:      6 / 63 loss=5.608, nll_loss=1.978, ppl=3.94, wps=1191.4, ups=0.4, wpb=2979.5, bsz=132, num_updates=1770, lr=2.124e-05, gnorm=1.763, train_wall=5, gb_free=11.6, wall=6588
2024-08-11 18:44:45 | INFO | train_inner | epoch 029:      8 / 63 loss=5.72, nll_loss=2.131, ppl=4.38, wps=1276.8, ups=0.45, wpb=2859.5, bsz=108, num_updates=1772, lr=2.1264e-05, gnorm=2.05, train_wall=4, gb_free=13.3, wall=6593
2024-08-11 18:44:51 | INFO | train_inner | epoch 029:     10 / 63 loss=5.762, nll_loss=2.201, ppl=4.6, wps=1391, ups=0.38, wpb=3666, bsz=188, num_updates=1774, lr=2.1288e-05, gnorm=1.834, train_wall=5, gb_free=11.9, wall=6598
2024-08-11 18:44:56 | INFO | train_inner | epoch 029:     12 / 63 loss=5.701, nll_loss=2.086, ppl=4.25, wps=1275, ups=0.39, wpb=3260.5, bsz=96, num_updates=1776, lr=2.1312e-05, gnorm=1.846, train_wall=5, gb_free=10.5, wall=6603
2024-08-11 18:45:01 | INFO | train_inner | epoch 029:     14 / 63 loss=5.553, nll_loss=1.938, ppl=3.83, wps=1294.9, ups=0.37, wpb=3457.5, bsz=244, num_updates=1778, lr=2.1336e-05, gnorm=1.618, train_wall=5, gb_free=11.3, wall=6609
2024-08-11 18:45:06 | INFO | train_inner | epoch 029:     16 / 63 loss=5.811, nll_loss=2.236, ppl=4.71, wps=1407.2, ups=0.39, wpb=3630.5, bsz=156, num_updates=1780, lr=2.136e-05, gnorm=2.243, train_wall=5, gb_free=12.8, wall=6614
2024-08-11 18:45:11 | INFO | train_inner | epoch 029:     18 / 63 loss=5.65, nll_loss=2.044, ppl=4.12, wps=1297.6, ups=0.39, wpb=3326, bsz=176, num_updates=1782, lr=2.1384e-05, gnorm=1.755, train_wall=5, gb_free=11.8, wall=6619
2024-08-11 18:45:17 | INFO | train_inner | epoch 029:     20 / 63 loss=5.782, nll_loss=2.205, ppl=4.61, wps=1457.6, ups=0.39, wpb=3764, bsz=128, num_updates=1784, lr=2.1408e-05, gnorm=1.762, train_wall=5, gb_free=12.8, wall=6624
2024-08-11 18:45:27 | INFO | train_inner | epoch 029:     22 / 63 loss=5.833, nll_loss=2.274, ppl=4.84, wps=613, ups=0.2, wpb=3108, bsz=92, num_updates=1786, lr=2.1432e-05, gnorm=1.944, train_wall=10, gb_free=10.3, wall=6634
2024-08-11 18:45:32 | INFO | train_inner | epoch 029:     24 / 63 loss=5.626, nll_loss=2.008, ppl=4.02, wps=1351.8, ups=0.38, wpb=3522.5, bsz=160, num_updates=1788, lr=2.1456e-05, gnorm=1.609, train_wall=5, gb_free=11.3, wall=6639
2024-08-11 18:45:37 | INFO | train_inner | epoch 029:     26 / 63 loss=5.823, nll_loss=2.267, ppl=4.81, wps=1612.5, ups=0.37, wpb=4327.5, bsz=224, num_updates=1790, lr=2.148e-05, gnorm=1.772, train_wall=5, gb_free=10, wall=6645
2024-08-11 18:45:42 | INFO | train_inner | epoch 029:     28 / 63 loss=5.799, nll_loss=2.215, ppl=4.64, wps=1289.4, ups=0.41, wpb=3157.5, bsz=116, num_updates=1792, lr=2.1504e-05, gnorm=1.92, train_wall=5, gb_free=12.2, wall=6650
2024-08-11 18:45:47 | INFO | train_inner | epoch 029:     30 / 63 loss=5.62, nll_loss=1.995, ppl=3.99, wps=1176.6, ups=0.39, wpb=3024, bsz=88, num_updates=1794, lr=2.1528e-05, gnorm=1.852, train_wall=5, gb_free=13.7, wall=6655
2024-08-11 18:45:52 | INFO | train_inner | epoch 029:     32 / 63 loss=5.741, nll_loss=2.141, ppl=4.41, wps=1467.2, ups=0.43, wpb=3374.5, bsz=104, num_updates=1796, lr=2.1552e-05, gnorm=1.923, train_wall=5, gb_free=17, wall=6659
2024-08-11 18:45:57 | INFO | train_inner | epoch 029:     34 / 63 loss=5.571, nll_loss=1.968, ppl=3.91, wps=1328.8, ups=0.37, wpb=3602, bsz=280, num_updates=1798, lr=2.1576e-05, gnorm=1.736, train_wall=5, gb_free=12.6, wall=6665
2024-08-11 18:46:02 | INFO | train_inner | epoch 029:     36 / 63 loss=5.804, nll_loss=2.214, ppl=4.64, wps=1255.3, ups=0.4, wpb=3128, bsz=76, num_updates=1800, lr=2.16e-05, gnorm=2.001, train_wall=5, gb_free=13.5, wall=6670
2024-08-11 18:46:08 | INFO | train_inner | epoch 029:     38 / 63 loss=5.692, nll_loss=2.107, ppl=4.31, wps=1627.9, ups=0.38, wpb=4328.5, bsz=260, num_updates=1802, lr=2.1624e-05, gnorm=1.572, train_wall=5, gb_free=11, wall=6675
2024-08-11 18:46:13 | INFO | train_inner | epoch 029:     40 / 63 loss=5.688, nll_loss=2.089, ppl=4.25, wps=1292.1, ups=0.4, wpb=3266, bsz=112, num_updates=1804, lr=2.1648e-05, gnorm=1.867, train_wall=5, gb_free=15.1, wall=6680
2024-08-11 18:46:18 | INFO | train_inner | epoch 029:     42 / 63 loss=5.822, nll_loss=2.254, ppl=4.77, wps=1433.6, ups=0.4, wpb=3628.5, bsz=108, num_updates=1806, lr=2.1672e-05, gnorm=1.818, train_wall=5, gb_free=14.8, wall=6685
2024-08-11 18:46:22 | INFO | train_inner | epoch 029:     44 / 63 loss=5.823, nll_loss=2.26, ppl=4.79, wps=1562.5, ups=0.5, wpb=3100, bsz=116.5, num_updates=1808, lr=2.1696e-05, gnorm=2.107, train_wall=4, gb_free=12.9, wall=6689
2024-08-11 18:46:27 | INFO | train_inner | epoch 029:     46 / 63 loss=5.699, nll_loss=2.105, ppl=4.3, wps=1445.2, ups=0.37, wpb=3953, bsz=192, num_updates=1810, lr=2.172e-05, gnorm=1.746, train_wall=5, gb_free=13.4, wall=6695
2024-08-11 18:46:32 | INFO | train_inner | epoch 029:     48 / 63 loss=5.646, nll_loss=2.034, ppl=4.1, wps=1540.9, ups=0.39, wpb=4000.5, bsz=212, num_updates=1812, lr=2.1744e-05, gnorm=1.552, train_wall=5, gb_free=10.3, wall=6700
2024-08-11 18:46:38 | INFO | train_inner | epoch 029:     50 / 63 loss=5.814, nll_loss=2.233, ppl=4.7, wps=1357, ups=0.4, wpb=3417.5, bsz=96, num_updates=1814, lr=2.1768e-05, gnorm=1.949, train_wall=5, gb_free=13.5, wall=6705
2024-08-11 18:46:43 | INFO | train_inner | epoch 029:     52 / 63 loss=5.671, nll_loss=2.066, ppl=4.19, wps=1035.3, ups=0.39, wpb=2638, bsz=96, num_updates=1816, lr=2.1792e-05, gnorm=2.127, train_wall=5, gb_free=11.8, wall=6710
2024-08-11 18:46:48 | INFO | train_inner | epoch 029:     54 / 63 loss=5.693, nll_loss=2.107, ppl=4.31, wps=1382.7, ups=0.38, wpb=3685, bsz=184, num_updates=1818, lr=2.1816e-05, gnorm=1.782, train_wall=5, gb_free=12.4, wall=6715
2024-08-11 18:46:53 | INFO | train_inner | epoch 029:     56 / 63 loss=5.804, nll_loss=2.238, ppl=4.72, wps=1428.5, ups=0.4, wpb=3561, bsz=148, num_updates=1820, lr=2.184e-05, gnorm=2.013, train_wall=5, gb_free=15.8, wall=6720
2024-08-11 18:46:58 | INFO | train_inner | epoch 029:     58 / 63 loss=5.623, nll_loss=1.992, ppl=3.98, wps=1193.9, ups=0.4, wpb=3019, bsz=72, num_updates=1822, lr=2.1864e-05, gnorm=1.909, train_wall=5, gb_free=16, wall=6725
2024-08-11 18:47:03 | INFO | train_inner | epoch 029:     60 / 63 loss=5.724, nll_loss=2.153, ppl=4.45, wps=1152.2, ups=0.36, wpb=3161.5, bsz=188, num_updates=1824, lr=2.1888e-05, gnorm=1.87, train_wall=5, gb_free=10.5, wall=6731
2024-08-11 18:47:09 | INFO | train_inner | epoch 029:     62 / 63 loss=5.688, nll_loss=2.09, ppl=4.26, wps=1151.3, ups=0.38, wpb=3008, bsz=144, num_updates=1826, lr=2.1912e-05, gnorm=1.769, train_wall=5, gb_free=11.3, wall=6736
2024-08-11 18:47:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26380.05859375Mb; avail=228686.14453125Mb
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000532
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26380.05859375Mb; avail=228686.14453125Mb
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004720
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26380.05859375Mb; avail=228686.14453125Mb
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004003
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009580
2024-08-11 18:47:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26380.0546875Mb; avail=228686.14453125Mb
2024-08-11 18:47:18 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.209 | nll_loss 2.525 | ppl 5.76 | wps 3058.1 | wpb 1463.4 | bsz 64.9 | num_updates 1827 | best_loss 6.197
2024-08-11 18:47:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1827 updates
2024-08-11 18:47:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 18:47:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 18:47:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 29 @ 1827 updates, score 6.209) (writing took 38.374880047980696 seconds)
2024-08-11 18:47:57 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-08-11 18:47:57 | INFO | train | epoch 029 | loss 5.711 | nll_loss 2.118 | ppl 4.34 | wps 1013.6 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 1827 | lr 2.1924e-05 | gnorm 1.856 | train_wall 164 | gb_free 18.8 | wall 6784
2024-08-11 18:47:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:47:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:47:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:47:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000761
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28742.16015625Mb; avail=226324.04296875Mb
2024-08-11 18:47:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000179
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001286
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28742.16015625Mb; avail=226324.04296875Mb
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28742.16015625Mb; avail=226324.04296875Mb
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000418
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002028
2024-08-11 18:47:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28742.16015625Mb; avail=226324.04296875Mb
2024-08-11 18:47:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:47:57 | INFO | fairseq.trainer | begin training epoch 30
2024-08-11 18:47:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:47:59 | INFO | train_inner | epoch 030:      1 / 63 loss=5.684, nll_loss=2.063, ppl=4.18, wps=72.1, ups=0.04, wpb=1817.5, bsz=52, num_updates=1828, lr=2.1936e-05, gnorm=2.593, train_wall=4, gb_free=11.9, wall=6786
2024-08-11 18:48:04 | INFO | train_inner | epoch 030:      3 / 63 loss=5.746, nll_loss=2.151, ppl=4.44, wps=1468.9, ups=0.44, wpb=3364.5, bsz=112, num_updates=1830, lr=2.196e-05, gnorm=2.009, train_wall=5, gb_free=15.8, wall=6791
2024-08-11 18:48:09 | INFO | train_inner | epoch 030:      5 / 63 loss=5.718, nll_loss=2.145, ppl=4.42, wps=1507, ups=0.41, wpb=3678, bsz=136, num_updates=1832, lr=2.1984e-05, gnorm=1.823, train_wall=5, gb_free=12.3, wall=6796
2024-08-11 18:48:14 | INFO | train_inner | epoch 030:      7 / 63 loss=5.612, nll_loss=1.979, ppl=3.94, wps=1474.9, ups=0.36, wpb=4116, bsz=148, num_updates=1834, lr=2.2008e-05, gnorm=1.579, train_wall=6, gb_free=11.5, wall=6802
2024-08-11 18:48:19 | INFO | train_inner | epoch 030:      9 / 63 loss=5.676, nll_loss=2.065, ppl=4.19, wps=1395.1, ups=0.38, wpb=3638.5, bsz=152, num_updates=1836, lr=2.2032e-05, gnorm=1.764, train_wall=5, gb_free=10.8, wall=6807
2024-08-11 18:48:25 | INFO | train_inner | epoch 030:     11 / 63 loss=5.619, nll_loss=1.982, ppl=3.95, wps=1065.3, ups=0.36, wpb=2925.5, bsz=120, num_updates=1838, lr=2.2056e-05, gnorm=2.118, train_wall=5, gb_free=9.1, wall=6812
2024-08-11 18:48:30 | INFO | train_inner | epoch 030:     13 / 63 loss=5.738, nll_loss=2.164, ppl=4.48, wps=1617.6, ups=0.37, wpb=4412.5, bsz=228, num_updates=1840, lr=2.208e-05, gnorm=1.547, train_wall=5, gb_free=12.5, wall=6818
2024-08-11 18:48:36 | INFO | train_inner | epoch 030:     15 / 63 loss=5.689, nll_loss=2.087, ppl=4.25, wps=1340.1, ups=0.39, wpb=3463.5, bsz=156, num_updates=1842, lr=2.2104e-05, gnorm=1.813, train_wall=5, gb_free=11.4, wall=6823
2024-08-11 18:48:41 | INFO | train_inner | epoch 030:     17 / 63 loss=5.578, nll_loss=1.951, ppl=3.87, wps=1532.8, ups=0.4, wpb=3860, bsz=212, num_updates=1844, lr=2.2128e-05, gnorm=1.578, train_wall=5, gb_free=11.9, wall=6828
2024-08-11 18:48:46 | INFO | train_inner | epoch 030:     19 / 63 loss=5.565, nll_loss=1.921, ppl=3.79, wps=1195.7, ups=0.36, wpb=3350, bsz=128, num_updates=1846, lr=2.2152e-05, gnorm=1.685, train_wall=6, gb_free=9, wall=6833
2024-08-11 18:48:51 | INFO | train_inner | epoch 030:     21 / 63 loss=5.74, nll_loss=2.151, ppl=4.44, wps=1365.8, ups=0.38, wpb=3554.5, bsz=136, num_updates=1848, lr=2.2176e-05, gnorm=1.733, train_wall=5, gb_free=13.7, wall=6839
2024-08-11 18:49:02 | INFO | train_inner | epoch 030:     23 / 63 loss=5.677, nll_loss=2.068, ppl=4.19, wps=642, ups=0.19, wpb=3411.5, bsz=124, num_updates=1850, lr=2.22e-05, gnorm=1.802, train_wall=11, gb_free=10.4, wall=6849
2024-08-11 18:49:07 | INFO | train_inner | epoch 030:     25 / 63 loss=5.693, nll_loss=2.101, ppl=4.29, wps=1337.4, ups=0.42, wpb=3200.5, bsz=124, num_updates=1852, lr=2.2224e-05, gnorm=1.791, train_wall=5, gb_free=13.2, wall=6854
2024-08-11 18:49:12 | INFO | train_inner | epoch 030:     27 / 63 loss=5.636, nll_loss=2.025, ppl=4.07, wps=1263.4, ups=0.38, wpb=3285, bsz=160, num_updates=1854, lr=2.2248e-05, gnorm=1.709, train_wall=5, gb_free=11.4, wall=6859
2024-08-11 18:49:17 | INFO | train_inner | epoch 030:     29 / 63 loss=5.638, nll_loss=1.995, ppl=3.99, wps=1190.8, ups=0.38, wpb=3173.5, bsz=128, num_updates=1856, lr=2.2272e-05, gnorm=1.779, train_wall=5, gb_free=12.6, wall=6865
2024-08-11 18:49:23 | INFO | train_inner | epoch 030:     31 / 63 loss=5.429, nll_loss=1.776, ppl=3.43, wps=1234.4, ups=0.35, wpb=3556, bsz=256, num_updates=1858, lr=2.2296e-05, gnorm=1.456, train_wall=6, gb_free=8.9, wall=6870
2024-08-11 18:49:28 | INFO | train_inner | epoch 030:     33 / 63 loss=5.68, nll_loss=2.094, ppl=4.27, wps=1403.4, ups=0.42, wpb=3336, bsz=128, num_updates=1860, lr=2.232e-05, gnorm=1.766, train_wall=5, gb_free=11.9, wall=6875
2024-08-11 18:49:33 | INFO | train_inner | epoch 030:     35 / 63 loss=5.63, nll_loss=2.039, ppl=4.11, wps=1452.5, ups=0.37, wpb=3914, bsz=184, num_updates=1862, lr=2.2344e-05, gnorm=1.536, train_wall=5, gb_free=12.2, wall=6881
2024-08-11 18:49:38 | INFO | train_inner | epoch 030:     37 / 63 loss=5.649, nll_loss=2.075, ppl=4.21, wps=1214.4, ups=0.38, wpb=3162, bsz=168, num_updates=1864, lr=2.2368e-05, gnorm=1.812, train_wall=5, gb_free=12.2, wall=6886
2024-08-11 18:49:43 | INFO | train_inner | epoch 030:     39 / 63 loss=5.591, nll_loss=1.96, ppl=3.89, wps=1179.3, ups=0.4, wpb=2957, bsz=144, num_updates=1866, lr=2.2392e-05, gnorm=1.795, train_wall=5, gb_free=12.6, wall=6891
2024-08-11 18:49:47 | INFO | train_inner | epoch 030:     41 / 63 loss=5.717, nll_loss=2.097, ppl=4.28, wps=1485.4, ups=0.52, wpb=2865, bsz=132.5, num_updates=1868, lr=2.2416e-05, gnorm=2.208, train_wall=4, gb_free=12, wall=6895
2024-08-11 18:49:52 | INFO | train_inner | epoch 030:     43 / 63 loss=5.741, nll_loss=2.122, ppl=4.35, wps=1318.9, ups=0.45, wpb=2952.5, bsz=136, num_updates=1870, lr=2.244e-05, gnorm=2.293, train_wall=4, gb_free=13, wall=6899
2024-08-11 18:49:57 | INFO | train_inner | epoch 030:     45 / 63 loss=5.72, nll_loss=2.123, ppl=4.36, wps=1448.5, ups=0.4, wpb=3666.5, bsz=192, num_updates=1872, lr=2.2464e-05, gnorm=1.725, train_wall=5, gb_free=10.8, wall=6904
2024-08-11 18:50:02 | INFO | train_inner | epoch 030:     47 / 63 loss=5.74, nll_loss=2.166, ppl=4.49, wps=1339.6, ups=0.38, wpb=3550.5, bsz=152, num_updates=1874, lr=2.2488e-05, gnorm=1.834, train_wall=5, gb_free=11.1, wall=6909
2024-08-11 18:50:07 | INFO | train_inner | epoch 030:     49 / 63 loss=5.708, nll_loss=2.114, ppl=4.33, wps=1243.8, ups=0.41, wpb=3060.5, bsz=84, num_updates=1876, lr=2.2512e-05, gnorm=1.957, train_wall=5, gb_free=12, wall=6914
2024-08-11 18:50:12 | INFO | train_inner | epoch 030:     51 / 63 loss=5.55, nll_loss=1.903, ppl=3.74, wps=1257, ups=0.38, wpb=3270.5, bsz=112, num_updates=1878, lr=2.2536e-05, gnorm=1.735, train_wall=5, gb_free=13.5, wall=6920
2024-08-11 18:50:17 | INFO | train_inner | epoch 030:     53 / 63 loss=5.667, nll_loss=2.037, ppl=4.1, wps=1453.8, ups=0.39, wpb=3732.5, bsz=140, num_updates=1880, lr=2.256e-05, gnorm=1.636, train_wall=5, gb_free=13.3, wall=6925
2024-08-11 18:50:23 | INFO | train_inner | epoch 030:     55 / 63 loss=5.691, nll_loss=2.092, ppl=4.26, wps=1450.8, ups=0.37, wpb=3917, bsz=204, num_updates=1882, lr=2.2584e-05, gnorm=1.634, train_wall=5, gb_free=12.9, wall=6930
2024-08-11 18:50:28 | INFO | train_inner | epoch 030:     57 / 63 loss=5.687, nll_loss=2.098, ppl=4.28, wps=1522, ups=0.39, wpb=3892.5, bsz=220, num_updates=1884, lr=2.2608e-05, gnorm=1.673, train_wall=5, gb_free=11.5, wall=6935
2024-08-11 18:50:33 | INFO | train_inner | epoch 030:     59 / 63 loss=5.635, nll_loss=2.015, ppl=4.04, wps=1345.3, ups=0.4, wpb=3348.5, bsz=104, num_updates=1886, lr=2.2632e-05, gnorm=1.846, train_wall=5, gb_free=12.8, wall=6940
2024-08-11 18:50:38 | INFO | train_inner | epoch 030:     61 / 63 loss=5.571, nll_loss=1.954, ppl=3.88, wps=1318.2, ups=0.4, wpb=3315.5, bsz=148, num_updates=1888, lr=2.2656e-05, gnorm=1.712, train_wall=5, gb_free=15.6, wall=6945
2024-08-11 18:50:42 | INFO | train_inner | epoch 030:     63 / 63 loss=5.51, nll_loss=1.869, ppl=3.65, wps=1040.7, ups=0.53, wpb=1964, bsz=64, num_updates=1890, lr=2.268e-05, gnorm=2.635, train_wall=4, gb_free=17.8, wall=6949
2024-08-11 18:50:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23230.25390625Mb; avail=231835.94921875Mb
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000536
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23230.25390625Mb; avail=231835.94921875Mb
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004764
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23230.25390625Mb; avail=231835.94921875Mb
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004051
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009758
2024-08-11 18:50:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23230.25390625Mb; avail=231835.94921875Mb
2024-08-11 18:50:50 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.192 | nll_loss 2.519 | ppl 5.73 | wps 3059 | wpb 1463.4 | bsz 64.9 | num_updates 1890 | best_loss 6.192
2024-08-11 18:50:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1890 updates
2024-08-11 18:50:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:51:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:51:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 30 @ 1890 updates, score 6.192) (writing took 62.21797908702865 seconds)
2024-08-11 18:51:52 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-08-11 18:51:52 | INFO | train | epoch 030 | loss 5.655 | nll_loss 2.046 | ppl 4.13 | wps 909 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1890 | lr 2.268e-05 | gnorm 1.81 | train_wall 165 | gb_free 17.8 | wall 7020
2024-08-11 18:51:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:51:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:51:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:51:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000742
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16593.2109375Mb; avail=238473.03125Mb
2024-08-11 18:51:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000158
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001280
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16593.2109375Mb; avail=238473.03125Mb
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16593.2109375Mb; avail=238473.03125Mb
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000454
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002081
2024-08-11 18:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16593.2109375Mb; avail=238473.03125Mb
2024-08-11 18:51:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:51:52 | INFO | fairseq.trainer | begin training epoch 31
2024-08-11 18:51:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:51:57 | INFO | train_inner | epoch 031:      2 / 63 loss=5.564, nll_loss=1.943, ppl=3.84, wps=91.3, ups=0.03, wpb=3442, bsz=172, num_updates=1892, lr=2.2704e-05, gnorm=1.607, train_wall=5, gb_free=11.3, wall=7024
2024-08-11 18:52:02 | INFO | train_inner | epoch 031:      4 / 63 loss=5.625, nll_loss=2, ppl=4, wps=1419.4, ups=0.4, wpb=3531.5, bsz=152, num_updates=1894, lr=2.2728e-05, gnorm=1.651, train_wall=5, gb_free=14.1, wall=7029
2024-08-11 18:52:07 | INFO | train_inner | epoch 031:      6 / 63 loss=5.763, nll_loss=2.176, ppl=4.52, wps=1673.7, ups=0.39, wpb=4341, bsz=200, num_updates=1896, lr=2.2752e-05, gnorm=1.89, train_wall=5, gb_free=10.9, wall=7035
2024-08-11 18:52:12 | INFO | train_inner | epoch 031:      8 / 63 loss=5.586, nll_loss=1.945, ppl=3.85, wps=1449.6, ups=0.4, wpb=3665, bsz=160, num_updates=1898, lr=2.2776e-05, gnorm=1.759, train_wall=5, gb_free=9.2, wall=7040
2024-08-11 18:52:16 | INFO | train_inner | epoch 031:     10 / 63 loss=5.668, nll_loss=2.046, ppl=4.13, wps=1323.6, ups=0.51, wpb=2608, bsz=64.5, num_updates=1900, lr=2.28e-05, gnorm=2.31, train_wall=4, gb_free=11.3, wall=7044
2024-08-11 18:52:21 | INFO | train_inner | epoch 031:     12 / 63 loss=5.623, nll_loss=2.01, ppl=4.03, wps=1379.3, ups=0.41, wpb=3369, bsz=152, num_updates=1902, lr=2.2824e-05, gnorm=1.725, train_wall=5, gb_free=13, wall=7048
2024-08-11 18:52:26 | INFO | train_inner | epoch 031:     14 / 63 loss=5.612, nll_loss=1.979, ppl=3.94, wps=1241.3, ups=0.39, wpb=3162.5, bsz=84, num_updates=1904, lr=2.2848e-05, gnorm=1.851, train_wall=5, gb_free=10.8, wall=7054
2024-08-11 18:52:31 | INFO | train_inner | epoch 031:     16 / 63 loss=5.623, nll_loss=1.997, ppl=3.99, wps=1401.3, ups=0.47, wpb=2974, bsz=100, num_updates=1906, lr=2.2872e-05, gnorm=2.088, train_wall=4, gb_free=12.3, wall=7058
2024-08-11 18:52:36 | INFO | train_inner | epoch 031:     18 / 63 loss=5.608, nll_loss=2.003, ppl=4.01, wps=1388.7, ups=0.37, wpb=3714.5, bsz=192, num_updates=1908, lr=2.2896e-05, gnorm=1.776, train_wall=5, gb_free=12.4, wall=7063
2024-08-11 18:52:41 | INFO | train_inner | epoch 031:     20 / 63 loss=5.57, nll_loss=1.933, ppl=3.82, wps=1259.7, ups=0.4, wpb=3125, bsz=112, num_updates=1910, lr=2.292e-05, gnorm=1.858, train_wall=5, gb_free=13.1, wall=7068
2024-08-11 18:52:45 | INFO | train_inner | epoch 031:     22 / 63 loss=5.641, nll_loss=2.01, ppl=4.03, wps=1350.8, ups=0.43, wpb=3150.5, bsz=104, num_updates=1912, lr=2.2944e-05, gnorm=1.909, train_wall=5, gb_free=17.1, wall=7073
2024-08-11 18:52:51 | INFO | train_inner | epoch 031:     24 / 63 loss=5.681, nll_loss=2.038, ppl=4.11, wps=1327, ups=0.37, wpb=3590, bsz=120, num_updates=1914, lr=2.2968e-05, gnorm=1.813, train_wall=5, gb_free=11.9, wall=7078
2024-08-11 18:52:56 | INFO | train_inner | epoch 031:     26 / 63 loss=5.683, nll_loss=2.074, ppl=4.21, wps=1379.9, ups=0.37, wpb=3701.5, bsz=168, num_updates=1916, lr=2.2992e-05, gnorm=1.675, train_wall=5, gb_free=9.9, wall=7084
2024-08-11 18:53:02 | INFO | train_inner | epoch 031:     28 / 63 loss=5.623, nll_loss=2.021, ppl=4.06, wps=1453.8, ups=0.37, wpb=3978, bsz=192, num_updates=1918, lr=2.3016e-05, gnorm=1.504, train_wall=5, gb_free=11.9, wall=7089
2024-08-11 18:53:07 | INFO | train_inner | epoch 031:     30 / 63 loss=5.573, nll_loss=1.942, ppl=3.84, wps=1268.9, ups=0.39, wpb=3268, bsz=116, num_updates=1920, lr=2.304e-05, gnorm=1.938, train_wall=5, gb_free=10.5, wall=7094
2024-08-11 18:53:17 | INFO | train_inner | epoch 031:     32 / 63 loss=5.506, nll_loss=1.877, ppl=3.67, wps=740.7, ups=0.19, wpb=3917, bsz=216, num_updates=1922, lr=2.3064e-05, gnorm=1.499, train_wall=11, gb_free=11.1, wall=7105
2024-08-11 18:53:23 | INFO | train_inner | epoch 031:     34 / 63 loss=5.625, nll_loss=2.006, ppl=4.02, wps=1315.8, ups=0.38, wpb=3493.5, bsz=112, num_updates=1924, lr=2.3088e-05, gnorm=1.718, train_wall=5, gb_free=14.2, wall=7110
2024-08-11 18:53:27 | INFO | train_inner | epoch 031:     36 / 63 loss=5.634, nll_loss=1.987, ppl=3.97, wps=1391.8, ups=0.42, wpb=3277, bsz=92, num_updates=1926, lr=2.3112e-05, gnorm=1.806, train_wall=5, gb_free=15.6, wall=7115
2024-08-11 18:53:33 | INFO | train_inner | epoch 031:     38 / 63 loss=5.513, nll_loss=1.885, ppl=3.69, wps=1364.6, ups=0.38, wpb=3606.5, bsz=268, num_updates=1928, lr=2.3136e-05, gnorm=1.551, train_wall=5, gb_free=10.8, wall=7120
2024-08-11 18:53:39 | INFO | train_inner | epoch 031:     40 / 63 loss=5.457, nll_loss=1.84, ppl=3.58, wps=1349.8, ups=0.35, wpb=3907.5, bsz=316, num_updates=1930, lr=2.316e-05, gnorm=1.452, train_wall=6, gb_free=11.5, wall=7126
2024-08-11 18:53:44 | INFO | train_inner | epoch 031:     42 / 63 loss=5.537, nll_loss=1.93, ppl=3.81, wps=1367, ups=0.38, wpb=3553.5, bsz=244, num_updates=1932, lr=2.3184e-05, gnorm=1.635, train_wall=5, gb_free=13.8, wall=7131
2024-08-11 18:53:49 | INFO | train_inner | epoch 031:     44 / 63 loss=5.587, nll_loss=1.976, ppl=3.93, wps=1586.4, ups=0.38, wpb=4196, bsz=244, num_updates=1934, lr=2.3208e-05, gnorm=1.572, train_wall=5, gb_free=14.1, wall=7136
2024-08-11 18:53:54 | INFO | train_inner | epoch 031:     46 / 63 loss=5.726, nll_loss=2.126, ppl=4.37, wps=1483, ups=0.41, wpb=3621, bsz=108, num_updates=1936, lr=2.3232e-05, gnorm=1.902, train_wall=5, gb_free=12.7, wall=7141
2024-08-11 18:53:59 | INFO | train_inner | epoch 031:     48 / 63 loss=5.738, nll_loss=2.151, ppl=4.44, wps=1420, ups=0.38, wpb=3749.5, bsz=116, num_updates=1938, lr=2.3256e-05, gnorm=1.84, train_wall=5, gb_free=12.5, wall=7147
2024-08-11 18:54:04 | INFO | train_inner | epoch 031:     50 / 63 loss=5.562, nll_loss=1.919, ppl=3.78, wps=1156.1, ups=0.39, wpb=2985.5, bsz=104, num_updates=1940, lr=2.328e-05, gnorm=1.843, train_wall=5, gb_free=12.4, wall=7152
2024-08-11 18:54:09 | INFO | train_inner | epoch 031:     52 / 63 loss=5.588, nll_loss=1.965, ppl=3.9, wps=1175.9, ups=0.43, wpb=2708.5, bsz=88, num_updates=1942, lr=2.3304e-05, gnorm=2.047, train_wall=5, gb_free=13.6, wall=7156
2024-08-11 18:54:14 | INFO | train_inner | epoch 031:     54 / 63 loss=5.569, nll_loss=1.923, ppl=3.79, wps=1211.4, ups=0.4, wpb=3008.5, bsz=120, num_updates=1944, lr=2.3328e-05, gnorm=1.849, train_wall=5, gb_free=14, wall=7161
2024-08-11 18:54:19 | INFO | train_inner | epoch 031:     56 / 63 loss=5.65, nll_loss=2.013, ppl=4.04, wps=1251.4, ups=0.37, wpb=3370.5, bsz=112, num_updates=1946, lr=2.3352e-05, gnorm=1.959, train_wall=5, gb_free=10.4, wall=7167
2024-08-11 18:54:25 | INFO | train_inner | epoch 031:     58 / 63 loss=5.587, nll_loss=1.951, ppl=3.87, wps=1110, ups=0.37, wpb=2996.5, bsz=148, num_updates=1948, lr=2.3376e-05, gnorm=1.809, train_wall=5, gb_free=11.3, wall=7172
2024-08-11 18:54:30 | INFO | train_inner | epoch 031:     60 / 63 loss=5.568, nll_loss=1.928, ppl=3.8, wps=1304.2, ups=0.38, wpb=3389, bsz=120, num_updates=1950, lr=2.34e-05, gnorm=1.681, train_wall=5, gb_free=11.9, wall=7177
2024-08-11 18:54:35 | INFO | train_inner | epoch 031:     62 / 63 loss=5.516, nll_loss=1.886, ppl=3.7, wps=1246.2, ups=0.41, wpb=3066, bsz=120, num_updates=1952, lr=2.3424e-05, gnorm=1.9, train_wall=5, gb_free=11.7, wall=7182
2024-08-11 18:54:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21030.19140625Mb; avail=234036.01171875Mb
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000534
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21030.19140625Mb; avail=234036.01171875Mb
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004717
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21030.19140625Mb; avail=234036.01171875Mb
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004042
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009621
2024-08-11 18:54:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21030.19140625Mb; avail=234036.01171875Mb
2024-08-11 18:54:45 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.183 | nll_loss 2.518 | ppl 5.73 | wps 3055.9 | wpb 1463.4 | bsz 64.9 | num_updates 1953 | best_loss 6.183
2024-08-11 18:54:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1953 updates
2024-08-11 18:54:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:55:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:55:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 31 @ 1953 updates, score 6.183) (writing took 62.98673215182498 seconds)
2024-08-11 18:55:48 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-08-11 18:55:48 | INFO | train | epoch 031 | loss 5.607 | nll_loss 1.985 | ppl 3.96 | wps 910.6 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 1953 | lr 2.3436e-05 | gnorm 1.799 | train_wall 163 | gb_free 16.7 | wall 7255
2024-08-11 18:55:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:55:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:55:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:55:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000830
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31498.1015625Mb; avail=223568.140625Mb
2024-08-11 18:55:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000199
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001422
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31501.0546875Mb; avail=223565.1875Mb
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000061
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31501.546875Mb; avail=223564.6953125Mb
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000461
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002242
2024-08-11 18:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31503.0234375Mb; avail=223563.21875Mb
2024-08-11 18:55:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:55:48 | INFO | fairseq.trainer | begin training epoch 32
2024-08-11 18:55:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:55:50 | INFO | train_inner | epoch 032:      1 / 63 loss=5.467, nll_loss=1.812, ppl=3.51, wps=64.1, ups=0.03, wpb=2423, bsz=96, num_updates=1954, lr=2.3448e-05, gnorm=2.098, train_wall=4, gb_free=10.4, wall=7258
2024-08-11 18:55:55 | INFO | train_inner | epoch 032:      3 / 63 loss=5.604, nll_loss=1.981, ppl=3.95, wps=1455.3, ups=0.44, wpb=3309, bsz=100, num_updates=1956, lr=2.3472e-05, gnorm=1.794, train_wall=5, gb_free=12.4, wall=7262
2024-08-11 18:56:00 | INFO | train_inner | epoch 032:      5 / 63 loss=5.491, nll_loss=1.859, ppl=3.63, wps=1528.7, ups=0.38, wpb=4022.5, bsz=260, num_updates=1958, lr=2.3496e-05, gnorm=1.522, train_wall=5, gb_free=11.9, wall=7268
2024-08-11 18:56:05 | INFO | train_inner | epoch 032:      7 / 63 loss=5.523, nll_loss=1.877, ppl=3.67, wps=1441, ups=0.41, wpb=3527.5, bsz=148, num_updates=1960, lr=2.352e-05, gnorm=1.656, train_wall=5, gb_free=12.4, wall=7273
2024-08-11 18:56:11 | INFO | train_inner | epoch 032:      9 / 63 loss=5.585, nll_loss=1.943, ppl=3.84, wps=1652.2, ups=0.37, wpb=4496.5, bsz=216, num_updates=1962, lr=2.3544e-05, gnorm=1.514, train_wall=5, gb_free=11.1, wall=7278
2024-08-11 18:56:16 | INFO | train_inner | epoch 032:     11 / 63 loss=5.445, nll_loss=1.762, ppl=3.39, wps=1347.2, ups=0.37, wpb=3627, bsz=196, num_updates=1964, lr=2.3568e-05, gnorm=1.629, train_wall=5, gb_free=13.9, wall=7283
2024-08-11 18:56:21 | INFO | train_inner | epoch 032:     13 / 63 loss=5.594, nll_loss=1.967, ppl=3.91, wps=1457.2, ups=0.39, wpb=3781.5, bsz=160, num_updates=1966, lr=2.3592e-05, gnorm=1.721, train_wall=5, gb_free=11.1, wall=7289
2024-08-11 18:56:26 | INFO | train_inner | epoch 032:     15 / 63 loss=5.518, nll_loss=1.883, ppl=3.69, wps=1291.1, ups=0.42, wpb=3064, bsz=128, num_updates=1968, lr=2.3616e-05, gnorm=1.748, train_wall=5, gb_free=12.8, wall=7293
2024-08-11 18:56:36 | INFO | train_inner | epoch 032:     17 / 63 loss=5.445, nll_loss=1.794, ppl=3.47, wps=685, ups=0.2, wpb=3445, bsz=124, num_updates=1970, lr=2.364e-05, gnorm=1.953, train_wall=10, gb_free=12.7, wall=7303
2024-08-11 18:56:41 | INFO | train_inner | epoch 032:     19 / 63 loss=5.53, nll_loss=1.882, ppl=3.69, wps=1393.6, ups=0.37, wpb=3772.5, bsz=172, num_updates=1972, lr=2.3664e-05, gnorm=1.614, train_wall=5, gb_free=10.7, wall=7309
2024-08-11 18:56:46 | INFO | train_inner | epoch 032:     21 / 63 loss=5.516, nll_loss=1.843, ppl=3.59, wps=1370.2, ups=0.39, wpb=3477.5, bsz=124, num_updates=1974, lr=2.3688e-05, gnorm=1.627, train_wall=5, gb_free=12, wall=7314
2024-08-11 18:56:51 | INFO | train_inner | epoch 032:     23 / 63 loss=5.645, nll_loss=2.022, ppl=4.06, wps=1161.6, ups=0.41, wpb=2841, bsz=104, num_updates=1976, lr=2.3712e-05, gnorm=1.931, train_wall=5, gb_free=11.5, wall=7319
2024-08-11 18:56:56 | INFO | train_inner | epoch 032:     25 / 63 loss=5.672, nll_loss=2.071, ppl=4.2, wps=1240, ups=0.39, wpb=3169, bsz=116, num_updates=1978, lr=2.3736e-05, gnorm=2.006, train_wall=5, gb_free=12.9, wall=7324
2024-08-11 18:57:02 | INFO | train_inner | epoch 032:     27 / 63 loss=5.354, nll_loss=1.678, ppl=3.2, wps=1319.5, ups=0.36, wpb=3622, bsz=224, num_updates=1980, lr=2.376e-05, gnorm=1.576, train_wall=5, gb_free=11.3, wall=7329
2024-08-11 18:57:07 | INFO | train_inner | epoch 032:     29 / 63 loss=5.525, nll_loss=1.871, ppl=3.66, wps=1167.3, ups=0.38, wpb=3071.5, bsz=84, num_updates=1982, lr=2.3784e-05, gnorm=1.774, train_wall=5, gb_free=14.8, wall=7335
2024-08-11 18:57:12 | INFO | train_inner | epoch 032:     31 / 63 loss=5.719, nll_loss=2.117, ppl=4.34, wps=1475, ups=0.39, wpb=3768, bsz=124, num_updates=1984, lr=2.3808e-05, gnorm=1.824, train_wall=5, gb_free=13.1, wall=7340
2024-08-11 18:57:17 | INFO | train_inner | epoch 032:     33 / 63 loss=5.596, nll_loss=1.963, ppl=3.9, wps=1168.8, ups=0.4, wpb=2940.5, bsz=156, num_updates=1986, lr=2.3832e-05, gnorm=2.04, train_wall=5, gb_free=11.5, wall=7345
2024-08-11 18:57:22 | INFO | train_inner | epoch 032:     35 / 63 loss=5.38, nll_loss=1.675, ppl=3.19, wps=1070.3, ups=0.42, wpb=2528.5, bsz=108, num_updates=1988, lr=2.3856e-05, gnorm=1.843, train_wall=5, gb_free=11.5, wall=7349
2024-08-11 18:57:27 | INFO | train_inner | epoch 032:     37 / 63 loss=5.523, nll_loss=1.864, ppl=3.64, wps=1494.9, ups=0.39, wpb=3785, bsz=128, num_updates=1990, lr=2.388e-05, gnorm=1.594, train_wall=5, gb_free=12.9, wall=7355
2024-08-11 18:57:33 | INFO | train_inner | epoch 032:     39 / 63 loss=5.672, nll_loss=2.093, ppl=4.27, wps=1602.2, ups=0.37, wpb=4371, bsz=228, num_updates=1992, lr=2.3904e-05, gnorm=1.578, train_wall=5, gb_free=12.1, wall=7360
2024-08-11 18:57:38 | INFO | train_inner | epoch 032:     41 / 63 loss=5.486, nll_loss=1.867, ppl=3.65, wps=1419.2, ups=0.38, wpb=3736, bsz=212, num_updates=1994, lr=2.3928e-05, gnorm=1.637, train_wall=5, gb_free=13.9, wall=7365
2024-08-11 18:57:43 | INFO | train_inner | epoch 032:     43 / 63 loss=5.393, nll_loss=1.709, ppl=3.27, wps=1269.1, ups=0.37, wpb=3403.5, bsz=180, num_updates=1996, lr=2.3952e-05, gnorm=1.631, train_wall=5, gb_free=12.1, wall=7371
2024-08-11 18:57:48 | INFO | train_inner | epoch 032:     45 / 63 loss=5.5, nll_loss=1.838, ppl=3.57, wps=1250.9, ups=0.39, wpb=3192, bsz=160, num_updates=1998, lr=2.3976e-05, gnorm=1.742, train_wall=5, gb_free=12.9, wall=7376
2024-08-11 18:57:53 | INFO | train_inner | epoch 032:     47 / 63 loss=5.458, nll_loss=1.776, ppl=3.42, wps=1071.3, ups=0.39, wpb=2743, bsz=80, num_updates=2000, lr=2.4e-05, gnorm=1.872, train_wall=5, gb_free=10.5, wall=7381
2024-08-11 18:57:59 | INFO | train_inner | epoch 032:     49 / 63 loss=5.439, nll_loss=1.792, ppl=3.46, wps=1279.5, ups=0.39, wpb=3262.5, bsz=184, num_updates=2002, lr=2.4024e-05, gnorm=1.628, train_wall=5, gb_free=11.7, wall=7386
2024-08-11 18:58:04 | INFO | train_inner | epoch 032:     51 / 63 loss=5.42, nll_loss=1.765, ppl=3.4, wps=1188.2, ups=0.38, wpb=3120.5, bsz=164, num_updates=2004, lr=2.4048e-05, gnorm=1.573, train_wall=5, gb_free=11.9, wall=7391
2024-08-11 18:58:09 | INFO | train_inner | epoch 032:     53 / 63 loss=5.603, nll_loss=1.996, ppl=3.99, wps=1151.5, ups=0.39, wpb=2954.5, bsz=116, num_updates=2006, lr=2.4072e-05, gnorm=1.817, train_wall=5, gb_free=12.1, wall=7396
2024-08-11 18:58:14 | INFO | train_inner | epoch 032:     55 / 63 loss=5.519, nll_loss=1.872, ppl=3.66, wps=1505.4, ups=0.41, wpb=3692, bsz=176, num_updates=2008, lr=2.4096e-05, gnorm=1.622, train_wall=5, gb_free=12.3, wall=7401
2024-08-11 18:58:19 | INFO | train_inner | epoch 032:     57 / 63 loss=5.573, nll_loss=1.91, ppl=3.76, wps=1268.7, ups=0.37, wpb=3455.5, bsz=100, num_updates=2010, lr=2.412e-05, gnorm=1.774, train_wall=5, gb_free=12.6, wall=7407
2024-08-11 18:58:25 | INFO | train_inner | epoch 032:     59 / 63 loss=5.575, nll_loss=1.949, ppl=3.86, wps=1435.5, ups=0.38, wpb=3761, bsz=168, num_updates=2012, lr=2.4144e-05, gnorm=1.557, train_wall=5, gb_free=13.3, wall=7412
2024-08-11 18:58:29 | INFO | train_inner | epoch 032:     61 / 63 loss=5.648, nll_loss=2.045, ppl=4.13, wps=1235.9, ups=0.45, wpb=2725, bsz=100.5, num_updates=2014, lr=2.4168e-05, gnorm=2.052, train_wall=4, gb_free=16.2, wall=7416
2024-08-11 18:58:33 | INFO | train_inner | epoch 032:     63 / 63 loss=5.499, nll_loss=1.848, ppl=3.6, wps=1354.6, ups=0.5, wpb=2689.5, bsz=84, num_updates=2016, lr=2.4192e-05, gnorm=1.821, train_wall=4, gb_free=17.6, wall=7420
2024-08-11 18:58:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21010.71484375Mb; avail=234055.48828125Mb
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000529
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21010.71484375Mb; avail=234055.48828125Mb
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004694
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21010.71484375Mb; avail=234055.48828125Mb
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004042
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009589
2024-08-11 18:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21010.71484375Mb; avail=234055.48828125Mb
2024-08-11 18:58:41 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.175 | nll_loss 2.487 | ppl 5.61 | wps 3051.8 | wpb 1463.4 | bsz 64.9 | num_updates 2016 | best_loss 6.175
2024-08-11 18:58:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2016 updates
2024-08-11 18:58:41 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:59:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 18:59:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 32 @ 2016 updates, score 6.175) (writing took 62.820489092729986 seconds)
2024-08-11 18:59:44 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-08-11 18:59:44 | INFO | train | epoch 032 | loss 5.531 | nll_loss 1.889 | ppl 3.7 | wps 905.6 | ups 0.27 | wpb 3400.7 | bsz 148.2 | num_updates 2016 | lr 2.4192e-05 | gnorm 1.731 | train_wall 165 | gb_free 17.6 | wall 7492
2024-08-11 18:59:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 18:59:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 18:59:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 18:59:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000764
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25292.7890625Mb; avail=229773.453125Mb
2024-08-11 18:59:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000139
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001215
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25273.1015625Mb; avail=229793.140625Mb
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25273.1015625Mb; avail=229794.125Mb
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000429
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001969
2024-08-11 18:59:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25253.90625Mb; avail=229812.3359375Mb
2024-08-11 18:59:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 18:59:44 | INFO | fairseq.trainer | begin training epoch 33
2024-08-11 18:59:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 18:59:49 | INFO | train_inner | epoch 033:      2 / 63 loss=5.494, nll_loss=1.82, ppl=3.53, wps=97, ups=0.03, wpb=3711.5, bsz=156, num_updates=2018, lr=2.4216e-05, gnorm=1.642, train_wall=5, gb_free=11.7, wall=7497
2024-08-11 18:59:55 | INFO | train_inner | epoch 033:      4 / 63 loss=5.465, nll_loss=1.799, ppl=3.48, wps=1440.8, ups=0.39, wpb=3652, bsz=168, num_updates=2020, lr=2.424e-05, gnorm=1.571, train_wall=5, gb_free=13.5, wall=7502
2024-08-11 19:00:00 | INFO | train_inner | epoch 033:      6 / 63 loss=5.455, nll_loss=1.823, ppl=3.54, wps=1355.3, ups=0.38, wpb=3579, bsz=228, num_updates=2022, lr=2.4264e-05, gnorm=1.614, train_wall=5, gb_free=12.2, wall=7507
2024-08-11 19:00:05 | INFO | train_inner | epoch 033:      8 / 63 loss=5.387, nll_loss=1.716, ppl=3.29, wps=1355.2, ups=0.4, wpb=3357.5, bsz=192, num_updates=2024, lr=2.4288e-05, gnorm=1.563, train_wall=5, gb_free=12.5, wall=7512
2024-08-11 19:00:10 | INFO | train_inner | epoch 033:     10 / 63 loss=5.473, nll_loss=1.813, ppl=3.51, wps=1471.6, ups=0.37, wpb=4011, bsz=204, num_updates=2026, lr=2.4312e-05, gnorm=1.545, train_wall=5, gb_free=12.4, wall=7518
2024-08-11 19:00:21 | INFO | train_inner | epoch 033:     12 / 63 loss=5.48, nll_loss=1.8, ppl=3.48, wps=642.4, ups=0.19, wpb=3317.5, bsz=104, num_updates=2028, lr=2.4336e-05, gnorm=1.709, train_wall=10, gb_free=11.9, wall=7528
2024-08-11 19:00:25 | INFO | train_inner | epoch 033:     14 / 63 loss=5.44, nll_loss=1.764, ppl=3.4, wps=1274.1, ups=0.42, wpb=3006.5, bsz=108, num_updates=2030, lr=2.436e-05, gnorm=1.813, train_wall=5, gb_free=13.9, wall=7533
2024-08-11 19:00:30 | INFO | train_inner | epoch 033:     16 / 63 loss=5.489, nll_loss=1.847, ppl=3.6, wps=1243.9, ups=0.4, wpb=3107.5, bsz=144, num_updates=2032, lr=2.4384e-05, gnorm=1.674, train_wall=5, gb_free=12.2, wall=7538
2024-08-11 19:00:36 | INFO | train_inner | epoch 033:     18 / 63 loss=5.351, nll_loss=1.682, ppl=3.21, wps=1352.8, ups=0.38, wpb=3539.5, bsz=204, num_updates=2034, lr=2.4408e-05, gnorm=1.604, train_wall=5, gb_free=11.1, wall=7543
2024-08-11 19:00:40 | INFO | train_inner | epoch 033:     20 / 63 loss=5.525, nll_loss=1.883, ppl=3.69, wps=1469.6, ups=0.42, wpb=3511, bsz=140, num_updates=2036, lr=2.4432e-05, gnorm=1.706, train_wall=5, gb_free=12.7, wall=7548
2024-08-11 19:00:46 | INFO | train_inner | epoch 033:     22 / 63 loss=5.477, nll_loss=1.812, ppl=3.51, wps=1393.7, ups=0.38, wpb=3653.5, bsz=184, num_updates=2038, lr=2.4456e-05, gnorm=1.6, train_wall=5, gb_free=11.4, wall=7553
2024-08-11 19:00:50 | INFO | train_inner | epoch 033:     24 / 63 loss=5.463, nll_loss=1.807, ppl=3.5, wps=1438.3, ups=0.47, wpb=3029, bsz=148.5, num_updates=2040, lr=2.448e-05, gnorm=2.068, train_wall=4, gb_free=14.1, wall=7557
2024-08-11 19:00:55 | INFO | train_inner | epoch 033:     26 / 63 loss=5.545, nll_loss=1.886, ppl=3.7, wps=1592.3, ups=0.4, wpb=3935, bsz=192, num_updates=2042, lr=2.4504e-05, gnorm=1.683, train_wall=5, gb_free=9.7, wall=7562
2024-08-11 19:01:00 | INFO | train_inner | epoch 033:     28 / 63 loss=5.408, nll_loss=1.725, ppl=3.3, wps=1196.4, ups=0.39, wpb=3102.5, bsz=96, num_updates=2044, lr=2.4528e-05, gnorm=1.642, train_wall=5, gb_free=10.9, wall=7567
2024-08-11 19:01:05 | INFO | train_inner | epoch 033:     30 / 63 loss=5.47, nll_loss=1.833, ppl=3.56, wps=1274.4, ups=0.37, wpb=3417.5, bsz=144, num_updates=2046, lr=2.4552e-05, gnorm=1.769, train_wall=5, gb_free=14.1, wall=7573
2024-08-11 19:01:11 | INFO | train_inner | epoch 033:     32 / 63 loss=5.461, nll_loss=1.8, ppl=3.48, wps=1548, ups=0.37, wpb=4144, bsz=176, num_updates=2048, lr=2.4576e-05, gnorm=1.599, train_wall=5, gb_free=12.3, wall=7578
2024-08-11 19:01:16 | INFO | train_inner | epoch 033:     34 / 63 loss=5.493, nll_loss=1.843, ppl=3.59, wps=1373.5, ups=0.38, wpb=3568.5, bsz=156, num_updates=2050, lr=2.46e-05, gnorm=1.608, train_wall=5, gb_free=13.2, wall=7583
2024-08-11 19:01:20 | INFO | train_inner | epoch 033:     36 / 63 loss=5.522, nll_loss=1.852, ppl=3.61, wps=1170, ups=0.43, wpb=2722.5, bsz=76, num_updates=2052, lr=2.4624e-05, gnorm=1.964, train_wall=5, gb_free=15.5, wall=7588
2024-08-11 19:01:25 | INFO | train_inner | epoch 033:     38 / 63 loss=5.61, nll_loss=1.987, ppl=3.96, wps=1482.9, ups=0.42, wpb=3514, bsz=128, num_updates=2054, lr=2.4648e-05, gnorm=1.831, train_wall=5, gb_free=12.1, wall=7593
2024-08-11 19:01:30 | INFO | train_inner | epoch 033:     40 / 63 loss=5.453, nll_loss=1.786, ppl=3.45, wps=1395.5, ups=0.39, wpb=3586, bsz=132, num_updates=2056, lr=2.4672e-05, gnorm=1.61, train_wall=5, gb_free=13.2, wall=7598
2024-08-11 19:01:36 | INFO | train_inner | epoch 033:     42 / 63 loss=5.5, nll_loss=1.859, ppl=3.63, wps=1465.1, ups=0.36, wpb=4082, bsz=200, num_updates=2058, lr=2.4696e-05, gnorm=1.562, train_wall=6, gb_free=11.8, wall=7603
2024-08-11 19:01:41 | INFO | train_inner | epoch 033:     44 / 63 loss=5.51, nll_loss=1.867, ppl=3.65, wps=1451.7, ups=0.37, wpb=3954.5, bsz=204, num_updates=2060, lr=2.472e-05, gnorm=1.5, train_wall=5, gb_free=11.4, wall=7609
2024-08-11 19:01:46 | INFO | train_inner | epoch 033:     46 / 63 loss=5.381, nll_loss=1.677, ppl=3.2, wps=986.1, ups=0.39, wpb=2510, bsz=88, num_updates=2062, lr=2.4744e-05, gnorm=1.9, train_wall=5, gb_free=12.7, wall=7614
2024-08-11 19:01:52 | INFO | train_inner | epoch 033:     48 / 63 loss=5.497, nll_loss=1.84, ppl=3.58, wps=1332.9, ups=0.36, wpb=3717.5, bsz=164, num_updates=2064, lr=2.4768e-05, gnorm=1.633, train_wall=6, gb_free=10.1, wall=7619
2024-08-11 19:01:57 | INFO | train_inner | epoch 033:     50 / 63 loss=5.432, nll_loss=1.751, ppl=3.37, wps=1058.6, ups=0.39, wpb=2701.5, bsz=76, num_updates=2066, lr=2.4792e-05, gnorm=1.934, train_wall=5, gb_free=12.3, wall=7624
2024-08-11 19:02:02 | INFO | train_inner | epoch 033:     52 / 63 loss=5.481, nll_loss=1.837, ppl=3.57, wps=1179.9, ups=0.38, wpb=3110.5, bsz=132, num_updates=2068, lr=2.4816e-05, gnorm=1.744, train_wall=5, gb_free=8.9, wall=7630
2024-08-11 19:02:08 | INFO | train_inner | epoch 033:     54 / 63 loss=5.466, nll_loss=1.798, ppl=3.48, wps=1301.2, ups=0.39, wpb=3353, bsz=100, num_updates=2070, lr=2.484e-05, gnorm=1.702, train_wall=5, gb_free=12.7, wall=7635
2024-08-11 19:02:13 | INFO | train_inner | epoch 033:     56 / 63 loss=5.5, nll_loss=1.838, ppl=3.58, wps=1314.9, ups=0.35, wpb=3731.5, bsz=140, num_updates=2072, lr=2.4864e-05, gnorm=1.648, train_wall=6, gb_free=10.9, wall=7641
2024-08-11 19:02:18 | INFO | train_inner | epoch 033:     58 / 63 loss=5.391, nll_loss=1.721, ppl=3.3, wps=1201.9, ups=0.38, wpb=3157.5, bsz=200, num_updates=2074, lr=2.4888e-05, gnorm=1.728, train_wall=5, gb_free=13.1, wall=7646
2024-08-11 19:02:24 | INFO | train_inner | epoch 033:     60 / 63 loss=5.472, nll_loss=1.846, ppl=3.59, wps=1188.9, ups=0.39, wpb=3034.5, bsz=164, num_updates=2076, lr=2.4912e-05, gnorm=1.669, train_wall=5, gb_free=15.2, wall=7651
2024-08-11 19:02:29 | INFO | train_inner | epoch 033:     62 / 63 loss=5.521, nll_loss=1.855, ppl=3.62, wps=1252.3, ups=0.39, wpb=3244.5, bsz=80, num_updates=2078, lr=2.4936e-05, gnorm=1.772, train_wall=5, gb_free=11.3, wall=7656
2024-08-11 19:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21846.140625Mb; avail=233220.05078125Mb
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000529
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21847.125Mb; avail=233219.06640625Mb
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004716
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21854.5078125Mb; avail=233211.68359375Mb
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003971
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009544
2024-08-11 19:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21860.90625Mb; avail=233205.28515625Mb
2024-08-11 19:02:39 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.167 | nll_loss 2.482 | ppl 5.59 | wps 3055.4 | wpb 1463.4 | bsz 64.9 | num_updates 2079 | best_loss 6.167
2024-08-11 19:02:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2079 updates
2024-08-11 19:02:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 19:03:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-11 19:03:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 33 @ 2079 updates, score 6.167) (writing took 67.56984617328271 seconds)
2024-08-11 19:03:46 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-08-11 19:03:46 | INFO | train | epoch 033 | loss 5.475 | nll_loss 1.816 | ppl 3.52 | wps 885.1 | ups 0.26 | wpb 3400.7 | bsz 148.2 | num_updates 2079 | lr 2.4948e-05 | gnorm 1.708 | train_wall 166 | gb_free 15.2 | wall 7734
2024-08-11 19:03:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:03:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:03:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:03:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000769
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25900.66015625Mb; avail=229165.54296875Mb
2024-08-11 19:03:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000149
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001345
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25900.66015625Mb; avail=229165.54296875Mb
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000058
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25900.66015625Mb; avail=229165.54296875Mb
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000479
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002292
2024-08-11 19:03:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25900.66015625Mb; avail=229165.54296875Mb
2024-08-11 19:03:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:03:46 | INFO | fairseq.trainer | begin training epoch 34
2024-08-11 19:03:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:03:49 | INFO | train_inner | epoch 034:      1 / 63 loss=5.559, nll_loss=1.909, ppl=3.76, wps=74.4, ups=0.02, wpb=2981, bsz=108, num_updates=2080, lr=2.496e-05, gnorm=2.014, train_wall=4, gb_free=11.8, wall=7736
2024-08-11 19:03:54 | INFO | train_inner | epoch 034:      3 / 63 loss=5.466, nll_loss=1.824, ppl=3.54, wps=1395.4, ups=0.42, wpb=3324.5, bsz=160, num_updates=2082, lr=2.4984e-05, gnorm=1.787, train_wall=5, gb_free=12.7, wall=7741
2024-08-11 19:03:59 | INFO | train_inner | epoch 034:      5 / 63 loss=5.45, nll_loss=1.805, ppl=3.49, wps=1398.7, ups=0.37, wpb=3752.5, bsz=196, num_updates=2084, lr=2.5008e-05, gnorm=1.495, train_wall=5, gb_free=10.5, wall=7746
2024-08-11 19:04:04 | INFO | train_inner | epoch 034:      7 / 63 loss=5.404, nll_loss=1.706, ppl=3.26, wps=1310.1, ups=0.39, wpb=3349, bsz=84, num_updates=2086, lr=2.5032e-05, gnorm=1.681, train_wall=5, gb_free=10.3, wall=7752
2024-08-11 19:04:09 | INFO | train_inner | epoch 034:      9 / 63 loss=5.423, nll_loss=1.745, ppl=3.35, wps=1210.2, ups=0.38, wpb=3214, bsz=156, num_updates=2088, lr=2.5056e-05, gnorm=1.724, train_wall=5, gb_free=12.6, wall=7757
2024-08-11 19:04:14 | INFO | train_inner | epoch 034:     11 / 63 loss=5.396, nll_loss=1.704, ppl=3.26, wps=1315.7, ups=0.42, wpb=3150.5, bsz=116, num_updates=2090, lr=2.508e-05, gnorm=1.763, train_wall=5, gb_free=13.1, wall=7762
2024-08-11 19:04:19 | INFO | train_inner | epoch 034:     13 / 63 loss=5.368, nll_loss=1.68, ppl=3.2, wps=1381.6, ups=0.39, wpb=3516.5, bsz=140, num_updates=2092, lr=2.5104e-05, gnorm=1.684, train_wall=5, gb_free=12.4, wall=7767
2024-08-11 19:04:25 | INFO | train_inner | epoch 034:     15 / 63 loss=5.482, nll_loss=1.854, ppl=3.61, wps=1437.2, ups=0.37, wpb=3833.5, bsz=240, num_updates=2094, lr=2.5128e-05, gnorm=1.544, train_wall=5, gb_free=12.9, wall=7772
2024-08-11 19:04:30 | INFO | train_inner | epoch 034:     17 / 63 loss=5.549, nll_loss=1.921, ppl=3.79, wps=1687.7, ups=0.39, wpb=4371, bsz=212, num_updates=2096, lr=2.5152e-05, gnorm=1.6, train_wall=5, gb_free=11.9, wall=7777
2024-08-11 19:04:35 | INFO | train_inner | epoch 034:     19 / 63 loss=5.376, nll_loss=1.689, ppl=3.22, wps=1467.8, ups=0.37, wpb=3973, bsz=192, num_updates=2098, lr=2.5176e-05, gnorm=1.52, train_wall=5, gb_free=10.5, wall=7783
2024-08-11 19:04:40 | INFO | train_inner | epoch 034:     21 / 63 loss=5.488, nll_loss=1.81, ppl=3.51, wps=1282.7, ups=0.43, wpb=3011, bsz=104, num_updates=2100, lr=2.52e-05, gnorm=1.951, train_wall=5, gb_free=18.8, wall=7787
2024-08-11 19:04:45 | INFO | train_inner | epoch 034:     23 / 63 loss=5.429, nll_loss=1.741, ppl=3.34, wps=1443.5, ups=0.39, wpb=3722, bsz=128, num_updates=2102, lr=2.5224e-05, gnorm=1.565, train_wall=5, gb_free=12, wall=7792
2024-08-11 19:04:51 | INFO | train_inner | epoch 034:     25 / 63 loss=5.387, nll_loss=1.732, ppl=3.32, wps=1405.9, ups=0.36, wpb=3881.5, bsz=252, num_updates=2104, lr=2.5248e-05, gnorm=1.524, train_wall=6, gb_free=12.3, wall=7798
2024-08-11 19:04:56 | INFO | train_inner | epoch 034:     27 / 63 loss=5.466, nll_loss=1.839, ppl=3.58, wps=1393.4, ups=0.4, wpb=3511, bsz=184, num_updates=2106, lr=2.5272e-05, gnorm=1.727, train_wall=5, gb_free=11.1, wall=7803
2024-08-11 19:05:01 | INFO | train_inner | epoch 034:     29 / 63 loss=5.243, nll_loss=1.511, ppl=2.85, wps=1103, ups=0.38, wpb=2933, bsz=124, num_updates=2108, lr=2.5296e-05, gnorm=1.641, train_wall=5, gb_free=12.5, wall=7808
2024-08-11 19:05:06 | INFO | train_inner | epoch 034:     31 / 63 loss=5.409, nll_loss=1.725, ppl=3.3, wps=1327, ups=0.39, wpb=3401, bsz=132, num_updates=2110, lr=2.532e-05, gnorm=1.7, train_wall=5, gb_free=11.6, wall=7813
2024-08-11 19:05:11 | INFO | train_inner | epoch 034:     33 / 63 loss=5.378, nll_loss=1.709, ppl=3.27, wps=1274.4, ups=0.39, wpb=3269.5, bsz=220, num_updates=2112, lr=2.5344e-05, gnorm=1.666, train_wall=5, gb_free=12.9, wall=7819
2024-08-11 19:05:17 | INFO | train_inner | epoch 034:     35 / 63 loss=5.443, nll_loss=1.765, ppl=3.4, wps=1372.3, ups=0.37, wpb=3667.5, bsz=152, num_updates=2114, lr=2.5368e-05, gnorm=1.691, train_wall=5, gb_free=13.5, wall=7824
2024-08-11 19:05:22 | INFO | train_inner | epoch 034:     37 / 63 loss=5.475, nll_loss=1.82, ppl=3.53, wps=1593.9, ups=0.4, wpb=3997.5, bsz=176, num_updates=2116, lr=2.5392e-05, gnorm=1.545, train_wall=5, gb_free=16.4, wall=7829
2024-08-11 19:05:27 | INFO | train_inner | epoch 034:     39 / 63 loss=5.533, nll_loss=1.903, ppl=3.74, wps=1399.2, ups=0.39, wpb=3568, bsz=204, num_updates=2118, lr=2.5416e-05, gnorm=1.714, train_wall=5, gb_free=13.3, wall=7834
2024-08-11 19:05:32 | INFO | train_inner | epoch 034:     41 / 63 loss=5.521, nll_loss=1.863, ppl=3.64, wps=1353.9, ups=0.37, wpb=3678, bsz=100, num_updates=2120, lr=2.544e-05, gnorm=1.688, train_wall=5, gb_free=12, wall=7840
2024-08-11 19:05:36 | INFO | train_inner | epoch 034:     43 / 63 loss=5.515, nll_loss=1.86, ppl=3.63, wps=1366.9, ups=0.51, wpb=2664.5, bsz=64.5, num_updates=2122, lr=2.5464e-05, gnorm=2.282, train_wall=4, gb_free=15.8, wall=7843
2024-08-11 19:05:41 | INFO | train_inner | epoch 034:     45 / 63 loss=5.449, nll_loss=1.788, ppl=3.45, wps=1213.5, ups=0.39, wpb=3130.5, bsz=136, num_updates=2124, lr=2.5488e-05, gnorm=1.837, train_wall=5, gb_free=13.4, wall=7849
2024-08-11 19:05:47 | INFO | train_inner | epoch 034:     47 / 63 loss=5.4, nll_loss=1.728, ppl=3.31, wps=1358.2, ups=0.38, wpb=3579.5, bsz=176, num_updates=2126, lr=2.5512e-05, gnorm=1.766, train_wall=5, gb_free=15.4, wall=7854
2024-08-11 19:05:52 | INFO | train_inner | epoch 034:     49 / 63 loss=5.332, nll_loss=1.611, ppl=3.05, wps=1181.2, ups=0.37, wpb=3205, bsz=76, num_updates=2128, lr=2.5536e-05, gnorm=1.784, train_wall=5, gb_free=12, wall=7859
2024-08-11 19:05:57 | INFO | train_inner | epoch 034:     51 / 63 loss=5.322, nll_loss=1.615, ppl=3.06, wps=1311.1, ups=0.38, wpb=3452.5, bsz=192, num_updates=2130, lr=2.556e-05, gnorm=1.515, train_wall=5, gb_free=11.3, wall=7865
2024-08-11 19:06:02 | INFO | train_inner | epoch 034:     53 / 63 loss=5.456, nll_loss=1.78, ppl=3.43, wps=1302.6, ups=0.46, wpb=2854, bsz=76, num_updates=2132, lr=2.5584e-05, gnorm=2.088, train_wall=4, gb_free=11, wall=7869
2024-08-11 19:06:07 | INFO | train_inner | epoch 034:     55 / 63 loss=5.505, nll_loss=1.867, ppl=3.65, wps=1318.9, ups=0.39, wpb=3348, bsz=92, num_updates=2134, lr=2.5608e-05, gnorm=1.717, train_wall=5, gb_free=13.1, wall=7874
2024-08-11 19:06:12 | INFO | train_inner | epoch 034:     57 / 63 loss=5.297, nll_loss=1.617, ppl=3.07, wps=1105.6, ups=0.38, wpb=2928, bsz=224, num_updates=2136, lr=2.5632e-05, gnorm=1.684, train_wall=5, gb_free=11, wall=7879
2024-08-11 19:06:17 | INFO | train_inner | epoch 034:     59 / 63 loss=5.447, nll_loss=1.773, ppl=3.42, wps=1226.5, ups=0.39, wpb=3167.5, bsz=92, num_updates=2138, lr=2.5656e-05, gnorm=1.764, train_wall=5, gb_free=12.1, wall=7884
2024-08-11 19:06:22 | INFO | train_inner | epoch 034:     61 / 63 loss=5.434, nll_loss=1.748, ppl=3.36, wps=1155.7, ups=0.39, wpb=2959, bsz=76, num_updates=2140, lr=2.568e-05, gnorm=1.906, train_wall=5, gb_free=12.7, wall=7890
2024-08-11 19:06:26 | INFO | train_inner | epoch 034:     63 / 63 loss=5.548, nll_loss=1.902, ppl=3.74, wps=1319, ups=0.47, wpb=2787.5, bsz=124, num_updates=2142, lr=2.5704e-05, gnorm=1.923, train_wall=4, gb_free=14.7, wall=7894
2024-08-11 19:06:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36277.2890625Mb; avail=218788.91015625Mb
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000611
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36277.78125Mb; avail=218788.41796875Mb
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004792
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36277.78125Mb; avail=218788.41796875Mb
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004049
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009787
2024-08-11 19:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36277.78125Mb; avail=218788.41796875Mb
2024-08-11 19:06:35 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.173 | nll_loss 2.484 | ppl 5.6 | wps 3057.6 | wpb 1463.4 | bsz 64.9 | num_updates 2142 | best_loss 6.167
2024-08-11 19:06:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2142 updates
2024-08-11 19:06:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:07:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:07:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 34 @ 2142 updates, score 6.173) (writing took 42.48660866590217 seconds)
2024-08-11 19:07:17 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-08-11 19:07:17 | INFO | train | epoch 034 | loss 5.435 | nll_loss 1.767 | ppl 3.4 | wps 1014.8 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2142 | lr 2.5704e-05 | gnorm 1.724 | train_wall 160 | gb_free 14.7 | wall 7945
2024-08-11 19:07:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:07:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:07:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:07:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000792
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28304.65625Mb; avail=226761.49609375Mb
2024-08-11 19:07:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000139
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001238
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28304.65625Mb; avail=226761.49609375Mb
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28304.65625Mb; avail=226761.49609375Mb
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000434
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001994
2024-08-11 19:07:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28304.65625Mb; avail=226761.49609375Mb
2024-08-11 19:07:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:07:23 | INFO | fairseq.trainer | begin training epoch 35
2024-08-11 19:07:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:07:28 | INFO | train_inner | epoch 035:      2 / 63 loss=5.428, nll_loss=1.75, ppl=3.36, wps=103.5, ups=0.03, wpb=3164, bsz=124, num_updates=2144, lr=2.5728e-05, gnorm=1.681, train_wall=5, gb_free=13.5, wall=7955
2024-08-11 19:07:33 | INFO | train_inner | epoch 035:      4 / 63 loss=5.303, nll_loss=1.604, ppl=3.04, wps=1394.3, ups=0.38, wpb=3651.5, bsz=224, num_updates=2146, lr=2.5752e-05, gnorm=1.477, train_wall=5, gb_free=13.3, wall=7960
2024-08-11 19:07:38 | INFO | train_inner | epoch 035:      6 / 63 loss=5.292, nll_loss=1.63, ppl=3.1, wps=1243.3, ups=0.37, wpb=3353.5, bsz=236, num_updates=2148, lr=2.5776e-05, gnorm=1.555, train_wall=5, gb_free=11.9, wall=7966
2024-08-11 19:07:44 | INFO | train_inner | epoch 035:      8 / 63 loss=5.464, nll_loss=1.81, ppl=3.51, wps=1509.1, ups=0.38, wpb=3966, bsz=144, num_updates=2150, lr=2.58e-05, gnorm=1.698, train_wall=5, gb_free=12.9, wall=7971
2024-08-11 19:07:49 | INFO | train_inner | epoch 035:     10 / 63 loss=5.506, nll_loss=1.852, ppl=3.61, wps=1521.8, ups=0.39, wpb=3871, bsz=156, num_updates=2152, lr=2.5824e-05, gnorm=1.719, train_wall=5, gb_free=12, wall=7976
2024-08-11 19:07:54 | INFO | train_inner | epoch 035:     12 / 63 loss=5.486, nll_loss=1.808, ppl=3.5, wps=1480.3, ups=0.37, wpb=3965, bsz=184, num_updates=2154, lr=2.5848e-05, gnorm=1.528, train_wall=5, gb_free=11.4, wall=7981
2024-08-11 19:07:59 | INFO | train_inner | epoch 035:     14 / 63 loss=5.428, nll_loss=1.719, ppl=3.29, wps=1378.9, ups=0.39, wpb=3542, bsz=112, num_updates=2156, lr=2.5872e-05, gnorm=1.762, train_wall=5, gb_free=12.1, wall=7986
2024-08-11 19:08:04 | INFO | train_inner | epoch 035:     16 / 63 loss=5.302, nll_loss=1.609, ppl=3.05, wps=1284.8, ups=0.39, wpb=3302.5, bsz=200, num_updates=2158, lr=2.5896e-05, gnorm=1.564, train_wall=5, gb_free=12.2, wall=7992
2024-08-11 19:08:09 | INFO | train_inner | epoch 035:     18 / 63 loss=5.273, nll_loss=1.587, ppl=3, wps=1294.9, ups=0.41, wpb=3166, bsz=180, num_updates=2160, lr=2.592e-05, gnorm=1.632, train_wall=5, gb_free=11.7, wall=7996
2024-08-11 19:08:14 | INFO | train_inner | epoch 035:     20 / 63 loss=5.362, nll_loss=1.697, ppl=3.24, wps=1318.1, ups=0.38, wpb=3475.5, bsz=124, num_updates=2162, lr=2.5944e-05, gnorm=1.675, train_wall=5, gb_free=11.3, wall=8002
2024-08-11 19:08:20 | INFO | train_inner | epoch 035:     22 / 63 loss=5.414, nll_loss=1.726, ppl=3.31, wps=1452.8, ups=0.39, wpb=3754.5, bsz=132, num_updates=2164, lr=2.5968e-05, gnorm=1.595, train_wall=5, gb_free=12.9, wall=8007
2024-08-11 19:08:25 | INFO | train_inner | epoch 035:     24 / 63 loss=5.436, nll_loss=1.748, ppl=3.36, wps=1435.5, ups=0.39, wpb=3689.5, bsz=156, num_updates=2166, lr=2.5992e-05, gnorm=1.616, train_wall=5, gb_free=16.3, wall=8012
2024-08-11 19:08:30 | INFO | train_inner | epoch 035:     26 / 63 loss=5.349, nll_loss=1.635, ppl=3.11, wps=1408.7, ups=0.4, wpb=3486.5, bsz=152, num_updates=2168, lr=2.6016e-05, gnorm=1.575, train_wall=5, gb_free=10.9, wall=8017
2024-08-11 19:08:35 | INFO | train_inner | epoch 035:     28 / 63 loss=5.412, nll_loss=1.745, ppl=3.35, wps=1256.9, ups=0.39, wpb=3196.5, bsz=140, num_updates=2170, lr=2.604e-05, gnorm=1.694, train_wall=5, gb_free=13.6, wall=8022
2024-08-11 19:08:40 | INFO | train_inner | epoch 035:     30 / 63 loss=5.507, nll_loss=1.883, ppl=3.69, wps=1326, ups=0.39, wpb=3384.5, bsz=152, num_updates=2172, lr=2.6064e-05, gnorm=1.722, train_wall=5, gb_free=12.9, wall=8027
2024-08-11 19:08:45 | INFO | train_inner | epoch 035:     32 / 63 loss=5.339, nll_loss=1.651, ppl=3.14, wps=1326.7, ups=0.42, wpb=3127.5, bsz=120, num_updates=2174, lr=2.6088e-05, gnorm=1.683, train_wall=5, gb_free=16.7, wall=8032
2024-08-11 19:08:50 | INFO | train_inner | epoch 035:     34 / 63 loss=5.323, nll_loss=1.621, ppl=3.08, wps=1208.9, ups=0.4, wpb=3055, bsz=140, num_updates=2176, lr=2.6112e-05, gnorm=1.772, train_wall=5, gb_free=15.5, wall=8037
2024-08-11 19:08:55 | INFO | train_inner | epoch 035:     36 / 63 loss=5.401, nll_loss=1.701, ppl=3.25, wps=1488, ups=0.37, wpb=3994.5, bsz=204, num_updates=2178, lr=2.6136e-05, gnorm=1.49, train_wall=5, gb_free=12.7, wall=8042
2024-08-11 19:09:00 | INFO | train_inner | epoch 035:     38 / 63 loss=5.28, nll_loss=1.544, ppl=2.92, wps=1202.4, ups=0.42, wpb=2862.5, bsz=96, num_updates=2180, lr=2.616e-05, gnorm=1.615, train_wall=5, gb_free=11.3, wall=8047
2024-08-11 19:09:05 | INFO | train_inner | epoch 035:     40 / 63 loss=5.324, nll_loss=1.651, ppl=3.14, wps=1183, ups=0.38, wpb=3111.5, bsz=172, num_updates=2182, lr=2.6184e-05, gnorm=1.603, train_wall=5, gb_free=13.2, wall=8052
2024-08-11 19:09:10 | INFO | train_inner | epoch 035:     42 / 63 loss=5.317, nll_loss=1.631, ppl=3.1, wps=1285.6, ups=0.38, wpb=3375.5, bsz=160, num_updates=2184, lr=2.6208e-05, gnorm=1.626, train_wall=5, gb_free=11.2, wall=8058
2024-08-11 19:09:16 | INFO | train_inner | epoch 035:     44 / 63 loss=5.421, nll_loss=1.745, ppl=3.35, wps=1416.4, ups=0.38, wpb=3743, bsz=132, num_updates=2186, lr=2.6232e-05, gnorm=1.7, train_wall=5, gb_free=13.3, wall=8063
2024-08-11 19:09:20 | INFO | train_inner | epoch 035:     46 / 63 loss=5.324, nll_loss=1.621, ppl=3.08, wps=1250.2, ups=0.46, wpb=2693, bsz=84, num_updates=2188, lr=2.6256e-05, gnorm=1.919, train_wall=4, gb_free=13, wall=8067
2024-08-11 19:09:25 | INFO | train_inner | epoch 035:     48 / 63 loss=5.395, nll_loss=1.697, ppl=3.24, wps=1416.5, ups=0.4, wpb=3568.5, bsz=132, num_updates=2190, lr=2.628e-05, gnorm=1.643, train_wall=5, gb_free=13.3, wall=8072
2024-08-11 19:09:30 | INFO | train_inner | epoch 035:     50 / 63 loss=5.364, nll_loss=1.647, ppl=3.13, wps=1279.2, ups=0.38, wpb=3336, bsz=80, num_updates=2192, lr=2.6304e-05, gnorm=1.692, train_wall=5, gb_free=13.5, wall=8077
2024-08-11 19:09:36 | INFO | train_inner | epoch 035:     52 / 63 loss=5.371, nll_loss=1.679, ppl=3.2, wps=1274.2, ups=0.37, wpb=3451, bsz=116, num_updates=2194, lr=2.6328e-05, gnorm=1.675, train_wall=5, gb_free=12.2, wall=8083
2024-08-11 19:09:40 | INFO | train_inner | epoch 035:     54 / 63 loss=5.486, nll_loss=1.833, ppl=3.56, wps=1348.2, ups=0.45, wpb=2992.5, bsz=108.5, num_updates=2196, lr=2.6352e-05, gnorm=1.987, train_wall=4, gb_free=12.5, wall=8087
2024-08-11 19:09:45 | INFO | train_inner | epoch 035:     56 / 63 loss=5.299, nll_loss=1.596, ppl=3.02, wps=1203.7, ups=0.36, wpb=3327.5, bsz=144, num_updates=2198, lr=2.6376e-05, gnorm=1.657, train_wall=6, gb_free=9.9, wall=8093
2024-08-11 19:09:51 | INFO | train_inner | epoch 035:     58 / 63 loss=5.443, nll_loss=1.801, ppl=3.48, wps=1332.5, ups=0.36, wpb=3653.5, bsz=200, num_updates=2200, lr=2.64e-05, gnorm=1.646, train_wall=5, gb_free=9.7, wall=8098
2024-08-11 19:09:56 | INFO | train_inner | epoch 035:     60 / 63 loss=5.402, nll_loss=1.726, ppl=3.31, wps=1468.4, ups=0.38, wpb=3896.5, bsz=160, num_updates=2202, lr=2.6424e-05, gnorm=1.607, train_wall=5, gb_free=12.6, wall=8104
2024-08-11 19:10:01 | INFO | train_inner | epoch 035:     62 / 63 loss=5.37, nll_loss=1.696, ppl=3.24, wps=1317.3, ups=0.4, wpb=3318, bsz=144, num_updates=2204, lr=2.6448e-05, gnorm=1.685, train_wall=5, gb_free=12.6, wall=8109
2024-08-11 19:10:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26972.42578125Mb; avail=228093.734375Mb
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000603
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26972.91796875Mb; avail=228093.2421875Mb
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004704
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26972.91796875Mb; avail=228093.2421875Mb
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004001
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009656
2024-08-11 19:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26972.91796875Mb; avail=228093.2421875Mb
2024-08-11 19:10:11 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.198 | nll_loss 2.496 | ppl 5.64 | wps 3057 | wpb 1463.4 | bsz 64.9 | num_updates 2205 | best_loss 6.167
2024-08-11 19:10:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2205 updates
2024-08-11 19:10:11 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:10:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 35 @ 2205 updates, score 6.198) (writing took 38.41249344497919 seconds)
2024-08-11 19:10:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-08-11 19:10:49 | INFO | train | epoch 035 | loss 5.383 | nll_loss 1.7 | ppl 3.25 | wps 1010.1 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2205 | lr 2.646e-05 | gnorm 1.668 | train_wall 160 | gb_free 16.9 | wall 8157
2024-08-11 19:10:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:10:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:10:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:10:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000765
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29339.5234375Mb; avail=225726.63671875Mb
2024-08-11 19:10:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000138
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001241
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29339.5234375Mb; avail=225726.63671875Mb
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000042
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29339.5234375Mb; avail=225726.63671875Mb
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000424
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001991
2024-08-11 19:10:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29339.5234375Mb; avail=225726.63671875Mb
2024-08-11 19:10:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:10:50 | INFO | fairseq.trainer | begin training epoch 36
2024-08-11 19:10:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:10:52 | INFO | train_inner | epoch 036:      1 / 63 loss=5.222, nll_loss=1.487, ppl=2.8, wps=106.1, ups=0.04, wpb=2701, bsz=164, num_updates=2206, lr=2.6472e-05, gnorm=1.743, train_wall=4, gb_free=12.8, wall=8160
2024-08-11 19:10:56 | INFO | train_inner | epoch 036:      3 / 63 loss=5.33, nll_loss=1.625, ppl=3.08, wps=1317, ups=0.49, wpb=2697.5, bsz=176.5, num_updates=2208, lr=2.6496e-05, gnorm=2.394, train_wall=4, gb_free=13.3, wall=8164
2024-08-11 19:11:11 | INFO | train_inner | epoch 036:      5 / 63 loss=5.308, nll_loss=1.594, ppl=3.02, wps=465.6, ups=0.14, wpb=3445, bsz=140, num_updates=2210, lr=2.652e-05, gnorm=1.696, train_wall=15, gb_free=13.5, wall=8178
2024-08-11 19:11:16 | INFO | train_inner | epoch 036:      7 / 63 loss=5.316, nll_loss=1.637, ppl=3.11, wps=1466.2, ups=0.38, wpb=3899.5, bsz=220, num_updates=2212, lr=2.6544e-05, gnorm=1.584, train_wall=5, gb_free=12.2, wall=8184
2024-08-11 19:11:22 | INFO | train_inner | epoch 036:      9 / 63 loss=5.305, nll_loss=1.613, ppl=3.06, wps=1111.6, ups=0.39, wpb=2827, bsz=104, num_updates=2214, lr=2.6568e-05, gnorm=1.777, train_wall=5, gb_free=13.6, wall=8189
2024-08-11 19:11:27 | INFO | train_inner | epoch 036:     11 / 63 loss=5.324, nll_loss=1.632, ppl=3.1, wps=1417, ups=0.39, wpb=3673.5, bsz=160, num_updates=2216, lr=2.6592e-05, gnorm=1.573, train_wall=5, gb_free=15, wall=8194
2024-08-11 19:11:32 | INFO | train_inner | epoch 036:     13 / 63 loss=5.428, nll_loss=1.749, ppl=3.36, wps=1413.1, ups=0.4, wpb=3530, bsz=140, num_updates=2218, lr=2.6616e-05, gnorm=1.705, train_wall=5, gb_free=11.4, wall=8199
2024-08-11 19:11:37 | INFO | train_inner | epoch 036:     15 / 63 loss=5.321, nll_loss=1.601, ppl=3.03, wps=1430.3, ups=0.37, wpb=3878.5, bsz=132, num_updates=2220, lr=2.664e-05, gnorm=1.54, train_wall=5, gb_free=9.9, wall=8204
2024-08-11 19:11:42 | INFO | train_inner | epoch 036:     17 / 63 loss=5.279, nll_loss=1.55, ppl=2.93, wps=1393.1, ups=0.4, wpb=3444, bsz=120, num_updates=2222, lr=2.6664e-05, gnorm=1.719, train_wall=5, gb_free=12.7, wall=8209
2024-08-11 19:11:47 | INFO | train_inner | epoch 036:     19 / 63 loss=5.396, nll_loss=1.727, ppl=3.31, wps=1421.4, ups=0.38, wpb=3749, bsz=176, num_updates=2224, lr=2.6688e-05, gnorm=1.738, train_wall=5, gb_free=12.4, wall=8215
2024-08-11 19:11:53 | INFO | train_inner | epoch 036:     21 / 63 loss=5.249, nll_loss=1.518, ppl=2.86, wps=1196.5, ups=0.38, wpb=3176.5, bsz=144, num_updates=2226, lr=2.6712e-05, gnorm=1.625, train_wall=5, gb_free=12, wall=8220
2024-08-11 19:11:58 | INFO | train_inner | epoch 036:     23 / 63 loss=5.251, nll_loss=1.533, ppl=2.89, wps=1168.6, ups=0.38, wpb=3081, bsz=112, num_updates=2228, lr=2.6736e-05, gnorm=1.579, train_wall=5, gb_free=12.6, wall=8225
2024-08-11 19:12:03 | INFO | train_inner | epoch 036:     25 / 63 loss=5.209, nll_loss=1.502, ppl=2.83, wps=1112.4, ups=0.38, wpb=2889.5, bsz=172, num_updates=2230, lr=2.676e-05, gnorm=1.626, train_wall=5, gb_free=11.8, wall=8230
2024-08-11 19:12:08 | INFO | train_inner | epoch 036:     27 / 63 loss=5.346, nll_loss=1.659, ppl=3.16, wps=1362.3, ups=0.39, wpb=3458, bsz=168, num_updates=2232, lr=2.6784e-05, gnorm=1.609, train_wall=5, gb_free=13.7, wall=8236
2024-08-11 19:12:14 | INFO | train_inner | epoch 036:     29 / 63 loss=5.386, nll_loss=1.694, ppl=3.24, wps=1391.4, ups=0.36, wpb=3837, bsz=156, num_updates=2234, lr=2.6808e-05, gnorm=1.507, train_wall=6, gb_free=9.3, wall=8241
2024-08-11 19:12:19 | INFO | train_inner | epoch 036:     31 / 63 loss=5.405, nll_loss=1.705, ppl=3.26, wps=1109.4, ups=0.41, wpb=2710.5, bsz=76, num_updates=2236, lr=2.6832e-05, gnorm=1.788, train_wall=5, gb_free=13.5, wall=8246
2024-08-11 19:12:24 | INFO | train_inner | epoch 036:     33 / 63 loss=5.41, nll_loss=1.736, ppl=3.33, wps=1404.6, ups=0.37, wpb=3843.5, bsz=128, num_updates=2238, lr=2.6856e-05, gnorm=1.756, train_wall=5, gb_free=10.7, wall=8251
2024-08-11 19:12:29 | INFO | train_inner | epoch 036:     35 / 63 loss=5.336, nll_loss=1.622, ppl=3.08, wps=1178.5, ups=0.43, wpb=2722.5, bsz=76, num_updates=2240, lr=2.688e-05, gnorm=2.09, train_wall=5, gb_free=13.3, wall=8256
2024-08-11 19:12:33 | INFO | train_inner | epoch 036:     37 / 63 loss=5.263, nll_loss=1.536, ppl=2.9, wps=1457, ups=0.45, wpb=3231, bsz=108, num_updates=2242, lr=2.6904e-05, gnorm=1.619, train_wall=4, gb_free=16.8, wall=8260
2024-08-11 19:12:39 | INFO | train_inner | epoch 036:     39 / 63 loss=5.434, nll_loss=1.76, ppl=3.39, wps=1460.3, ups=0.37, wpb=3985, bsz=160, num_updates=2244, lr=2.6928e-05, gnorm=1.591, train_wall=5, gb_free=10.2, wall=8266
2024-08-11 19:12:43 | INFO | train_inner | epoch 036:     41 / 63 loss=5.279, nll_loss=1.546, ppl=2.92, wps=1350.9, ups=0.42, wpb=3249.5, bsz=100, num_updates=2246, lr=2.6952e-05, gnorm=1.711, train_wall=5, gb_free=15.6, wall=8271
2024-08-11 19:12:48 | INFO | train_inner | epoch 036:     43 / 63 loss=5.347, nll_loss=1.652, ppl=3.14, wps=1408.7, ups=0.41, wpb=3418.5, bsz=136, num_updates=2248, lr=2.6976e-05, gnorm=1.57, train_wall=5, gb_free=12, wall=8276
2024-08-11 19:12:54 | INFO | train_inner | epoch 036:     45 / 63 loss=5.346, nll_loss=1.654, ppl=3.15, wps=1287.7, ups=0.38, wpb=3379, bsz=116, num_updates=2250, lr=2.7e-05, gnorm=1.66, train_wall=5, gb_free=10.4, wall=8281
2024-08-11 19:12:58 | INFO | train_inner | epoch 036:     47 / 63 loss=5.316, nll_loss=1.637, ppl=3.11, wps=1324.8, ups=0.43, wpb=3058.5, bsz=172, num_updates=2252, lr=2.7024e-05, gnorm=1.605, train_wall=5, gb_free=15.9, wall=8285
2024-08-11 19:13:03 | INFO | train_inner | epoch 036:     49 / 63 loss=5.402, nll_loss=1.727, ppl=3.31, wps=1371, ups=0.37, wpb=3676, bsz=172, num_updates=2254, lr=2.7048e-05, gnorm=1.557, train_wall=5, gb_free=13.3, wall=8291
2024-08-11 19:13:09 | INFO | train_inner | epoch 036:     51 / 63 loss=5.468, nll_loss=1.811, ppl=3.51, wps=1491.3, ups=0.37, wpb=4057, bsz=156, num_updates=2256, lr=2.7072e-05, gnorm=1.691, train_wall=5, gb_free=10.7, wall=8296
2024-08-11 19:13:14 | INFO | train_inner | epoch 036:     53 / 63 loss=5.38, nll_loss=1.697, ppl=3.24, wps=1463.4, ups=0.38, wpb=3853, bsz=228, num_updates=2258, lr=2.7096e-05, gnorm=1.456, train_wall=5, gb_free=13.3, wall=8302
2024-08-11 19:13:20 | INFO | train_inner | epoch 036:     55 / 63 loss=5.247, nll_loss=1.541, ppl=2.91, wps=1320.5, ups=0.36, wpb=3699.5, bsz=276, num_updates=2260, lr=2.712e-05, gnorm=1.411, train_wall=6, gb_free=10.6, wall=8307
2024-08-11 19:13:25 | INFO | train_inner | epoch 036:     57 / 63 loss=5.228, nll_loss=1.512, ppl=2.85, wps=1293.4, ups=0.37, wpb=3524.5, bsz=216, num_updates=2262, lr=2.7144e-05, gnorm=1.436, train_wall=5, gb_free=12, wall=8313
2024-08-11 19:13:31 | INFO | train_inner | epoch 036:     59 / 63 loss=5.321, nll_loss=1.613, ppl=3.06, wps=1373, ups=0.37, wpb=3664.5, bsz=144, num_updates=2264, lr=2.7168e-05, gnorm=1.521, train_wall=5, gb_free=11.7, wall=8318
2024-08-11 19:13:36 | INFO | train_inner | epoch 036:     61 / 63 loss=5.426, nll_loss=1.769, ppl=3.41, wps=1304.3, ups=0.4, wpb=3243, bsz=104, num_updates=2266, lr=2.7192e-05, gnorm=1.702, train_wall=5, gb_free=12.9, wall=8323
2024-08-11 19:13:39 | INFO | train_inner | epoch 036:     63 / 63 loss=5.346, nll_loss=1.65, ppl=3.14, wps=1186.8, ups=0.55, wpb=2167, bsz=76, num_updates=2268, lr=2.7216e-05, gnorm=1.99, train_wall=4, gb_free=15.5, wall=8327
2024-08-11 19:13:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6922.81640625Mb; avail=248159.41015625Mb
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000546
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6922.81640625Mb; avail=248159.41015625Mb
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004702
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6922.81640625Mb; avail=248159.41015625Mb
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004001
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009576
2024-08-11 19:13:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6922.81640625Mb; avail=248159.41015625Mb
2024-08-11 19:13:48 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.185 | nll_loss 2.502 | ppl 5.66 | wps 3058.3 | wpb 1463.4 | bsz 64.9 | num_updates 2268 | best_loss 6.167
2024-08-11 19:13:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2268 updates
2024-08-11 19:13:48 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:14:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 36 @ 2268 updates, score 6.185) (writing took 39.00172437308356 seconds)
2024-08-11 19:14:27 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-08-11 19:14:27 | INFO | train | epoch 036 | loss 5.337 | nll_loss 1.64 | ppl 3.12 | wps 986.6 | ups 0.29 | wpb 3400.7 | bsz 148.2 | num_updates 2268 | lr 2.7216e-05 | gnorm 1.667 | train_wall 169 | gb_free 15.5 | wall 8374
2024-08-11 19:14:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:14:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:14:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:14:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000694
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9277.10546875Mb; avail=245805.12109375Mb
2024-08-11 19:14:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000167
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001339
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9277.10546875Mb; avail=245805.12109375Mb
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9277.10546875Mb; avail=245805.12109375Mb
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000439
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002175
2024-08-11 19:14:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9277.10546875Mb; avail=245805.12109375Mb
2024-08-11 19:14:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:14:27 | INFO | fairseq.trainer | begin training epoch 37
2024-08-11 19:14:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:14:32 | INFO | train_inner | epoch 037:      2 / 63 loss=5.208, nll_loss=1.478, ppl=2.78, wps=149.7, ups=0.04, wpb=3941.5, bsz=244, num_updates=2270, lr=2.724e-05, gnorm=1.391, train_wall=5, gb_free=11.3, wall=8379
2024-08-11 19:14:37 | INFO | train_inner | epoch 037:      4 / 63 loss=5.196, nll_loss=1.447, ppl=2.73, wps=1365.7, ups=0.4, wpb=3443.5, bsz=128, num_updates=2272, lr=2.7264e-05, gnorm=1.478, train_wall=5, gb_free=10, wall=8384
2024-08-11 19:14:42 | INFO | train_inner | epoch 037:      6 / 63 loss=5.362, nll_loss=1.677, ppl=3.2, wps=1442.1, ups=0.39, wpb=3701, bsz=176, num_updates=2274, lr=2.7288e-05, gnorm=1.643, train_wall=5, gb_free=12.6, wall=8389
2024-08-11 19:14:47 | INFO | train_inner | epoch 037:      8 / 63 loss=5.221, nll_loss=1.51, ppl=2.85, wps=1493.8, ups=0.38, wpb=3887, bsz=220, num_updates=2276, lr=2.7312e-05, gnorm=1.48, train_wall=5, gb_free=11.3, wall=8395
2024-08-11 19:14:52 | INFO | train_inner | epoch 037:     10 / 63 loss=5.335, nll_loss=1.657, ppl=3.15, wps=1409.1, ups=0.4, wpb=3545, bsz=216, num_updates=2278, lr=2.7336e-05, gnorm=1.538, train_wall=5, gb_free=13.4, wall=8400
2024-08-11 19:14:57 | INFO | train_inner | epoch 037:     12 / 63 loss=5.361, nll_loss=1.656, ppl=3.15, wps=1518, ups=0.4, wpb=3805.5, bsz=152, num_updates=2280, lr=2.736e-05, gnorm=2.013, train_wall=5, gb_free=14.3, wall=8405
2024-08-11 19:15:02 | INFO | train_inner | epoch 037:     14 / 63 loss=5.228, nll_loss=1.494, ppl=2.82, wps=1368.2, ups=0.39, wpb=3471, bsz=120, num_updates=2282, lr=2.7384e-05, gnorm=1.581, train_wall=5, gb_free=11.2, wall=8410
2024-08-11 19:15:08 | INFO | train_inner | epoch 037:     16 / 63 loss=5.25, nll_loss=1.536, ppl=2.9, wps=1372.4, ups=0.36, wpb=3817, bsz=180, num_updates=2284, lr=2.7408e-05, gnorm=1.524, train_wall=6, gb_free=10.4, wall=8415
2024-08-11 19:15:13 | INFO | train_inner | epoch 037:     18 / 63 loss=5.288, nll_loss=1.566, ppl=2.96, wps=1293.3, ups=0.38, wpb=3413, bsz=68, num_updates=2286, lr=2.7432e-05, gnorm=1.639, train_wall=5, gb_free=12.1, wall=8421
2024-08-11 19:15:18 | INFO | train_inner | epoch 037:     20 / 63 loss=5.252, nll_loss=1.531, ppl=2.89, wps=1311.4, ups=0.4, wpb=3315.5, bsz=124, num_updates=2288, lr=2.7456e-05, gnorm=1.71, train_wall=5, gb_free=12.5, wall=8426
2024-08-11 19:15:23 | INFO | train_inner | epoch 037:     22 / 63 loss=5.25, nll_loss=1.526, ppl=2.88, wps=1246.4, ups=0.39, wpb=3159, bsz=100, num_updates=2290, lr=2.748e-05, gnorm=1.614, train_wall=5, gb_free=13.6, wall=8431
2024-08-11 19:15:29 | INFO | train_inner | epoch 037:     24 / 63 loss=5.164, nll_loss=1.435, ppl=2.7, wps=1183.3, ups=0.35, wpb=3373, bsz=244, num_updates=2292, lr=2.7504e-05, gnorm=1.374, train_wall=6, gb_free=10.9, wall=8436
2024-08-11 19:15:34 | INFO | train_inner | epoch 037:     26 / 63 loss=5.284, nll_loss=1.561, ppl=2.95, wps=1320.8, ups=0.38, wpb=3485, bsz=112, num_updates=2294, lr=2.7528e-05, gnorm=1.636, train_wall=5, gb_free=11.5, wall=8442
2024-08-11 19:15:39 | INFO | train_inner | epoch 037:     28 / 63 loss=5.25, nll_loss=1.521, ppl=2.87, wps=1313.5, ups=0.4, wpb=3305.5, bsz=108, num_updates=2296, lr=2.7552e-05, gnorm=1.625, train_wall=5, gb_free=16.3, wall=8447
2024-08-11 19:15:45 | INFO | train_inner | epoch 037:     30 / 63 loss=5.266, nll_loss=1.55, ppl=2.93, wps=1427.6, ups=0.38, wpb=3785, bsz=156, num_updates=2298, lr=2.7576e-05, gnorm=1.565, train_wall=5, gb_free=10.5, wall=8452
2024-08-11 19:15:50 | INFO | train_inner | epoch 037:     32 / 63 loss=5.3, nll_loss=1.59, ppl=3.01, wps=1378.9, ups=0.38, wpb=3675, bsz=140, num_updates=2300, lr=2.76e-05, gnorm=1.581, train_wall=5, gb_free=11.3, wall=8457
2024-08-11 19:15:55 | INFO | train_inner | epoch 037:     34 / 63 loss=5.213, nll_loss=1.469, ppl=2.77, wps=1152.5, ups=0.41, wpb=2782.5, bsz=84, num_updates=2302, lr=2.7624e-05, gnorm=1.695, train_wall=5, gb_free=12.5, wall=8462
2024-08-11 19:16:00 | INFO | train_inner | epoch 037:     36 / 63 loss=5.378, nll_loss=1.687, ppl=3.22, wps=1414.3, ups=0.42, wpb=3355.5, bsz=108, num_updates=2304, lr=2.7648e-05, gnorm=1.734, train_wall=5, gb_free=14.6, wall=8467
2024-08-11 19:16:04 | INFO | train_inner | epoch 037:     38 / 63 loss=5.24, nll_loss=1.489, ppl=2.81, wps=1293.8, ups=0.47, wpb=2733, bsz=68, num_updates=2306, lr=2.7672e-05, gnorm=1.941, train_wall=4, gb_free=14.7, wall=8471
2024-08-11 19:16:09 | INFO | train_inner | epoch 037:     40 / 63 loss=5.357, nll_loss=1.689, ppl=3.22, wps=1579.6, ups=0.37, wpb=4224.5, bsz=188, num_updates=2308, lr=2.7696e-05, gnorm=1.504, train_wall=5, gb_free=10.4, wall=8476
2024-08-11 19:16:14 | INFO | train_inner | epoch 037:     42 / 63 loss=5.251, nll_loss=1.544, ppl=2.92, wps=1040.8, ups=0.4, wpb=2590.5, bsz=144, num_updates=2310, lr=2.772e-05, gnorm=1.813, train_wall=5, gb_free=11.4, wall=8481
2024-08-11 19:16:19 | INFO | train_inner | epoch 037:     44 / 63 loss=5.296, nll_loss=1.572, ppl=2.97, wps=1270, ups=0.39, wpb=3223.5, bsz=112, num_updates=2312, lr=2.7744e-05, gnorm=1.688, train_wall=5, gb_free=11.5, wall=8487
2024-08-11 19:16:24 | INFO | train_inner | epoch 037:     46 / 63 loss=5.327, nll_loss=1.622, ppl=3.08, wps=1402.5, ups=0.41, wpb=3386, bsz=148, num_updates=2314, lr=2.7768e-05, gnorm=1.743, train_wall=5, gb_free=12.6, wall=8491
2024-08-11 19:16:29 | INFO | train_inner | epoch 037:     48 / 63 loss=5.118, nll_loss=1.366, ppl=2.58, wps=1168.9, ups=0.37, wpb=3188.5, bsz=196, num_updates=2316, lr=2.7792e-05, gnorm=1.456, train_wall=5, gb_free=12.2, wall=8497
2024-08-11 19:16:34 | INFO | train_inner | epoch 037:     50 / 63 loss=5.211, nll_loss=1.51, ppl=2.85, wps=1218.8, ups=0.43, wpb=2837, bsz=164, num_updates=2318, lr=2.7816e-05, gnorm=1.741, train_wall=5, gb_free=13.6, wall=8501
2024-08-11 19:16:39 | INFO | train_inner | epoch 037:     52 / 63 loss=5.3, nll_loss=1.598, ppl=3.03, wps=1258.8, ups=0.38, wpb=3335.5, bsz=152, num_updates=2320, lr=2.784e-05, gnorm=1.666, train_wall=5, gb_free=11.4, wall=8507
2024-08-11 19:16:44 | INFO | train_inner | epoch 037:     54 / 63 loss=5.434, nll_loss=1.748, ppl=3.36, wps=1427.5, ups=0.46, wpb=3091.5, bsz=116.5, num_updates=2322, lr=2.7864e-05, gnorm=2.069, train_wall=4, gb_free=13.9, wall=8511
2024-08-11 19:16:49 | INFO | train_inner | epoch 037:     56 / 63 loss=5.304, nll_loss=1.59, ppl=3.01, wps=1518.3, ups=0.37, wpb=4098, bsz=184, num_updates=2324, lr=2.7888e-05, gnorm=1.475, train_wall=5, gb_free=13, wall=8516
2024-08-11 19:16:55 | INFO | train_inner | epoch 037:     58 / 63 loss=5.259, nll_loss=1.551, ppl=2.93, wps=1242, ups=0.37, wpb=3326, bsz=128, num_updates=2326, lr=2.7912e-05, gnorm=1.625, train_wall=5, gb_free=12.1, wall=8522
2024-08-11 19:17:00 | INFO | train_inner | epoch 037:     60 / 63 loss=5.261, nll_loss=1.573, ppl=2.98, wps=1569.8, ups=0.37, wpb=4200, bsz=260, num_updates=2328, lr=2.7936e-05, gnorm=1.345, train_wall=5, gb_free=11.4, wall=8527
2024-08-11 19:17:05 | INFO | train_inner | epoch 037:     62 / 63 loss=5.256, nll_loss=1.529, ppl=2.89, wps=1139.2, ups=0.4, wpb=2882, bsz=112, num_updates=2330, lr=2.796e-05, gnorm=1.909, train_wall=5, gb_free=12.5, wall=8532
2024-08-11 19:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9316.4609375Mb; avail=245765.7265625Mb
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000649
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9316.4609375Mb; avail=245765.7265625Mb
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004703
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9316.4609375Mb; avail=245765.7265625Mb
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004016
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009713
2024-08-11 19:17:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9316.953125Mb; avail=245765.234375Mb
2024-08-11 19:17:15 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.196 | nll_loss 2.506 | ppl 5.68 | wps 3066.1 | wpb 1463.4 | bsz 64.9 | num_updates 2331 | best_loss 6.167
2024-08-11 19:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2331 updates
2024-08-11 19:17:15 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:17:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:17:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 37 @ 2331 updates, score 6.196) (writing took 38.7060486888513 seconds)
2024-08-11 19:17:53 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-08-11 19:17:53 | INFO | train | epoch 037 | loss 5.272 | nll_loss 1.559 | ppl 2.95 | wps 1036.3 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2331 | lr 2.7972e-05 | gnorm 1.648 | train_wall 159 | gb_free 16.6 | wall 8581
2024-08-11 19:17:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:17:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:17:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:17:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000835
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12398.14453125Mb; avail=242684.02734375Mb
2024-08-11 19:17:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000163
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001287
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12398.14453125Mb; avail=242684.02734375Mb
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000041
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12398.14453125Mb; avail=242684.02734375Mb
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000443
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002069
2024-08-11 19:17:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12398.14453125Mb; avail=242684.02734375Mb
2024-08-11 19:17:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:17:53 | INFO | fairseq.trainer | begin training epoch 38
2024-08-11 19:17:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:17:56 | INFO | train_inner | epoch 038:      1 / 63 loss=5.202, nll_loss=1.445, ppl=2.72, wps=100, ups=0.04, wpb=2553.5, bsz=96, num_updates=2332, lr=2.7984e-05, gnorm=1.891, train_wall=4, gb_free=12.3, wall=8583
2024-08-11 19:18:01 | INFO | train_inner | epoch 038:      3 / 63 loss=5.207, nll_loss=1.469, ppl=2.77, wps=1353.2, ups=0.39, wpb=3491.5, bsz=140, num_updates=2334, lr=2.8008e-05, gnorm=1.57, train_wall=5, gb_free=9.7, wall=8588
2024-08-11 19:18:06 | INFO | train_inner | epoch 038:      5 / 63 loss=5.181, nll_loss=1.435, ppl=2.7, wps=1404.1, ups=0.41, wpb=3453.5, bsz=156, num_updates=2336, lr=2.8032e-05, gnorm=1.497, train_wall=5, gb_free=13.1, wall=8593
2024-08-11 19:18:21 | INFO | train_inner | epoch 038:      7 / 63 loss=5.27, nll_loss=1.574, ppl=2.98, wps=486.1, ups=0.13, wpb=3672, bsz=132, num_updates=2338, lr=2.8056e-05, gnorm=1.677, train_wall=15, gb_free=16.2, wall=8609
2024-08-11 19:18:26 | INFO | train_inner | epoch 038:      9 / 63 loss=5.162, nll_loss=1.443, ppl=2.72, wps=1377.7, ups=0.41, wpb=3344, bsz=188, num_updates=2340, lr=2.808e-05, gnorm=1.701, train_wall=5, gb_free=12.3, wall=8613
2024-08-11 19:18:31 | INFO | train_inner | epoch 038:     11 / 63 loss=5.281, nll_loss=1.554, ppl=2.94, wps=1395.4, ups=0.37, wpb=3794.5, bsz=144, num_updates=2342, lr=2.8104e-05, gnorm=1.482, train_wall=5, gb_free=11.5, wall=8619
2024-08-11 19:18:37 | INFO | train_inner | epoch 038:     13 / 63 loss=5.006, nll_loss=1.215, ppl=2.32, wps=1037, ups=0.39, wpb=2632.5, bsz=188, num_updates=2344, lr=2.8128e-05, gnorm=1.532, train_wall=5, gb_free=11.1, wall=8624
2024-08-11 19:18:42 | INFO | train_inner | epoch 038:     15 / 63 loss=5.124, nll_loss=1.354, ppl=2.56, wps=1235.1, ups=0.38, wpb=3249.5, bsz=128, num_updates=2346, lr=2.8152e-05, gnorm=1.435, train_wall=5, gb_free=12.3, wall=8629
2024-08-11 19:18:46 | INFO | train_inner | epoch 038:     17 / 63 loss=5.169, nll_loss=1.439, ppl=2.71, wps=1402.6, ups=0.46, wpb=3021.5, bsz=88, num_updates=2348, lr=2.8176e-05, gnorm=1.676, train_wall=4, gb_free=12, wall=8633
2024-08-11 19:18:51 | INFO | train_inner | epoch 038:     19 / 63 loss=5.191, nll_loss=1.478, ppl=2.79, wps=1121.8, ups=0.4, wpb=2809.5, bsz=132, num_updates=2350, lr=2.82e-05, gnorm=1.686, train_wall=5, gb_free=11.5, wall=8638
2024-08-11 19:18:56 | INFO | train_inner | epoch 038:     21 / 63 loss=5.198, nll_loss=1.456, ppl=2.74, wps=1226.1, ups=0.4, wpb=3076.5, bsz=124, num_updates=2352, lr=2.8224e-05, gnorm=1.7, train_wall=5, gb_free=12.8, wall=8643
2024-08-11 19:19:01 | INFO | train_inner | epoch 038:     23 / 63 loss=5.204, nll_loss=1.471, ppl=2.77, wps=1451, ups=0.38, wpb=3868.5, bsz=252, num_updates=2354, lr=2.8248e-05, gnorm=1.357, train_wall=5, gb_free=11.4, wall=8649
2024-08-11 19:19:06 | INFO | train_inner | epoch 038:     25 / 63 loss=5.24, nll_loss=1.487, ppl=2.8, wps=1344.9, ups=0.46, wpb=2952.5, bsz=112, num_updates=2356, lr=2.8272e-05, gnorm=1.832, train_wall=4, gb_free=13.9, wall=8653
2024-08-11 19:19:11 | INFO | train_inner | epoch 038:     27 / 63 loss=5.227, nll_loss=1.518, ppl=2.86, wps=1379.9, ups=0.36, wpb=3796, bsz=228, num_updates=2358, lr=2.8296e-05, gnorm=1.394, train_wall=5, gb_free=12.8, wall=8659
2024-08-11 19:19:16 | INFO | train_inner | epoch 038:     29 / 63 loss=5.23, nll_loss=1.512, ppl=2.85, wps=1382.3, ups=0.41, wpb=3345.5, bsz=104, num_updates=2360, lr=2.832e-05, gnorm=1.737, train_wall=5, gb_free=13.2, wall=8664
2024-08-11 19:19:22 | INFO | train_inner | epoch 038:     31 / 63 loss=5.257, nll_loss=1.56, ppl=2.95, wps=1486.5, ups=0.38, wpb=3936.5, bsz=164, num_updates=2362, lr=2.8344e-05, gnorm=1.461, train_wall=5, gb_free=13.3, wall=8669
2024-08-11 19:19:26 | INFO | train_inner | epoch 038:     33 / 63 loss=5.261, nll_loss=1.536, ppl=2.9, wps=1485.9, ups=0.42, wpb=3555, bsz=120, num_updates=2364, lr=2.8368e-05, gnorm=1.611, train_wall=5, gb_free=12.7, wall=8674
2024-08-11 19:19:32 | INFO | train_inner | epoch 038:     35 / 63 loss=5.13, nll_loss=1.368, ppl=2.58, wps=1068.3, ups=0.38, wpb=2777, bsz=120, num_updates=2366, lr=2.8392e-05, gnorm=1.606, train_wall=5, gb_free=11.1, wall=8679
2024-08-11 19:19:37 | INFO | train_inner | epoch 038:     37 / 63 loss=5.31, nll_loss=1.602, ppl=3.04, wps=1449.3, ups=0.36, wpb=3977, bsz=172, num_updates=2368, lr=2.8416e-05, gnorm=1.505, train_wall=5, gb_free=10.6, wall=8684
2024-08-11 19:19:42 | INFO | train_inner | epoch 038:     39 / 63 loss=5.177, nll_loss=1.457, ppl=2.75, wps=1400.3, ups=0.38, wpb=3695, bsz=228, num_updates=2370, lr=2.844e-05, gnorm=1.576, train_wall=5, gb_free=12.1, wall=8690
2024-08-11 19:19:48 | INFO | train_inner | epoch 038:     41 / 63 loss=5.192, nll_loss=1.473, ppl=2.78, wps=1446, ups=0.36, wpb=3995, bsz=264, num_updates=2372, lr=2.8464e-05, gnorm=1.379, train_wall=6, gb_free=12.7, wall=8695
2024-08-11 19:19:53 | INFO | train_inner | epoch 038:     43 / 63 loss=5.221, nll_loss=1.492, ppl=2.81, wps=1301.9, ups=0.38, wpb=3432.5, bsz=104, num_updates=2374, lr=2.8488e-05, gnorm=1.609, train_wall=5, gb_free=11.8, wall=8700
2024-08-11 19:19:57 | INFO | train_inner | epoch 038:     45 / 63 loss=5.324, nll_loss=1.634, ppl=3.1, wps=1265.7, ups=0.49, wpb=2603.5, bsz=120.5, num_updates=2376, lr=2.8512e-05, gnorm=2.087, train_wall=4, gb_free=16.5, wall=8705
2024-08-11 19:20:03 | INFO | train_inner | epoch 038:     47 / 63 loss=5.291, nll_loss=1.56, ppl=2.95, wps=1337.5, ups=0.37, wpb=3577, bsz=108, num_updates=2378, lr=2.8536e-05, gnorm=1.803, train_wall=5, gb_free=12, wall=8710
2024-08-11 19:20:07 | INFO | train_inner | epoch 038:     49 / 63 loss=5.145, nll_loss=1.394, ppl=2.63, wps=1193.1, ups=0.42, wpb=2846, bsz=116, num_updates=2380, lr=2.856e-05, gnorm=1.596, train_wall=5, gb_free=15.3, wall=8715
2024-08-11 19:20:12 | INFO | train_inner | epoch 038:     51 / 63 loss=5.312, nll_loss=1.606, ppl=3.04, wps=1310.2, ups=0.41, wpb=3211.5, bsz=84, num_updates=2382, lr=2.8584e-05, gnorm=1.889, train_wall=5, gb_free=10.8, wall=8720
2024-08-11 19:20:17 | INFO | train_inner | epoch 038:     53 / 63 loss=5.31, nll_loss=1.609, ppl=3.05, wps=1377.5, ups=0.42, wpb=3289.5, bsz=124, num_updates=2384, lr=2.8608e-05, gnorm=1.668, train_wall=5, gb_free=12.6, wall=8724
2024-08-11 19:20:22 | INFO | train_inner | epoch 038:     55 / 63 loss=5.169, nll_loss=1.424, ppl=2.68, wps=1358.5, ups=0.39, wpb=3517.5, bsz=152, num_updates=2386, lr=2.8632e-05, gnorm=1.598, train_wall=5, gb_free=10.5, wall=8729
2024-08-11 19:20:27 | INFO | train_inner | epoch 038:     57 / 63 loss=5.302, nll_loss=1.596, ppl=3.02, wps=1460.2, ups=0.39, wpb=3743, bsz=180, num_updates=2388, lr=2.8656e-05, gnorm=1.537, train_wall=5, gb_free=11.6, wall=8735
2024-08-11 19:20:33 | INFO | train_inner | epoch 038:     59 / 63 loss=5.297, nll_loss=1.571, ppl=2.97, wps=1422.4, ups=0.38, wpb=3736.5, bsz=132, num_updates=2390, lr=2.868e-05, gnorm=2.052, train_wall=5, gb_free=13, wall=8740
2024-08-11 19:20:38 | INFO | train_inner | epoch 038:     61 / 63 loss=5.181, nll_loss=1.418, ppl=2.67, wps=1330.2, ups=0.37, wpb=3564.5, bsz=96, num_updates=2392, lr=2.8704e-05, gnorm=1.646, train_wall=5, gb_free=11.6, wall=8745
2024-08-11 19:20:42 | INFO | train_inner | epoch 038:     63 / 63 loss=5.419, nll_loss=1.765, ppl=3.4, wps=1572.9, ups=0.47, wpb=3349.5, bsz=188, num_updates=2394, lr=2.8728e-05, gnorm=1.697, train_wall=4, gb_free=14, wall=8749
2024-08-11 19:20:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=7619.5859375Mb; avail=247462.64453125Mb
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000534
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7619.5859375Mb; avail=247462.64453125Mb
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004776
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7619.5859375Mb; avail=247462.64453125Mb
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003986
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009661
2024-08-11 19:20:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=7619.5859375Mb; avail=247462.64453125Mb
2024-08-11 19:20:51 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.178 | nll_loss 2.528 | ppl 5.77 | wps 3062.2 | wpb 1463.4 | bsz 64.9 | num_updates 2394 | best_loss 6.167
2024-08-11 19:20:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2394 updates
2024-08-11 19:20:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:21:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 38 @ 2394 updates, score 6.178) (writing took 38.61427046125755 seconds)
2024-08-11 19:21:29 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-08-11 19:21:29 | INFO | train | epoch 038 | loss 5.229 | nll_loss 1.503 | ppl 2.83 | wps 992.8 | ups 0.29 | wpb 3400.7 | bsz 148.2 | num_updates 2394 | lr 2.8728e-05 | gnorm 1.631 | train_wall 168 | gb_free 14 | wall 8797
2024-08-11 19:21:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:21:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:21:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:21:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000756
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9984.4296875Mb; avail=245097.796875Mb
2024-08-11 19:21:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000137
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001240
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9984.4296875Mb; avail=245097.796875Mb
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000042
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9984.4296875Mb; avail=245097.796875Mb
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000438
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002003
2024-08-11 19:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9984.42578125Mb; avail=245097.796875Mb
2024-08-11 19:21:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:21:29 | INFO | fairseq.trainer | begin training epoch 39
2024-08-11 19:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:21:34 | INFO | train_inner | epoch 039:      2 / 63 loss=5.035, nll_loss=1.285, ppl=2.44, wps=115.7, ups=0.04, wpb=3011.5, bsz=176, num_updates=2396, lr=2.8752e-05, gnorm=1.517, train_wall=5, gb_free=10.4, wall=8802
2024-08-11 19:21:39 | INFO | train_inner | epoch 039:      4 / 63 loss=5.092, nll_loss=1.327, ppl=2.51, wps=1092.7, ups=0.4, wpb=2754.5, bsz=84, num_updates=2398, lr=2.8776e-05, gnorm=1.952, train_wall=5, gb_free=12.3, wall=8807
2024-08-11 19:21:45 | INFO | train_inner | epoch 039:      6 / 63 loss=5.126, nll_loss=1.351, ppl=2.55, wps=1271.6, ups=0.38, wpb=3369, bsz=104, num_updates=2400, lr=2.88e-05, gnorm=1.543, train_wall=5, gb_free=12.5, wall=8812
2024-08-11 19:21:49 | INFO | train_inner | epoch 039:      8 / 63 loss=5.171, nll_loss=1.425, ppl=2.68, wps=1280.9, ups=0.44, wpb=2930.5, bsz=120, num_updates=2402, lr=2.8824e-05, gnorm=1.661, train_wall=5, gb_free=12.8, wall=8816
2024-08-11 19:21:55 | INFO | train_inner | epoch 039:     10 / 63 loss=5.235, nll_loss=1.526, ppl=2.88, wps=1596.6, ups=0.35, wpb=4598.5, bsz=272, num_updates=2404, lr=2.8848e-05, gnorm=1.445, train_wall=6, gb_free=9.9, wall=8822
2024-08-11 19:22:00 | INFO | train_inner | epoch 039:     12 / 63 loss=5.109, nll_loss=1.341, ppl=2.53, wps=1273.7, ups=0.39, wpb=3302.5, bsz=116, num_updates=2406, lr=2.8872e-05, gnorm=1.589, train_wall=5, gb_free=11.6, wall=8827
2024-08-11 19:22:10 | INFO | train_inner | epoch 039:     14 / 63 loss=5.295, nll_loss=1.581, ppl=2.99, wps=733.1, ups=0.19, wpb=3765, bsz=120, num_updates=2408, lr=2.8896e-05, gnorm=1.613, train_wall=10, gb_free=15.7, wall=8838
2024-08-11 19:22:16 | INFO | train_inner | epoch 039:     16 / 63 loss=5.16, nll_loss=1.42, ppl=2.68, wps=1465, ups=0.38, wpb=3859, bsz=208, num_updates=2410, lr=2.892e-05, gnorm=1.35, train_wall=5, gb_free=12.9, wall=8843
2024-08-11 19:22:20 | INFO | train_inner | epoch 039:     18 / 63 loss=5.171, nll_loss=1.417, ppl=2.67, wps=1275.3, ups=0.44, wpb=2926.5, bsz=112, num_updates=2412, lr=2.8944e-05, gnorm=1.65, train_wall=5, gb_free=11.3, wall=8848
2024-08-11 19:22:25 | INFO | train_inner | epoch 039:     20 / 63 loss=5.129, nll_loss=1.404, ppl=2.65, wps=1280.6, ups=0.38, wpb=3336, bsz=196, num_updates=2414, lr=2.8968e-05, gnorm=1.456, train_wall=5, gb_free=13.7, wall=8853
2024-08-11 19:22:31 | INFO | train_inner | epoch 039:     22 / 63 loss=5.234, nll_loss=1.521, ppl=2.87, wps=1369.1, ups=0.38, wpb=3595, bsz=160, num_updates=2416, lr=2.8992e-05, gnorm=1.553, train_wall=5, gb_free=11.4, wall=8858
2024-08-11 19:22:35 | INFO | train_inner | epoch 039:     24 / 63 loss=5.256, nll_loss=1.527, ppl=2.88, wps=1516.9, ups=0.42, wpb=3609, bsz=136, num_updates=2418, lr=2.9016e-05, gnorm=1.584, train_wall=5, gb_free=16.8, wall=8863
2024-08-11 19:22:40 | INFO | train_inner | epoch 039:     26 / 63 loss=5.142, nll_loss=1.368, ppl=2.58, wps=1431.1, ups=0.45, wpb=3160, bsz=88, num_updates=2420, lr=2.904e-05, gnorm=1.827, train_wall=4, gb_free=15.7, wall=8867
2024-08-11 19:22:45 | INFO | train_inner | epoch 039:     28 / 63 loss=5.178, nll_loss=1.442, ppl=2.72, wps=1246.8, ups=0.37, wpb=3359, bsz=124, num_updates=2422, lr=2.9064e-05, gnorm=1.722, train_wall=5, gb_free=9.6, wall=8873
2024-08-11 19:22:50 | INFO | train_inner | epoch 039:     30 / 63 loss=5.209, nll_loss=1.45, ppl=2.73, wps=1373, ups=0.39, wpb=3534.5, bsz=104, num_updates=2424, lr=2.9088e-05, gnorm=1.643, train_wall=5, gb_free=13.1, wall=8878
2024-08-11 19:22:55 | INFO | train_inner | epoch 039:     32 / 63 loss=5.179, nll_loss=1.416, ppl=2.67, wps=1267.3, ups=0.44, wpb=2892, bsz=60, num_updates=2426, lr=2.9112e-05, gnorm=1.764, train_wall=5, gb_free=19.3, wall=8882
2024-08-11 19:23:00 | INFO | train_inner | epoch 039:     34 / 63 loss=5.294, nll_loss=1.6, ppl=3.03, wps=1338.9, ups=0.39, wpb=3417, bsz=152, num_updates=2428, lr=2.9136e-05, gnorm=2.718, train_wall=5, gb_free=12.4, wall=8887
2024-08-11 19:23:05 | INFO | train_inner | epoch 039:     36 / 63 loss=5.205, nll_loss=1.469, ppl=2.77, wps=1359.4, ups=0.37, wpb=3637, bsz=164, num_updates=2430, lr=2.916e-05, gnorm=1.547, train_wall=5, gb_free=12.8, wall=8893
2024-08-11 19:23:10 | INFO | train_inner | epoch 039:     38 / 63 loss=5.201, nll_loss=1.483, ppl=2.8, wps=1293.5, ups=0.4, wpb=3263, bsz=152, num_updates=2432, lr=2.9184e-05, gnorm=1.663, train_wall=5, gb_free=13.5, wall=8898
2024-08-11 19:23:14 | INFO | train_inner | epoch 039:     40 / 63 loss=5.21, nll_loss=1.486, ppl=2.8, wps=1272.1, ups=0.5, wpb=2524.5, bsz=64.5, num_updates=2434, lr=2.9208e-05, gnorm=2.428, train_wall=4, gb_free=12.8, wall=8902
2024-08-11 19:23:20 | INFO | train_inner | epoch 039:     42 / 63 loss=5.205, nll_loss=1.468, ppl=2.77, wps=1317.8, ups=0.37, wpb=3522, bsz=156, num_updates=2436, lr=2.9232e-05, gnorm=1.589, train_wall=5, gb_free=11.1, wall=8907
2024-08-11 19:23:25 | INFO | train_inner | epoch 039:     44 / 63 loss=5.151, nll_loss=1.401, ppl=2.64, wps=1430.5, ups=0.36, wpb=3951.5, bsz=188, num_updates=2438, lr=2.9256e-05, gnorm=1.343, train_wall=6, gb_free=11.1, wall=8913
2024-08-11 19:23:31 | INFO | train_inner | epoch 039:     46 / 63 loss=5.039, nll_loss=1.264, ppl=2.4, wps=1207.1, ups=0.37, wpb=3304.5, bsz=244, num_updates=2440, lr=2.928e-05, gnorm=1.464, train_wall=5, gb_free=11.4, wall=8918
2024-08-11 19:23:36 | INFO | train_inner | epoch 039:     48 / 63 loss=5.146, nll_loss=1.386, ppl=2.61, wps=1276.8, ups=0.4, wpb=3192, bsz=92, num_updates=2442, lr=2.9304e-05, gnorm=1.689, train_wall=5, gb_free=14.1, wall=8923
2024-08-11 19:23:41 | INFO | train_inner | epoch 039:     50 / 63 loss=5.271, nll_loss=1.571, ppl=2.97, wps=1483.2, ups=0.37, wpb=4002, bsz=184, num_updates=2444, lr=2.9328e-05, gnorm=1.516, train_wall=5, gb_free=12.7, wall=8929
2024-08-11 19:23:47 | INFO | train_inner | epoch 039:     52 / 63 loss=5.149, nll_loss=1.44, ppl=2.71, wps=1349.2, ups=0.36, wpb=3702, bsz=252, num_updates=2446, lr=2.9352e-05, gnorm=1.39, train_wall=5, gb_free=12.1, wall=8934
2024-08-11 19:23:52 | INFO | train_inner | epoch 039:     54 / 63 loss=5.191, nll_loss=1.444, ppl=2.72, wps=1217.2, ups=0.41, wpb=2986.5, bsz=120, num_updates=2448, lr=2.9376e-05, gnorm=1.63, train_wall=5, gb_free=10.8, wall=8939
2024-08-11 19:23:56 | INFO | train_inner | epoch 039:     56 / 63 loss=5.196, nll_loss=1.466, ppl=2.76, wps=1585.9, ups=0.41, wpb=3880, bsz=232, num_updates=2450, lr=2.94e-05, gnorm=1.425, train_wall=5, gb_free=12.9, wall=8944
2024-08-11 19:24:02 | INFO | train_inner | epoch 039:     58 / 63 loss=5.269, nll_loss=1.545, ppl=2.92, wps=1339.9, ups=0.39, wpb=3393.5, bsz=132, num_updates=2452, lr=2.9424e-05, gnorm=1.668, train_wall=5, gb_free=12.4, wall=8949
2024-08-11 19:24:07 | INFO | train_inner | epoch 039:     60 / 63 loss=5.205, nll_loss=1.465, ppl=2.76, wps=1409.5, ups=0.39, wpb=3586.5, bsz=124, num_updates=2454, lr=2.9448e-05, gnorm=1.535, train_wall=5, gb_free=11.7, wall=8954
2024-08-11 19:24:12 | INFO | train_inner | epoch 039:     62 / 63 loss=5.343, nll_loss=1.663, ppl=3.17, wps=1559.2, ups=0.39, wpb=4017.5, bsz=212, num_updates=2456, lr=2.9472e-05, gnorm=1.478, train_wall=5, gb_free=13.5, wall=8959
2024-08-11 19:24:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6911.97265625Mb; avail=248170.30078125Mb
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000528
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6911.97265625Mb; avail=248170.30078125Mb
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004643
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6911.97265625Mb; avail=248170.30078125Mb
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003976
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009475
2024-08-11 19:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6911.97265625Mb; avail=248170.30078125Mb
2024-08-11 19:24:22 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.191 | nll_loss 2.517 | ppl 5.72 | wps 3065.4 | wpb 1463.4 | bsz 64.9 | num_updates 2457 | best_loss 6.167
2024-08-11 19:24:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2457 updates
2024-08-11 19:24:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:24:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:25:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 39 @ 2457 updates, score 6.191) (writing took 38.808418586384505 seconds)
2024-08-11 19:25:00 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-08-11 19:25:00 | INFO | train | epoch 039 | loss 5.19 | nll_loss 1.454 | ppl 2.74 | wps 1014.4 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2457 | lr 2.9484e-05 | gnorm 1.656 | train_wall 164 | gb_free 15.2 | wall 9008
2024-08-11 19:25:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:25:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:25:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:25:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000725
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9286.796875Mb; avail=245795.4765625Mb
2024-08-11 19:25:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000140
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001237
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9286.796875Mb; avail=245795.4765625Mb
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000041
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9286.796875Mb; avail=245795.4765625Mb
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000439
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002015
2024-08-11 19:25:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9286.796875Mb; avail=245795.4765625Mb
2024-08-11 19:25:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:25:00 | INFO | fairseq.trainer | begin training epoch 40
2024-08-11 19:25:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:25:03 | INFO | train_inner | epoch 040:      1 / 63 loss=4.981, nll_loss=1.191, ppl=2.28, wps=102.9, ups=0.04, wpb=2650.5, bsz=196, num_updates=2458, lr=2.9496e-05, gnorm=1.794, train_wall=4, gb_free=10.8, wall=9011
2024-08-11 19:25:09 | INFO | train_inner | epoch 040:      3 / 63 loss=5.17, nll_loss=1.436, ppl=2.71, wps=1450.6, ups=0.38, wpb=3867, bsz=176, num_updates=2460, lr=2.952e-05, gnorm=1.484, train_wall=5, gb_free=10.5, wall=9016
2024-08-11 19:25:14 | INFO | train_inner | epoch 040:      5 / 63 loss=5.2, nll_loss=1.485, ppl=2.8, wps=1266.7, ups=0.41, wpb=3118, bsz=100, num_updates=2462, lr=2.9544e-05, gnorm=1.822, train_wall=5, gb_free=13.9, wall=9021
2024-08-11 19:25:18 | INFO | train_inner | epoch 040:      7 / 63 loss=5.163, nll_loss=1.423, ppl=2.68, wps=1329, ups=0.41, wpb=3225.5, bsz=132, num_updates=2464, lr=2.9568e-05, gnorm=1.554, train_wall=5, gb_free=12.2, wall=9026
2024-08-11 19:25:24 | INFO | train_inner | epoch 040:      9 / 63 loss=5.094, nll_loss=1.31, ppl=2.48, wps=1455.7, ups=0.37, wpb=3941, bsz=196, num_updates=2466, lr=2.9592e-05, gnorm=1.452, train_wall=5, gb_free=12, wall=9031
2024-08-11 19:25:29 | INFO | train_inner | epoch 040:     11 / 63 loss=5.113, nll_loss=1.337, ppl=2.53, wps=1354.9, ups=0.42, wpb=3237, bsz=92, num_updates=2468, lr=2.9616e-05, gnorm=1.688, train_wall=5, gb_free=11.3, wall=9036
2024-08-11 19:25:32 | INFO | train_inner | epoch 040:     13 / 63 loss=5.372, nll_loss=1.717, ppl=3.29, wps=1336.2, ups=0.52, wpb=2548, bsz=108.5, num_updates=2470, lr=2.964e-05, gnorm=2.39, train_wall=4, gb_free=20.3, wall=9040
2024-08-11 19:25:38 | INFO | train_inner | epoch 040:     15 / 63 loss=5.184, nll_loss=1.449, ppl=2.73, wps=1419.7, ups=0.38, wpb=3713, bsz=168, num_updates=2472, lr=2.9664e-05, gnorm=1.574, train_wall=5, gb_free=12.5, wall=9045
2024-08-11 19:25:48 | INFO | train_inner | epoch 040:     17 / 63 loss=5.136, nll_loss=1.37, ppl=2.58, wps=657.7, ups=0.19, wpb=3411, bsz=116, num_updates=2474, lr=2.9688e-05, gnorm=1.633, train_wall=10, gb_free=10.5, wall=9055
2024-08-11 19:25:53 | INFO | train_inner | epoch 040:     19 / 63 loss=5.166, nll_loss=1.412, ppl=2.66, wps=1549.4, ups=0.38, wpb=4121, bsz=188, num_updates=2476, lr=2.9712e-05, gnorm=1.429, train_wall=5, gb_free=12.1, wall=9061
2024-08-11 19:25:58 | INFO | train_inner | epoch 040:     21 / 63 loss=5.077, nll_loss=1.305, ppl=2.47, wps=1287.6, ups=0.4, wpb=3188.5, bsz=156, num_updates=2478, lr=2.9736e-05, gnorm=1.466, train_wall=5, gb_free=13.1, wall=9066
2024-08-11 19:26:04 | INFO | train_inner | epoch 040:     23 / 63 loss=5.125, nll_loss=1.419, ppl=2.67, wps=1482.8, ups=0.37, wpb=3968.5, bsz=256, num_updates=2480, lr=2.976e-05, gnorm=1.901, train_wall=5, gb_free=13.3, wall=9071
2024-08-11 19:26:08 | INFO | train_inner | epoch 040:     25 / 63 loss=5.17, nll_loss=1.436, ppl=2.71, wps=1357.3, ups=0.44, wpb=3100, bsz=124, num_updates=2482, lr=2.9784e-05, gnorm=1.727, train_wall=5, gb_free=11.4, wall=9076
2024-08-11 19:26:14 | INFO | train_inner | epoch 040:     27 / 63 loss=5.211, nll_loss=1.459, ppl=2.75, wps=1381.7, ups=0.37, wpb=3777, bsz=164, num_updates=2484, lr=2.9808e-05, gnorm=1.444, train_wall=5, gb_free=12.8, wall=9081
2024-08-11 19:26:19 | INFO | train_inner | epoch 040:     29 / 63 loss=5.17, nll_loss=1.416, ppl=2.67, wps=1476.6, ups=0.4, wpb=3666.5, bsz=220, num_updates=2486, lr=2.9832e-05, gnorm=1.428, train_wall=5, gb_free=14.9, wall=9086
2024-08-11 19:26:24 | INFO | train_inner | epoch 040:     31 / 63 loss=5.118, nll_loss=1.346, ppl=2.54, wps=1285.5, ups=0.4, wpb=3187.5, bsz=120, num_updates=2488, lr=2.9856e-05, gnorm=1.638, train_wall=5, gb_free=14, wall=9091
2024-08-11 19:26:29 | INFO | train_inner | epoch 040:     33 / 63 loss=5.144, nll_loss=1.407, ppl=2.65, wps=1426, ups=0.37, wpb=3821, bsz=172, num_updates=2490, lr=2.988e-05, gnorm=1.469, train_wall=5, gb_free=13.2, wall=9096
2024-08-11 19:26:34 | INFO | train_inner | epoch 040:     35 / 63 loss=5.126, nll_loss=1.369, ppl=2.58, wps=1380.8, ups=0.41, wpb=3379, bsz=128, num_updates=2492, lr=2.9904e-05, gnorm=1.492, train_wall=5, gb_free=15.8, wall=9101
2024-08-11 19:26:39 | INFO | train_inner | epoch 040:     37 / 63 loss=5.12, nll_loss=1.375, ppl=2.59, wps=1372.8, ups=0.4, wpb=3473.5, bsz=136, num_updates=2494, lr=2.9928e-05, gnorm=1.586, train_wall=5, gb_free=11.1, wall=9106
2024-08-11 19:26:44 | INFO | train_inner | epoch 040:     39 / 63 loss=5.12, nll_loss=1.361, ppl=2.57, wps=1377.5, ups=0.37, wpb=3734.5, bsz=156, num_updates=2496, lr=2.9952e-05, gnorm=1.479, train_wall=5, gb_free=11.9, wall=9112
2024-08-11 19:26:49 | INFO | train_inner | epoch 040:     41 / 63 loss=4.969, nll_loss=1.173, ppl=2.25, wps=1215.2, ups=0.44, wpb=2777, bsz=208, num_updates=2498, lr=2.9976e-05, gnorm=1.373, train_wall=5, gb_free=11.1, wall=9116
2024-08-11 19:26:54 | INFO | train_inner | epoch 040:     43 / 63 loss=5.065, nll_loss=1.29, ppl=2.45, wps=1223.2, ups=0.4, wpb=3061.5, bsz=100, num_updates=2500, lr=3e-05, gnorm=1.57, train_wall=5, gb_free=12.5, wall=9121
2024-08-11 19:26:59 | INFO | train_inner | epoch 040:     45 / 63 loss=5.117, nll_loss=1.383, ppl=2.61, wps=1303.1, ups=0.42, wpb=3131.5, bsz=80, num_updates=2502, lr=2.9988e-05, gnorm=1.678, train_wall=5, gb_free=13.1, wall=9126
2024-08-11 19:27:04 | INFO | train_inner | epoch 040:     47 / 63 loss=5.278, nll_loss=1.588, ppl=3.01, wps=1562.3, ups=0.38, wpb=4105.5, bsz=208, num_updates=2504, lr=2.9976e-05, gnorm=1.59, train_wall=5, gb_free=14.1, wall=9131
2024-08-11 19:27:09 | INFO | train_inner | epoch 040:     49 / 63 loss=5.207, nll_loss=1.442, ppl=2.72, wps=1383, ups=0.41, wpb=3414.5, bsz=116, num_updates=2506, lr=2.99641e-05, gnorm=1.599, train_wall=5, gb_free=13.9, wall=9136
2024-08-11 19:27:14 | INFO | train_inner | epoch 040:     51 / 63 loss=5.095, nll_loss=1.31, ppl=2.48, wps=1367.9, ups=0.39, wpb=3491, bsz=160, num_updates=2508, lr=2.99521e-05, gnorm=1.456, train_wall=5, gb_free=12.7, wall=9141
2024-08-11 19:27:19 | INFO | train_inner | epoch 040:     53 / 63 loss=5.187, nll_loss=1.453, ppl=2.74, wps=1478.6, ups=0.37, wpb=3961, bsz=216, num_updates=2510, lr=2.99402e-05, gnorm=1.562, train_wall=5, gb_free=11.4, wall=9147
2024-08-11 19:27:25 | INFO | train_inner | epoch 040:     55 / 63 loss=5.165, nll_loss=1.431, ppl=2.7, wps=1173.5, ups=0.38, wpb=3062, bsz=60, num_updates=2512, lr=2.99283e-05, gnorm=2.06, train_wall=5, gb_free=13.7, wall=9152
2024-08-11 19:27:30 | INFO | train_inner | epoch 040:     57 / 63 loss=5.263, nll_loss=1.55, ppl=2.93, wps=1260.6, ups=0.38, wpb=3337, bsz=172, num_updates=2514, lr=2.99164e-05, gnorm=1.663, train_wall=5, gb_free=10.1, wall=9157
2024-08-11 19:27:35 | INFO | train_inner | epoch 040:     59 / 63 loss=5.069, nll_loss=1.265, ppl=2.4, wps=1109.5, ups=0.4, wpb=2778.5, bsz=72, num_updates=2516, lr=2.99045e-05, gnorm=1.949, train_wall=5, gb_free=10.4, wall=9162
2024-08-11 19:27:40 | INFO | train_inner | epoch 040:     61 / 63 loss=5.12, nll_loss=1.338, ppl=2.53, wps=1201.2, ups=0.37, wpb=3275, bsz=116, num_updates=2518, lr=2.98926e-05, gnorm=1.616, train_wall=5, gb_free=10.3, wall=9168
2024-08-11 19:27:44 | INFO | train_inner | epoch 040:     63 / 63 loss=5.201, nll_loss=1.49, ppl=2.81, wps=1374.1, ups=0.59, wpb=2339.5, bsz=76, num_updates=2520, lr=2.98807e-05, gnorm=2.063, train_wall=3, gb_free=18.2, wall=9171
2024-08-11 19:27:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6922.23046875Mb; avail=248160.0390625Mb
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000535
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6922.23046875Mb; avail=248160.0390625Mb
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004826
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6922.23046875Mb; avail=248160.0390625Mb
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004061
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009760
2024-08-11 19:27:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6922.23046875Mb; avail=248160.0390625Mb
2024-08-11 19:27:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.174 | nll_loss 2.538 | ppl 5.81 | wps 3065.4 | wpb 1463.4 | bsz 64.9 | num_updates 2520 | best_loss 6.167
2024-08-11 19:27:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2520 updates
2024-08-11 19:27:52 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:28:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 40 @ 2520 updates, score 6.174) (writing took 38.766177288256586 seconds)
2024-08-11 19:28:31 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-08-11 19:28:31 | INFO | train | epoch 040 | loss 5.149 | nll_loss 1.401 | ppl 2.64 | wps 1017.7 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2520 | lr 2.98807e-05 | gnorm 1.632 | train_wall 163 | gb_free 18.2 | wall 9218
2024-08-11 19:28:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:28:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:28:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:28:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000762
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9294.90625Mb; avail=245787.36328125Mb
2024-08-11 19:28:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000175
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001266
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9294.90625Mb; avail=245787.36328125Mb
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000041
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9294.90625Mb; avail=245787.36328125Mb
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000424
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002015
2024-08-11 19:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9294.90625Mb; avail=245787.36328125Mb
2024-08-11 19:28:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:28:31 | INFO | fairseq.trainer | begin training epoch 41
2024-08-11 19:28:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:28:36 | INFO | train_inner | epoch 041:      2 / 63 loss=5.024, nll_loss=1.238, ppl=2.36, wps=122.3, ups=0.04, wpb=3208.5, bsz=64, num_updates=2522, lr=2.98689e-05, gnorm=1.579, train_wall=5, gb_free=11.3, wall=9224
2024-08-11 19:28:41 | INFO | train_inner | epoch 041:      4 / 63 loss=5.176, nll_loss=1.415, ppl=2.67, wps=1475.6, ups=0.46, wpb=3188, bsz=128.5, num_updates=2524, lr=2.9857e-05, gnorm=1.827, train_wall=4, gb_free=10.3, wall=9228
2024-08-11 19:28:45 | INFO | train_inner | epoch 041:      6 / 63 loss=5.105, nll_loss=1.33, ppl=2.51, wps=1424.3, ups=0.41, wpb=3436, bsz=104, num_updates=2526, lr=2.98452e-05, gnorm=1.528, train_wall=5, gb_free=13, wall=9233
2024-08-11 19:28:50 | INFO | train_inner | epoch 041:      8 / 63 loss=5.035, nll_loss=1.223, ppl=2.33, wps=1236.2, ups=0.41, wpb=3015.5, bsz=72, num_updates=2528, lr=2.98334e-05, gnorm=1.669, train_wall=5, gb_free=10.8, wall=9238
2024-08-11 19:28:56 | INFO | train_inner | epoch 041:     10 / 63 loss=5.015, nll_loss=1.252, ppl=2.38, wps=1415, ups=0.37, wpb=3775.5, bsz=240, num_updates=2530, lr=2.98216e-05, gnorm=1.268, train_wall=5, gb_free=12.5, wall=9243
2024-08-11 19:29:01 | INFO | train_inner | epoch 041:     12 / 63 loss=5.072, nll_loss=1.312, ppl=2.48, wps=1345.1, ups=0.39, wpb=3456, bsz=88, num_updates=2532, lr=2.98098e-05, gnorm=1.574, train_wall=5, gb_free=13.6, wall=9248
2024-08-11 19:29:06 | INFO | train_inner | epoch 041:     14 / 63 loss=5.18, nll_loss=1.45, ppl=2.73, wps=1340.1, ups=0.38, wpb=3566.5, bsz=144, num_updates=2534, lr=2.97981e-05, gnorm=1.591, train_wall=5, gb_free=13.1, wall=9253
2024-08-11 19:29:11 | INFO | train_inner | epoch 041:     16 / 63 loss=5.102, nll_loss=1.318, ppl=2.49, wps=1286.4, ups=0.41, wpb=3159, bsz=140, num_updates=2536, lr=2.97863e-05, gnorm=1.586, train_wall=5, gb_free=13.1, wall=9258
2024-08-11 19:29:16 | INFO | train_inner | epoch 041:     18 / 63 loss=5.08, nll_loss=1.309, ppl=2.48, wps=1191.5, ups=0.41, wpb=2939, bsz=124, num_updates=2538, lr=2.97746e-05, gnorm=1.667, train_wall=5, gb_free=12.2, wall=9263
2024-08-11 19:29:21 | INFO | train_inner | epoch 041:     20 / 63 loss=5.073, nll_loss=1.297, ppl=2.46, wps=1152.5, ups=0.42, wpb=2739, bsz=92, num_updates=2540, lr=2.97628e-05, gnorm=1.742, train_wall=5, gb_free=13, wall=9268
2024-08-11 19:29:26 | INFO | train_inner | epoch 041:     22 / 63 loss=5.02, nll_loss=1.234, ppl=2.35, wps=1223.7, ups=0.4, wpb=3091, bsz=120, num_updates=2542, lr=2.97511e-05, gnorm=1.425, train_wall=5, gb_free=10.7, wall=9273
2024-08-11 19:29:31 | INFO | train_inner | epoch 041:     24 / 63 loss=5.081, nll_loss=1.333, ppl=2.52, wps=1474.8, ups=0.37, wpb=3953.5, bsz=164, num_updates=2544, lr=2.97394e-05, gnorm=1.36, train_wall=5, gb_free=11.8, wall=9278
2024-08-11 19:29:36 | INFO | train_inner | epoch 041:     26 / 63 loss=5.116, nll_loss=1.375, ppl=2.59, wps=1328.9, ups=0.42, wpb=3185, bsz=140, num_updates=2546, lr=2.97278e-05, gnorm=1.601, train_wall=5, gb_free=14.5, wall=9283
2024-08-11 19:29:46 | INFO | train_inner | epoch 041:     28 / 63 loss=5.093, nll_loss=1.306, ppl=2.47, wps=714.2, ups=0.19, wpb=3670.5, bsz=180, num_updates=2548, lr=2.97161e-05, gnorm=1.415, train_wall=10, gb_free=10.5, wall=9293
2024-08-11 19:29:51 | INFO | train_inner | epoch 041:     30 / 63 loss=5.111, nll_loss=1.346, ppl=2.54, wps=1483.1, ups=0.41, wpb=3617.5, bsz=176, num_updates=2550, lr=2.97044e-05, gnorm=1.501, train_wall=5, gb_free=10.6, wall=9298
2024-08-11 19:29:56 | INFO | train_inner | epoch 041:     32 / 63 loss=5.1, nll_loss=1.362, ppl=2.57, wps=1388.9, ups=0.37, wpb=3793.5, bsz=236, num_updates=2552, lr=2.96928e-05, gnorm=1.371, train_wall=5, gb_free=10.7, wall=9304
2024-08-11 19:30:02 | INFO | train_inner | epoch 041:     34 / 63 loss=5.158, nll_loss=1.408, ppl=2.65, wps=1349.1, ups=0.38, wpb=3544, bsz=120, num_updates=2554, lr=2.96812e-05, gnorm=1.631, train_wall=5, gb_free=13.4, wall=9309
2024-08-11 19:30:07 | INFO | train_inner | epoch 041:     36 / 63 loss=5.129, nll_loss=1.374, ppl=2.59, wps=1509.5, ups=0.37, wpb=4079.5, bsz=188, num_updates=2556, lr=2.96695e-05, gnorm=1.346, train_wall=5, gb_free=8.9, wall=9314
2024-08-11 19:30:12 | INFO | train_inner | epoch 041:     38 / 63 loss=5.182, nll_loss=1.44, ppl=2.71, wps=1558.6, ups=0.37, wpb=4185, bsz=192, num_updates=2558, lr=2.96579e-05, gnorm=1.59, train_wall=5, gb_free=11.1, wall=9320
2024-08-11 19:30:18 | INFO | train_inner | epoch 041:     40 / 63 loss=5.057, nll_loss=1.324, ppl=2.5, wps=1358.2, ups=0.36, wpb=3744.5, bsz=252, num_updates=2560, lr=2.96464e-05, gnorm=1.322, train_wall=6, gb_free=10.5, wall=9325
2024-08-11 19:30:23 | INFO | train_inner | epoch 041:     42 / 63 loss=5.116, nll_loss=1.373, ppl=2.59, wps=1315, ups=0.4, wpb=3291, bsz=156, num_updates=2562, lr=2.96348e-05, gnorm=1.539, train_wall=5, gb_free=13.3, wall=9330
2024-08-11 19:30:28 | INFO | train_inner | epoch 041:     44 / 63 loss=5.005, nll_loss=1.189, ppl=2.28, wps=1220.5, ups=0.38, wpb=3193, bsz=124, num_updates=2564, lr=2.96232e-05, gnorm=1.478, train_wall=5, gb_free=10.3, wall=9336
2024-08-11 19:30:34 | INFO | train_inner | epoch 041:     46 / 63 loss=5.194, nll_loss=1.446, ppl=2.72, wps=1390.7, ups=0.38, wpb=3708, bsz=132, num_updates=2566, lr=2.96117e-05, gnorm=1.603, train_wall=5, gb_free=10.6, wall=9341
2024-08-11 19:30:38 | INFO | train_inner | epoch 041:     48 / 63 loss=4.948, nll_loss=1.154, ppl=2.22, wps=1181.1, ups=0.42, wpb=2821, bsz=120, num_updates=2568, lr=2.96001e-05, gnorm=1.573, train_wall=5, gb_free=11.6, wall=9346
2024-08-11 19:30:43 | INFO | train_inner | epoch 041:     50 / 63 loss=5.099, nll_loss=1.353, ppl=2.55, wps=1209.9, ups=0.4, wpb=2992, bsz=128, num_updates=2570, lr=2.95886e-05, gnorm=1.554, train_wall=5, gb_free=11.9, wall=9351
2024-08-11 19:30:48 | INFO | train_inner | epoch 041:     52 / 63 loss=5.063, nll_loss=1.303, ppl=2.47, wps=1104.4, ups=0.43, wpb=2550, bsz=108, num_updates=2572, lr=2.95771e-05, gnorm=1.693, train_wall=5, gb_free=13.2, wall=9355
2024-08-11 19:30:53 | INFO | train_inner | epoch 041:     54 / 63 loss=5.052, nll_loss=1.29, ppl=2.44, wps=1347.3, ups=0.37, wpb=3641.5, bsz=216, num_updates=2574, lr=2.95656e-05, gnorm=1.417, train_wall=5, gb_free=13.1, wall=9361
2024-08-11 19:30:59 | INFO | train_inner | epoch 041:     56 / 63 loss=5.109, nll_loss=1.357, ppl=2.56, wps=1533.6, ups=0.38, wpb=4038.5, bsz=212, num_updates=2576, lr=2.95541e-05, gnorm=1.312, train_wall=5, gb_free=10.9, wall=9366
2024-08-11 19:31:03 | INFO | train_inner | epoch 041:     58 / 63 loss=5.054, nll_loss=1.278, ppl=2.43, wps=1241.5, ups=0.41, wpb=3057, bsz=108, num_updates=2578, lr=2.95427e-05, gnorm=1.817, train_wall=5, gb_free=13, wall=9371
2024-08-11 19:31:09 | INFO | train_inner | epoch 041:     60 / 63 loss=5.102, nll_loss=1.348, ppl=2.55, wps=1534.6, ups=0.38, wpb=4019.5, bsz=180, num_updates=2580, lr=2.95312e-05, gnorm=1.388, train_wall=5, gb_free=12.3, wall=9376
2024-08-11 19:31:13 | INFO | train_inner | epoch 041:     62 / 63 loss=5.166, nll_loss=1.424, ppl=2.68, wps=1548.9, ups=0.42, wpb=3686.5, bsz=200, num_updates=2582, lr=2.95198e-05, gnorm=1.459, train_wall=5, gb_free=16.1, wall=9381
2024-08-11 19:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=6937.90625Mb; avail=248144.3671875Mb
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000522
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.90625Mb; avail=248144.3671875Mb
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004701
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.90625Mb; avail=248144.3671875Mb
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004018
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009570
2024-08-11 19:31:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=6937.90625Mb; avail=248144.3671875Mb
2024-08-11 19:31:23 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.201 | nll_loss 2.529 | ppl 5.77 | wps 3068.1 | wpb 1463.4 | bsz 64.9 | num_updates 2583 | best_loss 6.167
2024-08-11 19:31:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2583 updates
2024-08-11 19:31:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:32:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:32:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 41 @ 2583 updates, score 6.201) (writing took 39.039108539931476 seconds)
2024-08-11 19:32:02 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-08-11 19:32:02 | INFO | train | epoch 041 | loss 5.093 | nll_loss 1.331 | ppl 2.52 | wps 1014.6 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2583 | lr 2.95141e-05 | gnorm 1.558 | train_wall 163 | gb_free 20.2 | wall 9429
2024-08-11 19:32:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:32:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:32:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:32:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000719
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12051.59765625Mb; avail=243030.640625Mb
2024-08-11 19:32:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000138
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001240
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12051.59765625Mb; avail=243030.640625Mb
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12051.59765625Mb; avail=243030.640625Mb
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000424
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001993
2024-08-11 19:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12051.59765625Mb; avail=243030.640625Mb
2024-08-11 19:32:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:32:02 | INFO | fairseq.trainer | begin training epoch 42
2024-08-11 19:32:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:32:05 | INFO | train_inner | epoch 042:      1 / 63 loss=4.913, nll_loss=1.082, ppl=2.12, wps=85.5, ups=0.04, wpb=2195.5, bsz=104, num_updates=2584, lr=2.95084e-05, gnorm=2.338, train_wall=4, gb_free=10, wall=9432
2024-08-11 19:32:10 | INFO | train_inner | epoch 042:      3 / 63 loss=5.039, nll_loss=1.286, ppl=2.44, wps=1291.7, ups=0.4, wpb=3232.5, bsz=160, num_updates=2586, lr=2.94969e-05, gnorm=1.439, train_wall=5, gb_free=11.3, wall=9437
2024-08-11 19:32:15 | INFO | train_inner | epoch 042:      5 / 63 loss=4.991, nll_loss=1.207, ppl=2.31, wps=1383.8, ups=0.36, wpb=3793, bsz=152, num_updates=2588, lr=2.94855e-05, gnorm=1.369, train_wall=5, gb_free=10.3, wall=9443
2024-08-11 19:32:20 | INFO | train_inner | epoch 042:      7 / 63 loss=4.964, nll_loss=1.168, ppl=2.25, wps=1295.3, ups=0.41, wpb=3143.5, bsz=128, num_updates=2590, lr=2.94742e-05, gnorm=1.438, train_wall=5, gb_free=15.7, wall=9448
2024-08-11 19:32:25 | INFO | train_inner | epoch 042:      9 / 63 loss=4.986, nll_loss=1.205, ppl=2.31, wps=1106.5, ups=0.42, wpb=2662, bsz=144, num_updates=2592, lr=2.94628e-05, gnorm=1.555, train_wall=5, gb_free=15.2, wall=9452
2024-08-11 19:32:30 | INFO | train_inner | epoch 042:     11 / 63 loss=5.093, nll_loss=1.322, ppl=2.5, wps=1449.8, ups=0.38, wpb=3778, bsz=136, num_updates=2594, lr=2.94514e-05, gnorm=1.548, train_wall=5, gb_free=11.4, wall=9458
2024-08-11 19:32:35 | INFO | train_inner | epoch 042:     13 / 63 loss=5.031, nll_loss=1.231, ppl=2.35, wps=1366.4, ups=0.43, wpb=3179, bsz=92, num_updates=2596, lr=2.94401e-05, gnorm=1.594, train_wall=5, gb_free=12.9, wall=9462
2024-08-11 19:32:40 | INFO | train_inner | epoch 042:     15 / 63 loss=5.169, nll_loss=1.419, ppl=2.67, wps=1335.2, ups=0.38, wpb=3503.5, bsz=112, num_updates=2598, lr=2.94287e-05, gnorm=1.616, train_wall=5, gb_free=14.1, wall=9467
2024-08-11 19:32:45 | INFO | train_inner | epoch 042:     17 / 63 loss=5.21, nll_loss=1.484, ppl=2.8, wps=1698, ups=0.38, wpb=4424, bsz=244, num_updates=2600, lr=2.94174e-05, gnorm=1.577, train_wall=5, gb_free=12.3, wall=9473
2024-08-11 19:32:50 | INFO | train_inner | epoch 042:     19 / 63 loss=4.981, nll_loss=1.18, ppl=2.27, wps=1350.7, ups=0.39, wpb=3501, bsz=160, num_updates=2602, lr=2.94061e-05, gnorm=1.338, train_wall=5, gb_free=13.3, wall=9478
2024-08-11 19:32:55 | INFO | train_inner | epoch 042:     21 / 63 loss=5.135, nll_loss=1.37, ppl=2.58, wps=1354.5, ups=0.4, wpb=3347, bsz=84, num_updates=2604, lr=2.93948e-05, gnorm=1.633, train_wall=5, gb_free=12, wall=9483
2024-08-11 19:33:00 | INFO | train_inner | epoch 042:     23 / 63 loss=4.96, nll_loss=1.166, ppl=2.24, wps=1144.8, ups=0.43, wpb=2670.5, bsz=96, num_updates=2606, lr=2.93835e-05, gnorm=1.669, train_wall=5, gb_free=12.3, wall=9487
2024-08-11 19:33:05 | INFO | train_inner | epoch 042:     25 / 63 loss=5.031, nll_loss=1.256, ppl=2.39, wps=1519.5, ups=0.39, wpb=3866, bsz=132, num_updates=2608, lr=2.93723e-05, gnorm=1.478, train_wall=5, gb_free=11.7, wall=9493
2024-08-11 19:33:10 | INFO | train_inner | epoch 042:     27 / 63 loss=5.082, nll_loss=1.302, ppl=2.47, wps=1368, ups=0.39, wpb=3495, bsz=108, num_updates=2610, lr=2.9361e-05, gnorm=1.631, train_wall=5, gb_free=11.9, wall=9498
2024-08-11 19:33:21 | INFO | train_inner | epoch 042:     29 / 63 loss=5.019, nll_loss=1.212, ppl=2.32, wps=716.5, ups=0.19, wpb=3805.5, bsz=196, num_updates=2612, lr=2.93498e-05, gnorm=1.416, train_wall=11, gb_free=12.3, wall=9508
2024-08-11 19:33:25 | INFO | train_inner | epoch 042:     31 / 63 loss=5.156, nll_loss=1.412, ppl=2.66, wps=1570, ups=0.45, wpb=3506, bsz=152, num_updates=2614, lr=2.93385e-05, gnorm=1.545, train_wall=4, gb_free=12.3, wall=9513
2024-08-11 19:33:31 | INFO | train_inner | epoch 042:     33 / 63 loss=5.079, nll_loss=1.323, ppl=2.5, wps=1258.3, ups=0.38, wpb=3326, bsz=144, num_updates=2616, lr=2.93273e-05, gnorm=1.503, train_wall=5, gb_free=10.7, wall=9518
2024-08-11 19:33:36 | INFO | train_inner | epoch 042:     35 / 63 loss=4.991, nll_loss=1.226, ppl=2.34, wps=1157.6, ups=0.41, wpb=2795.5, bsz=128, num_updates=2618, lr=2.93161e-05, gnorm=1.668, train_wall=5, gb_free=13.1, wall=9523
2024-08-11 19:33:40 | INFO | train_inner | epoch 042:     37 / 63 loss=5.083, nll_loss=1.321, ppl=2.5, wps=1322.8, ups=0.41, wpb=3202.5, bsz=96, num_updates=2620, lr=2.93049e-05, gnorm=1.61, train_wall=5, gb_free=12, wall=9528
2024-08-11 19:33:46 | INFO | train_inner | epoch 042:     39 / 63 loss=4.967, nll_loss=1.163, ppl=2.24, wps=1175.2, ups=0.37, wpb=3160, bsz=224, num_updates=2622, lr=2.92937e-05, gnorm=1.449, train_wall=5, gb_free=10.9, wall=9533
2024-08-11 19:33:51 | INFO | train_inner | epoch 042:     41 / 63 loss=5.065, nll_loss=1.269, ppl=2.41, wps=1481.6, ups=0.37, wpb=4018.5, bsz=196, num_updates=2624, lr=2.92826e-05, gnorm=1.367, train_wall=5, gb_free=11, wall=9538
2024-08-11 19:33:57 | INFO | train_inner | epoch 042:     43 / 63 loss=5.067, nll_loss=1.309, ppl=2.48, wps=1597.1, ups=0.36, wpb=4401, bsz=248, num_updates=2626, lr=2.92714e-05, gnorm=1.349, train_wall=6, gb_free=15.7, wall=9544
2024-08-11 19:34:02 | INFO | train_inner | epoch 042:     45 / 63 loss=5.122, nll_loss=1.432, ppl=2.7, wps=1411.1, ups=0.38, wpb=3759.5, bsz=244, num_updates=2628, lr=2.92603e-05, gnorm=1.484, train_wall=5, gb_free=11.8, wall=9549
2024-08-11 19:34:07 | INFO | train_inner | epoch 042:     47 / 63 loss=5.019, nll_loss=1.218, ppl=2.33, wps=1339.9, ups=0.41, wpb=3252, bsz=96, num_updates=2630, lr=2.92492e-05, gnorm=1.514, train_wall=5, gb_free=13, wall=9554
2024-08-11 19:34:12 | INFO | train_inner | epoch 042:     49 / 63 loss=5.155, nll_loss=1.381, ppl=2.6, wps=1440.4, ups=0.39, wpb=3677.5, bsz=172, num_updates=2632, lr=2.9238e-05, gnorm=1.585, train_wall=5, gb_free=12.8, wall=9559
2024-08-11 19:34:17 | INFO | train_inner | epoch 042:     51 / 63 loss=4.986, nll_loss=1.183, ppl=2.27, wps=1270.5, ups=0.37, wpb=3446.5, bsz=140, num_updates=2634, lr=2.92269e-05, gnorm=1.551, train_wall=5, gb_free=11.8, wall=9565
2024-08-11 19:34:22 | INFO | train_inner | epoch 042:     53 / 63 loss=5.022, nll_loss=1.246, ppl=2.37, wps=1354.5, ups=0.43, wpb=3184.5, bsz=100, num_updates=2636, lr=2.92159e-05, gnorm=1.595, train_wall=5, gb_free=14, wall=9569
2024-08-11 19:34:27 | INFO | train_inner | epoch 042:     55 / 63 loss=4.945, nll_loss=1.136, ppl=2.2, wps=1260.2, ups=0.41, wpb=3111, bsz=76, num_updates=2638, lr=2.92048e-05, gnorm=1.544, train_wall=5, gb_free=10.9, wall=9574
2024-08-11 19:34:31 | INFO | train_inner | epoch 042:     57 / 63 loss=5.173, nll_loss=1.454, ppl=2.74, wps=1410.1, ups=0.46, wpb=3033, bsz=132.5, num_updates=2640, lr=2.91937e-05, gnorm=1.658, train_wall=4, gb_free=13.5, wall=9579
2024-08-11 19:34:37 | INFO | train_inner | epoch 042:     59 / 63 loss=5.028, nll_loss=1.257, ppl=2.39, wps=1277, ups=0.38, wpb=3327, bsz=208, num_updates=2642, lr=2.91827e-05, gnorm=1.41, train_wall=5, gb_free=10.9, wall=9584
2024-08-11 19:34:41 | INFO | train_inner | epoch 042:     61 / 63 loss=4.966, nll_loss=1.133, ppl=2.19, wps=1416.7, ups=0.42, wpb=3384, bsz=128, num_updates=2644, lr=2.91716e-05, gnorm=1.418, train_wall=5, gb_free=15.8, wall=9589
2024-08-11 19:34:45 | INFO | train_inner | epoch 042:     63 / 63 loss=5.1, nll_loss=1.358, ppl=2.56, wps=1368.7, ups=0.5, wpb=2718.5, bsz=156, num_updates=2646, lr=2.91606e-05, gnorm=1.885, train_wall=4, gb_free=18.5, wall=9593
2024-08-11 19:34:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9682.6484375Mb; avail=245399.62109375Mb
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9682.6484375Mb; avail=245399.62109375Mb
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004762
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9682.6484375Mb; avail=245399.62109375Mb
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004017
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009666
2024-08-11 19:34:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9682.6484375Mb; avail=245399.62109375Mb
2024-08-11 19:34:54 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.193 | nll_loss 2.564 | ppl 5.91 | wps 3067.6 | wpb 1463.4 | bsz 64.9 | num_updates 2646 | best_loss 6.167
2024-08-11 19:34:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2646 updates
2024-08-11 19:34:54 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:35:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 42 @ 2646 updates, score 6.193) (writing took 38.35867923265323 seconds)
2024-08-11 19:35:32 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-08-11 19:35:32 | INFO | train | epoch 042 | loss 5.053 | nll_loss 1.279 | ppl 2.43 | wps 1020.3 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2646 | lr 2.91606e-05 | gnorm 1.527 | train_wall 163 | gb_free 18.5 | wall 9639
2024-08-11 19:35:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 9337}; raw total size: 9337
2024-08-11 19:35:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 9337}; resampled total size: 9337
2024-08-11 19:35:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-11 19:35:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000763
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15334.765625Mb; avail=239747.5078125Mb
2024-08-11 19:35:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000177
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001323
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15334.765625Mb; avail=239747.5078125Mb
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15334.765625Mb; avail=239747.5078125Mb
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000432
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002107
2024-08-11 19:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15334.765625Mb; avail=239747.5078125Mb
2024-08-11 19:35:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 63
2024-08-11 19:35:32 | INFO | fairseq.trainer | begin training epoch 43
2024-08-11 19:35:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-11 19:35:37 | INFO | train_inner | epoch 043:      2 / 63 loss=4.979, nll_loss=1.204, ppl=2.3, wps=132.2, ups=0.04, wpb=3424.5, bsz=124, num_updates=2648, lr=2.91496e-05, gnorm=1.502, train_wall=5, gb_free=12.5, wall=9644
2024-08-11 19:35:43 | INFO | train_inner | epoch 043:      4 / 63 loss=5.002, nll_loss=1.227, ppl=2.34, wps=1382.1, ups=0.37, wpb=3737.5, bsz=180, num_updates=2650, lr=2.91386e-05, gnorm=1.33, train_wall=5, gb_free=9.1, wall=9650
2024-08-11 19:35:48 | INFO | train_inner | epoch 043:      6 / 63 loss=4.892, nll_loss=1.054, ppl=2.08, wps=1312.4, ups=0.38, wpb=3474, bsz=124, num_updates=2652, lr=2.91276e-05, gnorm=1.323, train_wall=5, gb_free=10.6, wall=9655
2024-08-11 19:35:53 | INFO | train_inner | epoch 043:      8 / 63 loss=5.032, nll_loss=1.263, ppl=2.4, wps=1549.5, ups=0.39, wpb=3987.5, bsz=216, num_updates=2654, lr=2.91166e-05, gnorm=1.5, train_wall=5, gb_free=14, wall=9660
2024-08-11 19:35:58 | INFO | train_inner | epoch 043:     10 / 63 loss=4.948, nll_loss=1.137, ppl=2.2, wps=1455.5, ups=0.38, wpb=3849, bsz=208, num_updates=2656, lr=2.91056e-05, gnorm=1.227, train_wall=5, gb_free=12.8, wall=9666
2024-08-11 19:36:03 | INFO | train_inner | epoch 043:     12 / 63 loss=5.028, nll_loss=1.243, ppl=2.37, wps=1454.5, ups=0.38, wpb=3791.5, bsz=132, num_updates=2658, lr=2.90947e-05, gnorm=1.39, train_wall=5, gb_free=10.5, wall=9671
2024-08-11 19:36:08 | INFO | train_inner | epoch 043:     14 / 63 loss=4.932, nll_loss=1.128, ppl=2.19, wps=1341.4, ups=0.4, wpb=3335.5, bsz=152, num_updates=2660, lr=2.90838e-05, gnorm=1.304, train_wall=5, gb_free=13.6, wall=9676
2024-08-11 19:36:13 | INFO | train_inner | epoch 043:     16 / 63 loss=4.968, nll_loss=1.182, ppl=2.27, wps=1146.9, ups=0.4, wpb=2874.5, bsz=72, num_updates=2662, lr=2.90728e-05, gnorm=1.664, train_wall=5, gb_free=12.9, wall=9681
2024-08-11 19:36:19 | INFO | train_inner | epoch 043:     18 / 63 loss=4.956, nll_loss=1.167, ppl=2.25, wps=1393.2, ups=0.39, wpb=3533, bsz=168, num_updates=2664, lr=2.90619e-05, gnorm=1.498, train_wall=5, gb_free=9.9, wall=9686
2024-08-11 19:36:24 | INFO | train_inner | epoch 043:     20 / 63 loss=5.032, nll_loss=1.247, ppl=2.37, wps=1458.2, ups=0.38, wpb=3878.5, bsz=188, num_updates=2666, lr=2.9051e-05, gnorm=1.309, train_wall=5, gb_free=15.5, wall=9691
2024-08-11 19:36:33 | INFO | train_inner | epoch 043:     22 / 63 loss=5.037, nll_loss=1.233, ppl=2.35, wps=664.5, ups=0.21, wpb=3184, bsz=100, num_updates=2668, lr=2.90401e-05, gnorm=1.558, train_wall=10, gb_free=13.7, wall=9701
2024-08-11 19:36:39 | INFO | train_inner | epoch 043:     24 / 63 loss=4.962, nll_loss=1.161, ppl=2.24, wps=1208.6, ups=0.39, wpb=3078, bsz=132, num_updates=2670, lr=2.90292e-05, gnorm=1.53, train_wall=5, gb_free=11.5, wall=9706
2024-08-11 19:36:43 | INFO | train_inner | epoch 043:     26 / 63 loss=4.896, nll_loss=1.1, ppl=2.14, wps=1190, ups=0.41, wpb=2936, bsz=204, num_updates=2672, lr=2.90184e-05, gnorm=1.368, train_wall=5, gb_free=15.2, wall=9711
2024-08-11 19:36:48 | INFO | train_inner | epoch 043:     28 / 63 loss=4.971, nll_loss=1.191, ppl=2.28, wps=1183.1, ups=0.4, wpb=2985.5, bsz=92, num_updates=2674, lr=2.90075e-05, gnorm=1.604, train_wall=5, gb_free=12.3, wall=9716
2024-08-11 19:36:54 | INFO | train_inner | epoch 043:     30 / 63 loss=4.965, nll_loss=1.161, ppl=2.24, wps=1296.3, ups=0.36, wpb=3597.5, bsz=164, num_updates=2676, lr=2.89967e-05, gnorm=1.297, train_wall=6, gb_free=12.1, wall=9721
2024-08-11 19:36:58 | INFO | train_inner | epoch 043:     32 / 63 loss=5.016, nll_loss=1.201, ppl=2.3, wps=1324.2, ups=0.47, wpb=2791, bsz=60.5, num_updates=2678, lr=2.89858e-05, gnorm=1.809, train_wall=4, gb_free=16.9, wall=9726
2024-08-11 19:37:04 | INFO | train_inner | epoch 043:     34 / 63 loss=4.956, nll_loss=1.138, ppl=2.2, wps=1252.8, ups=0.38, wpb=3334, bsz=136, num_updates=2680, lr=2.8975e-05, gnorm=1.385, train_wall=5, gb_free=12.2, wall=9731
2024-08-11 19:37:09 | INFO | train_inner | epoch 043:     36 / 63 loss=5.073, nll_loss=1.341, ppl=2.53, wps=1445.6, ups=0.39, wpb=3742, bsz=208, num_updates=2682, lr=2.89642e-05, gnorm=1.571, train_wall=5, gb_free=14.1, wall=9736
2024-08-11 19:37:14 | INFO | train_inner | epoch 043:     38 / 63 loss=5.099, nll_loss=1.344, ppl=2.54, wps=1398.9, ups=0.41, wpb=3400, bsz=152, num_updates=2684, lr=2.89534e-05, gnorm=1.554, train_wall=5, gb_free=13.6, wall=9741
2024-08-11 19:37:19 | INFO | train_inner | epoch 043:     40 / 63 loss=4.996, nll_loss=1.227, ppl=2.34, wps=1398.6, ups=0.37, wpb=3814, bsz=224, num_updates=2686, lr=2.89426e-05, gnorm=1.3, train_wall=5, gb_free=11.8, wall=9746
2024-08-11 19:37:24 | INFO | train_inner | epoch 043:     42 / 63 loss=5.057, nll_loss=1.275, ppl=2.42, wps=1227.5, ups=0.4, wpb=3095.5, bsz=128, num_updates=2688, lr=2.89319e-05, gnorm=1.722, train_wall=5, gb_free=12.7, wall=9751
2024-08-11 19:37:29 | INFO | train_inner | epoch 043:     44 / 63 loss=4.997, nll_loss=1.196, ppl=2.29, wps=1422.3, ups=0.41, wpb=3471, bsz=88, num_updates=2690, lr=2.89211e-05, gnorm=1.542, train_wall=5, gb_free=11.4, wall=9756
2024-08-11 19:37:34 | INFO | train_inner | epoch 043:     46 / 63 loss=4.993, nll_loss=1.191, ppl=2.28, wps=1432.8, ups=0.43, wpb=3319.5, bsz=108, num_updates=2692, lr=2.89104e-05, gnorm=1.494, train_wall=5, gb_free=12.9, wall=9761
2024-08-11 19:37:39 | INFO | train_inner | epoch 043:     48 / 63 loss=5.026, nll_loss=1.266, ppl=2.41, wps=1396.7, ups=0.37, wpb=3820.5, bsz=216, num_updates=2694, lr=2.88996e-05, gnorm=1.269, train_wall=5, gb_free=11.2, wall=9766
2024-08-11 19:37:44 | INFO | train_inner | epoch 043:     50 / 63 loss=4.93, nll_loss=1.116, ppl=2.17, wps=1225.4, ups=0.4, wpb=3035.5, bsz=116, num_updates=2696, lr=2.88889e-05, gnorm=1.91, train_wall=5, gb_free=11.4, wall=9771
2024-08-11 19:37:49 | INFO | train_inner | epoch 043:     52 / 63 loss=5.035, nll_loss=1.263, ppl=2.4, wps=1392.1, ups=0.38, wpb=3700, bsz=152, num_updates=2698, lr=2.88782e-05, gnorm=1.396, train_wall=5, gb_free=13, wall=9777
2024-08-11 19:37:54 | INFO | train_inner | epoch 043:     54 / 63 loss=5.028, nll_loss=1.255, ppl=2.39, wps=1350.2, ups=0.46, wpb=2931.5, bsz=116, num_updates=2700, lr=2.88675e-05, gnorm=1.607, train_wall=4, gb_free=15.4, wall=9781
2024-08-11 19:37:59 | INFO | train_inner | epoch 043:     56 / 63 loss=5.048, nll_loss=1.254, ppl=2.39, wps=1314.2, ups=0.37, wpb=3514, bsz=172, num_updates=2702, lr=2.88568e-05, gnorm=1.471, train_wall=5, gb_free=11.7, wall=9786
2024-08-11 19:38:04 | INFO | train_inner | epoch 043:     58 / 63 loss=4.928, nll_loss=1.105, ppl=2.15, wps=1076.2, ups=0.39, wpb=2725.5, bsz=76, num_updates=2704, lr=2.88462e-05, gnorm=1.616, train_wall=5, gb_free=11.6, wall=9791
2024-08-11 19:38:09 | INFO | train_inner | epoch 043:     60 / 63 loss=5.098, nll_loss=1.367, ppl=2.58, wps=1307.5, ups=0.38, wpb=3407.5, bsz=160, num_updates=2706, lr=2.88355e-05, gnorm=1.562, train_wall=5, gb_free=10.9, wall=9797
2024-08-11 19:38:15 | INFO | train_inner | epoch 043:     62 / 63 loss=4.995, nll_loss=1.232, ppl=2.35, wps=1561.4, ups=0.37, wpb=4193, bsz=248, num_updates=2708, lr=2.88248e-05, gnorm=1.266, train_wall=5, gb_free=12.5, wall=9802
2024-08-11 19:38:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10216.74609375Mb; avail=244865.54296875Mb
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10216.74609375Mb; avail=244865.54296875Mb
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.004706
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10216.74609375Mb; avail=244865.54296875Mb
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.003996
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.009581
2024-08-11 19:38:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10216.74609375Mb; avail=244865.54296875Mb
2024-08-11 19:38:25 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.223 | nll_loss 2.558 | ppl 5.89 | wps 3068.1 | wpb 1463.4 | bsz 64.9 | num_updates 2709 | best_loss 6.167
2024-08-11 19:38:25 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2024-08-11 19:38:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2709 updates
2024-08-11 19:38:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:38:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-11 19:38:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/new_checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 43 @ 2709 updates, score 6.223) (writing took 34.262127517256886 seconds)
2024-08-11 19:38:59 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-08-11 19:38:59 | INFO | train | epoch 043 | loss 4.997 | nll_loss 1.21 | ppl 2.31 | wps 1035.6 | ups 0.3 | wpb 3400.7 | bsz 148.2 | num_updates 2709 | lr 2.88195e-05 | gnorm 1.48 | train_wall 164 | gb_free 15.9 | wall 9846
2024-08-11 19:38:59 | INFO | fairseq_cli.train | done training in 9839.2 seconds
2024-08-17 17:28:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='mr-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin', langs=['hi', 'mr'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-08-17 17:28:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'mr']
2024-08-17 17:28:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | [mr] dictionary: 128112 types
2024-08-17 17:28:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-08-17 17:28:31 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-08-17 17:28:31 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-08-17 17:28:31 | INFO | fairseq_cli.train | model: TransformerModel
2024-08-17 17:28:31 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-08-17 17:28:31 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-08-17 17:28:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-08-17 17:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-08-17 17:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16072.6953125Mb; avail=239009.1328125Mb
2024-08-17 17:28:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-08-17 17:28:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:mr-hi': 1}
2024-08-17 17:28:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-08-17 17:28:31 | INFO | fairseq.data.data_utils | loaded 1,405 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.mr-hi.mr
2024-08-17 17:28:31 | INFO | fairseq.data.data_utils | loaded 1,405 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/valid.mr-hi.hi
2024-08-17 17:28:31 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin valid mr-hi 1405 examples
2024-08-17 17:28:32 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-08-17 17:28:32 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-08-17 17:28:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-08-17 17:28:32 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-08-17 17:28:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-08-17 17:28:32 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-08-17 17:28:32 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-08-17 17:28:32 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/1.2B_last_checkpoint.pt: optimizer, lr scheduler, meters, dataloader will be reset
2024-08-17 17:28:32 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/1.2B_last_checkpoint.pt
2024-08-17 17:28:38 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-08-17 17:28:39 | INFO | fairseq.trainer | Loaded checkpoint /home/krish/content/1.2B_last_checkpoint.pt (epoch 81 @ 0 updates)
2024-08-17 17:28:39 | INFO | fairseq.trainer | loading train data for epoch 1
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12426.13671875Mb; avail=242647.87109375Mb
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:mr-hi': 1}
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:mr-hi src_langtok: 128063; tgt_langtok: 128036
2024-08-17 17:28:39 | INFO | fairseq.data.data_utils | loaded 11,221 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/train.mr-hi.mr
2024-08-17 17:28:39 | INFO | fairseq.data.data_utils | loaded 11,221 examples from: /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin/train.mr-hi.hi
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Marathi/wmt22_spm/wmt22_bin train mr-hi 11221 examples
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:mr-hi', 11221)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:mr-hi', 1.0)]
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000581
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12426.13671875Mb; avail=242647.87109375Mb
2024-08-17 17:28:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000146
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001428
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12426.13671875Mb; avail=242647.87109375Mb
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000053
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12426.13671875Mb; avail=242647.87109375Mb
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000957
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002752
2024-08-17 17:28:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12426.13671875Mb; avail=242647.87109375Mb
2024-08-17 17:28:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 17:28:39 | INFO | fairseq.trainer | begin training epoch 1
2024-08-17 17:28:39 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 17:28:44 | INFO | train_inner | epoch 001:      2 / 94 loss=10.01, nll_loss=7.465, ppl=176.68, wps=1007.2, ups=0.41, wpb=2786, bsz=100, num_updates=2, lr=2.4e-08, gnorm=5.386, train_wall=5, gb_free=10.7, wall=13
2024-08-17 17:28:48 | INFO | train_inner | epoch 001:      4 / 94 loss=10.715, nll_loss=8.343, ppl=324.82, wps=1520.8, ups=0.61, wpb=2494, bsz=68, num_updates=4, lr=4.8e-08, gnorm=8.535, train_wall=3, gb_free=17.7, wall=16
2024-08-17 17:28:52 | INFO | train_inner | epoch 001:      6 / 94 loss=10.049, nll_loss=7.516, ppl=183.07, wps=1466.1, ups=0.46, wpb=3185, bsz=108, num_updates=6, lr=7.2e-08, gnorm=4.738, train_wall=4, gb_free=15.2, wall=20
2024-08-17 17:28:57 | INFO | train_inner | epoch 001:      8 / 94 loss=9.973, nll_loss=7.422, ppl=171.48, wps=1194.4, ups=0.42, wpb=2847.5, bsz=92, num_updates=8, lr=9.6e-08, gnorm=5.698, train_wall=5, gb_free=12.3, wall=25
2024-08-17 17:29:01 | INFO | train_inner | epoch 001:     10 / 94 loss=9.774, nll_loss=7.169, ppl=143.9, wps=1272, ups=0.47, wpb=2698, bsz=128, num_updates=10, lr=1.2e-07, gnorm=6.746, train_wall=4, gb_free=17.4, wall=29
2024-08-17 17:29:05 | INFO | train_inner | epoch 001:     12 / 94 loss=9.762, nll_loss=7.162, ppl=143.2, wps=1755.4, ups=0.58, wpb=3028, bsz=132, num_updates=12, lr=1.44e-07, gnorm=5.026, train_wall=3, gb_free=19, wall=33
2024-08-17 17:29:09 | INFO | train_inner | epoch 001:     14 / 94 loss=9.856, nll_loss=7.277, ppl=155.06, wps=1433.2, ups=0.44, wpb=3226, bsz=120, num_updates=14, lr=1.68e-07, gnorm=4.915, train_wall=4, gb_free=14.4, wall=37
2024-08-17 17:29:14 | INFO | train_inner | epoch 001:     16 / 94 loss=9.872, nll_loss=7.304, ppl=158.07, wps=925, ups=0.45, wpb=2054, bsz=112, num_updates=16, lr=1.92e-07, gnorm=7.757, train_wall=4, gb_free=9.8, wall=42
2024-08-17 17:29:19 | INFO | train_inner | epoch 001:     18 / 94 loss=9.7, nll_loss=7.073, ppl=134.62, wps=1645.7, ups=0.39, wpb=4188, bsz=180, num_updates=18, lr=2.16e-07, gnorm=4.749, train_wall=5, gb_free=11.6, wall=47
2024-08-17 17:29:22 | INFO | train_inner | epoch 001:     20 / 94 loss=10.363, nll_loss=7.918, ppl=241.84, wps=947.8, ups=0.65, wpb=1457.5, bsz=56, num_updates=20, lr=2.4e-07, gnorm=7.314, train_wall=3, gb_free=20.5, wall=50
2024-08-17 17:29:26 | INFO | train_inner | epoch 001:     22 / 94 loss=10.143, nll_loss=7.639, ppl=199.37, wps=1475.5, ups=0.47, wpb=3124.5, bsz=96, num_updates=22, lr=2.64e-07, gnorm=5.703, train_wall=4, gb_free=12.7, wall=54
2024-08-17 17:29:30 | INFO | train_inner | epoch 001:     24 / 94 loss=10.236, nll_loss=7.763, ppl=217.17, wps=1090, ups=0.45, wpb=2406.5, bsz=72, num_updates=24, lr=2.88e-07, gnorm=5.011, train_wall=4, gb_free=13.9, wall=59
2024-08-17 17:29:35 | INFO | train_inner | epoch 001:     26 / 94 loss=9.596, nll_loss=6.953, ppl=123.88, wps=1655.7, ups=0.42, wpb=3963.5, bsz=200, num_updates=26, lr=3.12e-07, gnorm=5.461, train_wall=5, gb_free=10.6, wall=63
2024-08-17 17:29:39 | INFO | train_inner | epoch 001:     28 / 94 loss=9.768, nll_loss=7.178, ppl=144.77, wps=1330.1, ups=0.49, wpb=2734, bsz=116, num_updates=28, lr=3.36e-07, gnorm=7.733, train_wall=4, gb_free=19.7, wall=68
2024-08-17 17:29:44 | INFO | train_inner | epoch 001:     30 / 94 loss=9.84, nll_loss=7.265, ppl=153.84, wps=689.4, ups=0.39, wpb=1774, bsz=72, num_updates=30, lr=3.6e-07, gnorm=5.562, train_wall=5, gb_free=9.6, wall=73
2024-08-17 17:29:49 | INFO | train_inner | epoch 001:     32 / 94 loss=9.584, nll_loss=6.946, ppl=123.32, wps=1299.9, ups=0.4, wpb=3237, bsz=120, num_updates=32, lr=3.84e-07, gnorm=4.211, train_wall=5, gb_free=13, wall=78
2024-08-17 17:29:54 | INFO | train_inner | epoch 001:     34 / 94 loss=10.106, nll_loss=7.603, ppl=194.47, wps=1341.4, ups=0.45, wpb=3008, bsz=120, num_updates=34, lr=4.08e-07, gnorm=5.322, train_wall=4, gb_free=14.6, wall=82
2024-08-17 17:29:58 | INFO | train_inner | epoch 001:     36 / 94 loss=10.469, nll_loss=8.072, ppl=269.17, wps=1146.5, ups=0.44, wpb=2606, bsz=83, num_updates=36, lr=4.32e-07, gnorm=8.037, train_wall=5, gb_free=13.3, wall=87
2024-08-17 17:30:03 | INFO | train_inner | epoch 001:     38 / 94 loss=9.84, nll_loss=7.275, ppl=154.93, wps=1103.6, ups=0.48, wpb=2307.5, bsz=88, num_updates=38, lr=4.56e-07, gnorm=5.925, train_wall=4, gb_free=16.8, wall=91
2024-08-17 17:30:07 | INFO | train_inner | epoch 001:     40 / 94 loss=9.526, nll_loss=6.886, ppl=118.3, wps=1319.2, ups=0.45, wpb=2957, bsz=164, num_updates=40, lr=4.8e-07, gnorm=5.902, train_wall=4, gb_free=12.7, wall=95
2024-08-17 17:30:11 | INFO | train_inner | epoch 001:     42 / 94 loss=10.541, nll_loss=8.152, ppl=284.39, wps=871.9, ups=0.57, wpb=1533.5, bsz=56, num_updates=42, lr=5.04e-07, gnorm=8.123, train_wall=4, gb_free=15.8, wall=99
2024-08-17 17:30:16 | INFO | train_inner | epoch 001:     44 / 94 loss=9.382, nll_loss=6.702, ppl=104.1, wps=1124.2, ups=0.4, wpb=2802, bsz=132, num_updates=44, lr=5.28e-07, gnorm=5.023, train_wall=5, gb_free=12.5, wall=104
2024-08-17 17:30:20 | INFO | train_inner | epoch 001:     46 / 94 loss=9.424, nll_loss=6.758, ppl=108.2, wps=1180.7, ups=0.42, wpb=2818, bsz=160, num_updates=46, lr=5.52e-07, gnorm=5.1, train_wall=5, gb_free=12.3, wall=109
2024-08-17 17:30:26 | INFO | train_inner | epoch 001:     48 / 94 loss=9.654, nll_loss=7.052, ppl=132.67, wps=1453.9, ups=0.38, wpb=3876.5, bsz=144, num_updates=48, lr=5.76e-07, gnorm=3.689, train_wall=5, gb_free=12.3, wall=114
2024-08-17 17:30:30 | INFO | train_inner | epoch 001:     50 / 94 loss=9.829, nll_loss=7.273, ppl=154.61, wps=1542.9, ups=0.47, wpb=3315.5, bsz=108, num_updates=50, lr=6e-07, gnorm=4.439, train_wall=4, gb_free=16.1, wall=118
2024-08-17 17:30:34 | INFO | train_inner | epoch 001:     52 / 94 loss=9.845, nll_loss=7.304, ppl=158.07, wps=1439.3, ups=0.49, wpb=2919, bsz=112, num_updates=52, lr=6.24e-07, gnorm=4.512, train_wall=4, gb_free=15.2, wall=122
2024-08-17 17:30:39 | INFO | train_inner | epoch 001:     54 / 94 loss=9.739, nll_loss=7.177, ppl=144.76, wps=619.5, ups=0.43, wpb=1451.5, bsz=64, num_updates=54, lr=6.48e-07, gnorm=5.786, train_wall=5, gb_free=11.6, wall=127
2024-08-17 17:30:43 | INFO | train_inner | epoch 001:     56 / 94 loss=9.135, nll_loss=6.406, ppl=84.77, wps=1193.9, ups=0.42, wpb=2845, bsz=168, num_updates=56, lr=6.72e-07, gnorm=5.528, train_wall=5, gb_free=12.5, wall=132
2024-08-17 17:30:48 | INFO | train_inner | epoch 001:     58 / 94 loss=9.821, nll_loss=7.275, ppl=154.89, wps=1828.7, ups=0.45, wpb=4108, bsz=156, num_updates=58, lr=6.96e-07, gnorm=4.374, train_wall=4, gb_free=15.5, wall=136
2024-08-17 17:30:53 | INFO | train_inner | epoch 001:     60 / 94 loss=9.457, nll_loss=6.818, ppl=112.82, wps=1165.9, ups=0.42, wpb=2773, bsz=120, num_updates=60, lr=7.2e-07, gnorm=4.841, train_wall=5, gb_free=12.9, wall=141
2024-08-17 17:30:57 | INFO | train_inner | epoch 001:     62 / 94 loss=9.884, nll_loss=7.362, ppl=164.54, wps=1179.3, ups=0.43, wpb=2743.5, bsz=76, num_updates=62, lr=7.44e-07, gnorm=3.634, train_wall=5, gb_free=10.2, wall=146
2024-08-17 17:31:02 | INFO | train_inner | epoch 001:     64 / 94 loss=9.236, nll_loss=6.544, ppl=93.3, wps=1394.7, ups=0.42, wpb=3343.5, bsz=168, num_updates=64, lr=7.68e-07, gnorm=3.676, train_wall=5, gb_free=14.3, wall=150
2024-08-17 17:31:07 | INFO | train_inner | epoch 001:     66 / 94 loss=9.317, nll_loss=6.656, ppl=100.82, wps=1427.8, ups=0.42, wpb=3408.5, bsz=184, num_updates=66, lr=7.92e-07, gnorm=3.947, train_wall=5, gb_free=14.3, wall=155
2024-08-17 17:31:12 | INFO | train_inner | epoch 001:     68 / 94 loss=9.372, nll_loss=6.725, ppl=105.8, wps=1215.5, ups=0.39, wpb=3097, bsz=152, num_updates=68, lr=8.16e-07, gnorm=3.496, train_wall=5, gb_free=11.9, wall=160
2024-08-17 17:31:17 | INFO | train_inner | epoch 001:     70 / 94 loss=9.601, nll_loss=7.009, ppl=128.82, wps=1154.2, ups=0.43, wpb=2673.5, bsz=100, num_updates=70, lr=8.4e-07, gnorm=3.392, train_wall=5, gb_free=15.1, wall=165
2024-08-17 17:31:22 | INFO | train_inner | epoch 001:     72 / 94 loss=9.08, nll_loss=6.359, ppl=82.11, wps=981.6, ups=0.37, wpb=2685, bsz=144, num_updates=72, lr=8.64e-07, gnorm=3.846, train_wall=5, gb_free=13.5, wall=170
2024-08-17 17:31:26 | INFO | train_inner | epoch 001:     74 / 94 loss=9.7, nll_loss=7.151, ppl=142.11, wps=1506.1, ups=0.51, wpb=2969.5, bsz=104, num_updates=74, lr=8.88e-07, gnorm=4.059, train_wall=4, gb_free=14.3, wall=174
2024-08-17 17:31:30 | INFO | train_inner | epoch 001:     76 / 94 loss=9.813, nll_loss=7.288, ppl=156.28, wps=1186.1, ups=0.5, wpb=2378, bsz=72, num_updates=76, lr=9.12e-07, gnorm=3.95, train_wall=4, gb_free=13.3, wall=178
2024-08-17 17:31:35 | INFO | train_inner | epoch 001:     78 / 94 loss=9.896, nll_loss=7.397, ppl=168.5, wps=886.3, ups=0.45, wpb=1986, bsz=59.5, num_updates=78, lr=9.36e-07, gnorm=4.524, train_wall=4, gb_free=17.3, wall=183
2024-08-17 17:31:39 | INFO | train_inner | epoch 001:     80 / 94 loss=9.415, nll_loss=6.785, ppl=110.3, wps=704.9, ups=0.44, wpb=1587.5, bsz=68, num_updates=80, lr=9.6e-07, gnorm=4.63, train_wall=4, gb_free=14.1, wall=187
2024-08-17 17:31:44 | INFO | train_inner | epoch 001:     82 / 94 loss=9.395, nll_loss=6.766, ppl=108.87, wps=1463.4, ups=0.43, wpb=3386, bsz=140, num_updates=82, lr=9.84e-07, gnorm=3.549, train_wall=5, gb_free=9.7, wall=192
2024-08-17 17:31:48 | INFO | train_inner | epoch 001:     84 / 94 loss=9.003, nll_loss=6.277, ppl=77.55, wps=1088.5, ups=0.43, wpb=2531, bsz=156, num_updates=84, lr=1.008e-06, gnorm=3.595, train_wall=5, gb_free=15.6, wall=197
2024-08-17 17:31:53 | INFO | train_inner | epoch 001:     86 / 94 loss=9.508, nll_loss=6.918, ppl=120.97, wps=1089.7, ups=0.43, wpb=2557.5, bsz=96, num_updates=86, lr=1.032e-06, gnorm=3.23, train_wall=5, gb_free=10.9, wall=201
2024-08-17 17:31:58 | INFO | train_inner | epoch 001:     88 / 94 loss=9.21, nll_loss=6.541, ppl=93.14, wps=1134.8, ups=0.39, wpb=2946, bsz=132, num_updates=88, lr=1.056e-06, gnorm=2.981, train_wall=5, gb_free=11.6, wall=207
2024-08-17 17:32:04 | INFO | train_inner | epoch 001:     90 / 94 loss=9.087, nll_loss=6.38, ppl=83.31, wps=1503, ups=0.38, wpb=3993, bsz=192, num_updates=90, lr=1.08e-06, gnorm=3.03, train_wall=5, gb_free=10.1, wall=212
2024-08-17 17:32:09 | INFO | train_inner | epoch 001:     92 / 94 loss=9.058, nll_loss=6.345, ppl=81.31, wps=1619.5, ups=0.36, wpb=4504.5, bsz=200, num_updates=92, lr=1.104e-06, gnorm=2.738, train_wall=6, gb_free=10.8, wall=217
2024-08-17 17:32:13 | INFO | train_inner | epoch 001:     94 / 94 loss=9.086, nll_loss=6.389, ppl=83.78, wps=1095.2, ups=0.5, wpb=2175, bsz=120, num_updates=94, lr=1.128e-06, gnorm=3.329, train_wall=4, gb_free=15, wall=221
2024-08-17 17:32:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12661.4140625Mb; avail=242404.55859375Mb
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000561
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12661.4140625Mb; avail=242404.55859375Mb
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005508
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12661.4140625Mb; avail=242404.55859375Mb
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004813
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011229
2024-08-17 17:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12661.4140625Mb; avail=242404.55859375Mb
2024-08-17 17:32:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.851 | nll_loss 5.947 | ppl 61.71 | wps 2425.7 | wpb 944.1 | bsz 40.1 | num_updates 94
2024-08-17 17:32:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 94 updates
2024-08-17 17:32:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:33:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:33:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 1 @ 94 updates, score 8.851) (writing took 51.22470834106207 seconds)
2024-08-17 17:33:18 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-08-17 17:33:18 | INFO | train | epoch 001 | loss 9.662 | nll_loss 7.066 | ppl 133.96 | wps 956.9 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 94 | lr 1.128e-06 | gnorm 4.952 | train_wall 213 | gb_free 15 | wall 286
2024-08-17 17:33:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 17:33:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 17:33:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 17:33:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000811
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34953.94140625Mb; avail=220112.1171875Mb
2024-08-17 17:33:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000164
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001461
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34953.94140625Mb; avail=220112.1171875Mb
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34953.94140625Mb; avail=220112.1171875Mb
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000525
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002317
2024-08-17 17:33:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34953.94140625Mb; avail=220112.1171875Mb
2024-08-17 17:33:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 17:33:18 | INFO | fairseq.trainer | begin training epoch 2
2024-08-17 17:33:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 17:33:23 | INFO | train_inner | epoch 002:      2 / 94 loss=9.063, nll_loss=6.356, ppl=81.91, wps=90.3, ups=0.03, wpb=3151.5, bsz=176, num_updates=96, lr=1.152e-06, gnorm=2.876, train_wall=5, gb_free=15.2, wall=291
2024-08-17 17:33:28 | INFO | train_inner | epoch 002:      4 / 94 loss=9.297, nll_loss=6.661, ppl=101.17, wps=1241.1, ups=0.42, wpb=2961.5, bsz=120, num_updates=98, lr=1.176e-06, gnorm=3.309, train_wall=5, gb_free=14.1, wall=296
2024-08-17 17:33:32 | INFO | train_inner | epoch 002:      6 / 94 loss=9.825, nll_loss=7.327, ppl=160.53, wps=1279.6, ups=0.52, wpb=2467, bsz=84, num_updates=100, lr=1.2e-06, gnorm=4.175, train_wall=4, gb_free=17.1, wall=300
2024-08-17 17:33:36 | INFO | train_inner | epoch 002:      8 / 94 loss=9.54, nll_loss=6.966, ppl=125, wps=1406.9, ups=0.47, wpb=2998, bsz=100, num_updates=102, lr=1.224e-06, gnorm=3.305, train_wall=4, gb_free=16.3, wall=304
2024-08-17 17:33:40 | INFO | train_inner | epoch 002:     10 / 94 loss=9.172, nll_loss=6.502, ppl=90.61, wps=1757, ups=0.45, wpb=3920, bsz=180, num_updates=104, lr=1.248e-06, gnorm=3.111, train_wall=4, gb_free=12.1, wall=309
2024-08-17 17:33:45 | INFO | train_inner | epoch 002:     12 / 94 loss=9.385, nll_loss=6.781, ppl=109.97, wps=814.1, ups=0.4, wpb=2020.5, bsz=96, num_updates=106, lr=1.272e-06, gnorm=3.706, train_wall=5, gb_free=10.5, wall=314
2024-08-17 17:33:50 | INFO | train_inner | epoch 002:     14 / 94 loss=9.042, nll_loss=6.342, ppl=81.13, wps=1245.8, ups=0.42, wpb=2944, bsz=164, num_updates=108, lr=1.296e-06, gnorm=2.834, train_wall=5, gb_free=14.8, wall=318
2024-08-17 17:33:55 | INFO | train_inner | epoch 002:     16 / 94 loss=9.049, nll_loss=6.35, ppl=81.55, wps=1213.8, ups=0.41, wpb=2938, bsz=160, num_updates=110, lr=1.32e-06, gnorm=3.356, train_wall=5, gb_free=12.2, wall=323
2024-08-17 17:34:00 | INFO | train_inner | epoch 002:     18 / 94 loss=8.841, nll_loss=6.088, ppl=68.03, wps=1439, ups=0.38, wpb=3762, bsz=216, num_updates=112, lr=1.344e-06, gnorm=2.348, train_wall=5, gb_free=10.3, wall=328
2024-08-17 17:34:04 | INFO | train_inner | epoch 002:     20 / 94 loss=9.729, nll_loss=7.216, ppl=148.63, wps=1176.8, ups=0.5, wpb=2339.5, bsz=92, num_updates=114, lr=1.368e-06, gnorm=3.935, train_wall=4, gb_free=13.4, wall=332
2024-08-17 17:34:09 | INFO | train_inner | epoch 002:     22 / 94 loss=9.249, nll_loss=6.624, ppl=98.63, wps=1171.8, ups=0.4, wpb=2965, bsz=116, num_updates=116, lr=1.392e-06, gnorm=2.816, train_wall=5, gb_free=13, wall=337
2024-08-17 17:34:14 | INFO | train_inner | epoch 002:     24 / 94 loss=9.467, nll_loss=6.89, ppl=118.62, wps=1286.9, ups=0.45, wpb=2861, bsz=84, num_updates=118, lr=1.416e-06, gnorm=2.938, train_wall=4, gb_free=13.7, wall=342
2024-08-17 17:34:18 | INFO | train_inner | epoch 002:     26 / 94 loss=9.562, nll_loss=7.009, ppl=128.79, wps=1682.7, ups=0.44, wpb=3789.5, bsz=112, num_updates=120, lr=1.44e-06, gnorm=2.885, train_wall=4, gb_free=14.6, wall=346
2024-08-17 17:34:23 | INFO | train_inner | epoch 002:     28 / 94 loss=8.982, nll_loss=6.285, ppl=77.98, wps=1396.1, ups=0.37, wpb=3725.5, bsz=184, num_updates=122, lr=1.464e-06, gnorm=2.562, train_wall=5, gb_free=10.9, wall=352
2024-08-17 17:34:28 | INFO | train_inner | epoch 002:     30 / 94 loss=9.296, nll_loss=6.679, ppl=102.44, wps=1273.3, ups=0.41, wpb=3127, bsz=116, num_updates=124, lr=1.488e-06, gnorm=2.656, train_wall=5, gb_free=9.7, wall=357
2024-08-17 17:34:33 | INFO | train_inner | epoch 002:     32 / 94 loss=9.209, nll_loss=6.566, ppl=94.75, wps=1178.9, ups=0.41, wpb=2894.5, bsz=124, num_updates=126, lr=1.512e-06, gnorm=2.755, train_wall=5, gb_free=9.9, wall=361
2024-08-17 17:34:38 | INFO | train_inner | epoch 002:     34 / 94 loss=9.039, nll_loss=6.354, ppl=81.82, wps=1440, ups=0.38, wpb=3804.5, bsz=196, num_updates=128, lr=1.536e-06, gnorm=2.665, train_wall=5, gb_free=12.4, wall=367
2024-08-17 17:34:44 | INFO | train_inner | epoch 002:     36 / 94 loss=8.944, nll_loss=6.238, ppl=75.48, wps=1553.8, ups=0.39, wpb=3979, bsz=192, num_updates=130, lr=1.56e-06, gnorm=2.494, train_wall=5, gb_free=12.1, wall=372
2024-08-17 17:34:49 | INFO | train_inner | epoch 002:     38 / 94 loss=8.885, nll_loss=6.161, ppl=71.56, wps=997.6, ups=0.4, wpb=2518, bsz=144, num_updates=132, lr=1.584e-06, gnorm=2.964, train_wall=5, gb_free=15, wall=377
2024-08-17 17:34:52 | INFO | train_inner | epoch 002:     40 / 94 loss=9.696, nll_loss=7.19, ppl=145.97, wps=1651.9, ups=0.63, wpb=2615.5, bsz=68, num_updates=134, lr=1.608e-06, gnorm=3.307, train_wall=3, gb_free=18.6, wall=380
2024-08-17 17:34:56 | INFO | train_inner | epoch 002:     42 / 94 loss=9.636, nll_loss=7.114, ppl=138.48, wps=1165.3, ups=0.46, wpb=2520, bsz=80, num_updates=136, lr=1.632e-06, gnorm=3.266, train_wall=4, gb_free=13.8, wall=384
2024-08-17 17:35:00 | INFO | train_inner | epoch 002:     44 / 94 loss=9.396, nll_loss=6.821, ppl=113.1, wps=1018.6, ups=0.54, wpb=1883.5, bsz=60, num_updates=138, lr=1.656e-06, gnorm=3.2, train_wall=4, gb_free=11.2, wall=388
2024-08-17 17:35:05 | INFO | train_inner | epoch 002:     46 / 94 loss=9.659, nll_loss=7.14, ppl=141.01, wps=882.9, ups=0.42, wpb=2096, bsz=60, num_updates=140, lr=1.68e-06, gnorm=3.453, train_wall=5, gb_free=12, wall=393
2024-08-17 17:35:09 | INFO | train_inner | epoch 002:     48 / 94 loss=9.256, nll_loss=6.642, ppl=99.89, wps=717.1, ups=0.45, wpb=1586, bsz=71.5, num_updates=142, lr=1.704e-06, gnorm=3.698, train_wall=4, gb_free=14.4, wall=397
2024-08-17 17:35:13 | INFO | train_inner | epoch 002:     50 / 94 loss=9.445, nll_loss=6.875, ppl=117.36, wps=1271.1, ups=0.54, wpb=2337, bsz=84, num_updates=144, lr=1.728e-06, gnorm=3.148, train_wall=4, gb_free=15.1, wall=401
2024-08-17 17:35:17 | INFO | train_inner | epoch 002:     52 / 94 loss=9.16, nll_loss=6.519, ppl=91.68, wps=1182.4, ups=0.46, wpb=2545.5, bsz=92, num_updates=146, lr=1.752e-06, gnorm=2.99, train_wall=4, gb_free=11.9, wall=405
2024-08-17 17:35:22 | INFO | train_inner | epoch 002:     54 / 94 loss=9.195, nll_loss=6.564, ppl=94.59, wps=1351.5, ups=0.41, wpb=3278, bsz=136, num_updates=148, lr=1.776e-06, gnorm=2.638, train_wall=5, gb_free=13.8, wall=410
2024-08-17 17:35:27 | INFO | train_inner | epoch 002:     56 / 94 loss=8.918, nll_loss=6.216, ppl=74.33, wps=967.9, ups=0.38, wpb=2514.5, bsz=128, num_updates=150, lr=1.8e-06, gnorm=2.925, train_wall=5, gb_free=13.1, wall=415
2024-08-17 17:35:31 | INFO | train_inner | epoch 002:     58 / 94 loss=9.686, nll_loss=7.18, ppl=145, wps=942, ups=0.53, wpb=1775.5, bsz=56, num_updates=152, lr=1.824e-06, gnorm=3.499, train_wall=4, gb_free=12.3, wall=419
2024-08-17 17:35:35 | INFO | train_inner | epoch 002:     60 / 94 loss=9.019, nll_loss=6.346, ppl=81.36, wps=1356, ups=0.46, wpb=2954, bsz=132, num_updates=154, lr=1.848e-06, gnorm=2.462, train_wall=4, gb_free=10.8, wall=423
2024-08-17 17:35:40 | INFO | train_inner | epoch 002:     62 / 94 loss=9.06, nll_loss=6.398, ppl=84.35, wps=1048.1, ups=0.41, wpb=2526.5, bsz=124, num_updates=156, lr=1.872e-06, gnorm=3.099, train_wall=5, gb_free=13.9, wall=428
2024-08-17 17:35:44 | INFO | train_inner | epoch 002:     64 / 94 loss=9.177, nll_loss=6.543, ppl=93.22, wps=1321.3, ups=0.47, wpb=2818, bsz=100, num_updates=158, lr=1.896e-06, gnorm=2.592, train_wall=4, gb_free=15.3, wall=433
2024-08-17 17:35:48 | INFO | train_inner | epoch 002:     66 / 94 loss=9.525, nll_loss=6.977, ppl=125.97, wps=1730.2, ups=0.54, wpb=3222, bsz=92, num_updates=160, lr=1.92e-06, gnorm=2.722, train_wall=4, gb_free=13.9, wall=436
2024-08-17 17:35:52 | INFO | train_inner | epoch 002:     68 / 94 loss=9.512, nll_loss=6.96, ppl=124.48, wps=1160, ups=0.48, wpb=2438, bsz=68, num_updates=162, lr=1.944e-06, gnorm=2.791, train_wall=4, gb_free=13.5, wall=440
2024-08-17 17:35:57 | INFO | train_inner | epoch 002:     70 / 94 loss=9.222, nll_loss=6.596, ppl=96.73, wps=1052.1, ups=0.44, wpb=2405, bsz=96, num_updates=164, lr=1.968e-06, gnorm=3.018, train_wall=5, gb_free=8.9, wall=445
2024-08-17 17:36:02 | INFO | train_inner | epoch 002:     72 / 94 loss=9.009, nll_loss=6.337, ppl=80.82, wps=1020.9, ups=0.37, wpb=2735, bsz=124, num_updates=166, lr=1.992e-06, gnorm=2.594, train_wall=5, gb_free=8, wall=450
2024-08-17 17:36:06 | INFO | train_inner | epoch 002:     74 / 94 loss=9.317, nll_loss=6.723, ppl=105.64, wps=811.9, ups=0.47, wpb=1719.5, bsz=56, num_updates=168, lr=2.016e-06, gnorm=3.234, train_wall=4, gb_free=16.1, wall=455
2024-08-17 17:36:11 | INFO | train_inner | epoch 002:     76 / 94 loss=9.321, nll_loss=6.721, ppl=105.49, wps=1669.7, ups=0.42, wpb=3947, bsz=124, num_updates=170, lr=2.04e-06, gnorm=2.378, train_wall=5, gb_free=13.9, wall=459
2024-08-17 17:36:16 | INFO | train_inner | epoch 002:     78 / 94 loss=9.147, nll_loss=6.51, ppl=91.15, wps=1137.7, ups=0.45, wpb=2550, bsz=88, num_updates=172, lr=2.064e-06, gnorm=2.691, train_wall=4, gb_free=14.9, wall=464
2024-08-17 17:36:21 | INFO | train_inner | epoch 002:     80 / 94 loss=9.265, nll_loss=6.668, ppl=101.67, wps=1019.1, ups=0.39, wpb=2584.5, bsz=92, num_updates=174, lr=2.088e-06, gnorm=2.732, train_wall=5, gb_free=15.1, wall=469
2024-08-17 17:36:26 | INFO | train_inner | epoch 002:     82 / 94 loss=9.044, nll_loss=6.374, ppl=82.93, wps=1652.9, ups=0.37, wpb=4440, bsz=204, num_updates=176, lr=2.112e-06, gnorm=2.007, train_wall=5, gb_free=9.7, wall=474
2024-08-17 17:36:31 | INFO | train_inner | epoch 002:     84 / 94 loss=9.137, nll_loss=6.5, ppl=90.52, wps=1527.1, ups=0.42, wpb=3631, bsz=172, num_updates=178, lr=2.136e-06, gnorm=2.579, train_wall=5, gb_free=16.7, wall=479
2024-08-17 17:36:41 | INFO | train_inner | epoch 002:     86 / 94 loss=8.795, nll_loss=6.071, ppl=67.23, wps=890, ups=0.19, wpb=4568.5, bsz=240, num_updates=180, lr=2.16e-06, gnorm=2.046, train_wall=10, gb_free=12, wall=489
2024-08-17 17:36:45 | INFO | train_inner | epoch 002:     88 / 94 loss=9.284, nll_loss=6.688, ppl=103.08, wps=948.2, ups=0.53, wpb=1783, bsz=71, num_updates=182, lr=2.184e-06, gnorm=3.999, train_wall=4, gb_free=14.6, wall=493
2024-08-17 17:36:50 | INFO | train_inner | epoch 002:     90 / 94 loss=8.996, nll_loss=6.313, ppl=79.5, wps=1214.1, ups=0.38, wpb=3234, bsz=128, num_updates=184, lr=2.208e-06, gnorm=2.373, train_wall=5, gb_free=11.8, wall=498
2024-08-17 17:36:55 | INFO | train_inner | epoch 002:     92 / 94 loss=8.86, nll_loss=6.149, ppl=70.94, wps=774.3, ups=0.44, wpb=1778, bsz=100, num_updates=186, lr=2.232e-06, gnorm=3.377, train_wall=5, gb_free=13.9, wall=503
2024-08-17 17:36:58 | INFO | train_inner | epoch 002:     94 / 94 loss=8.697, nll_loss=5.948, ppl=61.75, wps=980.8, ups=0.53, wpb=1837.5, bsz=108, num_updates=188, lr=2.256e-06, gnorm=3.275, train_wall=4, gb_free=19.6, wall=507
2024-08-17 17:36:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32964.18359375Mb; avail=222102.375Mb
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000546
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32963.69140625Mb; avail=222101.6484375Mb
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005423
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32964.91015625Mb; avail=222101.15625Mb
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004765
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011084
2024-08-17 17:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32964.91015625Mb; avail=222101.15625Mb
2024-08-17 17:37:12 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.606 | nll_loss 5.687 | ppl 51.52 | wps 2434.2 | wpb 944.1 | bsz 40.1 | num_updates 188 | best_loss 8.606
2024-08-17 17:37:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 188 updates
2024-08-17 17:37:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:37:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:38:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 2 @ 188 updates, score 8.606) (writing took 66.6752610518597 seconds)
2024-08-17 17:38:19 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-08-17 17:38:19 | INFO | train | epoch 002 | loss 9.213 | nll_loss 6.578 | ppl 95.55 | wps 887.6 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 188 | lr 2.256e-06 | gnorm 2.974 | train_wall 220 | gb_free 19.6 | wall 587
2024-08-17 17:38:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 17:38:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 17:38:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 17:38:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000804
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24118.83203125Mb; avail=230947.2890625Mb
2024-08-17 17:38:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000162
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001613
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24118.83203125Mb; avail=230947.2890625Mb
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000054
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24118.83203125Mb; avail=230947.2890625Mb
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000555
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002528
2024-08-17 17:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24118.83203125Mb; avail=230947.2890625Mb
2024-08-17 17:38:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 17:38:19 | INFO | fairseq.trainer | begin training epoch 3
2024-08-17 17:38:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 17:38:23 | INFO | train_inner | epoch 003:      2 / 94 loss=8.818, nll_loss=6.11, ppl=69.05, wps=78.6, ups=0.02, wpb=3342.5, bsz=192, num_updates=190, lr=2.28e-06, gnorm=2.424, train_wall=5, gb_free=13.8, wall=592
2024-08-17 17:38:28 | INFO | train_inner | epoch 003:      4 / 94 loss=9.263, nll_loss=6.658, ppl=100.99, wps=1247.1, ups=0.46, wpb=2712.5, bsz=112, num_updates=192, lr=2.304e-06, gnorm=2.739, train_wall=4, gb_free=14, wall=596
2024-08-17 17:38:32 | INFO | train_inner | epoch 003:      6 / 94 loss=9.205, nll_loss=6.595, ppl=96.69, wps=1382.8, ups=0.43, wpb=3206.5, bsz=116, num_updates=194, lr=2.328e-06, gnorm=2.704, train_wall=5, gb_free=11.9, wall=601
2024-08-17 17:38:36 | INFO | train_inner | epoch 003:      8 / 94 loss=9.467, nll_loss=6.9, ppl=119.47, wps=1039.4, ups=0.53, wpb=1974, bsz=60, num_updates=196, lr=2.352e-06, gnorm=3.1, train_wall=4, gb_free=18.1, wall=605
2024-08-17 17:38:42 | INFO | train_inner | epoch 003:     10 / 94 loss=8.658, nll_loss=5.902, ppl=59.79, wps=1272.8, ups=0.38, wpb=3379.5, bsz=192, num_updates=198, lr=2.376e-06, gnorm=2.346, train_wall=5, gb_free=13, wall=610
2024-08-17 17:38:46 | INFO | train_inner | epoch 003:     12 / 94 loss=9.209, nll_loss=6.59, ppl=96.31, wps=1484.6, ups=0.46, wpb=3241, bsz=132, num_updates=200, lr=2.4e-06, gnorm=2.579, train_wall=4, gb_free=16, wall=614
2024-08-17 17:38:51 | INFO | train_inner | epoch 003:     14 / 94 loss=8.677, nll_loss=5.922, ppl=60.65, wps=1124.4, ups=0.39, wpb=2877.5, bsz=168, num_updates=202, lr=2.424e-06, gnorm=2.378, train_wall=5, gb_free=11.6, wall=619
2024-08-17 17:38:56 | INFO | train_inner | epoch 003:     16 / 94 loss=9.392, nll_loss=6.812, ppl=112.36, wps=1497.5, ups=0.45, wpb=3319, bsz=96, num_updates=204, lr=2.448e-06, gnorm=2.931, train_wall=4, gb_free=15.1, wall=624
2024-08-17 17:39:01 | INFO | train_inner | epoch 003:     18 / 94 loss=9.302, nll_loss=6.71, ppl=104.73, wps=815.1, ups=0.4, wpb=2044.5, bsz=60, num_updates=206, lr=2.472e-06, gnorm=3.071, train_wall=5, gb_free=12.9, wall=629
2024-08-17 17:39:05 | INFO | train_inner | epoch 003:     20 / 94 loss=9.074, nll_loss=6.425, ppl=85.9, wps=1285, ups=0.45, wpb=2886, bsz=124, num_updates=208, lr=2.496e-06, gnorm=2.425, train_wall=4, gb_free=16.5, wall=633
2024-08-17 17:39:11 | INFO | train_inner | epoch 003:     22 / 94 loss=8.695, nll_loss=5.946, ppl=61.66, wps=1472.7, ups=0.36, wpb=4051, bsz=208, num_updates=210, lr=2.52e-06, gnorm=2.001, train_wall=5, gb_free=12.8, wall=639
2024-08-17 17:39:15 | INFO | train_inner | epoch 003:     24 / 94 loss=8.737, nll_loss=6.005, ppl=64.24, wps=978, ups=0.47, wpb=2095.5, bsz=120, num_updates=212, lr=2.544e-06, gnorm=3.04, train_wall=4, gb_free=10.7, wall=643
2024-08-17 17:39:19 | INFO | train_inner | epoch 003:     26 / 94 loss=9.342, nll_loss=6.769, ppl=109.06, wps=1581.6, ups=0.46, wpb=3463, bsz=100, num_updates=214, lr=2.568e-06, gnorm=2.687, train_wall=4, gb_free=15.6, wall=647
2024-08-17 17:39:23 | INFO | train_inner | epoch 003:     28 / 94 loss=8.97, nll_loss=6.282, ppl=77.84, wps=1373.1, ups=0.47, wpb=2897, bsz=112, num_updates=216, lr=2.592e-06, gnorm=2.443, train_wall=4, gb_free=16.5, wall=652
2024-08-17 17:39:28 | INFO | train_inner | epoch 003:     30 / 94 loss=9.367, nll_loss=6.784, ppl=110.18, wps=746.3, ups=0.48, wpb=1565.5, bsz=56, num_updates=218, lr=2.616e-06, gnorm=3.49, train_wall=4, gb_free=17.4, wall=656
2024-08-17 17:39:32 | INFO | train_inner | epoch 003:     32 / 94 loss=9.272, nll_loss=6.668, ppl=101.72, wps=1151.2, ups=0.45, wpb=2584.5, bsz=84, num_updates=220, lr=2.64e-06, gnorm=2.458, train_wall=4, gb_free=13.7, wall=660
2024-08-17 17:39:37 | INFO | train_inner | epoch 003:     34 / 94 loss=9.074, nll_loss=6.431, ppl=86.31, wps=1565, ups=0.45, wpb=3474.5, bsz=164, num_updates=222, lr=2.664e-06, gnorm=2.254, train_wall=4, gb_free=18.5, wall=665
2024-08-17 17:39:41 | INFO | train_inner | epoch 003:     36 / 94 loss=9.093, nll_loss=6.426, ppl=85.99, wps=953, ups=0.44, wpb=2161.5, bsz=88, num_updates=224, lr=2.688e-06, gnorm=2.72, train_wall=5, gb_free=15.8, wall=669
2024-08-17 17:39:45 | INFO | train_inner | epoch 003:     38 / 94 loss=9.371, nll_loss=6.793, ppl=110.87, wps=1308.4, ups=0.51, wpb=2567.5, bsz=68, num_updates=226, lr=2.712e-06, gnorm=2.82, train_wall=4, gb_free=19.8, wall=673
2024-08-17 17:39:50 | INFO | train_inner | epoch 003:     40 / 94 loss=8.887, nll_loss=6.181, ppl=72.54, wps=1673.1, ups=0.37, wpb=4503, bsz=192, num_updates=228, lr=2.736e-06, gnorm=2.082, train_wall=5, gb_free=13.3, wall=679
2024-08-17 17:39:55 | INFO | train_inner | epoch 003:     42 / 94 loss=8.799, nll_loss=6.071, ppl=67.22, wps=935.1, ups=0.48, wpb=1950, bsz=116, num_updates=230, lr=2.76e-06, gnorm=2.969, train_wall=4, gb_free=18.1, wall=683
2024-08-17 17:39:59 | INFO | train_inner | epoch 003:     44 / 94 loss=9.039, nll_loss=6.382, ppl=83.39, wps=1112.7, ups=0.44, wpb=2512.5, bsz=104, num_updates=232, lr=2.784e-06, gnorm=2.56, train_wall=5, gb_free=15.1, wall=687
2024-08-17 17:40:04 | INFO | train_inner | epoch 003:     46 / 94 loss=8.985, nll_loss=6.312, ppl=79.46, wps=1243.8, ups=0.41, wpb=3050.5, bsz=128, num_updates=234, lr=2.808e-06, gnorm=2.329, train_wall=5, gb_free=15.2, wall=692
2024-08-17 17:40:09 | INFO | train_inner | epoch 003:     48 / 94 loss=9.083, nll_loss=6.434, ppl=86.47, wps=1465.5, ups=0.41, wpb=3609, bsz=140, num_updates=236, lr=2.832e-06, gnorm=2.176, train_wall=5, gb_free=13.6, wall=697
2024-08-17 17:40:14 | INFO | train_inner | epoch 003:     50 / 94 loss=9.006, nll_loss=6.348, ppl=81.48, wps=1167.7, ups=0.42, wpb=2801, bsz=124, num_updates=238, lr=2.856e-06, gnorm=2.339, train_wall=5, gb_free=13.7, wall=702
2024-08-17 17:40:18 | INFO | train_inner | epoch 003:     52 / 94 loss=8.993, nll_loss=6.327, ppl=80.26, wps=715, ups=0.52, wpb=1385, bsz=48, num_updates=240, lr=2.88e-06, gnorm=3.263, train_wall=4, gb_free=14.8, wall=706
2024-08-17 17:40:21 | INFO | train_inner | epoch 003:     54 / 94 loss=8.891, nll_loss=6.197, ppl=73.36, wps=1172.4, ups=0.52, wpb=2274, bsz=104, num_updates=242, lr=2.904e-06, gnorm=2.754, train_wall=4, gb_free=14.9, wall=710
2024-08-17 17:40:27 | INFO | train_inner | epoch 003:     56 / 94 loss=8.869, nll_loss=6.179, ppl=72.46, wps=1365.6, ups=0.39, wpb=3527, bsz=172, num_updates=244, lr=2.928e-06, gnorm=2.377, train_wall=5, gb_free=9.5, wall=715
2024-08-17 17:40:30 | INFO | train_inner | epoch 003:     58 / 94 loss=9.138, nll_loss=6.501, ppl=90.6, wps=813, ups=0.52, wpb=1560, bsz=48, num_updates=246, lr=2.952e-06, gnorm=3.414, train_wall=4, gb_free=22.9, wall=719
2024-08-17 17:40:41 | INFO | train_inner | epoch 003:     60 / 94 loss=8.988, nll_loss=6.317, ppl=79.71, wps=612.2, ups=0.2, wpb=3111.5, bsz=144, num_updates=248, lr=2.976e-06, gnorm=2.489, train_wall=10, gb_free=9.2, wall=729
2024-08-17 17:40:46 | INFO | train_inner | epoch 003:     62 / 94 loss=8.791, nll_loss=6.053, ppl=66.4, wps=1794.3, ups=0.35, wpb=5110, bsz=220, num_updates=250, lr=3e-06, gnorm=1.754, train_wall=6, gb_free=11.2, wall=735
2024-08-17 17:40:51 | INFO | train_inner | epoch 003:     64 / 94 loss=8.988, nll_loss=6.315, ppl=79.6, wps=1617.5, ups=0.43, wpb=3732, bsz=132, num_updates=252, lr=3.024e-06, gnorm=2.317, train_wall=5, gb_free=13.9, wall=739
2024-08-17 17:40:55 | INFO | train_inner | epoch 003:     66 / 94 loss=9.135, nll_loss=6.491, ppl=89.95, wps=1443.9, ups=0.46, wpb=3119.5, bsz=108, num_updates=254, lr=3.048e-06, gnorm=2.484, train_wall=4, gb_free=16.9, wall=744
2024-08-17 17:41:00 | INFO | train_inner | epoch 003:     68 / 94 loss=9.238, nll_loss=6.617, ppl=98.12, wps=1026, ups=0.47, wpb=2185.5, bsz=64, num_updates=256, lr=3.072e-06, gnorm=2.644, train_wall=4, gb_free=14.4, wall=748
2024-08-17 17:41:04 | INFO | train_inner | epoch 003:     70 / 94 loss=8.692, nll_loss=5.938, ppl=61.31, wps=1170.6, ups=0.41, wpb=2852, bsz=164, num_updates=258, lr=3.096e-06, gnorm=2.422, train_wall=5, gb_free=15.4, wall=753
2024-08-17 17:41:09 | INFO | train_inner | epoch 003:     72 / 94 loss=9.004, nll_loss=6.332, ppl=80.57, wps=1062.3, ups=0.42, wpb=2533, bsz=91, num_updates=260, lr=3.12e-06, gnorm=2.713, train_wall=5, gb_free=12.5, wall=757
2024-08-17 17:41:13 | INFO | train_inner | epoch 003:     74 / 94 loss=9.152, nll_loss=6.514, ppl=91.39, wps=1131.6, ups=0.56, wpb=2004, bsz=51.5, num_updates=262, lr=3.144e-06, gnorm=3.079, train_wall=4, gb_free=16.6, wall=761
2024-08-17 17:41:18 | INFO | train_inner | epoch 003:     76 / 94 loss=8.809, nll_loss=6.086, ppl=67.93, wps=1382.8, ups=0.4, wpb=3428, bsz=168, num_updates=264, lr=3.168e-06, gnorm=2.228, train_wall=5, gb_free=12.7, wall=766
2024-08-17 17:41:22 | INFO | train_inner | epoch 003:     78 / 94 loss=8.807, nll_loss=6.093, ppl=68.26, wps=1331.3, ups=0.44, wpb=3045, bsz=156, num_updates=266, lr=3.192e-06, gnorm=2.632, train_wall=5, gb_free=13.8, wall=771
2024-08-17 17:41:26 | INFO | train_inner | epoch 003:     80 / 94 loss=9.04, nll_loss=6.377, ppl=83.09, wps=1289, ups=0.47, wpb=2734, bsz=96, num_updates=268, lr=3.216e-06, gnorm=2.667, train_wall=4, gb_free=17.5, wall=775
2024-08-17 17:41:31 | INFO | train_inner | epoch 003:     82 / 94 loss=9.043, nll_loss=6.385, ppl=83.56, wps=1384.6, ups=0.49, wpb=2846, bsz=92, num_updates=270, lr=3.24e-06, gnorm=2.511, train_wall=4, gb_free=19.4, wall=779
2024-08-17 17:41:35 | INFO | train_inner | epoch 003:     84 / 94 loss=8.648, nll_loss=5.893, ppl=59.44, wps=1496.3, ups=0.41, wpb=3675.5, bsz=212, num_updates=272, lr=3.264e-06, gnorm=2.083, train_wall=5, gb_free=14, wall=784
2024-08-17 17:41:40 | INFO | train_inner | epoch 003:     86 / 94 loss=8.989, nll_loss=6.323, ppl=80.06, wps=1270, ups=0.45, wpb=2797.5, bsz=116, num_updates=274, lr=3.288e-06, gnorm=2.65, train_wall=4, gb_free=18, wall=788
2024-08-17 17:41:45 | INFO | train_inner | epoch 003:     88 / 94 loss=8.861, nll_loss=6.148, ppl=70.91, wps=1278, ups=0.4, wpb=3188.5, bsz=136, num_updates=276, lr=3.312e-06, gnorm=2.227, train_wall=5, gb_free=15, wall=793
2024-08-17 17:41:50 | INFO | train_inner | epoch 003:     90 / 94 loss=8.991, nll_loss=6.318, ppl=79.79, wps=723.4, ups=0.42, wpb=1735.5, bsz=76, num_updates=278, lr=3.336e-06, gnorm=2.88, train_wall=5, gb_free=10.7, wall=798
2024-08-17 17:41:54 | INFO | train_inner | epoch 003:     92 / 94 loss=8.957, nll_loss=6.279, ppl=77.66, wps=1168.5, ups=0.42, wpb=2811, bsz=112, num_updates=280, lr=3.36e-06, gnorm=2.444, train_wall=5, gb_free=13.2, wall=803
2024-08-17 17:41:58 | INFO | train_inner | epoch 003:     94 / 94 loss=9.28, nll_loss=6.695, ppl=103.58, wps=878.2, ups=0.56, wpb=1565.5, bsz=44, num_updates=282, lr=3.384e-06, gnorm=3.163, train_wall=4, gb_free=14.7, wall=806
2024-08-17 17:41:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20520.80078125Mb; avail=234545.31640625Mb
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000537
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20520.80078125Mb; avail=234545.31640625Mb
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005350
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20520.80078125Mb; avail=234545.31640625Mb
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004760
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.010989
2024-08-17 17:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20520.80078125Mb; avail=234545.31640625Mb
2024-08-17 17:42:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.472 | nll_loss 5.516 | ppl 45.75 | wps 2433.3 | wpb 944.1 | bsz 40.1 | num_updates 282 | best_loss 8.472
2024-08-17 17:42:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 282 updates
2024-08-17 17:42:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:42:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:43:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 3 @ 282 updates, score 8.472) (writing took 58.608677530195564 seconds)
2024-08-17 17:43:10 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-08-17 17:43:10 | INFO | train | epoch 003 | loss 9.001 | nll_loss 6.33 | ppl 80.46 | wps 915.7 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 282 | lr 3.384e-06 | gnorm 2.601 | train_wall 219 | gb_free 14.7 | wall 878
2024-08-17 17:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 17:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 17:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 17:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000933
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22388.76171875Mb; avail=232677.35546875Mb
2024-08-17 17:43:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000166
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001639
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22388.76171875Mb; avail=232677.35546875Mb
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22388.76171875Mb; avail=232677.35546875Mb
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000595
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002609
2024-08-17 17:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22388.76171875Mb; avail=232677.35546875Mb
2024-08-17 17:43:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 17:43:10 | INFO | fairseq.trainer | begin training epoch 4
2024-08-17 17:43:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 17:43:15 | INFO | train_inner | epoch 004:      2 / 94 loss=8.812, nll_loss=6.093, ppl=68.27, wps=68, ups=0.03, wpb=2607.5, bsz=124, num_updates=284, lr=3.408e-06, gnorm=2.572, train_wall=5, gb_free=13.9, wall=883
2024-08-17 17:43:19 | INFO | train_inner | epoch 004:      4 / 94 loss=8.722, nll_loss=5.982, ppl=63.19, wps=1594, ups=0.48, wpb=3315.5, bsz=176, num_updates=286, lr=3.432e-06, gnorm=2.161, train_wall=4, gb_free=14, wall=887
2024-08-17 17:43:23 | INFO | train_inner | epoch 004:      6 / 94 loss=9.148, nll_loss=6.507, ppl=90.94, wps=1541.2, ups=0.49, wpb=3162.5, bsz=92, num_updates=288, lr=3.456e-06, gnorm=2.472, train_wall=4, gb_free=17.1, wall=891
2024-08-17 17:43:27 | INFO | train_inner | epoch 004:      8 / 94 loss=8.963, nll_loss=6.272, ppl=77.3, wps=866.9, ups=0.51, wpb=1699, bsz=64, num_updates=290, lr=3.48e-06, gnorm=2.807, train_wall=4, gb_free=12.5, wall=895
2024-08-17 17:43:31 | INFO | train_inner | epoch 004:     10 / 94 loss=9.385, nll_loss=6.819, ppl=112.92, wps=1290.8, ups=0.48, wpb=2670, bsz=68, num_updates=292, lr=3.504e-06, gnorm=2.862, train_wall=4, gb_free=14.2, wall=899
2024-08-17 17:43:36 | INFO | train_inner | epoch 004:     12 / 94 loss=9.004, nll_loss=6.34, ppl=81.01, wps=900.7, ups=0.39, wpb=2296.5, bsz=76, num_updates=294, lr=3.528e-06, gnorm=2.548, train_wall=5, gb_free=12.1, wall=904
2024-08-17 17:43:41 | INFO | train_inner | epoch 004:     14 / 94 loss=8.955, nll_loss=6.275, ppl=77.41, wps=1841.2, ups=0.46, wpb=4035.5, bsz=144, num_updates=296, lr=3.552e-06, gnorm=2.1, train_wall=4, gb_free=17.4, wall=909
2024-08-17 17:43:46 | INFO | train_inner | epoch 004:     16 / 94 loss=8.574, nll_loss=5.802, ppl=55.81, wps=1573.8, ups=0.39, wpb=4054, bsz=240, num_updates=298, lr=3.576e-06, gnorm=2.183, train_wall=5, gb_free=14.8, wall=914
2024-08-17 17:43:51 | INFO | train_inner | epoch 004:     18 / 94 loss=8.615, nll_loss=5.853, ppl=57.8, wps=811.1, ups=0.42, wpb=1940.5, bsz=116, num_updates=300, lr=3.6e-06, gnorm=2.875, train_wall=5, gb_free=13.9, wall=919
2024-08-17 17:43:55 | INFO | train_inner | epoch 004:     20 / 94 loss=8.836, nll_loss=6.117, ppl=69.42, wps=1118.9, ups=0.44, wpb=2560.5, bsz=92, num_updates=302, lr=3.624e-06, gnorm=2.639, train_wall=5, gb_free=16.2, wall=923
2024-08-17 17:44:00 | INFO | train_inner | epoch 004:     22 / 94 loss=8.598, nll_loss=5.83, ppl=56.9, wps=855.6, ups=0.45, wpb=1888, bsz=116, num_updates=304, lr=3.648e-06, gnorm=3.009, train_wall=4, gb_free=13.8, wall=928
2024-08-17 17:44:14 | INFO | train_inner | epoch 004:     24 / 94 loss=9.008, nll_loss=6.343, ppl=81.19, wps=380.5, ups=0.14, wpb=2749, bsz=91, num_updates=306, lr=3.672e-06, gnorm=2.766, train_wall=14, gb_free=15.6, wall=942
2024-08-17 17:44:19 | INFO | train_inner | epoch 004:     26 / 94 loss=8.887, nll_loss=6.19, ppl=72.99, wps=631.8, ups=0.43, wpb=1455.5, bsz=64, num_updates=308, lr=3.696e-06, gnorm=2.949, train_wall=5, gb_free=10.7, wall=947
2024-08-17 17:44:24 | INFO | train_inner | epoch 004:     28 / 94 loss=8.824, nll_loss=6.107, ppl=68.92, wps=1297.5, ups=0.4, wpb=3207.5, bsz=148, num_updates=310, lr=3.72e-06, gnorm=2.207, train_wall=5, gb_free=15.3, wall=952
2024-08-17 17:44:27 | INFO | train_inner | epoch 004:     30 / 94 loss=9.312, nll_loss=6.714, ppl=104.97, wps=1674, ups=0.56, wpb=2991.5, bsz=64, num_updates=312, lr=3.744e-06, gnorm=2.612, train_wall=4, gb_free=12.9, wall=955
2024-08-17 17:44:32 | INFO | train_inner | epoch 004:     32 / 94 loss=8.873, nll_loss=6.177, ppl=72.33, wps=1419.6, ups=0.38, wpb=3690.5, bsz=164, num_updates=314, lr=3.768e-06, gnorm=2.263, train_wall=5, gb_free=9, wall=961
2024-08-17 17:44:37 | INFO | train_inner | epoch 004:     34 / 94 loss=8.862, nll_loss=6.15, ppl=70.99, wps=1259.6, ups=0.4, wpb=3132, bsz=132, num_updates=316, lr=3.792e-06, gnorm=2.216, train_wall=5, gb_free=8.8, wall=966
2024-08-17 17:44:42 | INFO | train_inner | epoch 004:     36 / 94 loss=8.855, nll_loss=6.12, ppl=69.57, wps=1469.9, ups=0.4, wpb=3629.5, bsz=152, num_updates=318, lr=3.816e-06, gnorm=2.026, train_wall=5, gb_free=11.3, wall=970
2024-08-17 17:44:47 | INFO | train_inner | epoch 004:     38 / 94 loss=8.653, nll_loss=5.897, ppl=59.6, wps=1176.3, ups=0.41, wpb=2870.5, bsz=156, num_updates=320, lr=3.84e-06, gnorm=2.561, train_wall=5, gb_free=18, wall=975
2024-08-17 17:44:51 | INFO | train_inner | epoch 004:     40 / 94 loss=8.756, nll_loss=6.013, ppl=64.57, wps=1536.7, ups=0.46, wpb=3339.5, bsz=128, num_updates=322, lr=3.864e-06, gnorm=2.313, train_wall=4, gb_free=15, wall=980
2024-08-17 17:44:55 | INFO | train_inner | epoch 004:     42 / 94 loss=8.937, nll_loss=6.253, ppl=76.26, wps=1200.1, ups=0.52, wpb=2313, bsz=76, num_updates=324, lr=3.888e-06, gnorm=2.623, train_wall=4, gb_free=13.8, wall=984
2024-08-17 17:45:01 | INFO | train_inner | epoch 004:     44 / 94 loss=8.755, nll_loss=6.027, ppl=65.2, wps=1352.8, ups=0.38, wpb=3553, bsz=164, num_updates=326, lr=3.912e-06, gnorm=2.037, train_wall=5, gb_free=13.9, wall=989
2024-08-17 17:45:06 | INFO | train_inner | epoch 004:     46 / 94 loss=8.645, nll_loss=5.893, ppl=59.43, wps=1052.7, ups=0.37, wpb=2845, bsz=144, num_updates=328, lr=3.936e-06, gnorm=2.144, train_wall=5, gb_free=13.3, wall=994
2024-08-17 17:45:11 | INFO | train_inner | epoch 004:     48 / 94 loss=8.811, nll_loss=6.098, ppl=68.5, wps=1250.9, ups=0.41, wpb=3024.5, bsz=116, num_updates=330, lr=3.96e-06, gnorm=2.294, train_wall=5, gb_free=18.5, wall=999
2024-08-17 17:45:16 | INFO | train_inner | epoch 004:     50 / 94 loss=8.87, nll_loss=6.177, ppl=72.34, wps=1143.5, ups=0.39, wpb=2961.5, bsz=108, num_updates=332, lr=3.984e-06, gnorm=2.238, train_wall=5, gb_free=10.4, wall=1004
2024-08-17 17:45:20 | INFO | train_inner | epoch 004:     52 / 94 loss=8.905, nll_loss=6.221, ppl=74.61, wps=1052.2, ups=0.51, wpb=2067.5, bsz=79.5, num_updates=334, lr=4.008e-06, gnorm=3.024, train_wall=4, gb_free=19.7, wall=1008
2024-08-17 17:45:25 | INFO | train_inner | epoch 004:     54 / 94 loss=8.97, nll_loss=6.289, ppl=78.18, wps=901.6, ups=0.42, wpb=2150.5, bsz=80, num_updates=336, lr=4.032e-06, gnorm=2.71, train_wall=5, gb_free=12.9, wall=1013
2024-08-17 17:45:28 | INFO | train_inner | epoch 004:     56 / 94 loss=9.335, nll_loss=6.74, ppl=106.9, wps=1322.3, ups=0.54, wpb=2453, bsz=64, num_updates=338, lr=4.056e-06, gnorm=2.883, train_wall=4, gb_free=18.2, wall=1017
2024-08-17 17:45:33 | INFO | train_inner | epoch 004:     58 / 94 loss=8.883, nll_loss=6.175, ppl=72.24, wps=1509.1, ups=0.43, wpb=3528.5, bsz=128, num_updates=340, lr=4.08e-06, gnorm=2.144, train_wall=5, gb_free=14.4, wall=1021
2024-08-17 17:45:38 | INFO | train_inner | epoch 004:     60 / 94 loss=8.867, nll_loss=6.162, ppl=71.6, wps=1288.7, ups=0.39, wpb=3262.5, bsz=104, num_updates=342, lr=4.104e-06, gnorm=2.383, train_wall=5, gb_free=11.3, wall=1026
2024-08-17 17:45:42 | INFO | train_inner | epoch 004:     62 / 94 loss=9.228, nll_loss=6.612, ppl=97.79, wps=1091.9, ups=0.54, wpb=2006, bsz=52, num_updates=344, lr=4.128e-06, gnorm=2.781, train_wall=4, gb_free=15.9, wall=1030
2024-08-17 17:45:46 | INFO | train_inner | epoch 004:     64 / 94 loss=8.909, nll_loss=6.21, ppl=74, wps=1180, ups=0.45, wpb=2617.5, bsz=84, num_updates=346, lr=4.152e-06, gnorm=2.932, train_wall=4, gb_free=14.3, wall=1034
2024-08-17 17:45:52 | INFO | train_inner | epoch 004:     66 / 94 loss=8.475, nll_loss=5.674, ppl=51.06, wps=1200, ups=0.37, wpb=3285, bsz=212, num_updates=348, lr=4.176e-06, gnorm=2.451, train_wall=5, gb_free=13.8, wall=1040
2024-08-17 17:45:56 | INFO | train_inner | epoch 004:     68 / 94 loss=8.65, nll_loss=5.898, ppl=59.65, wps=814.8, ups=0.45, wpb=1826.5, bsz=108, num_updates=350, lr=4.2e-06, gnorm=2.92, train_wall=4, gb_free=11.9, wall=1044
2024-08-17 17:46:01 | INFO | train_inner | epoch 004:     70 / 94 loss=8.677, nll_loss=5.932, ppl=61.04, wps=1408.4, ups=0.38, wpb=3747, bsz=184, num_updates=352, lr=4.224e-06, gnorm=2.031, train_wall=5, gb_free=12.3, wall=1050
2024-08-17 17:46:06 | INFO | train_inner | epoch 004:     72 / 94 loss=8.867, nll_loss=6.165, ppl=71.76, wps=1391.7, ups=0.45, wpb=3124, bsz=124, num_updates=354, lr=4.248e-06, gnorm=2.385, train_wall=4, gb_free=13.6, wall=1054
2024-08-17 17:46:11 | INFO | train_inner | epoch 004:     74 / 94 loss=8.563, nll_loss=5.784, ppl=55.1, wps=1218.7, ups=0.42, wpb=2885, bsz=168, num_updates=356, lr=4.272e-06, gnorm=2.601, train_wall=5, gb_free=10.1, wall=1059
2024-08-17 17:46:16 | INFO | train_inner | epoch 004:     76 / 94 loss=8.915, nll_loss=6.234, ppl=75.25, wps=1528.1, ups=0.42, wpb=3676, bsz=136, num_updates=358, lr=4.296e-06, gnorm=2.279, train_wall=5, gb_free=12, wall=1064
2024-08-17 17:46:19 | INFO | train_inner | epoch 004:     78 / 94 loss=8.901, nll_loss=6.21, ppl=74.04, wps=1564.7, ups=0.55, wpb=2838, bsz=108, num_updates=360, lr=4.32e-06, gnorm=2.706, train_wall=4, gb_free=18.3, wall=1067
2024-08-17 17:46:25 | INFO | train_inner | epoch 004:     80 / 94 loss=8.529, nll_loss=5.746, ppl=53.67, wps=1300.8, ups=0.35, wpb=3668.5, bsz=212, num_updates=362, lr=4.344e-06, gnorm=1.99, train_wall=6, gb_free=9.8, wall=1073
2024-08-17 17:46:29 | INFO | train_inner | epoch 004:     82 / 94 loss=8.698, nll_loss=5.948, ppl=61.73, wps=1549.2, ups=0.47, wpb=3305, bsz=124, num_updates=364, lr=4.368e-06, gnorm=2.118, train_wall=4, gb_free=10.5, wall=1077
2024-08-17 17:46:34 | INFO | train_inner | epoch 004:     84 / 94 loss=8.727, nll_loss=5.994, ppl=63.73, wps=1029.6, ups=0.42, wpb=2427.5, bsz=112, num_updates=366, lr=4.392e-06, gnorm=2.628, train_wall=5, gb_free=15.5, wall=1082
2024-08-17 17:46:38 | INFO | train_inner | epoch 004:     86 / 94 loss=8.966, nll_loss=6.28, ppl=77.73, wps=1568.5, ups=0.45, wpb=3472, bsz=96, num_updates=368, lr=4.416e-06, gnorm=2.311, train_wall=4, gb_free=15.5, wall=1086
2024-08-17 17:46:43 | INFO | train_inner | epoch 004:     88 / 94 loss=8.755, nll_loss=6.016, ppl=64.7, wps=1114.4, ups=0.45, wpb=2483, bsz=104, num_updates=370, lr=4.44e-06, gnorm=2.327, train_wall=4, gb_free=16.6, wall=1091
2024-08-17 17:46:47 | INFO | train_inner | epoch 004:     90 / 94 loss=8.88, nll_loss=6.187, ppl=72.87, wps=830.2, ups=0.47, wpb=1780.5, bsz=80, num_updates=372, lr=4.464e-06, gnorm=2.913, train_wall=4, gb_free=15.2, wall=1095
2024-08-17 17:46:51 | INFO | train_inner | epoch 004:     92 / 94 loss=8.76, nll_loss=6.03, ppl=65.35, wps=1119.4, ups=0.51, wpb=2192, bsz=88, num_updates=374, lr=4.488e-06, gnorm=2.91, train_wall=4, gb_free=18.6, wall=1099
2024-08-17 17:46:55 | INFO | train_inner | epoch 004:     94 / 94 loss=8.439, nll_loss=5.624, ppl=49.32, wps=1389, ups=0.51, wpb=2701.5, bsz=148, num_updates=376, lr=4.512e-06, gnorm=2.425, train_wall=4, gb_free=13.7, wall=1103
2024-08-17 17:46:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17644.32421875Mb; avail=237421.7109375Mb
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000555
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17644.32421875Mb; avail=237421.7109375Mb
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005327
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17644.32421875Mb; avail=237421.7109375Mb
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004760
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.010984
2024-08-17 17:46:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17644.32421875Mb; avail=237421.7109375Mb
2024-08-17 17:47:08 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.376 | nll_loss 5.386 | ppl 41.82 | wps 2427.3 | wpb 944.1 | bsz 40.1 | num_updates 376 | best_loss 8.376
2024-08-17 17:47:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 376 updates
2024-08-17 17:47:08 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:47:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:48:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 4 @ 376 updates, score 8.376) (writing took 63.48920759279281 seconds)
2024-08-17 17:48:12 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-08-17 17:48:12 | INFO | train | epoch 004 | loss 8.835 | nll_loss 6.122 | ppl 69.67 | wps 885.3 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 376 | lr 4.512e-06 | gnorm 2.496 | train_wall 224 | gb_free 13.7 | wall 1180
2024-08-17 17:48:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 17:48:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 17:48:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 17:48:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000806
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40657.04296875Mb; avail=214409.0234375Mb
2024-08-17 17:48:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000189
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001529
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40657.53515625Mb; avail=214408.53125Mb
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000061
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40657.04296875Mb; avail=214409.0234375Mb
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000557
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002480
2024-08-17 17:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40657.04296875Mb; avail=214409.0234375Mb
2024-08-17 17:48:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 17:48:12 | INFO | fairseq.trainer | begin training epoch 5
2024-08-17 17:48:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 17:48:16 | INFO | train_inner | epoch 005:      2 / 94 loss=8.944, nll_loss=6.256, ppl=76.41, wps=60.4, ups=0.02, wpb=2443, bsz=84, num_updates=378, lr=4.536e-06, gnorm=2.702, train_wall=4, gb_free=23.8, wall=1184
2024-08-17 17:48:20 | INFO | train_inner | epoch 005:      4 / 94 loss=8.812, nll_loss=6.085, ppl=67.86, wps=1063.5, ups=0.49, wpb=2154.5, bsz=72, num_updates=380, lr=4.56e-06, gnorm=2.559, train_wall=4, gb_free=15.7, wall=1188
2024-08-17 17:48:24 | INFO | train_inner | epoch 005:      6 / 94 loss=8.562, nll_loss=5.784, ppl=55.1, wps=1408, ups=0.44, wpb=3228.5, bsz=187.5, num_updates=382, lr=4.584e-06, gnorm=2.329, train_wall=5, gb_free=11.1, wall=1193
2024-08-17 17:48:29 | INFO | train_inner | epoch 005:      8 / 94 loss=8.716, nll_loss=5.963, ppl=62.36, wps=1092.9, ups=0.41, wpb=2661.5, bsz=96, num_updates=384, lr=4.608e-06, gnorm=2.314, train_wall=5, gb_free=13.2, wall=1197
2024-08-17 17:48:34 | INFO | train_inner | epoch 005:     10 / 94 loss=8.831, nll_loss=6.107, ppl=68.94, wps=1560.7, ups=0.44, wpb=3528, bsz=128, num_updates=386, lr=4.632e-06, gnorm=2.447, train_wall=5, gb_free=12.1, wall=1202
2024-08-17 17:48:39 | INFO | train_inner | epoch 005:     12 / 94 loss=8.522, nll_loss=5.728, ppl=53.01, wps=1169.7, ups=0.38, wpb=3077, bsz=172, num_updates=388, lr=4.656e-06, gnorm=2.104, train_wall=5, gb_free=12.8, wall=1207
2024-08-17 17:48:44 | INFO | train_inner | epoch 005:     14 / 94 loss=8.576, nll_loss=5.806, ppl=55.93, wps=1320.5, ups=0.41, wpb=3220.5, bsz=176, num_updates=390, lr=4.68e-06, gnorm=2.26, train_wall=5, gb_free=9.7, wall=1212
2024-08-17 17:48:47 | INFO | train_inner | epoch 005:     16 / 94 loss=8.909, nll_loss=6.216, ppl=74.33, wps=1219.9, ups=0.55, wpb=2211, bsz=56, num_updates=392, lr=4.704e-06, gnorm=2.997, train_wall=4, gb_free=13.6, wall=1216
2024-08-17 17:48:52 | INFO | train_inner | epoch 005:     18 / 94 loss=8.999, nll_loss=6.336, ppl=80.8, wps=994.6, ups=0.48, wpb=2074.5, bsz=68, num_updates=394, lr=4.728e-06, gnorm=2.879, train_wall=4, gb_free=16.9, wall=1220
2024-08-17 17:48:56 | INFO | train_inner | epoch 005:     20 / 94 loss=8.817, nll_loss=6.105, ppl=68.85, wps=1492.4, ups=0.5, wpb=3011, bsz=108, num_updates=396, lr=4.752e-06, gnorm=2.326, train_wall=4, gb_free=15.2, wall=1224
2024-08-17 17:49:01 | INFO | train_inner | epoch 005:     22 / 94 loss=8.62, nll_loss=5.86, ppl=58.08, wps=1689.2, ups=0.37, wpb=4609, bsz=224, num_updates=398, lr=4.776e-06, gnorm=1.975, train_wall=5, gb_free=10.6, wall=1229
2024-08-17 17:49:06 | INFO | train_inner | epoch 005:     24 / 94 loss=8.558, nll_loss=5.781, ppl=54.99, wps=1193, ups=0.42, wpb=2809.5, bsz=140, num_updates=400, lr=4.8e-06, gnorm=2.267, train_wall=5, gb_free=11.6, wall=1234
2024-08-17 17:49:10 | INFO | train_inner | epoch 005:     26 / 94 loss=8.854, nll_loss=6.149, ppl=70.95, wps=1603.3, ups=0.43, wpb=3742, bsz=124, num_updates=402, lr=4.824e-06, gnorm=2.308, train_wall=5, gb_free=16, wall=1239
2024-08-17 17:49:15 | INFO | train_inner | epoch 005:     28 / 94 loss=8.639, nll_loss=5.88, ppl=58.91, wps=1215.1, ups=0.42, wpb=2863, bsz=136, num_updates=404, lr=4.848e-06, gnorm=2.393, train_wall=5, gb_free=14.2, wall=1243
2024-08-17 17:49:20 | INFO | train_inner | epoch 005:     30 / 94 loss=8.57, nll_loss=5.796, ppl=55.55, wps=1307.9, ups=0.38, wpb=3457, bsz=180, num_updates=406, lr=4.872e-06, gnorm=2.072, train_wall=5, gb_free=11.4, wall=1249
2024-08-17 17:49:25 | INFO | train_inner | epoch 005:     32 / 94 loss=8.68, nll_loss=5.932, ppl=61.03, wps=1371.5, ups=0.42, wpb=3258.5, bsz=148, num_updates=408, lr=4.896e-06, gnorm=2.219, train_wall=5, gb_free=13.8, wall=1253
2024-08-17 17:49:30 | INFO | train_inner | epoch 005:     34 / 94 loss=8.813, nll_loss=6.083, ppl=67.78, wps=1483.6, ups=0.42, wpb=3502, bsz=120, num_updates=410, lr=4.92e-06, gnorm=2.313, train_wall=5, gb_free=15.2, wall=1258
2024-08-17 17:49:35 | INFO | train_inner | epoch 005:     36 / 94 loss=8.696, nll_loss=5.945, ppl=61.59, wps=1399, ups=0.43, wpb=3278, bsz=148, num_updates=412, lr=4.944e-06, gnorm=2.295, train_wall=5, gb_free=14.1, wall=1263
2024-08-17 17:49:39 | INFO | train_inner | epoch 005:     38 / 94 loss=8.807, nll_loss=6.076, ppl=67.47, wps=1263.9, ups=0.51, wpb=2473, bsz=80, num_updates=414, lr=4.968e-06, gnorm=2.544, train_wall=4, gb_free=15.8, wall=1267
2024-08-17 17:49:44 | INFO | train_inner | epoch 005:     40 / 94 loss=8.498, nll_loss=5.699, ppl=51.93, wps=1156.5, ups=0.37, wpb=3158.5, bsz=160, num_updates=416, lr=4.992e-06, gnorm=2.344, train_wall=5, gb_free=9.8, wall=1272
2024-08-17 17:49:49 | INFO | train_inner | epoch 005:     42 / 94 loss=8.657, nll_loss=5.899, ppl=59.67, wps=1608.7, ups=0.38, wpb=4221.5, bsz=188, num_updates=418, lr=5.016e-06, gnorm=1.997, train_wall=5, gb_free=10.5, wall=1278
2024-08-17 17:49:54 | INFO | train_inner | epoch 005:     44 / 94 loss=8.83, nll_loss=6.126, ppl=69.83, wps=936.7, ups=0.42, wpb=2244, bsz=84, num_updates=420, lr=5.04e-06, gnorm=2.505, train_wall=5, gb_free=15.2, wall=1282
2024-08-17 17:49:59 | INFO | train_inner | epoch 005:     46 / 94 loss=8.527, nll_loss=5.737, ppl=53.32, wps=1111.9, ups=0.4, wpb=2806, bsz=152, num_updates=422, lr=5.064e-06, gnorm=2.512, train_wall=5, gb_free=15.3, wall=1287
2024-08-17 17:50:03 | INFO | train_inner | epoch 005:     48 / 94 loss=8.971, nll_loss=6.294, ppl=78.47, wps=1532.3, ups=0.55, wpb=2808.5, bsz=64, num_updates=424, lr=5.088e-06, gnorm=2.484, train_wall=4, gb_free=16.4, wall=1291
2024-08-17 17:50:07 | INFO | train_inner | epoch 005:     50 / 94 loss=8.74, nll_loss=6.02, ppl=64.88, wps=496.7, ups=0.45, wpb=1108, bsz=56, num_updates=426, lr=5.112e-06, gnorm=3.449, train_wall=4, gb_free=11.4, wall=1295
2024-08-17 17:50:11 | INFO | train_inner | epoch 005:     52 / 94 loss=9.196, nll_loss=6.579, ppl=95.57, wps=1080.4, ups=0.5, wpb=2169, bsz=43, num_updates=428, lr=5.136e-06, gnorm=3.386, train_wall=4, gb_free=15.7, wall=1299
2024-08-17 17:50:16 | INFO | train_inner | epoch 005:     54 / 94 loss=8.806, nll_loss=6.078, ppl=67.55, wps=910.6, ups=0.44, wpb=2081, bsz=64, num_updates=430, lr=5.16e-06, gnorm=2.668, train_wall=5, gb_free=14.6, wall=1304
2024-08-17 17:50:21 | INFO | train_inner | epoch 005:     56 / 94 loss=8.916, nll_loss=6.212, ppl=74.16, wps=1006.9, ups=0.39, wpb=2591, bsz=92, num_updates=432, lr=5.184e-06, gnorm=2.407, train_wall=5, gb_free=10.8, wall=1309
2024-08-17 17:50:25 | INFO | train_inner | epoch 005:     58 / 94 loss=8.677, nll_loss=5.914, ppl=60.3, wps=929.2, ups=0.46, wpb=2036.5, bsz=80, num_updates=434, lr=5.208e-06, gnorm=2.696, train_wall=4, gb_free=14, wall=1314
2024-08-17 17:50:30 | INFO | train_inner | epoch 005:     60 / 94 loss=8.73, nll_loss=5.979, ppl=63.06, wps=1194.7, ups=0.45, wpb=2632, bsz=104, num_updates=436, lr=5.232e-06, gnorm=2.318, train_wall=4, gb_free=15.4, wall=1318
2024-08-17 17:50:35 | INFO | train_inner | epoch 005:     62 / 94 loss=8.375, nll_loss=5.546, ppl=46.73, wps=1255.8, ups=0.37, wpb=3359, bsz=228, num_updates=438, lr=5.256e-06, gnorm=2.183, train_wall=5, gb_free=13.5, wall=1323
2024-08-17 17:50:39 | INFO | train_inner | epoch 005:     64 / 94 loss=8.902, nll_loss=6.198, ppl=73.41, wps=1492.1, ups=0.56, wpb=2659.5, bsz=76, num_updates=440, lr=5.28e-06, gnorm=2.544, train_wall=4, gb_free=14.4, wall=1327
2024-08-17 17:50:42 | INFO | train_inner | epoch 005:     66 / 94 loss=8.849, nll_loss=6.141, ppl=70.55, wps=1258, ups=0.61, wpb=2070, bsz=60, num_updates=442, lr=5.304e-06, gnorm=2.544, train_wall=3, gb_free=18.5, wall=1330
2024-08-17 17:50:46 | INFO | train_inner | epoch 005:     68 / 94 loss=8.677, nll_loss=5.93, ppl=60.96, wps=813.7, ups=0.49, wpb=1645, bsz=56, num_updates=444, lr=5.328e-06, gnorm=2.945, train_wall=4, gb_free=13.2, wall=1334
2024-08-17 17:50:51 | INFO | train_inner | epoch 005:     70 / 94 loss=8.752, nll_loss=6.03, ppl=65.32, wps=1355.9, ups=0.43, wpb=3169, bsz=116, num_updates=446, lr=5.352e-06, gnorm=2.158, train_wall=5, gb_free=14, wall=1339
2024-08-17 17:50:56 | INFO | train_inner | epoch 005:     72 / 94 loss=8.353, nll_loss=5.538, ppl=46.46, wps=1176.5, ups=0.39, wpb=3025, bsz=216, num_updates=448, lr=5.376e-06, gnorm=2.315, train_wall=5, gb_free=12.5, wall=1344
2024-08-17 17:51:00 | INFO | train_inner | epoch 005:     74 / 94 loss=8.611, nll_loss=5.86, ppl=58.08, wps=614.1, ups=0.45, wpb=1378.5, bsz=60, num_updates=450, lr=5.4e-06, gnorm=3.216, train_wall=4, gb_free=13, wall=1349
2024-08-17 17:51:05 | INFO | train_inner | epoch 005:     76 / 94 loss=8.755, nll_loss=6.004, ppl=64.17, wps=1154.4, ups=0.47, wpb=2457.5, bsz=84, num_updates=452, lr=5.424e-06, gnorm=2.484, train_wall=4, gb_free=14.6, wall=1353
2024-08-17 17:51:10 | INFO | train_inner | epoch 005:     78 / 94 loss=8.673, nll_loss=5.915, ppl=60.36, wps=1584.4, ups=0.38, wpb=4134, bsz=144, num_updates=454, lr=5.448e-06, gnorm=1.965, train_wall=5, gb_free=11.8, wall=1358
2024-08-17 17:51:14 | INFO | train_inner | epoch 005:     80 / 94 loss=8.772, nll_loss=6.045, ppl=66.01, wps=1301.1, ups=0.5, wpb=2597, bsz=84, num_updates=456, lr=5.472e-06, gnorm=2.781, train_wall=4, gb_free=19, wall=1362
2024-08-17 17:51:19 | INFO | train_inner | epoch 005:     82 / 94 loss=8.487, nll_loss=5.683, ppl=51.37, wps=1020.8, ups=0.4, wpb=2539, bsz=148, num_updates=458, lr=5.496e-06, gnorm=2.571, train_wall=5, gb_free=12.9, wall=1367
2024-08-17 17:51:24 | INFO | train_inner | epoch 005:     84 / 94 loss=8.529, nll_loss=5.748, ppl=53.76, wps=1346.4, ups=0.4, wpb=3344, bsz=180, num_updates=460, lr=5.52e-06, gnorm=2.382, train_wall=5, gb_free=10.7, wall=1372
2024-08-17 17:51:33 | INFO | train_inner | epoch 005:     86 / 94 loss=8.593, nll_loss=5.823, ppl=56.61, wps=651.3, ups=0.21, wpb=3143.5, bsz=148, num_updates=462, lr=5.544e-06, gnorm=2.236, train_wall=10, gb_free=13.6, wall=1382
2024-08-17 17:51:38 | INFO | train_inner | epoch 005:     88 / 94 loss=8.636, nll_loss=5.874, ppl=58.65, wps=1923.1, ups=0.43, wpb=4469, bsz=180, num_updates=464, lr=5.568e-06, gnorm=1.9, train_wall=5, gb_free=15.1, wall=1386
2024-08-17 17:51:42 | INFO | train_inner | epoch 005:     90 / 94 loss=8.689, nll_loss=5.943, ppl=61.52, wps=1386.2, ups=0.44, wpb=3125.5, bsz=128, num_updates=466, lr=5.592e-06, gnorm=2.146, train_wall=5, gb_free=12.8, wall=1391
2024-08-17 17:51:48 | INFO | train_inner | epoch 005:     92 / 94 loss=8.575, nll_loss=5.792, ppl=55.41, wps=995.3, ups=0.39, wpb=2580, bsz=108, num_updates=468, lr=5.616e-06, gnorm=2.357, train_wall=5, gb_free=11.1, wall=1396
2024-08-17 17:51:51 | INFO | train_inner | epoch 005:     94 / 94 loss=8.862, nll_loss=6.16, ppl=71.5, wps=1418.2, ups=0.61, wpb=2336, bsz=60, num_updates=470, lr=5.64e-06, gnorm=2.879, train_wall=3, gb_free=19.5, wall=1399
2024-08-17 17:51:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35629.78515625Mb; avail=219436.3125Mb
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000539
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35629.78515625Mb; avail=219436.3125Mb
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005401
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35629.29296875Mb; avail=219436.8046875Mb
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004740
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011024
2024-08-17 17:51:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35629.78515625Mb; avail=219436.3125Mb
2024-08-17 17:52:04 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.288 | nll_loss 5.281 | ppl 38.88 | wps 2432.9 | wpb 944.1 | bsz 40.1 | num_updates 470 | best_loss 8.288
2024-08-17 17:52:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 470 updates
2024-08-17 17:52:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:52:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 5 @ 470 updates, score 8.288) (writing took 63.22437815601006 seconds)
2024-08-17 17:53:08 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-08-17 17:53:08 | INFO | train | epoch 005 | loss 8.7 | nll_loss 5.954 | ppl 61.98 | wps 902.2 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 470 | lr 5.64e-06 | gnorm 2.461 | train_wall 219 | gb_free 19.5 | wall 1476
2024-08-17 17:53:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 17:53:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 17:53:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 17:53:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000829
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25681.50390625Mb; avail=229384.62109375Mb
2024-08-17 17:53:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000164
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001542
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25681.50390625Mb; avail=229384.62109375Mb
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000051
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25681.50390625Mb; avail=229384.62109375Mb
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000626
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002571
2024-08-17 17:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25681.50390625Mb; avail=229384.62109375Mb
2024-08-17 17:53:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 17:53:08 | INFO | fairseq.trainer | begin training epoch 6
2024-08-17 17:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 17:53:13 | INFO | train_inner | epoch 006:      2 / 94 loss=8.617, nll_loss=5.845, ppl=57.48, wps=110.3, ups=0.02, wpb=4517.5, bsz=184, num_updates=472, lr=5.664e-06, gnorm=1.697, train_wall=5, gb_free=10.7, wall=1481
2024-08-17 17:53:18 | INFO | train_inner | epoch 006:      4 / 94 loss=8.483, nll_loss=5.686, ppl=51.49, wps=1278.6, ups=0.41, wpb=3112.5, bsz=188, num_updates=474, lr=5.688e-06, gnorm=2.252, train_wall=5, gb_free=13.5, wall=1486
2024-08-17 17:53:22 | INFO | train_inner | epoch 006:      6 / 94 loss=8.757, nll_loss=6.011, ppl=64.51, wps=1308.4, ups=0.5, wpb=2603.5, bsz=80, num_updates=476, lr=5.712e-06, gnorm=2.659, train_wall=4, gb_free=13.4, wall=1490
2024-08-17 17:53:27 | INFO | train_inner | epoch 006:      8 / 94 loss=8.551, nll_loss=5.771, ppl=54.59, wps=1395.6, ups=0.41, wpb=3393.5, bsz=164, num_updates=478, lr=5.736e-06, gnorm=2.073, train_wall=5, gb_free=15.7, wall=1495
2024-08-17 17:53:31 | INFO | train_inner | epoch 006:     10 / 94 loss=8.581, nll_loss=5.801, ppl=55.73, wps=1173.2, ups=0.49, wpb=2372.5, bsz=116, num_updates=480, lr=5.76e-06, gnorm=2.355, train_wall=4, gb_free=14.2, wall=1499
2024-08-17 17:53:36 | INFO | train_inner | epoch 006:     12 / 94 loss=8.679, nll_loss=5.927, ppl=60.85, wps=1495, ups=0.4, wpb=3730.5, bsz=144, num_updates=482, lr=5.784e-06, gnorm=2.08, train_wall=5, gb_free=10, wall=1504
2024-08-17 17:53:40 | INFO | train_inner | epoch 006:     14 / 94 loss=8.525, nll_loss=5.712, ppl=52.41, wps=1170, ups=0.45, wpb=2608, bsz=120, num_updates=484, lr=5.808e-06, gnorm=2.367, train_wall=4, gb_free=14, wall=1508
2024-08-17 17:53:45 | INFO | train_inner | epoch 006:     16 / 94 loss=8.549, nll_loss=5.766, ppl=54.41, wps=1213.1, ups=0.4, wpb=3056, bsz=148, num_updates=486, lr=5.832e-06, gnorm=2.176, train_wall=5, gb_free=15.3, wall=1513
2024-08-17 17:53:49 | INFO | train_inner | epoch 006:     18 / 94 loss=8.572, nll_loss=5.79, ppl=55.35, wps=1854.4, ups=0.48, wpb=3866, bsz=168, num_updates=488, lr=5.856e-06, gnorm=2.118, train_wall=4, gb_free=18.7, wall=1518
2024-08-17 17:53:54 | INFO | train_inner | epoch 006:     20 / 94 loss=8.526, nll_loss=5.744, ppl=53.59, wps=937.9, ups=0.4, wpb=2325.5, bsz=120, num_updates=490, lr=5.88e-06, gnorm=2.411, train_wall=5, gb_free=10.7, wall=1523
2024-08-17 17:53:58 | INFO | train_inner | epoch 006:     22 / 94 loss=8.828, nll_loss=6.112, ppl=69.19, wps=1512.9, ups=0.54, wpb=2798.5, bsz=60, num_updates=492, lr=5.904e-06, gnorm=2.34, train_wall=4, gb_free=14.3, wall=1526
2024-08-17 17:54:03 | INFO | train_inner | epoch 006:     24 / 94 loss=8.558, nll_loss=5.779, ppl=54.9, wps=1367.7, ups=0.41, wpb=3346.5, bsz=160, num_updates=494, lr=5.928e-06, gnorm=1.978, train_wall=5, gb_free=11.1, wall=1531
2024-08-17 17:54:08 | INFO | train_inner | epoch 006:     26 / 94 loss=8.347, nll_loss=5.52, ppl=45.9, wps=1189.3, ups=0.37, wpb=3231, bsz=204, num_updates=496, lr=5.952e-06, gnorm=2.399, train_wall=5, gb_free=13.2, wall=1537
2024-08-17 17:54:13 | INFO | train_inner | epoch 006:     28 / 94 loss=8.413, nll_loss=5.599, ppl=48.47, wps=1378.6, ups=0.44, wpb=3166, bsz=164, num_updates=498, lr=5.976e-06, gnorm=2.038, train_wall=5, gb_free=12, wall=1541
2024-08-17 17:54:18 | INFO | train_inner | epoch 006:     30 / 94 loss=8.455, nll_loss=5.653, ppl=50.31, wps=859.2, ups=0.41, wpb=2094.5, bsz=120, num_updates=500, lr=6e-06, gnorm=2.561, train_wall=5, gb_free=13.2, wall=1546
2024-08-17 17:54:23 | INFO | train_inner | epoch 006:     32 / 94 loss=8.321, nll_loss=5.488, ppl=44.88, wps=1012.3, ups=0.37, wpb=2713.5, bsz=180, num_updates=502, lr=6.024e-06, gnorm=2.07, train_wall=5, gb_free=9, wall=1551
2024-08-17 17:54:28 | INFO | train_inner | epoch 006:     34 / 94 loss=8.61, nll_loss=5.848, ppl=57.62, wps=1318, ups=0.45, wpb=2958, bsz=115, num_updates=504, lr=6.048e-06, gnorm=2.491, train_wall=4, gb_free=16.3, wall=1556
2024-08-17 17:54:32 | INFO | train_inner | epoch 006:     36 / 94 loss=8.522, nll_loss=5.732, ppl=53.17, wps=1085.9, ups=0.42, wpb=2601.5, bsz=120, num_updates=506, lr=6.072e-06, gnorm=2.332, train_wall=5, gb_free=13.9, wall=1561
2024-08-17 17:54:37 | INFO | train_inner | epoch 006:     38 / 94 loss=8.615, nll_loss=5.854, ppl=57.83, wps=852.4, ups=0.42, wpb=2052.5, bsz=100, num_updates=508, lr=6.096e-06, gnorm=2.706, train_wall=5, gb_free=14.2, wall=1565
2024-08-17 17:54:43 | INFO | train_inner | epoch 006:     40 / 94 loss=8.559, nll_loss=5.78, ppl=54.93, wps=1455.6, ups=0.37, wpb=3939, bsz=172, num_updates=510, lr=6.12e-06, gnorm=2.176, train_wall=5, gb_free=13.4, wall=1571
2024-08-17 17:54:48 | INFO | train_inner | epoch 006:     42 / 94 loss=8.692, nll_loss=5.942, ppl=61.49, wps=887, ups=0.4, wpb=2224.5, bsz=88, num_updates=512, lr=6.144e-06, gnorm=2.542, train_wall=5, gb_free=12.6, wall=1576
2024-08-17 17:54:53 | INFO | train_inner | epoch 006:     44 / 94 loss=8.498, nll_loss=5.699, ppl=51.95, wps=1275.2, ups=0.4, wpb=3196, bsz=148, num_updates=514, lr=6.168e-06, gnorm=2.034, train_wall=5, gb_free=14.3, wall=1581
2024-08-17 17:54:57 | INFO | train_inner | epoch 006:     46 / 94 loss=8.643, nll_loss=5.878, ppl=58.81, wps=1307.2, ups=0.43, wpb=3039.5, bsz=120, num_updates=516, lr=6.192e-06, gnorm=2.318, train_wall=5, gb_free=16.6, wall=1586
2024-08-17 17:55:03 | INFO | train_inner | epoch 006:     48 / 94 loss=8.573, nll_loss=5.79, ppl=55.33, wps=1704.1, ups=0.36, wpb=4688.5, bsz=184, num_updates=518, lr=6.216e-06, gnorm=1.732, train_wall=5, gb_free=12.9, wall=1591
2024-08-17 17:55:07 | INFO | train_inner | epoch 006:     50 / 94 loss=8.394, nll_loss=5.57, ppl=47.51, wps=1159.1, ups=0.45, wpb=2547.5, bsz=139.5, num_updates=520, lr=6.24e-06, gnorm=2.837, train_wall=4, gb_free=12.1, wall=1595
2024-08-17 17:55:12 | INFO | train_inner | epoch 006:     52 / 94 loss=8.565, nll_loss=5.781, ppl=55, wps=1874, ups=0.4, wpb=4670.5, bsz=184, num_updates=522, lr=6.264e-06, gnorm=1.723, train_wall=5, gb_free=9.8, wall=1600
2024-08-17 17:55:21 | INFO | train_inner | epoch 006:     54 / 94 loss=8.669, nll_loss=5.923, ppl=60.67, wps=442.5, ups=0.23, wpb=1958.5, bsz=68, num_updates=524, lr=6.288e-06, gnorm=2.877, train_wall=9, gb_free=15.5, wall=1609
2024-08-17 17:55:25 | INFO | train_inner | epoch 006:     56 / 94 loss=8.4, nll_loss=5.573, ppl=47.59, wps=823.9, ups=0.47, wpb=1747, bsz=80, num_updates=526, lr=6.312e-06, gnorm=2.633, train_wall=4, gb_free=11.7, wall=1614
2024-08-17 17:55:29 | INFO | train_inner | epoch 006:     58 / 94 loss=8.54, nll_loss=5.758, ppl=54.1, wps=795.5, ups=0.49, wpb=1613, bsz=64, num_updates=528, lr=6.336e-06, gnorm=3.113, train_wall=4, gb_free=14.7, wall=1618
2024-08-17 17:55:34 | INFO | train_inner | epoch 006:     60 / 94 loss=8.589, nll_loss=5.807, ppl=55.99, wps=1488.8, ups=0.42, wpb=3534.5, bsz=124, num_updates=530, lr=6.36e-06, gnorm=2.124, train_wall=5, gb_free=12.4, wall=1622
2024-08-17 17:55:39 | INFO | train_inner | epoch 006:     62 / 94 loss=8.515, nll_loss=5.711, ppl=52.38, wps=1216, ups=0.39, wpb=3087.5, bsz=136, num_updates=532, lr=6.384e-06, gnorm=2.15, train_wall=5, gb_free=13.4, wall=1627
2024-08-17 17:55:43 | INFO | train_inner | epoch 006:     64 / 94 loss=8.737, nll_loss=5.982, ppl=63.2, wps=1058.3, ups=0.49, wpb=2153.5, bsz=60, num_updates=534, lr=6.408e-06, gnorm=2.913, train_wall=4, gb_free=14.6, wall=1632
2024-08-17 17:55:47 | INFO | train_inner | epoch 006:     66 / 94 loss=8.785, nll_loss=6.053, ppl=66.41, wps=1259, ups=0.52, wpb=2404.5, bsz=60, num_updates=536, lr=6.432e-06, gnorm=2.538, train_wall=4, gb_free=18.6, wall=1635
2024-08-17 17:55:51 | INFO | train_inner | epoch 006:     68 / 94 loss=8.798, nll_loss=6.077, ppl=67.52, wps=1027.3, ups=0.5, wpb=2075, bsz=60, num_updates=538, lr=6.456e-06, gnorm=3.221, train_wall=4, gb_free=19.9, wall=1639
2024-08-17 17:55:56 | INFO | train_inner | epoch 006:     70 / 94 loss=8.517, nll_loss=5.722, ppl=52.79, wps=1683, ups=0.4, wpb=4204, bsz=172, num_updates=540, lr=6.48e-06, gnorm=2.016, train_wall=5, gb_free=12.9, wall=1644
2024-08-17 17:56:01 | INFO | train_inner | epoch 006:     72 / 94 loss=8.575, nll_loss=5.797, ppl=55.59, wps=974, ups=0.43, wpb=2244, bsz=96, num_updates=542, lr=6.504e-06, gnorm=2.412, train_wall=5, gb_free=9.6, wall=1649
2024-08-17 17:56:05 | INFO | train_inner | epoch 006:     74 / 94 loss=8.57, nll_loss=5.796, ppl=55.57, wps=991.3, ups=0.46, wpb=2139.5, bsz=100, num_updates=544, lr=6.528e-06, gnorm=2.654, train_wall=4, gb_free=14.5, wall=1653
2024-08-17 17:56:09 | INFO | train_inner | epoch 006:     76 / 94 loss=8.614, nll_loss=5.847, ppl=57.57, wps=1113.9, ups=0.55, wpb=2037, bsz=64, num_updates=546, lr=6.552e-06, gnorm=2.864, train_wall=4, gb_free=11.9, wall=1657
2024-08-17 17:56:13 | INFO | train_inner | epoch 006:     78 / 94 loss=8.578, nll_loss=5.806, ppl=55.94, wps=1251.4, ups=0.43, wpb=2912.5, bsz=116, num_updates=548, lr=6.576e-06, gnorm=2.194, train_wall=5, gb_free=10.8, wall=1662
2024-08-17 17:56:19 | INFO | train_inner | epoch 006:     80 / 94 loss=8.64, nll_loss=5.88, ppl=58.91, wps=1157.9, ups=0.38, wpb=3053.5, bsz=92, num_updates=550, lr=6.6e-06, gnorm=2.26, train_wall=5, gb_free=10.8, wall=1667
2024-08-17 17:56:22 | INFO | train_inner | epoch 006:     82 / 94 loss=8.806, nll_loss=6.092, ppl=68.22, wps=1212.4, ups=0.55, wpb=2193.5, bsz=52, num_updates=552, lr=6.624e-06, gnorm=2.595, train_wall=4, gb_free=16.4, wall=1670
2024-08-17 17:56:26 | INFO | train_inner | epoch 006:     84 / 94 loss=8.524, nll_loss=5.729, ppl=53.05, wps=1121.3, ups=0.48, wpb=2323.5, bsz=80, num_updates=554, lr=6.648e-06, gnorm=2.555, train_wall=4, gb_free=11.4, wall=1675
2024-08-17 17:56:31 | INFO | train_inner | epoch 006:     86 / 94 loss=8.669, nll_loss=5.917, ppl=60.42, wps=1503.3, ups=0.47, wpb=3191, bsz=120, num_updates=556, lr=6.672e-06, gnorm=2.184, train_wall=4, gb_free=15.7, wall=1679
2024-08-17 17:56:35 | INFO | train_inner | epoch 006:     88 / 94 loss=8.665, nll_loss=5.906, ppl=59.98, wps=1598.1, ups=0.46, wpb=3493.5, bsz=104, num_updates=558, lr=6.696e-06, gnorm=2.259, train_wall=4, gb_free=12.4, wall=1683
2024-08-17 17:56:39 | INFO | train_inner | epoch 006:     90 / 94 loss=8.686, nll_loss=5.94, ppl=61.38, wps=1043.9, ups=0.45, wpb=2312.5, bsz=84, num_updates=560, lr=6.72e-06, gnorm=2.567, train_wall=4, gb_free=13.9, wall=1688
2024-08-17 17:56:44 | INFO | train_inner | epoch 006:     92 / 94 loss=8.634, nll_loss=5.884, ppl=59.05, wps=981, ups=0.42, wpb=2337, bsz=108, num_updates=562, lr=6.744e-06, gnorm=2.813, train_wall=5, gb_free=11.3, wall=1692
2024-08-17 17:56:48 | INFO | train_inner | epoch 006:     94 / 94 loss=8.483, nll_loss=5.692, ppl=51.71, wps=932.2, ups=0.58, wpb=1621, bsz=80, num_updates=564, lr=6.768e-06, gnorm=3.307, train_wall=3, gb_free=19.5, wall=1696
2024-08-17 17:56:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19432.66015625Mb; avail=235633.46484375Mb
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000541
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19432.66015625Mb; avail=235633.46484375Mb
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005364
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19432.66015625Mb; avail=235633.46484375Mb
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004796
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011058
2024-08-17 17:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19432.66015625Mb; avail=235633.46484375Mb
2024-08-17 17:57:01 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.215 | nll_loss 5.194 | ppl 36.61 | wps 2430.5 | wpb 944.1 | bsz 40.1 | num_updates 564 | best_loss 8.215
2024-08-17 17:57:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 564 updates
2024-08-17 17:57:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:57:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 17:58:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 6 @ 564 updates, score 8.215) (writing took 61.92732185032219 seconds)
2024-08-17 17:58:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-08-17 17:58:03 | INFO | train | epoch 006 | loss 8.581 | nll_loss 5.804 | ppl 55.88 | wps 903.8 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 564 | lr 6.768e-06 | gnorm 2.398 | train_wall 220 | gb_free 19.5 | wall 1771
2024-08-17 17:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 17:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 17:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 17:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000842
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21822.91796875Mb; avail=233243.20703125Mb
2024-08-17 17:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000230
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001571
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21822.91796875Mb; avail=233243.20703125Mb
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000042
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21822.91796875Mb; avail=233243.20703125Mb
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000568
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002491
2024-08-17 17:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21822.91796875Mb; avail=233243.20703125Mb
2024-08-17 17:58:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 17:58:03 | INFO | fairseq.trainer | begin training epoch 7
2024-08-17 17:58:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 17:58:08 | INFO | train_inner | epoch 007:      2 / 94 loss=8.57, nll_loss=5.791, ppl=55.37, wps=86.4, ups=0.03, wpb=3456, bsz=104, num_updates=566, lr=6.792e-06, gnorm=2.126, train_wall=5, gb_free=14.4, wall=1776
2024-08-17 17:58:13 | INFO | train_inner | epoch 007:      4 / 94 loss=8.41, nll_loss=5.589, ppl=48.14, wps=1524.6, ups=0.39, wpb=3895, bsz=184, num_updates=568, lr=6.816e-06, gnorm=2.016, train_wall=5, gb_free=11.8, wall=1781
2024-08-17 17:58:18 | INFO | train_inner | epoch 007:      6 / 94 loss=8.413, nll_loss=5.594, ppl=48.29, wps=1223.9, ups=0.38, wpb=3222.5, bsz=164, num_updates=570, lr=6.84e-06, gnorm=2.037, train_wall=5, gb_free=14.4, wall=1786
2024-08-17 17:58:23 | INFO | train_inner | epoch 007:      8 / 94 loss=8.413, nll_loss=5.595, ppl=48.34, wps=1640.7, ups=0.42, wpb=3939.5, bsz=184, num_updates=572, lr=6.864e-06, gnorm=2.02, train_wall=5, gb_free=12.8, wall=1791
2024-08-17 17:58:26 | INFO | train_inner | epoch 007:     10 / 94 loss=8.817, nll_loss=6.098, ppl=68.49, wps=1368.1, ups=0.72, wpb=1896.5, bsz=48, num_updates=574, lr=6.888e-06, gnorm=3.174, train_wall=3, gb_free=19.8, wall=1794
2024-08-17 17:58:30 | INFO | train_inner | epoch 007:     12 / 94 loss=8.501, nll_loss=5.71, ppl=52.36, wps=1408, ups=0.45, wpb=3121, bsz=136, num_updates=576, lr=6.912e-06, gnorm=2.39, train_wall=4, gb_free=16.7, wall=1798
2024-08-17 17:58:34 | INFO | train_inner | epoch 007:     14 / 94 loss=8.364, nll_loss=5.543, ppl=46.63, wps=1199.9, ups=0.45, wpb=2686.5, bsz=144, num_updates=578, lr=6.936e-06, gnorm=2.502, train_wall=4, gb_free=10.8, wall=1803
2024-08-17 17:58:39 | INFO | train_inner | epoch 007:     16 / 94 loss=8.62, nll_loss=5.846, ppl=57.53, wps=1137.5, ups=0.49, wpb=2334, bsz=68, num_updates=580, lr=6.96e-06, gnorm=2.423, train_wall=4, gb_free=13.2, wall=1807
2024-08-17 17:58:44 | INFO | train_inner | epoch 007:     18 / 94 loss=8.469, nll_loss=5.68, ppl=51.26, wps=1045.4, ups=0.38, wpb=2758, bsz=152, num_updates=582, lr=6.984e-06, gnorm=2.317, train_wall=5, gb_free=12.3, wall=1812
2024-08-17 17:58:49 | INFO | train_inner | epoch 007:     20 / 94 loss=8.443, nll_loss=5.639, ppl=49.82, wps=1099, ups=0.43, wpb=2547, bsz=132, num_updates=584, lr=7.008e-06, gnorm=2.576, train_wall=5, gb_free=13.3, wall=1817
2024-08-17 17:58:54 | INFO | train_inner | epoch 007:     22 / 94 loss=8.498, nll_loss=5.699, ppl=51.95, wps=1536.3, ups=0.4, wpb=3852.5, bsz=148, num_updates=586, lr=7.032e-06, gnorm=1.994, train_wall=5, gb_free=10.1, wall=1822
2024-08-17 17:59:02 | INFO | train_inner | epoch 007:     24 / 94 loss=8.519, nll_loss=5.732, ppl=53.13, wps=580.3, ups=0.23, wpb=2554.5, bsz=112, num_updates=588, lr=7.056e-06, gnorm=2.332, train_wall=9, gb_free=14.2, wall=1831
2024-08-17 17:59:07 | INFO | train_inner | epoch 007:     26 / 94 loss=8.509, nll_loss=5.724, ppl=52.87, wps=1137.6, ups=0.4, wpb=2852, bsz=128, num_updates=590, lr=7.08e-06, gnorm=2.238, train_wall=5, gb_free=14.4, wall=1836
2024-08-17 17:59:12 | INFO | train_inner | epoch 007:     28 / 94 loss=8.531, nll_loss=5.745, ppl=53.63, wps=985.1, ups=0.45, wpb=2182, bsz=92, num_updates=592, lr=7.104e-06, gnorm=2.454, train_wall=4, gb_free=14, wall=1840
2024-08-17 17:59:16 | INFO | train_inner | epoch 007:     30 / 94 loss=8.492, nll_loss=5.69, ppl=51.62, wps=1476, ups=0.45, wpb=3287, bsz=132, num_updates=594, lr=7.128e-06, gnorm=1.98, train_wall=4, gb_free=11.8, wall=1844
2024-08-17 17:59:21 | INFO | train_inner | epoch 007:     32 / 94 loss=8.47, nll_loss=5.664, ppl=50.71, wps=1126.2, ups=0.41, wpb=2753, bsz=136, num_updates=596, lr=7.152e-06, gnorm=2.138, train_wall=5, gb_free=14.7, wall=1849
2024-08-17 17:59:26 | INFO | train_inner | epoch 007:     34 / 94 loss=8.468, nll_loss=5.664, ppl=50.7, wps=599.8, ups=0.44, wpb=1353.5, bsz=47.5, num_updates=598, lr=7.176e-06, gnorm=2.975, train_wall=5, gb_free=10.9, wall=1854
2024-08-17 17:59:30 | INFO | train_inner | epoch 007:     36 / 94 loss=8.649, nll_loss=5.894, ppl=59.46, wps=910.3, ups=0.43, wpb=2134, bsz=60, num_updates=600, lr=7.2e-06, gnorm=2.531, train_wall=5, gb_free=12.5, wall=1859
2024-08-17 17:59:35 | INFO | train_inner | epoch 007:     38 / 94 loss=8.492, nll_loss=5.689, ppl=51.58, wps=1690.8, ups=0.44, wpb=3874.5, bsz=152, num_updates=602, lr=7.224e-06, gnorm=1.949, train_wall=5, gb_free=14.5, wall=1863
2024-08-17 17:59:39 | INFO | train_inner | epoch 007:     40 / 94 loss=8.756, nll_loss=6.034, ppl=65.51, wps=1012.5, ups=0.55, wpb=1852, bsz=48, num_updates=604, lr=7.248e-06, gnorm=2.818, train_wall=4, gb_free=13.9, wall=1867
2024-08-17 17:59:44 | INFO | train_inner | epoch 007:     42 / 94 loss=8.506, nll_loss=5.711, ppl=52.37, wps=1124.9, ups=0.4, wpb=2815, bsz=116, num_updates=606, lr=7.272e-06, gnorm=2.15, train_wall=5, gb_free=9.9, wall=1872
2024-08-17 17:59:49 | INFO | train_inner | epoch 007:     44 / 94 loss=8.427, nll_loss=5.611, ppl=48.87, wps=1247.9, ups=0.36, wpb=3477.5, bsz=168, num_updates=608, lr=7.296e-06, gnorm=2.121, train_wall=6, gb_free=9.1, wall=1877
2024-08-17 17:59:54 | INFO | train_inner | epoch 007:     46 / 94 loss=8.437, nll_loss=5.631, ppl=49.54, wps=1075.5, ups=0.43, wpb=2483.5, bsz=124, num_updates=610, lr=7.32e-06, gnorm=2.584, train_wall=5, gb_free=16.8, wall=1882
2024-08-17 17:59:58 | INFO | train_inner | epoch 007:     48 / 94 loss=8.534, nll_loss=5.73, ppl=53.06, wps=1584.6, ups=0.45, wpb=3530, bsz=92, num_updates=612, lr=7.344e-06, gnorm=2.085, train_wall=4, gb_free=15, wall=1886
2024-08-17 18:00:03 | INFO | train_inner | epoch 007:     50 / 94 loss=8.563, nll_loss=5.785, ppl=55.13, wps=1309.4, ups=0.43, wpb=3014, bsz=128, num_updates=614, lr=7.368e-06, gnorm=2.447, train_wall=5, gb_free=13.7, wall=1891
2024-08-17 18:00:07 | INFO | train_inner | epoch 007:     52 / 94 loss=8.491, nll_loss=5.688, ppl=51.55, wps=1205.1, ups=0.5, wpb=2396.5, bsz=72, num_updates=616, lr=7.392e-06, gnorm=2.717, train_wall=4, gb_free=12.9, wall=1895
2024-08-17 18:00:11 | INFO | train_inner | epoch 007:     54 / 94 loss=8.585, nll_loss=5.806, ppl=55.93, wps=817.7, ups=0.47, wpb=1741, bsz=68, num_updates=618, lr=7.416e-06, gnorm=2.757, train_wall=4, gb_free=18, wall=1899
2024-08-17 18:00:16 | INFO | train_inner | epoch 007:     56 / 94 loss=8.587, nll_loss=5.811, ppl=56.15, wps=1120.9, ups=0.43, wpb=2636.5, bsz=84, num_updates=620, lr=7.44e-06, gnorm=2.45, train_wall=5, gb_free=10.5, wall=1904
2024-08-17 18:00:20 | INFO | train_inner | epoch 007:     58 / 94 loss=8.294, nll_loss=5.454, ppl=43.84, wps=1284.6, ups=0.43, wpb=3018, bsz=188, num_updates=622, lr=7.464e-06, gnorm=2.59, train_wall=5, gb_free=14.2, wall=1909
2024-08-17 18:00:25 | INFO | train_inner | epoch 007:     60 / 94 loss=8.259, nll_loss=5.416, ppl=42.68, wps=1199.7, ups=0.44, wpb=2730.5, bsz=180, num_updates=624, lr=7.488e-06, gnorm=2.326, train_wall=5, gb_free=14.9, wall=1913
2024-08-17 18:00:30 | INFO | train_inner | epoch 007:     62 / 94 loss=8.56, nll_loss=5.771, ppl=54.62, wps=1221.8, ups=0.42, wpb=2900.5, bsz=84, num_updates=626, lr=7.512e-06, gnorm=2.371, train_wall=5, gb_free=9.6, wall=1918
2024-08-17 18:00:34 | INFO | train_inner | epoch 007:     64 / 94 loss=8.455, nll_loss=5.644, ppl=50, wps=801.1, ups=0.49, wpb=1650.5, bsz=72, num_updates=628, lr=7.536e-06, gnorm=3.059, train_wall=4, gb_free=14.3, wall=1922
2024-08-17 18:00:38 | INFO | train_inner | epoch 007:     66 / 94 loss=8.645, nll_loss=5.885, ppl=59.11, wps=1552.7, ups=0.48, wpb=3229.5, bsz=91, num_updates=630, lr=7.56e-06, gnorm=2.432, train_wall=4, gb_free=11.3, wall=1926
2024-08-17 18:00:43 | INFO | train_inner | epoch 007:     68 / 94 loss=8.47, nll_loss=5.671, ppl=50.96, wps=1080.8, ups=0.42, wpb=2583, bsz=108, num_updates=632, lr=7.584e-06, gnorm=2.255, train_wall=5, gb_free=15.2, wall=1931
2024-08-17 18:00:48 | INFO | train_inner | epoch 007:     70 / 94 loss=8.427, nll_loss=5.617, ppl=49.09, wps=1492.5, ups=0.39, wpb=3869, bsz=192, num_updates=634, lr=7.608e-06, gnorm=1.849, train_wall=5, gb_free=12.8, wall=1936
2024-08-17 18:00:52 | INFO | train_inner | epoch 007:     72 / 94 loss=8.515, nll_loss=5.734, ppl=53.24, wps=1426, ups=0.46, wpb=3114, bsz=152, num_updates=636, lr=7.632e-06, gnorm=2.077, train_wall=4, gb_free=14, wall=1941
2024-08-17 18:00:56 | INFO | train_inner | epoch 007:     74 / 94 loss=8.675, nll_loss=5.928, ppl=60.89, wps=1423.9, ups=0.51, wpb=2794, bsz=80, num_updates=638, lr=7.656e-06, gnorm=2.488, train_wall=4, gb_free=15.1, wall=1945
2024-08-17 18:01:02 | INFO | train_inner | epoch 007:     76 / 94 loss=8.419, nll_loss=5.607, ppl=48.74, wps=1086.9, ups=0.37, wpb=2950.5, bsz=116, num_updates=640, lr=7.68e-06, gnorm=2.09, train_wall=5, gb_free=13.9, wall=1950
2024-08-17 18:01:06 | INFO | train_inner | epoch 007:     78 / 94 loss=8.768, nll_loss=6.056, ppl=66.55, wps=1442.3, ups=0.48, wpb=3021, bsz=80, num_updates=642, lr=7.704e-06, gnorm=2.57, train_wall=4, gb_free=16.5, wall=1954
2024-08-17 18:01:10 | INFO | train_inner | epoch 007:     80 / 94 loss=8.305, nll_loss=5.471, ppl=44.37, wps=1017.9, ups=0.45, wpb=2244.5, bsz=124, num_updates=644, lr=7.728e-06, gnorm=2.611, train_wall=4, gb_free=15.1, wall=1959
2024-08-17 18:01:13 | INFO | train_inner | epoch 007:     82 / 94 loss=8.686, nll_loss=5.929, ppl=60.92, wps=1189.6, ups=0.64, wpb=1862.5, bsz=44, num_updates=646, lr=7.752e-06, gnorm=2.978, train_wall=3, gb_free=23.1, wall=1962
2024-08-17 18:01:18 | INFO | train_inner | epoch 007:     84 / 94 loss=8.442, nll_loss=5.625, ppl=49.36, wps=1141.2, ups=0.41, wpb=2783.5, bsz=128, num_updates=648, lr=7.776e-06, gnorm=2.182, train_wall=5, gb_free=14.9, wall=1967
2024-08-17 18:01:23 | INFO | train_inner | epoch 007:     86 / 94 loss=8.361, nll_loss=5.521, ppl=45.92, wps=1467.2, ups=0.41, wpb=3587.5, bsz=184, num_updates=650, lr=7.8e-06, gnorm=1.861, train_wall=5, gb_free=14.9, wall=1971
2024-08-17 18:01:27 | INFO | train_inner | epoch 007:     88 / 94 loss=8.484, nll_loss=5.679, ppl=51.24, wps=1303.9, ups=0.48, wpb=2716.5, bsz=96, num_updates=652, lr=7.824e-06, gnorm=2.256, train_wall=4, gb_free=14.2, wall=1976
2024-08-17 18:01:32 | INFO | train_inner | epoch 007:     90 / 94 loss=8.358, nll_loss=5.543, ppl=46.63, wps=1235, ups=0.44, wpb=2832.5, bsz=180, num_updates=654, lr=7.848e-06, gnorm=2.6, train_wall=5, gb_free=14.5, wall=1980
2024-08-17 18:01:38 | INFO | train_inner | epoch 007:     92 / 94 loss=8.435, nll_loss=5.62, ppl=49.19, wps=1380.2, ups=0.33, wpb=4139, bsz=176, num_updates=656, lr=7.872e-06, gnorm=1.911, train_wall=6, gb_free=8.3, wall=1986
2024-08-17 18:01:42 | INFO | train_inner | epoch 007:     94 / 94 loss=8.419, nll_loss=5.603, ppl=48.62, wps=1453.3, ups=0.52, wpb=2817.5, bsz=112, num_updates=658, lr=7.896e-06, gnorm=2.139, train_wall=4, gb_free=13.7, wall=1990
2024-08-17 18:01:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18676.8125Mb; avail=236389.3125Mb
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000544
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18676.8125Mb; avail=236389.3125Mb
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005337
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18676.8125Mb; avail=236389.3125Mb
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004771
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011023
2024-08-17 18:01:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18676.8125Mb; avail=236389.3125Mb
2024-08-17 18:01:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.181 | nll_loss 5.148 | ppl 35.46 | wps 2428.2 | wpb 944.1 | bsz 40.1 | num_updates 658 | best_loss 8.181
2024-08-17 18:01:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 658 updates
2024-08-17 18:01:55 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:02:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:02:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 7 @ 658 updates, score 8.181) (writing took 61.91932536801323 seconds)
2024-08-17 18:02:57 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-08-17 18:02:57 | INFO | train | epoch 007 | loss 8.492 | nll_loss 5.695 | ppl 51.81 | wps 907.5 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 658 | lr 7.896e-06 | gnorm 2.361 | train_wall 218 | gb_free 13.7 | wall 2065
2024-08-17 18:02:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:02:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:02:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:02:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000778
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26627.06640625Mb; avail=228439.1015625Mb
2024-08-17 18:02:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000163
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001507
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26630.01953125Mb; avail=228435.65625Mb
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26630.51171875Mb; avail=228435.1640625Mb
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000555
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002420
2024-08-17 18:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26632.48046875Mb; avail=228433.6875Mb
2024-08-17 18:02:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:02:57 | INFO | fairseq.trainer | begin training epoch 8
2024-08-17 18:02:57 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:03:02 | INFO | train_inner | epoch 008:      2 / 94 loss=8.518, nll_loss=5.727, ppl=52.96, wps=71.9, ups=0.03, wpb=2866.5, bsz=100, num_updates=660, lr=7.92e-06, gnorm=2.256, train_wall=4, gb_free=11.3, wall=2070
2024-08-17 18:03:06 | INFO | train_inner | epoch 008:      4 / 94 loss=8.497, nll_loss=5.696, ppl=51.83, wps=989.9, ups=0.43, wpb=2314.5, bsz=72, num_updates=662, lr=7.944e-06, gnorm=2.408, train_wall=5, gb_free=11.8, wall=2075
2024-08-17 18:03:11 | INFO | train_inner | epoch 008:      6 / 94 loss=8.34, nll_loss=5.504, ppl=45.39, wps=1389.4, ups=0.39, wpb=3547.5, bsz=180, num_updates=664, lr=7.968e-06, gnorm=1.949, train_wall=5, gb_free=8.4, wall=2080
2024-08-17 18:03:16 | INFO | train_inner | epoch 008:      8 / 94 loss=8.316, nll_loss=5.475, ppl=44.47, wps=1044, ups=0.43, wpb=2433.5, bsz=132, num_updates=666, lr=7.992e-06, gnorm=2.612, train_wall=5, gb_free=14.7, wall=2084
2024-08-17 18:03:21 | INFO | train_inner | epoch 008:     10 / 94 loss=8.369, nll_loss=5.532, ppl=46.28, wps=1343.8, ups=0.38, wpb=3521.5, bsz=152, num_updates=668, lr=8.016e-06, gnorm=1.848, train_wall=5, gb_free=11.4, wall=2090
2024-08-17 18:03:26 | INFO | train_inner | epoch 008:     12 / 94 loss=8.634, nll_loss=5.868, ppl=58.42, wps=1449.4, ups=0.46, wpb=3126.5, bsz=68, num_updates=670, lr=8.04e-06, gnorm=2.314, train_wall=4, gb_free=13.9, wall=2094
2024-08-17 18:03:30 | INFO | train_inner | epoch 008:     14 / 94 loss=8.354, nll_loss=5.516, ppl=45.76, wps=1584.7, ups=0.44, wpb=3571.5, bsz=152, num_updates=672, lr=8.064e-06, gnorm=1.827, train_wall=4, gb_free=10.2, wall=2098
2024-08-17 18:03:35 | INFO | train_inner | epoch 008:     16 / 94 loss=8.388, nll_loss=5.566, ppl=47.37, wps=1545.4, ups=0.4, wpb=3822.5, bsz=184, num_updates=674, lr=8.088e-06, gnorm=1.91, train_wall=5, gb_free=11.2, wall=2103
2024-08-17 18:03:39 | INFO | train_inner | epoch 008:     18 / 94 loss=8.384, nll_loss=5.545, ppl=46.69, wps=852.6, ups=0.5, wpb=1715, bsz=60, num_updates=676, lr=8.112e-06, gnorm=2.771, train_wall=4, gb_free=13.7, wall=2107
2024-08-17 18:03:44 | INFO | train_inner | epoch 008:     20 / 94 loss=8.28, nll_loss=5.429, ppl=43.09, wps=1229.7, ups=0.37, wpb=3315, bsz=192, num_updates=678, lr=8.136e-06, gnorm=2.019, train_wall=5, gb_free=11.3, wall=2113
2024-08-17 18:03:49 | INFO | train_inner | epoch 008:     22 / 94 loss=8.478, nll_loss=5.663, ppl=50.67, wps=1266.4, ups=0.44, wpb=2882, bsz=80, num_updates=680, lr=8.16e-06, gnorm=2.179, train_wall=5, gb_free=13.7, wall=2117
2024-08-17 18:03:54 | INFO | train_inner | epoch 008:     24 / 94 loss=8.298, nll_loss=5.458, ppl=43.97, wps=1484, ups=0.37, wpb=3966.5, bsz=220, num_updates=682, lr=8.184e-06, gnorm=1.766, train_wall=5, gb_free=13.7, wall=2123
2024-08-17 18:03:59 | INFO | train_inner | epoch 008:     26 / 94 loss=8.497, nll_loss=5.715, ppl=52.52, wps=907.5, ups=0.43, wpb=2099.5, bsz=84, num_updates=684, lr=8.208e-06, gnorm=2.661, train_wall=5, gb_free=13.3, wall=2127
2024-08-17 18:04:04 | INFO | train_inner | epoch 008:     28 / 94 loss=8.358, nll_loss=5.526, ppl=46.08, wps=1258.9, ups=0.4, wpb=3148, bsz=128, num_updates=686, lr=8.232e-06, gnorm=2.104, train_wall=5, gb_free=9, wall=2132
2024-08-17 18:04:09 | INFO | train_inner | epoch 008:     30 / 94 loss=8.385, nll_loss=5.577, ppl=47.74, wps=1271.8, ups=0.38, wpb=3309, bsz=184, num_updates=688, lr=8.256e-06, gnorm=2.001, train_wall=5, gb_free=14.1, wall=2137
2024-08-17 18:04:13 | INFO | train_inner | epoch 008:     32 / 94 loss=8.392, nll_loss=5.567, ppl=47.41, wps=1796.4, ups=0.52, wpb=3460.5, bsz=116, num_updates=690, lr=8.28e-06, gnorm=2.254, train_wall=4, gb_free=22.4, wall=2141
2024-08-17 18:04:18 | INFO | train_inner | epoch 008:     34 / 94 loss=8.476, nll_loss=5.672, ppl=50.97, wps=1432.6, ups=0.43, wpb=3321.5, bsz=128, num_updates=692, lr=8.304e-06, gnorm=2.084, train_wall=5, gb_free=14.6, wall=2146
2024-08-17 18:04:23 | INFO | train_inner | epoch 008:     36 / 94 loss=8.423, nll_loss=5.6, ppl=48.51, wps=1446.7, ups=0.41, wpb=3546.5, bsz=124, num_updates=694, lr=8.328e-06, gnorm=2.088, train_wall=5, gb_free=13.4, wall=2151
2024-08-17 18:04:27 | INFO | train_inner | epoch 008:     38 / 94 loss=8.453, nll_loss=5.642, ppl=49.95, wps=738.6, ups=0.42, wpb=1771.5, bsz=64, num_updates=696, lr=8.352e-06, gnorm=2.697, train_wall=5, gb_free=14.4, wall=2156
2024-08-17 18:04:32 | INFO | train_inner | epoch 008:     40 / 94 loss=8.366, nll_loss=5.53, ppl=46.19, wps=1801.8, ups=0.41, wpb=4445.5, bsz=176, num_updates=698, lr=8.376e-06, gnorm=1.667, train_wall=5, gb_free=12.1, wall=2161
2024-08-17 18:04:36 | INFO | train_inner | epoch 008:     42 / 94 loss=8.542, nll_loss=5.756, ppl=54.05, wps=1261.7, ups=0.53, wpb=2378, bsz=68, num_updates=700, lr=8.4e-06, gnorm=2.66, train_wall=4, gb_free=13.6, wall=2164
2024-08-17 18:04:41 | INFO | train_inner | epoch 008:     44 / 94 loss=8.348, nll_loss=5.517, ppl=45.81, wps=1059.4, ups=0.4, wpb=2675.5, bsz=116, num_updates=702, lr=8.424e-06, gnorm=2.238, train_wall=5, gb_free=14.7, wall=2169
2024-08-17 18:04:46 | INFO | train_inner | epoch 008:     46 / 94 loss=8.383, nll_loss=5.563, ppl=47.28, wps=825.7, ups=0.45, wpb=1841, bsz=88, num_updates=704, lr=8.448e-06, gnorm=2.689, train_wall=4, gb_free=14.4, wall=2174
2024-08-17 18:04:50 | INFO | train_inner | epoch 008:     48 / 94 loss=8.449, nll_loss=5.643, ppl=49.99, wps=1144.7, ups=0.41, wpb=2797.5, bsz=128, num_updates=706, lr=8.472e-06, gnorm=2.32, train_wall=5, gb_free=15.5, wall=2179
2024-08-17 18:04:55 | INFO | train_inner | epoch 008:     50 / 94 loss=8.359, nll_loss=5.521, ppl=45.92, wps=1333.2, ups=0.45, wpb=2937.5, bsz=128, num_updates=708, lr=8.496e-06, gnorm=2.112, train_wall=4, gb_free=17.9, wall=2183
2024-08-17 18:05:00 | INFO | train_inner | epoch 008:     52 / 94 loss=8.391, nll_loss=5.568, ppl=47.45, wps=947.1, ups=0.38, wpb=2486, bsz=120, num_updates=710, lr=8.52e-06, gnorm=2.255, train_wall=5, gb_free=14.4, wall=2188
2024-08-17 18:05:04 | INFO | train_inner | epoch 008:     54 / 94 loss=8.459, nll_loss=5.647, ppl=50.09, wps=1193.1, ups=0.58, wpb=2057.5, bsz=72, num_updates=712, lr=8.544e-06, gnorm=2.594, train_wall=3, gb_free=18.8, wall=2192
2024-08-17 18:05:08 | INFO | train_inner | epoch 008:     56 / 94 loss=8.537, nll_loss=5.749, ppl=53.78, wps=688.6, ups=0.5, wpb=1389.5, bsz=40, num_updates=714, lr=8.568e-06, gnorm=3.09, train_wall=4, gb_free=12.6, wall=2196
2024-08-17 18:05:11 | INFO | train_inner | epoch 008:     58 / 94 loss=8.411, nll_loss=5.598, ppl=48.43, wps=840.1, ups=0.52, wpb=1602, bsz=44, num_updates=716, lr=8.592e-06, gnorm=2.862, train_wall=4, gb_free=17, wall=2200
2024-08-17 18:05:16 | INFO | train_inner | epoch 008:     60 / 94 loss=8.466, nll_loss=5.67, ppl=50.91, wps=1114.2, ups=0.42, wpb=2671, bsz=104, num_updates=718, lr=8.616e-06, gnorm=2.575, train_wall=5, gb_free=13.6, wall=2204
2024-08-17 18:05:22 | INFO | train_inner | epoch 008:     62 / 94 loss=8.42, nll_loss=5.601, ppl=48.55, wps=952.1, ups=0.38, wpb=2530, bsz=92, num_updates=720, lr=8.64e-06, gnorm=2.329, train_wall=5, gb_free=10.2, wall=2210
2024-08-17 18:05:25 | INFO | train_inner | epoch 008:     64 / 94 loss=8.602, nll_loss=5.831, ppl=56.91, wps=1519, ups=0.57, wpb=2664.5, bsz=60, num_updates=722, lr=8.664e-06, gnorm=2.337, train_wall=3, gb_free=16.2, wall=2213
2024-08-17 18:05:30 | INFO | train_inner | epoch 008:     66 / 94 loss=8.271, nll_loss=5.431, ppl=43.13, wps=901.8, ups=0.42, wpb=2134.5, bsz=136, num_updates=724, lr=8.688e-06, gnorm=2.503, train_wall=5, gb_free=15.1, wall=2218
2024-08-17 18:05:35 | INFO | train_inner | epoch 008:     68 / 94 loss=8.254, nll_loss=5.403, ppl=42.32, wps=1193.8, ups=0.38, wpb=3108.5, bsz=180, num_updates=726, lr=8.712e-06, gnorm=2.034, train_wall=5, gb_free=13.1, wall=2223
2024-08-17 18:05:39 | INFO | train_inner | epoch 008:     70 / 94 loss=8.503, nll_loss=5.708, ppl=52.29, wps=1281.5, ups=0.48, wpb=2689.5, bsz=79, num_updates=728, lr=8.736e-06, gnorm=2.381, train_wall=4, gb_free=12.6, wall=2227
2024-08-17 18:05:43 | INFO | train_inner | epoch 008:     72 / 94 loss=8.412, nll_loss=5.595, ppl=48.33, wps=1484, ups=0.49, wpb=3026, bsz=124, num_updates=730, lr=8.76e-06, gnorm=2.145, train_wall=4, gb_free=17.5, wall=2232
2024-08-17 18:05:48 | INFO | train_inner | epoch 008:     74 / 94 loss=8.485, nll_loss=5.691, ppl=51.65, wps=1464.1, ups=0.46, wpb=3183, bsz=136, num_updates=732, lr=8.784e-06, gnorm=2.175, train_wall=4, gb_free=15, wall=2236
2024-08-17 18:05:52 | INFO | train_inner | epoch 008:     76 / 94 loss=8.36, nll_loss=5.527, ppl=46.11, wps=1242.5, ups=0.43, wpb=2858.5, bsz=132, num_updates=734, lr=8.808e-06, gnorm=2.094, train_wall=5, gb_free=14, wall=2240
2024-08-17 18:05:57 | INFO | train_inner | epoch 008:     78 / 94 loss=8.272, nll_loss=5.426, ppl=42.99, wps=1208.4, ups=0.42, wpb=2887.5, bsz=164, num_updates=736, lr=8.832e-06, gnorm=2.149, train_wall=5, gb_free=13.4, wall=2245
2024-08-17 18:06:02 | INFO | train_inner | epoch 008:     80 / 94 loss=8.476, nll_loss=5.675, ppl=51.1, wps=1598, ups=0.43, wpb=3721, bsz=136, num_updates=738, lr=8.856e-06, gnorm=2.129, train_wall=5, gb_free=12.8, wall=2250
2024-08-17 18:06:06 | INFO | train_inner | epoch 008:     82 / 94 loss=8.389, nll_loss=5.573, ppl=47.62, wps=1267, ups=0.41, wpb=3065.5, bsz=140, num_updates=740, lr=8.88e-06, gnorm=2.254, train_wall=5, gb_free=14.3, wall=2255
2024-08-17 18:06:12 | INFO | train_inner | epoch 008:     84 / 94 loss=8.28, nll_loss=5.437, ppl=43.31, wps=1539.6, ups=0.39, wpb=3921, bsz=224, num_updates=742, lr=8.904e-06, gnorm=1.827, train_wall=5, gb_free=13, wall=2260
2024-08-17 18:06:16 | INFO | train_inner | epoch 008:     86 / 94 loss=8.433, nll_loss=5.613, ppl=48.93, wps=1149.3, ups=0.43, wpb=2658, bsz=84, num_updates=744, lr=8.928e-06, gnorm=2.403, train_wall=5, gb_free=15.2, wall=2264
2024-08-17 18:06:21 | INFO | train_inner | epoch 008:     88 / 94 loss=8.429, nll_loss=5.621, ppl=49.22, wps=1115.4, ups=0.45, wpb=2456.5, bsz=120, num_updates=746, lr=8.952e-06, gnorm=2.495, train_wall=4, gb_free=11, wall=2269
2024-08-17 18:06:25 | INFO | train_inner | epoch 008:     90 / 94 loss=8.412, nll_loss=5.594, ppl=48.31, wps=953.4, ups=0.41, wpb=2330.5, bsz=96, num_updates=748, lr=8.976e-06, gnorm=2.434, train_wall=5, gb_free=14.5, wall=2274
2024-08-17 18:06:30 | INFO | train_inner | epoch 008:     92 / 94 loss=8.579, nll_loss=5.8, ppl=55.71, wps=1813.8, ups=0.49, wpb=3739, bsz=100, num_updates=750, lr=9e-06, gnorm=2.087, train_wall=4, gb_free=14.1, wall=2278
2024-08-17 18:06:33 | INFO | train_inner | epoch 008:     94 / 94 loss=8.187, nll_loss=5.317, ppl=39.85, wps=992.5, ups=0.66, wpb=1495.5, bsz=103.5, num_updates=752, lr=9.024e-06, gnorm=3.279, train_wall=3, gb_free=22.4, wall=2281
2024-08-17 18:06:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20527.53515625Mb; avail=234538.546875Mb
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000637
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20527.53515625Mb; avail=234538.546875Mb
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005347
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20527.53515625Mb; avail=234538.546875Mb
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004808
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011149
2024-08-17 18:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20527.53515625Mb; avail=234538.546875Mb
2024-08-17 18:06:46 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.136 | nll_loss 5.09 | ppl 34.07 | wps 2426 | wpb 944.1 | bsz 40.1 | num_updates 752 | best_loss 8.136
2024-08-17 18:06:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 752 updates
2024-08-17 18:06:46 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:07:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:07:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 8 @ 752 updates, score 8.136) (writing took 71.77596138976514 seconds)
2024-08-17 18:07:58 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-08-17 18:07:58 | INFO | train | epoch 008 | loss 8.409 | nll_loss 5.59 | ppl 48.18 | wps 887.9 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 752 | lr 9.024e-06 | gnorm 2.295 | train_wall 215 | gb_free 22.4 | wall 2366
2024-08-17 18:07:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:07:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:07:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:07:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000760
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31356.73046875Mb; avail=223709.3984375Mb
2024-08-17 18:07:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000158
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001495
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31356.73046875Mb; avail=223709.3984375Mb
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000064
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31356.73046875Mb; avail=223709.3984375Mb
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000551
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002426
2024-08-17 18:07:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31356.73046875Mb; avail=223709.3984375Mb
2024-08-17 18:07:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:07:58 | INFO | fairseq.trainer | begin training epoch 9
2024-08-17 18:07:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:08:02 | INFO | train_inner | epoch 009:      2 / 94 loss=8.364, nll_loss=5.547, ppl=46.75, wps=46.1, ups=0.02, wpb=2062, bsz=100, num_updates=754, lr=9.048e-06, gnorm=2.603, train_wall=4, gb_free=16.4, wall=2370
2024-08-17 18:08:07 | INFO | train_inner | epoch 009:      4 / 94 loss=8.186, nll_loss=5.319, ppl=39.92, wps=1325.5, ups=0.4, wpb=3318.5, bsz=208, num_updates=756, lr=9.072e-06, gnorm=2.141, train_wall=5, gb_free=14.4, wall=2375
2024-08-17 18:08:11 | INFO | train_inner | epoch 009:      6 / 94 loss=8.549, nll_loss=5.764, ppl=54.34, wps=1630.8, ups=0.46, wpb=3535.5, bsz=84, num_updates=758, lr=9.096e-06, gnorm=2.172, train_wall=4, gb_free=15.5, wall=2380
2024-08-17 18:08:17 | INFO | train_inner | epoch 009:      8 / 94 loss=8.307, nll_loss=5.46, ppl=44.01, wps=1475.7, ups=0.39, wpb=3756.5, bsz=184, num_updates=760, lr=9.12e-06, gnorm=1.999, train_wall=5, gb_free=13.3, wall=2385
2024-08-17 18:08:21 | INFO | train_inner | epoch 009:     10 / 94 loss=8.19, nll_loss=5.323, ppl=40.02, wps=1089.8, ups=0.41, wpb=2652, bsz=180, num_updates=762, lr=9.144e-06, gnorm=2.33, train_wall=5, gb_free=14.9, wall=2390
2024-08-17 18:08:26 | INFO | train_inner | epoch 009:     12 / 94 loss=8.377, nll_loss=5.549, ppl=46.81, wps=1529.8, ups=0.48, wpb=3211.5, bsz=132, num_updates=764, lr=9.168e-06, gnorm=2.114, train_wall=4, gb_free=14.5, wall=2394
2024-08-17 18:08:30 | INFO | train_inner | epoch 009:     14 / 94 loss=8.591, nll_loss=5.827, ppl=56.78, wps=927.8, ups=0.43, wpb=2163.5, bsz=51, num_updates=766, lr=9.192e-06, gnorm=2.753, train_wall=5, gb_free=8.2, wall=2399
2024-08-17 18:08:35 | INFO | train_inner | epoch 009:     16 / 94 loss=8.297, nll_loss=5.453, ppl=43.8, wps=1478.8, ups=0.45, wpb=3305, bsz=152, num_updates=768, lr=9.216e-06, gnorm=1.941, train_wall=4, gb_free=12.4, wall=2403
2024-08-17 18:08:39 | INFO | train_inner | epoch 009:     18 / 94 loss=8.319, nll_loss=5.483, ppl=44.74, wps=1332.6, ups=0.5, wpb=2682.5, bsz=112, num_updates=770, lr=9.24e-06, gnorm=2.366, train_wall=4, gb_free=15.9, wall=2407
2024-08-17 18:08:44 | INFO | train_inner | epoch 009:     20 / 94 loss=8.3, nll_loss=5.442, ppl=43.48, wps=1444.4, ups=0.38, wpb=3758, bsz=196, num_updates=772, lr=9.264e-06, gnorm=1.865, train_wall=5, gb_free=10.1, wall=2412
2024-08-17 18:08:48 | INFO | train_inner | epoch 009:     22 / 94 loss=8.498, nll_loss=5.686, ppl=51.47, wps=1047.4, ups=0.52, wpb=1997.5, bsz=44, num_updates=774, lr=9.288e-06, gnorm=2.688, train_wall=4, gb_free=13.5, wall=2416
2024-08-17 18:08:52 | INFO | train_inner | epoch 009:     24 / 94 loss=8.311, nll_loss=5.454, ppl=43.85, wps=1134.7, ups=0.43, wpb=2656.5, bsz=112, num_updates=776, lr=9.312e-06, gnorm=2.223, train_wall=5, gb_free=13.8, wall=2421
2024-08-17 18:08:57 | INFO | train_inner | epoch 009:     26 / 94 loss=8.318, nll_loss=5.48, ppl=44.63, wps=1134.4, ups=0.45, wpb=2547, bsz=116, num_updates=778, lr=9.336e-06, gnorm=2.367, train_wall=4, gb_free=12.4, wall=2425
2024-08-17 18:09:01 | INFO | train_inner | epoch 009:     28 / 94 loss=8.469, nll_loss=5.669, ppl=50.86, wps=977.3, ups=0.51, wpb=1933.5, bsz=52, num_updates=780, lr=9.36e-06, gnorm=2.634, train_wall=4, gb_free=13.8, wall=2429
2024-08-17 18:09:06 | INFO | train_inner | epoch 009:     30 / 94 loss=8.383, nll_loss=5.567, ppl=47.4, wps=908.3, ups=0.41, wpb=2205, bsz=104, num_updates=782, lr=9.384e-06, gnorm=2.371, train_wall=5, gb_free=13, wall=2434
2024-08-17 18:09:10 | INFO | train_inner | epoch 009:     32 / 94 loss=8.423, nll_loss=5.614, ppl=48.97, wps=1645.4, ups=0.45, wpb=3695, bsz=128, num_updates=784, lr=9.408e-06, gnorm=1.922, train_wall=4, gb_free=14.3, wall=2439
2024-08-17 18:09:14 | INFO | train_inner | epoch 009:     34 / 94 loss=8.307, nll_loss=5.478, ppl=44.58, wps=896.8, ups=0.49, wpb=1840.5, bsz=104, num_updates=786, lr=9.432e-06, gnorm=2.468, train_wall=4, gb_free=14.4, wall=2443
2024-08-17 18:09:19 | INFO | train_inner | epoch 009:     36 / 94 loss=8.309, nll_loss=5.478, ppl=44.56, wps=1478.2, ups=0.42, wpb=3537, bsz=184, num_updates=788, lr=9.456e-06, gnorm=2.055, train_wall=5, gb_free=13, wall=2447
2024-08-17 18:09:23 | INFO | train_inner | epoch 009:     38 / 94 loss=8.334, nll_loss=5.499, ppl=45.22, wps=799.4, ups=0.53, wpb=1497.5, bsz=68, num_updates=790, lr=9.48e-06, gnorm=2.761, train_wall=4, gb_free=19.3, wall=2451
2024-08-17 18:09:28 | INFO | train_inner | epoch 009:     40 / 94 loss=8.335, nll_loss=5.491, ppl=44.97, wps=1092.3, ups=0.4, wpb=2698, bsz=108, num_updates=792, lr=9.504e-06, gnorm=2.35, train_wall=5, gb_free=17.3, wall=2456
2024-08-17 18:09:32 | INFO | train_inner | epoch 009:     42 / 94 loss=8.319, nll_loss=5.473, ppl=44.41, wps=1280.8, ups=0.5, wpb=2571.5, bsz=88, num_updates=794, lr=9.528e-06, gnorm=2.299, train_wall=4, gb_free=16.1, wall=2460
2024-08-17 18:09:37 | INFO | train_inner | epoch 009:     44 / 94 loss=8.235, nll_loss=5.374, ppl=41.48, wps=1499.8, ups=0.39, wpb=3836.5, bsz=212, num_updates=796, lr=9.552e-06, gnorm=1.773, train_wall=5, gb_free=12.1, wall=2465
2024-08-17 18:09:43 | INFO | train_inner | epoch 009:     46 / 94 loss=8.279, nll_loss=5.424, ppl=42.95, wps=1165.9, ups=0.35, wpb=3291.5, bsz=164, num_updates=798, lr=9.576e-06, gnorm=1.961, train_wall=6, gb_free=9.7, wall=2471
2024-08-17 18:09:48 | INFO | train_inner | epoch 009:     48 / 94 loss=8.413, nll_loss=5.599, ppl=48.45, wps=1215.5, ups=0.4, wpb=3063.5, bsz=120, num_updates=800, lr=9.6e-06, gnorm=2.061, train_wall=5, gb_free=10.1, wall=2476
2024-08-17 18:09:53 | INFO | train_inner | epoch 009:     50 / 94 loss=8.412, nll_loss=5.596, ppl=48.37, wps=1438.3, ups=0.41, wpb=3521, bsz=128, num_updates=802, lr=9.624e-06, gnorm=2.05, train_wall=5, gb_free=9.6, wall=2481
2024-08-17 18:09:57 | INFO | train_inner | epoch 009:     52 / 94 loss=8.363, nll_loss=5.529, ppl=46.18, wps=883.8, ups=0.5, wpb=1780.5, bsz=60, num_updates=804, lr=9.648e-06, gnorm=3.196, train_wall=4, gb_free=18.3, wall=2485
2024-08-17 18:10:01 | INFO | train_inner | epoch 009:     54 / 94 loss=8.444, nll_loss=5.624, ppl=49.31, wps=1423.2, ups=0.42, wpb=3422, bsz=104, num_updates=806, lr=9.672e-06, gnorm=1.988, train_wall=5, gb_free=15.5, wall=2490
2024-08-17 18:10:06 | INFO | train_inner | epoch 009:     56 / 94 loss=8.34, nll_loss=5.491, ppl=44.96, wps=893.3, ups=0.44, wpb=2028, bsz=68, num_updates=808, lr=9.696e-06, gnorm=2.4, train_wall=5, gb_free=14.1, wall=2494
2024-08-17 18:10:10 | INFO | train_inner | epoch 009:     58 / 94 loss=8.335, nll_loss=5.495, ppl=45.11, wps=979.6, ups=0.5, wpb=1940, bsz=72, num_updates=810, lr=9.72e-06, gnorm=2.631, train_wall=4, gb_free=9.8, wall=2498
2024-08-17 18:10:14 | INFO | train_inner | epoch 009:     60 / 94 loss=8.494, nll_loss=5.694, ppl=51.76, wps=1265, ups=0.47, wpb=2689.5, bsz=84, num_updates=812, lr=9.744e-06, gnorm=2.337, train_wall=4, gb_free=14.4, wall=2502
2024-08-17 18:10:19 | INFO | train_inner | epoch 009:     62 / 94 loss=8.322, nll_loss=5.478, ppl=44.57, wps=1317.6, ups=0.39, wpb=3389.5, bsz=136, num_updates=814, lr=9.768e-06, gnorm=2.157, train_wall=5, gb_free=12.6, wall=2508
2024-08-17 18:10:25 | INFO | train_inner | epoch 009:     64 / 94 loss=8.384, nll_loss=5.564, ppl=47.32, wps=1122.5, ups=0.38, wpb=2927.5, bsz=92, num_updates=816, lr=9.792e-06, gnorm=2.196, train_wall=5, gb_free=12.9, wall=2513
2024-08-17 18:10:30 | INFO | train_inner | epoch 009:     66 / 94 loss=8.309, nll_loss=5.469, ppl=44.28, wps=1168.1, ups=0.37, wpb=3132, bsz=152, num_updates=818, lr=9.816e-06, gnorm=2.161, train_wall=5, gb_free=11.5, wall=2518
2024-08-17 18:10:34 | INFO | train_inner | epoch 009:     68 / 94 loss=8.426, nll_loss=5.613, ppl=48.95, wps=1453.7, ups=0.49, wpb=2978, bsz=88, num_updates=820, lr=9.84e-06, gnorm=2.445, train_wall=4, gb_free=16.9, wall=2522
2024-08-17 18:10:39 | INFO | train_inner | epoch 009:     70 / 94 loss=8.359, nll_loss=5.534, ppl=46.34, wps=967.3, ups=0.39, wpb=2471, bsz=88, num_updates=822, lr=9.864e-06, gnorm=2.49, train_wall=5, gb_free=11.7, wall=2527
2024-08-17 18:10:43 | INFO | train_inner | epoch 009:     72 / 94 loss=8.333, nll_loss=5.5, ppl=45.27, wps=1089.6, ups=0.49, wpb=2227.5, bsz=103.5, num_updates=824, lr=9.888e-06, gnorm=2.591, train_wall=4, gb_free=11.2, wall=2531
2024-08-17 18:10:48 | INFO | train_inner | epoch 009:     74 / 94 loss=8.335, nll_loss=5.496, ppl=45.13, wps=1413, ups=0.44, wpb=3184.5, bsz=136, num_updates=826, lr=9.912e-06, gnorm=2.211, train_wall=4, gb_free=14.7, wall=2536
2024-08-17 18:10:53 | INFO | train_inner | epoch 009:     76 / 94 loss=8.319, nll_loss=5.474, ppl=44.44, wps=1362.1, ups=0.41, wpb=3345, bsz=160, num_updates=828, lr=9.936e-06, gnorm=1.998, train_wall=5, gb_free=10.9, wall=2541
2024-08-17 18:10:57 | INFO | train_inner | epoch 009:     78 / 94 loss=8.393, nll_loss=5.549, ppl=46.81, wps=1489.8, ups=0.51, wpb=2913.5, bsz=80, num_updates=830, lr=9.96e-06, gnorm=2.139, train_wall=4, gb_free=13.6, wall=2545
2024-08-17 18:11:00 | INFO | train_inner | epoch 009:     80 / 94 loss=8.285, nll_loss=5.428, ppl=43.05, wps=1196.1, ups=0.55, wpb=2169, bsz=80, num_updates=832, lr=9.984e-06, gnorm=2.695, train_wall=4, gb_free=17, wall=2548
2024-08-17 18:11:05 | INFO | train_inner | epoch 009:     82 / 94 loss=8.305, nll_loss=5.463, ppl=44.11, wps=871.3, ups=0.45, wpb=1932.5, bsz=100, num_updates=834, lr=1.0008e-05, gnorm=2.553, train_wall=4, gb_free=16.7, wall=2553
2024-08-17 18:11:09 | INFO | train_inner | epoch 009:     84 / 94 loss=8.296, nll_loss=5.461, ppl=44.04, wps=1481.2, ups=0.42, wpb=3540, bsz=160, num_updates=836, lr=1.0032e-05, gnorm=1.989, train_wall=5, gb_free=10, wall=2558
2024-08-17 18:11:14 | INFO | train_inner | epoch 009:     86 / 94 loss=8.397, nll_loss=5.587, ppl=48.07, wps=1641.4, ups=0.41, wpb=3989.5, bsz=168, num_updates=838, lr=1.0056e-05, gnorm=2.046, train_wall=5, gb_free=11.7, wall=2563
2024-08-17 18:11:19 | INFO | train_inner | epoch 009:     88 / 94 loss=8.379, nll_loss=5.58, ppl=47.83, wps=1229, ups=0.4, wpb=3046.5, bsz=152, num_updates=840, lr=1.008e-05, gnorm=2.268, train_wall=5, gb_free=12, wall=2567
2024-08-17 18:11:24 | INFO | train_inner | epoch 009:     90 / 94 loss=8.301, nll_loss=5.466, ppl=44.19, wps=1810.6, ups=0.4, wpb=4510, bsz=188, num_updates=842, lr=1.0104e-05, gnorm=1.881, train_wall=5, gb_free=14.9, wall=2572
2024-08-17 18:11:29 | INFO | train_inner | epoch 009:     92 / 94 loss=8.143, nll_loss=5.255, ppl=38.19, wps=1228.3, ups=0.44, wpb=2822, bsz=152, num_updates=844, lr=1.0128e-05, gnorm=1.929, train_wall=5, gb_free=13.9, wall=2577
2024-08-17 18:11:37 | INFO | train_inner | epoch 009:     94 / 94 loss=8.392, nll_loss=5.577, ppl=47.73, wps=394.5, ups=0.23, wpb=1714.5, bsz=56, num_updates=846, lr=1.0152e-05, gnorm=2.881, train_wall=9, gb_free=18.1, wall=2586
2024-08-17 18:11:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26706.484375Mb; avail=228359.6015625Mb
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000555
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26706.484375Mb; avail=228359.6015625Mb
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005347
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26628.2265625Mb; avail=228437.859375Mb
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004736
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.010980
2024-08-17 18:11:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26571.625Mb; avail=228496.4296875Mb
2024-08-17 18:11:51 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.107 | nll_loss 5.05 | ppl 33.14 | wps 2445 | wpb 944.1 | bsz 40.1 | num_updates 846 | best_loss 8.107
2024-08-17 18:11:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 846 updates
2024-08-17 18:11:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:12:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 9 @ 846 updates, score 8.107) (writing took 66.73019483918324 seconds)
2024-08-17 18:12:58 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-08-17 18:12:58 | INFO | train | epoch 009 | loss 8.348 | nll_loss 5.515 | ppl 45.73 | wps 890.9 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 846 | lr 1.0152e-05 | gnorm 2.286 | train_wall 219 | gb_free 18.1 | wall 2666
2024-08-17 18:12:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:12:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:12:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:12:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000929
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30762.4296875Mb; avail=224303.6953125Mb
2024-08-17 18:12:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000161
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001549
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30762.4296875Mb; avail=224303.6953125Mb
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000046
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30762.4296875Mb; avail=224303.6953125Mb
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000558
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002464
2024-08-17 18:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30762.4296875Mb; avail=224303.6953125Mb
2024-08-17 18:12:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:12:58 | INFO | fairseq.trainer | begin training epoch 10
2024-08-17 18:12:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:13:03 | INFO | train_inner | epoch 010:      2 / 94 loss=8.285, nll_loss=5.43, ppl=43.12, wps=89.1, ups=0.02, wpb=3791.5, bsz=156, num_updates=848, lr=1.0176e-05, gnorm=1.836, train_wall=5, gb_free=12, wall=2671
2024-08-17 18:13:07 | INFO | train_inner | epoch 010:      4 / 94 loss=8.287, nll_loss=5.427, ppl=43.01, wps=1564.7, ups=0.45, wpb=3439, bsz=116, num_updates=850, lr=1.02e-05, gnorm=2.056, train_wall=4, gb_free=16.3, wall=2675
2024-08-17 18:13:12 | INFO | train_inner | epoch 010:      6 / 94 loss=8.397, nll_loss=5.574, ppl=47.63, wps=1989.8, ups=0.39, wpb=5072.5, bsz=212, num_updates=852, lr=1.0224e-05, gnorm=1.679, train_wall=5, gb_free=10.5, wall=2680
2024-08-17 18:13:16 | INFO | train_inner | epoch 010:      8 / 94 loss=8.196, nll_loss=5.324, ppl=40.05, wps=1487.3, ups=0.45, wpb=3295, bsz=172, num_updates=854, lr=1.0248e-05, gnorm=1.936, train_wall=4, gb_free=15, wall=2685
2024-08-17 18:13:22 | INFO | train_inner | epoch 010:     10 / 94 loss=8.341, nll_loss=5.495, ppl=45.1, wps=1318, ups=0.38, wpb=3466.5, bsz=108, num_updates=856, lr=1.0272e-05, gnorm=2.126, train_wall=5, gb_free=13.1, wall=2690
2024-08-17 18:13:26 | INFO | train_inner | epoch 010:     12 / 94 loss=8.337, nll_loss=5.505, ppl=45.4, wps=1240.2, ups=0.44, wpb=2789.5, bsz=104, num_updates=858, lr=1.0296e-05, gnorm=2.274, train_wall=4, gb_free=8.5, wall=2694
2024-08-17 18:13:31 | INFO | train_inner | epoch 010:     14 / 94 loss=8.205, nll_loss=5.332, ppl=40.28, wps=792.5, ups=0.46, wpb=1729, bsz=84, num_updates=860, lr=1.032e-05, gnorm=2.892, train_wall=4, gb_free=14, wall=2699
2024-08-17 18:13:35 | INFO | train_inner | epoch 010:     16 / 94 loss=8.31, nll_loss=5.462, ppl=44.08, wps=1397.4, ups=0.41, wpb=3426, bsz=132, num_updates=862, lr=1.0344e-05, gnorm=2.104, train_wall=5, gb_free=10.1, wall=2704
2024-08-17 18:13:40 | INFO | train_inner | epoch 010:     18 / 94 loss=8.371, nll_loss=5.538, ppl=46.45, wps=1345.8, ups=0.45, wpb=2979.5, bsz=112, num_updates=864, lr=1.0368e-05, gnorm=2.345, train_wall=4, gb_free=17.7, wall=2708
2024-08-17 18:13:45 | INFO | train_inner | epoch 010:     20 / 94 loss=8.284, nll_loss=5.436, ppl=43.3, wps=1531.8, ups=0.43, wpb=3603, bsz=176, num_updates=866, lr=1.0392e-05, gnorm=1.998, train_wall=5, gb_free=12.4, wall=2713
2024-08-17 18:13:50 | INFO | train_inner | epoch 010:     22 / 94 loss=8.224, nll_loss=5.366, ppl=41.25, wps=1228.8, ups=0.4, wpb=3047.5, bsz=172, num_updates=868, lr=1.0416e-05, gnorm=2.088, train_wall=5, gb_free=13.2, wall=2718
2024-08-17 18:13:54 | INFO | train_inner | epoch 010:     24 / 94 loss=8.28, nll_loss=5.42, ppl=42.81, wps=1054.7, ups=0.45, wpb=2350, bsz=55, num_updates=870, lr=1.044e-05, gnorm=2.505, train_wall=4, gb_free=14.6, wall=2722
2024-08-17 18:13:58 | INFO | train_inner | epoch 010:     26 / 94 loss=8.189, nll_loss=5.32, ppl=39.95, wps=914.5, ups=0.52, wpb=1762, bsz=52, num_updates=872, lr=1.0464e-05, gnorm=2.862, train_wall=4, gb_free=19.1, wall=2726
2024-08-17 18:14:03 | INFO | train_inner | epoch 010:     28 / 94 loss=8.151, nll_loss=5.271, ppl=38.62, wps=902, ups=0.41, wpb=2216.5, bsz=128, num_updates=874, lr=1.0488e-05, gnorm=2.502, train_wall=5, gb_free=12.3, wall=2731
2024-08-17 18:14:08 | INFO | train_inner | epoch 010:     30 / 94 loss=8.156, nll_loss=5.267, ppl=38.5, wps=1023.6, ups=0.37, wpb=2794, bsz=128, num_updates=876, lr=1.0512e-05, gnorm=2.071, train_wall=5, gb_free=10.2, wall=2737
2024-08-17 18:14:13 | INFO | train_inner | epoch 010:     32 / 94 loss=8.202, nll_loss=5.33, ppl=40.22, wps=1211.8, ups=0.39, wpb=3082, bsz=167.5, num_updates=878, lr=1.0536e-05, gnorm=2.111, train_wall=5, gb_free=18, wall=2742
2024-08-17 18:14:18 | INFO | train_inner | epoch 010:     34 / 94 loss=8.371, nll_loss=5.532, ppl=46.26, wps=1022.8, ups=0.44, wpb=2300.5, bsz=84, num_updates=880, lr=1.056e-05, gnorm=2.387, train_wall=4, gb_free=14, wall=2746
2024-08-17 18:14:22 | INFO | train_inner | epoch 010:     36 / 94 loss=8.421, nll_loss=5.6, ppl=48.5, wps=1418.4, ups=0.49, wpb=2912, bsz=80, num_updates=882, lr=1.0584e-05, gnorm=2.121, train_wall=4, gb_free=16.1, wall=2750
2024-08-17 18:14:26 | INFO | train_inner | epoch 010:     38 / 94 loss=8.329, nll_loss=5.484, ppl=44.74, wps=1402, ups=0.45, wpb=3146, bsz=124, num_updates=884, lr=1.0608e-05, gnorm=2.038, train_wall=4, gb_free=13.6, wall=2755
2024-08-17 18:14:32 | INFO | train_inner | epoch 010:     40 / 94 loss=8.274, nll_loss=5.432, ppl=43.16, wps=1437, ups=0.39, wpb=3676.5, bsz=168, num_updates=886, lr=1.0632e-05, gnorm=1.831, train_wall=5, gb_free=16.9, wall=2760
2024-08-17 18:14:35 | INFO | train_inner | epoch 010:     42 / 94 loss=8.322, nll_loss=5.501, ppl=45.29, wps=1218.4, ups=0.52, wpb=2336, bsz=116, num_updates=888, lr=1.0656e-05, gnorm=2.711, train_wall=4, gb_free=18.3, wall=2764
2024-08-17 18:14:40 | INFO | train_inner | epoch 010:     44 / 94 loss=8.237, nll_loss=5.377, ppl=41.55, wps=1524.2, ups=0.42, wpb=3659.5, bsz=160, num_updates=890, lr=1.068e-05, gnorm=2.024, train_wall=5, gb_free=13.6, wall=2768
2024-08-17 18:14:45 | INFO | train_inner | epoch 010:     46 / 94 loss=8.119, nll_loss=5.232, ppl=37.59, wps=890.5, ups=0.4, wpb=2245, bsz=128, num_updates=892, lr=1.0704e-05, gnorm=2.211, train_wall=5, gb_free=13.4, wall=2774
2024-08-17 18:14:50 | INFO | train_inner | epoch 010:     48 / 94 loss=8.341, nll_loss=5.51, ppl=45.58, wps=1291.3, ups=0.39, wpb=3317.5, bsz=132, num_updates=894, lr=1.0728e-05, gnorm=2.361, train_wall=5, gb_free=13.2, wall=2779
2024-08-17 18:14:54 | INFO | train_inner | epoch 010:     50 / 94 loss=8.099, nll_loss=5.21, ppl=37.02, wps=952.7, ups=0.54, wpb=1772, bsz=96, num_updates=896, lr=1.0752e-05, gnorm=2.815, train_wall=4, gb_free=14.7, wall=2782
2024-08-17 18:14:59 | INFO | train_inner | epoch 010:     52 / 94 loss=8.138, nll_loss=5.245, ppl=37.92, wps=1390, ups=0.42, wpb=3305, bsz=180, num_updates=898, lr=1.0776e-05, gnorm=2.027, train_wall=5, gb_free=15.4, wall=2787
2024-08-17 18:15:04 | INFO | train_inner | epoch 010:     54 / 94 loss=8.237, nll_loss=5.355, ppl=40.94, wps=1421.3, ups=0.39, wpb=3689, bsz=152, num_updates=900, lr=1.08e-05, gnorm=2.063, train_wall=5, gb_free=10.8, wall=2792
2024-08-17 18:15:09 | INFO | train_inner | epoch 010:     56 / 94 loss=8.169, nll_loss=5.295, ppl=39.25, wps=1101.8, ups=0.37, wpb=2940, bsz=184, num_updates=902, lr=1.0824e-05, gnorm=2.139, train_wall=5, gb_free=13.7, wall=2798
2024-08-17 18:15:19 | INFO | train_inner | epoch 010:     58 / 94 loss=8.23, nll_loss=5.354, ppl=40.9, wps=472.8, ups=0.21, wpb=2246.5, bsz=76, num_updates=904, lr=1.0848e-05, gnorm=2.777, train_wall=9, gb_free=12.3, wall=2807
2024-08-17 18:15:23 | INFO | train_inner | epoch 010:     60 / 94 loss=8.226, nll_loss=5.358, ppl=41, wps=1475.5, ups=0.45, wpb=3299.5, bsz=144, num_updates=906, lr=1.0872e-05, gnorm=2.083, train_wall=4, gb_free=14.3, wall=2812
2024-08-17 18:15:28 | INFO | train_inner | epoch 010:     62 / 94 loss=8.26, nll_loss=5.417, ppl=42.71, wps=1115.3, ups=0.48, wpb=2316, bsz=128, num_updates=908, lr=1.0896e-05, gnorm=2.492, train_wall=4, gb_free=11.7, wall=2816
2024-08-17 18:15:32 | INFO | train_inner | epoch 010:     64 / 94 loss=8.341, nll_loss=5.507, ppl=45.47, wps=919, ups=0.42, wpb=2179.5, bsz=68, num_updates=910, lr=1.092e-05, gnorm=2.45, train_wall=5, gb_free=15.5, wall=2821
2024-08-17 18:15:38 | INFO | train_inner | epoch 010:     66 / 94 loss=8.297, nll_loss=5.452, ppl=43.78, wps=1235.3, ups=0.36, wpb=3409, bsz=152, num_updates=912, lr=1.0944e-05, gnorm=1.837, train_wall=6, gb_free=9.9, wall=2826
2024-08-17 18:15:42 | INFO | train_inner | epoch 010:     68 / 94 loss=8.262, nll_loss=5.407, ppl=42.43, wps=1428.3, ups=0.45, wpb=3207.5, bsz=128, num_updates=914, lr=1.0968e-05, gnorm=2.043, train_wall=4, gb_free=14.2, wall=2831
2024-08-17 18:15:46 | INFO | train_inner | epoch 010:     70 / 94 loss=8.414, nll_loss=5.592, ppl=48.25, wps=1220.1, ups=0.57, wpb=2144.5, bsz=60, num_updates=916, lr=1.0992e-05, gnorm=2.707, train_wall=4, gb_free=16.6, wall=2834
2024-08-17 18:15:50 | INFO | train_inner | epoch 010:     72 / 94 loss=8.254, nll_loss=5.397, ppl=42.15, wps=1073.1, ups=0.44, wpb=2441, bsz=112, num_updates=918, lr=1.1016e-05, gnorm=2.28, train_wall=5, gb_free=14.4, wall=2839
2024-08-17 18:15:54 | INFO | train_inner | epoch 010:     74 / 94 loss=8.211, nll_loss=5.336, ppl=40.4, wps=927.1, ups=0.56, wpb=1656.5, bsz=56, num_updates=920, lr=1.104e-05, gnorm=2.637, train_wall=4, gb_free=16.2, wall=2842
2024-08-17 18:15:58 | INFO | train_inner | epoch 010:     76 / 94 loss=8.352, nll_loss=5.521, ppl=45.93, wps=1645, ups=0.46, wpb=3598.5, bsz=120, num_updates=922, lr=1.1064e-05, gnorm=1.958, train_wall=4, gb_free=10.4, wall=2847
2024-08-17 18:16:03 | INFO | train_inner | epoch 010:     78 / 94 loss=8.193, nll_loss=5.325, ppl=40.09, wps=1311.1, ups=0.42, wpb=3130.5, bsz=132, num_updates=924, lr=1.1088e-05, gnorm=2.081, train_wall=5, gb_free=15.7, wall=2851
2024-08-17 18:16:07 | INFO | train_inner | epoch 010:     80 / 94 loss=8.341, nll_loss=5.515, ppl=45.72, wps=951.1, ups=0.49, wpb=1955.5, bsz=60, num_updates=926, lr=1.1112e-05, gnorm=2.591, train_wall=4, gb_free=14.9, wall=2855
2024-08-17 18:16:13 | INFO | train_inner | epoch 010:     82 / 94 loss=8.202, nll_loss=5.338, ppl=40.46, wps=1237.8, ups=0.34, wpb=3621.5, bsz=188, num_updates=928, lr=1.1136e-05, gnorm=1.909, train_wall=6, gb_free=8.3, wall=2861
2024-08-17 18:16:17 | INFO | train_inner | epoch 010:     84 / 94 loss=8.128, nll_loss=5.255, ppl=38.19, wps=808, ups=0.51, wpb=1588.5, bsz=100, num_updates=930, lr=1.116e-05, gnorm=2.905, train_wall=4, gb_free=17.8, wall=2865
2024-08-17 18:16:21 | INFO | train_inner | epoch 010:     86 / 94 loss=8.41, nll_loss=5.597, ppl=48.4, wps=975.6, ups=0.45, wpb=2166.5, bsz=60, num_updates=932, lr=1.1184e-05, gnorm=2.752, train_wall=4, gb_free=13.9, wall=2870
2024-08-17 18:16:25 | INFO | train_inner | epoch 010:     88 / 94 loss=8.288, nll_loss=5.44, ppl=43.4, wps=1088.3, ups=0.5, wpb=2189, bsz=96, num_updates=934, lr=1.1208e-05, gnorm=2.687, train_wall=4, gb_free=11.9, wall=2874
2024-08-17 18:16:29 | INFO | train_inner | epoch 010:     90 / 94 loss=8.364, nll_loss=5.52, ppl=45.88, wps=1609, ups=0.49, wpb=3276.5, bsz=72, num_updates=936, lr=1.1232e-05, gnorm=2.23, train_wall=4, gb_free=15.8, wall=2878
2024-08-17 18:16:34 | INFO | train_inner | epoch 010:     92 / 94 loss=8.315, nll_loss=5.465, ppl=44.18, wps=1335.7, ups=0.47, wpb=2846.5, bsz=100, num_updates=938, lr=1.1256e-05, gnorm=2.198, train_wall=4, gb_free=11.7, wall=2882
2024-08-17 18:16:38 | INFO | train_inner | epoch 010:     94 / 94 loss=8.269, nll_loss=5.407, ppl=42.42, wps=1098.2, ups=0.53, wpb=2073.5, bsz=80, num_updates=940, lr=1.128e-05, gnorm=2.937, train_wall=4, gb_free=13.8, wall=2886
2024-08-17 18:16:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20429.76171875Mb; avail=234636.36328125Mb
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20429.76171875Mb; avail=234636.36328125Mb
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005346
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20429.76171875Mb; avail=234636.36328125Mb
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004848
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011088
2024-08-17 18:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20429.76171875Mb; avail=234636.36328125Mb
2024-08-17 18:16:51 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.08 | nll_loss 5.016 | ppl 32.36 | wps 2429.1 | wpb 944.1 | bsz 40.1 | num_updates 940 | best_loss 8.08
2024-08-17 18:16:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 940 updates
2024-08-17 18:16:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:17:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:17:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 10 @ 940 updates, score 8.08) (writing took 64.53432708978653 seconds)
2024-08-17 18:17:56 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-08-17 18:17:56 | INFO | train | epoch 010 | loss 8.274 | nll_loss 5.42 | ppl 42.82 | wps 896 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 940 | lr 1.128e-05 | gnorm 2.291 | train_wall 219 | gb_free 13.8 | wall 2964
2024-08-17 18:17:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:17:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:17:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:17:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000861
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32560.74609375Mb; avail=222505.42578125Mb
2024-08-17 18:17:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000178
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001621
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32564.19140625Mb; avail=222501.98046875Mb
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000059
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32564.68359375Mb; avail=222501.48828125Mb
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000596
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002677
2024-08-17 18:17:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32566.65234375Mb; avail=222499.51953125Mb
2024-08-17 18:17:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:17:56 | INFO | fairseq.trainer | begin training epoch 11
2024-08-17 18:17:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:18:00 | INFO | train_inner | epoch 011:      2 / 94 loss=8.1, nll_loss=5.207, ppl=36.94, wps=61.7, ups=0.02, wpb=2554, bsz=152, num_updates=942, lr=1.1304e-05, gnorm=2.454, train_wall=5, gb_free=14.9, wall=2969
2024-08-17 18:18:04 | INFO | train_inner | epoch 011:      4 / 94 loss=8.208, nll_loss=5.338, ppl=40.45, wps=978.9, ups=0.57, wpb=1710, bsz=47.5, num_updates=944, lr=1.1328e-05, gnorm=2.731, train_wall=3, gb_free=20, wall=2972
2024-08-17 18:18:09 | INFO | train_inner | epoch 011:      6 / 94 loss=8.268, nll_loss=5.411, ppl=42.54, wps=901.4, ups=0.4, wpb=2228.5, bsz=72, num_updates=946, lr=1.1352e-05, gnorm=2.406, train_wall=5, gb_free=9.2, wall=2977
2024-08-17 18:18:14 | INFO | train_inner | epoch 011:      8 / 94 loss=8.166, nll_loss=5.288, ppl=39.06, wps=1021.1, ups=0.42, wpb=2437.5, bsz=116, num_updates=948, lr=1.1376e-05, gnorm=2.291, train_wall=5, gb_free=11.7, wall=2982
2024-08-17 18:18:18 | INFO | train_inner | epoch 011:     10 / 94 loss=8.143, nll_loss=5.255, ppl=38.18, wps=1326.3, ups=0.41, wpb=3220.5, bsz=124, num_updates=950, lr=1.14e-05, gnorm=2.054, train_wall=5, gb_free=9.5, wall=2987
2024-08-17 18:18:24 | INFO | train_inner | epoch 011:     12 / 94 loss=8.164, nll_loss=5.298, ppl=39.33, wps=1287.7, ups=0.39, wpb=3310, bsz=212, num_updates=952, lr=1.1424e-05, gnorm=1.874, train_wall=5, gb_free=11.5, wall=2992
2024-08-17 18:18:28 | INFO | train_inner | epoch 011:     14 / 94 loss=8.267, nll_loss=5.407, ppl=42.44, wps=1287.1, ups=0.42, wpb=3080.5, bsz=112, num_updates=954, lr=1.1448e-05, gnorm=2.117, train_wall=5, gb_free=9, wall=2997
2024-08-17 18:18:33 | INFO | train_inner | epoch 011:     16 / 94 loss=8.248, nll_loss=5.388, ppl=41.88, wps=665, ups=0.46, wpb=1461.5, bsz=64, num_updates=956, lr=1.1472e-05, gnorm=2.927, train_wall=4, gb_free=14.1, wall=3001
2024-08-17 18:18:37 | INFO | train_inner | epoch 011:     18 / 94 loss=8.128, nll_loss=5.231, ppl=37.56, wps=1470.7, ups=0.44, wpb=3371, bsz=152, num_updates=958, lr=1.1496e-05, gnorm=2.055, train_wall=5, gb_free=12.7, wall=3006
2024-08-17 18:18:42 | INFO | train_inner | epoch 011:     20 / 94 loss=8.338, nll_loss=5.496, ppl=45.13, wps=1497.8, ups=0.43, wpb=3467, bsz=112, num_updates=960, lr=1.152e-05, gnorm=2.041, train_wall=5, gb_free=12.4, wall=3010
2024-08-17 18:18:47 | INFO | train_inner | epoch 011:     22 / 94 loss=8.157, nll_loss=5.285, ppl=38.99, wps=1387.2, ups=0.42, wpb=3327, bsz=168, num_updates=962, lr=1.1544e-05, gnorm=1.963, train_wall=5, gb_free=13.8, wall=3015
2024-08-17 18:18:57 | INFO | train_inner | epoch 011:     24 / 94 loss=8.217, nll_loss=5.361, ppl=41.09, wps=761.9, ups=0.2, wpb=3832, bsz=196, num_updates=964, lr=1.1568e-05, gnorm=2.046, train_wall=10, gb_free=12.6, wall=3025
2024-08-17 18:19:01 | INFO | train_inner | epoch 011:     26 / 94 loss=8.27, nll_loss=5.422, ppl=42.87, wps=1056.8, ups=0.51, wpb=2066, bsz=68, num_updates=966, lr=1.1592e-05, gnorm=2.587, train_wall=4, gb_free=15.2, wall=3029
2024-08-17 18:19:05 | INFO | train_inner | epoch 011:     28 / 94 loss=8.22, nll_loss=5.352, ppl=40.86, wps=1118, ups=0.46, wpb=2435.5, bsz=71, num_updates=968, lr=1.1616e-05, gnorm=2.603, train_wall=4, gb_free=16.4, wall=3033
2024-08-17 18:19:10 | INFO | train_inner | epoch 011:     30 / 94 loss=8.233, nll_loss=5.357, ppl=40.99, wps=1346.6, ups=0.44, wpb=3056.5, bsz=140, num_updates=970, lr=1.164e-05, gnorm=1.935, train_wall=5, gb_free=13.4, wall=3038
2024-08-17 18:19:13 | INFO | train_inner | epoch 011:     32 / 94 loss=8.339, nll_loss=5.492, ppl=45.01, wps=1373.8, ups=0.52, wpb=2626, bsz=80, num_updates=972, lr=1.1664e-05, gnorm=2.388, train_wall=4, gb_free=18.8, wall=3042
2024-08-17 18:19:18 | INFO | train_inner | epoch 011:     34 / 94 loss=8.237, nll_loss=5.354, ppl=40.91, wps=997.3, ups=0.43, wpb=2323, bsz=80, num_updates=974, lr=1.1688e-05, gnorm=2.467, train_wall=5, gb_free=15.4, wall=3046
2024-08-17 18:19:22 | INFO | train_inner | epoch 011:     36 / 94 loss=8.208, nll_loss=5.328, ppl=40.17, wps=1450, ups=0.46, wpb=3128, bsz=116, num_updates=976, lr=1.1712e-05, gnorm=2.044, train_wall=4, gb_free=15, wall=3051
2024-08-17 18:19:27 | INFO | train_inner | epoch 011:     38 / 94 loss=8.078, nll_loss=5.177, ppl=36.19, wps=861.5, ups=0.48, wpb=1809, bsz=96, num_updates=978, lr=1.1736e-05, gnorm=2.514, train_wall=4, gb_free=16.9, wall=3055
2024-08-17 18:19:31 | INFO | train_inner | epoch 011:     40 / 94 loss=8.015, nll_loss=5.1, ppl=34.29, wps=963.8, ups=0.51, wpb=1877.5, bsz=100, num_updates=980, lr=1.176e-05, gnorm=2.733, train_wall=4, gb_free=17.1, wall=3059
2024-08-17 18:19:35 | INFO | train_inner | epoch 011:     42 / 94 loss=8.212, nll_loss=5.346, ppl=40.66, wps=1299.8, ups=0.49, wpb=2654.5, bsz=116, num_updates=982, lr=1.1784e-05, gnorm=2.212, train_wall=4, gb_free=13.3, wall=3063
2024-08-17 18:19:39 | INFO | train_inner | epoch 011:     44 / 94 loss=8.275, nll_loss=5.425, ppl=42.96, wps=1417.9, ups=0.44, wpb=3254.5, bsz=120, num_updates=984, lr=1.1808e-05, gnorm=2.137, train_wall=5, gb_free=14.7, wall=3067
2024-08-17 18:19:44 | INFO | train_inner | epoch 011:     46 / 94 loss=8.315, nll_loss=5.479, ppl=44.61, wps=1179.8, ups=0.44, wpb=2658, bsz=100, num_updates=986, lr=1.1832e-05, gnorm=2.399, train_wall=4, gb_free=14.9, wall=3072
2024-08-17 18:19:48 | INFO | train_inner | epoch 011:     48 / 94 loss=8.254, nll_loss=5.396, ppl=42.12, wps=1016.6, ups=0.47, wpb=2168.5, bsz=68, num_updates=988, lr=1.1856e-05, gnorm=2.493, train_wall=4, gb_free=10.7, wall=3076
2024-08-17 18:19:53 | INFO | train_inner | epoch 011:     50 / 94 loss=8.191, nll_loss=5.316, ppl=39.82, wps=1263.9, ups=0.41, wpb=3088, bsz=124, num_updates=990, lr=1.188e-05, gnorm=2.064, train_wall=5, gb_free=13.3, wall=3081
2024-08-17 18:19:57 | INFO | train_inner | epoch 011:     52 / 94 loss=8.233, nll_loss=5.369, ppl=41.33, wps=1377.8, ups=0.45, wpb=3086.5, bsz=112, num_updates=992, lr=1.1904e-05, gnorm=2.051, train_wall=4, gb_free=18.2, wall=3086
2024-08-17 18:20:02 | INFO | train_inner | epoch 011:     54 / 94 loss=8.273, nll_loss=5.423, ppl=42.9, wps=950.9, ups=0.46, wpb=2074, bsz=72, num_updates=994, lr=1.1928e-05, gnorm=2.585, train_wall=4, gb_free=15.9, wall=3090
2024-08-17 18:20:06 | INFO | train_inner | epoch 011:     56 / 94 loss=8.255, nll_loss=5.394, ppl=42.05, wps=1517.6, ups=0.44, wpb=3488, bsz=164, num_updates=996, lr=1.1952e-05, gnorm=2.036, train_wall=5, gb_free=13.9, wall=3095
2024-08-17 18:20:10 | INFO | train_inner | epoch 011:     58 / 94 loss=8.277, nll_loss=5.42, ppl=42.82, wps=1565.1, ups=0.59, wpb=2663, bsz=100, num_updates=998, lr=1.1976e-05, gnorm=2.421, train_wall=3, gb_free=15.8, wall=3098
2024-08-17 18:20:14 | INFO | train_inner | epoch 011:     60 / 94 loss=8.241, nll_loss=5.363, ppl=41.16, wps=1203.6, ups=0.48, wpb=2524, bsz=76, num_updates=1000, lr=1.2e-05, gnorm=2.208, train_wall=4, gb_free=17.5, wall=3102
2024-08-17 18:20:19 | INFO | train_inner | epoch 011:     62 / 94 loss=8.275, nll_loss=5.416, ppl=42.71, wps=1375.3, ups=0.43, wpb=3213.5, bsz=136, num_updates=1002, lr=1.2024e-05, gnorm=2.098, train_wall=5, gb_free=14.5, wall=3107
2024-08-17 18:20:24 | INFO | train_inner | epoch 011:     64 / 94 loss=8.37, nll_loss=5.532, ppl=46.27, wps=1325.8, ups=0.4, wpb=3333.5, bsz=100, num_updates=1004, lr=1.2048e-05, gnorm=2.099, train_wall=5, gb_free=13.8, wall=3112
2024-08-17 18:20:29 | INFO | train_inner | epoch 011:     66 / 94 loss=8.123, nll_loss=5.239, ppl=37.77, wps=1266.9, ups=0.4, wpb=3201, bsz=168, num_updates=1006, lr=1.2072e-05, gnorm=1.908, train_wall=5, gb_free=17.9, wall=3117
2024-08-17 18:20:34 | INFO | train_inner | epoch 011:     68 / 94 loss=8.199, nll_loss=5.353, ppl=40.87, wps=1335.1, ups=0.39, wpb=3383.5, bsz=196, num_updates=1008, lr=1.2096e-05, gnorm=2.134, train_wall=5, gb_free=11, wall=3122
2024-08-17 18:20:39 | INFO | train_inner | epoch 011:     70 / 94 loss=8.208, nll_loss=5.346, ppl=40.67, wps=1120.7, ups=0.37, wpb=2991.5, bsz=128, num_updates=1010, lr=1.212e-05, gnorm=1.943, train_wall=5, gb_free=9.3, wall=3127
2024-08-17 18:20:43 | INFO | train_inner | epoch 011:     72 / 94 loss=7.986, nll_loss=5.069, ppl=33.56, wps=827.2, ups=0.49, wpb=1701.5, bsz=88, num_updates=1012, lr=1.2144e-05, gnorm=2.537, train_wall=4, gb_free=17.7, wall=3131
2024-08-17 18:20:48 | INFO | train_inner | epoch 011:     74 / 94 loss=8.225, nll_loss=5.36, ppl=41.06, wps=1268.6, ups=0.43, wpb=2978.5, bsz=132, num_updates=1014, lr=1.2168e-05, gnorm=2.072, train_wall=5, gb_free=14.4, wall=3136
2024-08-17 18:20:52 | INFO | train_inner | epoch 011:     76 / 94 loss=8.285, nll_loss=5.424, ppl=42.95, wps=1150, ups=0.47, wpb=2469.5, bsz=72, num_updates=1016, lr=1.2192e-05, gnorm=2.38, train_wall=4, gb_free=12.5, wall=3140
2024-08-17 18:20:58 | INFO | train_inner | epoch 011:     78 / 94 loss=8.028, nll_loss=5.124, ppl=34.88, wps=1399.7, ups=0.36, wpb=3942.5, bsz=288, num_updates=1018, lr=1.2216e-05, gnorm=1.741, train_wall=6, gb_free=13.5, wall=3146
2024-08-17 18:21:03 | INFO | train_inner | epoch 011:     80 / 94 loss=8.306, nll_loss=5.468, ppl=44.26, wps=1365.1, ups=0.42, wpb=3256.5, bsz=104, num_updates=1020, lr=1.224e-05, gnorm=2.267, train_wall=5, gb_free=13.3, wall=3151
2024-08-17 18:21:07 | INFO | train_inner | epoch 011:     82 / 94 loss=8.24, nll_loss=5.371, ppl=41.4, wps=1494.3, ups=0.44, wpb=3376, bsz=124, num_updates=1022, lr=1.2264e-05, gnorm=1.938, train_wall=5, gb_free=10.9, wall=3155
2024-08-17 18:21:12 | INFO | train_inner | epoch 011:     84 / 94 loss=8.271, nll_loss=5.41, ppl=42.5, wps=1785, ups=0.45, wpb=3997, bsz=120, num_updates=1024, lr=1.2288e-05, gnorm=1.839, train_wall=4, gb_free=12.4, wall=3160
2024-08-17 18:21:17 | INFO | train_inner | epoch 011:     86 / 94 loss=8.113, nll_loss=5.221, ppl=37.29, wps=1468.7, ups=0.4, wpb=3634, bsz=200, num_updates=1026, lr=1.2312e-05, gnorm=2.121, train_wall=5, gb_free=13.3, wall=3165
2024-08-17 18:21:21 | INFO | train_inner | epoch 011:     88 / 94 loss=8.281, nll_loss=5.43, ppl=43.13, wps=1309.9, ups=0.4, wpb=3241.5, bsz=132, num_updates=1028, lr=1.2336e-05, gnorm=2, train_wall=5, gb_free=11.9, wall=3170
2024-08-17 18:21:27 | INFO | train_inner | epoch 011:     90 / 94 loss=8.293, nll_loss=5.444, ppl=43.53, wps=1226.2, ups=0.39, wpb=3107.5, bsz=96, num_updates=1030, lr=1.236e-05, gnorm=1.999, train_wall=5, gb_free=10.7, wall=3175
2024-08-17 18:21:32 | INFO | train_inner | epoch 011:     92 / 94 loss=8.157, nll_loss=5.272, ppl=38.64, wps=900.4, ups=0.37, wpb=2409.5, bsz=104, num_updates=1032, lr=1.2384e-05, gnorm=2.292, train_wall=5, gb_free=10, wall=3180
2024-08-17 18:21:35 | INFO | train_inner | epoch 011:     94 / 94 loss=8.241, nll_loss=5.379, ppl=41.61, wps=1282.8, ups=0.56, wpb=2272, bsz=92, num_updates=1034, lr=1.2408e-05, gnorm=2.33, train_wall=4, gb_free=15.5, wall=3184
2024-08-17 18:21:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22322.39453125Mb; avail=232743.73046875Mb
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22322.39453125Mb; avail=232743.73046875Mb
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005411
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22322.39453125Mb; avail=232743.73046875Mb
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004750
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011058
2024-08-17 18:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22322.39453125Mb; avail=232743.73046875Mb
2024-08-17 18:21:49 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.045 | nll_loss 4.978 | ppl 31.51 | wps 2424 | wpb 944.1 | bsz 40.1 | num_updates 1034 | best_loss 8.045
2024-08-17 18:21:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1034 updates
2024-08-17 18:21:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:22:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:22:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 11 @ 1034 updates, score 8.045) (writing took 67.11836144281551 seconds)
2024-08-17 18:22:56 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-08-17 18:22:56 | INFO | train | epoch 011 | loss 8.218 | nll_loss 5.351 | ppl 40.81 | wps 888.4 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 1034 | lr 1.2408e-05 | gnorm 2.224 | train_wall 219 | gb_free 15.5 | wall 3264
2024-08-17 18:22:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:22:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:22:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:22:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000772
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34294.6171875Mb; avail=220771.51171875Mb
2024-08-17 18:22:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000170
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001557
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34294.6171875Mb; avail=220771.51171875Mb
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000051
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34294.6171875Mb; avail=220771.51171875Mb
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000554
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002474
2024-08-17 18:22:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34294.6171875Mb; avail=220771.51171875Mb
2024-08-17 18:22:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:22:56 | INFO | fairseq.trainer | begin training epoch 12
2024-08-17 18:22:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:23:01 | INFO | train_inner | epoch 012:      2 / 94 loss=8.152, nll_loss=5.27, ppl=38.58, wps=93, ups=0.02, wpb=3973, bsz=156, num_updates=1036, lr=1.2432e-05, gnorm=1.818, train_wall=5, gb_free=11.8, wall=3269
2024-08-17 18:23:06 | INFO | train_inner | epoch 012:      4 / 94 loss=8.192, nll_loss=5.319, ppl=39.92, wps=1369.8, ups=0.42, wpb=3223, bsz=132, num_updates=1038, lr=1.2456e-05, gnorm=2.031, train_wall=5, gb_free=13.1, wall=3274
2024-08-17 18:23:10 | INFO | train_inner | epoch 012:      6 / 94 loss=8.215, nll_loss=5.35, ppl=40.78, wps=1598.8, ups=0.42, wpb=3820.5, bsz=176, num_updates=1040, lr=1.248e-05, gnorm=1.73, train_wall=5, gb_free=10.6, wall=3279
2024-08-17 18:23:14 | INFO | train_inner | epoch 012:      8 / 94 loss=8.302, nll_loss=5.453, ppl=43.81, wps=1623.9, ups=0.51, wpb=3193, bsz=84, num_updates=1042, lr=1.2504e-05, gnorm=2.145, train_wall=4, gb_free=14.7, wall=3283
2024-08-17 18:23:19 | INFO | train_inner | epoch 012:     10 / 94 loss=8.019, nll_loss=5.102, ppl=34.36, wps=1348.8, ups=0.45, wpb=3004, bsz=180, num_updates=1044, lr=1.2528e-05, gnorm=2.154, train_wall=4, gb_free=11.5, wall=3287
2024-08-17 18:23:23 | INFO | train_inner | epoch 012:     12 / 94 loss=7.97, nll_loss=5.039, ppl=32.88, wps=822.1, ups=0.44, wpb=1855.5, bsz=100, num_updates=1046, lr=1.2552e-05, gnorm=2.609, train_wall=5, gb_free=12.6, wall=3292
2024-08-17 18:23:28 | INFO | train_inner | epoch 012:     14 / 94 loss=8.069, nll_loss=5.151, ppl=35.53, wps=761.8, ups=0.39, wpb=1963, bsz=88, num_updates=1048, lr=1.2576e-05, gnorm=2.367, train_wall=5, gb_free=10.8, wall=3297
2024-08-17 18:23:33 | INFO | train_inner | epoch 012:     16 / 94 loss=8.303, nll_loss=5.46, ppl=44, wps=1477.7, ups=0.46, wpb=3215, bsz=124, num_updates=1050, lr=1.26e-05, gnorm=2.357, train_wall=4, gb_free=15.7, wall=3301
2024-08-17 18:23:37 | INFO | train_inner | epoch 012:     18 / 94 loss=8.036, nll_loss=5.128, ppl=34.98, wps=902.5, ups=0.45, wpb=2024, bsz=120, num_updates=1052, lr=1.2624e-05, gnorm=2.497, train_wall=4, gb_free=15.1, wall=3306
2024-08-17 18:23:42 | INFO | train_inner | epoch 012:     20 / 94 loss=8.018, nll_loss=5.092, ppl=34.12, wps=933.2, ups=0.46, wpb=2016, bsz=108, num_updates=1054, lr=1.2648e-05, gnorm=2.368, train_wall=4, gb_free=18.3, wall=3310
2024-08-17 18:23:45 | INFO | train_inner | epoch 012:     22 / 94 loss=8.088, nll_loss=5.169, ppl=35.98, wps=941.1, ups=0.51, wpb=1832.5, bsz=44, num_updates=1056, lr=1.2672e-05, gnorm=2.887, train_wall=4, gb_free=17.2, wall=3314
2024-08-17 18:23:49 | INFO | train_inner | epoch 012:     24 / 94 loss=8.095, nll_loss=5.203, ppl=36.84, wps=1139.4, ups=0.51, wpb=2234, bsz=124, num_updates=1058, lr=1.2696e-05, gnorm=2.219, train_wall=4, gb_free=18.7, wall=3318
2024-08-17 18:23:55 | INFO | train_inner | epoch 012:     26 / 94 loss=8.16, nll_loss=5.284, ppl=38.95, wps=1503.2, ups=0.36, wpb=4183.5, bsz=200, num_updates=1060, lr=1.272e-05, gnorm=1.857, train_wall=6, gb_free=13.2, wall=3323
2024-08-17 18:24:00 | INFO | train_inner | epoch 012:     28 / 94 loss=8.191, nll_loss=5.316, ppl=39.83, wps=1043.7, ups=0.39, wpb=2662, bsz=100, num_updates=1062, lr=1.2744e-05, gnorm=2.146, train_wall=5, gb_free=11.1, wall=3328
2024-08-17 18:24:04 | INFO | train_inner | epoch 012:     30 / 94 loss=8.134, nll_loss=5.245, ppl=37.92, wps=1017.4, ups=0.46, wpb=2191, bsz=84, num_updates=1064, lr=1.2768e-05, gnorm=2.296, train_wall=4, gb_free=18, wall=3333
2024-08-17 18:24:09 | INFO | train_inner | epoch 012:     32 / 94 loss=8.171, nll_loss=5.293, ppl=39.2, wps=925.8, ups=0.41, wpb=2282.5, bsz=88, num_updates=1066, lr=1.2792e-05, gnorm=2.308, train_wall=5, gb_free=12.6, wall=3338
2024-08-17 18:24:14 | INFO | train_inner | epoch 012:     34 / 94 loss=8.034, nll_loss=5.134, ppl=35.11, wps=919.4, ups=0.44, wpb=2105.5, bsz=128, num_updates=1068, lr=1.2816e-05, gnorm=2.493, train_wall=5, gb_free=14.5, wall=3342
2024-08-17 18:24:18 | INFO | train_inner | epoch 012:     36 / 94 loss=7.892, nll_loss=4.956, ppl=31.04, wps=914.3, ups=0.44, wpb=2058, bsz=124, num_updates=1070, lr=1.284e-05, gnorm=2.385, train_wall=4, gb_free=11.5, wall=3347
2024-08-17 18:24:24 | INFO | train_inner | epoch 012:     38 / 94 loss=8.161, nll_loss=5.268, ppl=38.54, wps=1779.5, ups=0.39, wpb=4591, bsz=208, num_updates=1072, lr=1.2864e-05, gnorm=1.85, train_wall=5, gb_free=10.7, wall=3352
2024-08-17 18:24:28 | INFO | train_inner | epoch 012:     40 / 94 loss=8.254, nll_loss=5.382, ppl=41.69, wps=1334.5, ups=0.46, wpb=2918.5, bsz=124, num_updates=1074, lr=1.2888e-05, gnorm=2.302, train_wall=4, gb_free=14.3, wall=3356
2024-08-17 18:24:32 | INFO | train_inner | epoch 012:     42 / 94 loss=8.124, nll_loss=5.224, ppl=37.36, wps=815.4, ups=0.47, wpb=1721.5, bsz=72, num_updates=1076, lr=1.2912e-05, gnorm=2.831, train_wall=4, gb_free=9.5, wall=3360
2024-08-17 18:24:38 | INFO | train_inner | epoch 012:     44 / 94 loss=8.152, nll_loss=5.268, ppl=38.53, wps=1695.9, ups=0.37, wpb=4637.5, bsz=232, num_updates=1078, lr=1.2936e-05, gnorm=1.633, train_wall=5, gb_free=10.1, wall=3366
2024-08-17 18:24:42 | INFO | train_inner | epoch 012:     46 / 94 loss=8.223, nll_loss=5.352, ppl=40.85, wps=1810.4, ups=0.42, wpb=4319, bsz=172, num_updates=1080, lr=1.296e-05, gnorm=1.773, train_wall=5, gb_free=9.1, wall=3371
2024-08-17 18:24:46 | INFO | train_inner | epoch 012:     48 / 94 loss=8.26, nll_loss=5.401, ppl=42.25, wps=1637.3, ups=0.49, wpb=3367.5, bsz=124, num_updates=1082, lr=1.2984e-05, gnorm=1.908, train_wall=4, gb_free=15.6, wall=3375
2024-08-17 18:24:51 | INFO | train_inner | epoch 012:     50 / 94 loss=8.236, nll_loss=5.367, ppl=41.27, wps=1478.5, ups=0.45, wpb=3262.5, bsz=104, num_updates=1084, lr=1.3008e-05, gnorm=2.052, train_wall=4, gb_free=14.7, wall=3379
2024-08-17 18:24:55 | INFO | train_inner | epoch 012:     52 / 94 loss=8.287, nll_loss=5.433, ppl=43.19, wps=1301.6, ups=0.47, wpb=2786, bsz=64, num_updates=1086, lr=1.3032e-05, gnorm=2.108, train_wall=4, gb_free=17, wall=3383
2024-08-17 18:24:59 | INFO | train_inner | epoch 012:     54 / 94 loss=8.168, nll_loss=5.285, ppl=39, wps=1147, ups=0.47, wpb=2463, bsz=95.5, num_updates=1088, lr=1.3056e-05, gnorm=2.217, train_wall=4, gb_free=9, wall=3388
2024-08-17 18:25:03 | INFO | train_inner | epoch 012:     56 / 94 loss=8.267, nll_loss=5.415, ppl=42.68, wps=1481.8, ups=0.55, wpb=2705, bsz=104, num_updates=1090, lr=1.308e-05, gnorm=2.205, train_wall=4, gb_free=16.5, wall=3391
2024-08-17 18:25:08 | INFO | train_inner | epoch 012:     58 / 94 loss=8.039, nll_loss=5.132, ppl=35.07, wps=1129.8, ups=0.4, wpb=2824.5, bsz=172, num_updates=1092, lr=1.3104e-05, gnorm=2.156, train_wall=5, gb_free=17.9, wall=3396
2024-08-17 18:25:12 | INFO | train_inner | epoch 012:     60 / 94 loss=8.214, nll_loss=5.344, ppl=40.62, wps=1029.9, ups=0.48, wpb=2144.5, bsz=68, num_updates=1094, lr=1.3128e-05, gnorm=2.574, train_wall=4, gb_free=14, wall=3401
2024-08-17 18:25:18 | INFO | train_inner | epoch 012:     62 / 94 loss=8.208, nll_loss=5.346, ppl=40.68, wps=1256.2, ups=0.38, wpb=3319, bsz=148, num_updates=1096, lr=1.3152e-05, gnorm=2.033, train_wall=5, gb_free=10.2, wall=3406
2024-08-17 18:25:22 | INFO | train_inner | epoch 012:     64 / 94 loss=8.195, nll_loss=5.327, ppl=40.14, wps=1159.4, ups=0.41, wpb=2811.5, bsz=124, num_updates=1098, lr=1.3176e-05, gnorm=2.151, train_wall=5, gb_free=14, wall=3411
2024-08-17 18:25:27 | INFO | train_inner | epoch 012:     66 / 94 loss=8.186, nll_loss=5.31, ppl=39.67, wps=972.4, ups=0.43, wpb=2242.5, bsz=76, num_updates=1100, lr=1.32e-05, gnorm=2.493, train_wall=5, gb_free=13.6, wall=3415
2024-08-17 18:25:32 | INFO | train_inner | epoch 012:     68 / 94 loss=8.152, nll_loss=5.268, ppl=38.54, wps=1274.6, ups=0.45, wpb=2845.5, bsz=120, num_updates=1102, lr=1.3224e-05, gnorm=2.096, train_wall=4, gb_free=16.8, wall=3420
2024-08-17 18:25:37 | INFO | train_inner | epoch 012:     70 / 94 loss=8.097, nll_loss=5.196, ppl=36.65, wps=1234.6, ups=0.39, wpb=3132, bsz=128, num_updates=1104, lr=1.3248e-05, gnorm=2.007, train_wall=5, gb_free=14.9, wall=3425
2024-08-17 18:25:41 | INFO | train_inner | epoch 012:     72 / 94 loss=8.205, nll_loss=5.349, ppl=40.75, wps=1054.1, ups=0.44, wpb=2422.5, bsz=120, num_updates=1106, lr=1.3272e-05, gnorm=2.298, train_wall=5, gb_free=12.4, wall=3429
2024-08-17 18:25:45 | INFO | train_inner | epoch 012:     74 / 94 loss=8.226, nll_loss=5.357, ppl=40.98, wps=1514, ups=0.51, wpb=2941, bsz=72, num_updates=1108, lr=1.3296e-05, gnorm=2.236, train_wall=4, gb_free=13.8, wall=3433
2024-08-17 18:25:50 | INFO | train_inner | epoch 012:     76 / 94 loss=8.211, nll_loss=5.33, ppl=40.23, wps=1365.7, ups=0.42, wpb=3266.5, bsz=112, num_updates=1110, lr=1.332e-05, gnorm=1.983, train_wall=5, gb_free=12.7, wall=3438
2024-08-17 18:25:54 | INFO | train_inner | epoch 012:     78 / 94 loss=8.336, nll_loss=5.494, ppl=45.06, wps=1014.3, ups=0.49, wpb=2067, bsz=64, num_updates=1112, lr=1.3344e-05, gnorm=2.853, train_wall=4, gb_free=15.9, wall=3442
2024-08-17 18:25:57 | INFO | train_inner | epoch 012:     80 / 94 loss=8.291, nll_loss=5.447, ppl=43.63, wps=1154.5, ups=0.59, wpb=1972, bsz=48, num_updates=1114, lr=1.3368e-05, gnorm=2.672, train_wall=3, gb_free=13.2, wall=3446
2024-08-17 18:26:02 | INFO | train_inner | epoch 012:     82 / 94 loss=8.165, nll_loss=5.274, ppl=38.69, wps=1431.9, ups=0.42, wpb=3374.5, bsz=104, num_updates=1116, lr=1.3392e-05, gnorm=1.91, train_wall=5, gb_free=13.7, wall=3450
2024-08-17 18:26:07 | INFO | train_inner | epoch 012:     84 / 94 loss=8.087, nll_loss=5.173, ppl=36.09, wps=871.6, ups=0.38, wpb=2323.5, bsz=88, num_updates=1118, lr=1.3416e-05, gnorm=2.302, train_wall=5, gb_free=10.2, wall=3456
2024-08-17 18:26:12 | INFO | train_inner | epoch 012:     86 / 94 loss=8.189, nll_loss=5.3, ppl=39.4, wps=1781.3, ups=0.4, wpb=4463, bsz=164, num_updates=1120, lr=1.344e-05, gnorm=1.749, train_wall=5, gb_free=14.9, wall=3461
2024-08-17 18:26:18 | INFO | train_inner | epoch 012:     88 / 94 loss=8.218, nll_loss=5.356, ppl=40.95, wps=1208.6, ups=0.37, wpb=3267, bsz=168, num_updates=1122, lr=1.3464e-05, gnorm=1.993, train_wall=5, gb_free=14.1, wall=3466
2024-08-17 18:26:23 | INFO | train_inner | epoch 012:     90 / 94 loss=8.074, nll_loss=5.17, ppl=36.01, wps=1206.8, ups=0.39, wpb=3095, bsz=168, num_updates=1124, lr=1.3488e-05, gnorm=1.897, train_wall=5, gb_free=11.8, wall=3471
2024-08-17 18:26:27 | INFO | train_inner | epoch 012:     92 / 94 loss=8.107, nll_loss=5.224, ppl=37.37, wps=1432.2, ups=0.46, wpb=3106.5, bsz=176, num_updates=1126, lr=1.3512e-05, gnorm=2.191, train_wall=4, gb_free=19.8, wall=3476
2024-08-17 18:26:35 | INFO | train_inner | epoch 012:     94 / 94 loss=8.173, nll_loss=5.298, ppl=39.35, wps=311.2, ups=0.25, wpb=1263.5, bsz=31, num_updates=1128, lr=1.3536e-05, gnorm=3.506, train_wall=8, gb_free=23.2, wall=3484
2024-08-17 18:26:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25941.125Mb; avail=229125.01171875Mb
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25941.125Mb; avail=229125.01171875Mb
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005354
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25941.125Mb; avail=229125.01171875Mb
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004753
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011002
2024-08-17 18:26:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25941.125Mb; avail=229125.01171875Mb
2024-08-17 18:26:49 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.037 | nll_loss 4.973 | ppl 31.4 | wps 2443.9 | wpb 944.1 | bsz 40.1 | num_updates 1128 | best_loss 8.037
2024-08-17 18:26:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1128 updates
2024-08-17 18:26:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:27:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 12 @ 1128 updates, score 8.037) (writing took 62.95054504228756 seconds)
2024-08-17 18:27:52 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-08-17 18:27:52 | INFO | train | epoch 012 | loss 8.168 | nll_loss 5.287 | ppl 39.04 | wps 902.9 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 1128 | lr 1.3536e-05 | gnorm 2.227 | train_wall 219 | gb_free 23.2 | wall 3560
2024-08-17 18:27:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:27:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:27:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:27:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000894
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28318.03125Mb; avail=226748.10546875Mb
2024-08-17 18:27:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000163
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001480
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28318.03125Mb; avail=226748.10546875Mb
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000050
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28318.03125Mb; avail=226748.10546875Mb
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000552
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002376
2024-08-17 18:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28318.03125Mb; avail=226748.10546875Mb
2024-08-17 18:27:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:27:52 | INFO | fairseq.trainer | begin training epoch 13
2024-08-17 18:27:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:27:56 | INFO | train_inner | epoch 013:      2 / 94 loss=7.973, nll_loss=5.055, ppl=33.25, wps=32.1, ups=0.02, wpb=1297.5, bsz=52, num_updates=1130, lr=1.356e-05, gnorm=2.901, train_wall=4, gb_free=14.4, wall=3564
2024-08-17 18:28:00 | INFO | train_inner | epoch 013:      4 / 94 loss=8.148, nll_loss=5.263, ppl=38.39, wps=1131.4, ups=0.54, wpb=2102.5, bsz=76, num_updates=1132, lr=1.3584e-05, gnorm=2.554, train_wall=4, gb_free=13.1, wall=3568
2024-08-17 18:28:05 | INFO | train_inner | epoch 013:      6 / 94 loss=7.983, nll_loss=5.064, ppl=33.45, wps=1290.7, ups=0.4, wpb=3230.5, bsz=208, num_updates=1134, lr=1.3608e-05, gnorm=1.939, train_wall=5, gb_free=11.3, wall=3573
2024-08-17 18:28:09 | INFO | train_inner | epoch 013:      8 / 94 loss=8.091, nll_loss=5.196, ppl=36.66, wps=1326.9, ups=0.45, wpb=2979.5, bsz=152, num_updates=1136, lr=1.3632e-05, gnorm=2.273, train_wall=4, gb_free=13.6, wall=3578
2024-08-17 18:28:14 | INFO | train_inner | epoch 013:     10 / 94 loss=8.102, nll_loss=5.188, ppl=36.46, wps=1004.9, ups=0.42, wpb=2403, bsz=92, num_updates=1138, lr=1.3656e-05, gnorm=2.413, train_wall=5, gb_free=13.4, wall=3582
2024-08-17 18:28:18 | INFO | train_inner | epoch 013:     12 / 94 loss=8.23, nll_loss=5.353, ppl=40.87, wps=1254.3, ups=0.48, wpb=2602, bsz=60, num_updates=1140, lr=1.368e-05, gnorm=2.412, train_wall=4, gb_free=12.6, wall=3587
2024-08-17 18:28:23 | INFO | train_inner | epoch 013:     14 / 94 loss=8.038, nll_loss=5.121, ppl=34.8, wps=1058.3, ups=0.44, wpb=2382.5, bsz=128, num_updates=1142, lr=1.3704e-05, gnorm=2.335, train_wall=4, gb_free=15.9, wall=3591
2024-08-17 18:28:27 | INFO | train_inner | epoch 013:     16 / 94 loss=8.055, nll_loss=5.136, ppl=35.15, wps=1750.7, ups=0.46, wpb=3787, bsz=172, num_updates=1144, lr=1.3728e-05, gnorm=1.841, train_wall=4, gb_free=10.2, wall=3595
2024-08-17 18:28:32 | INFO | train_inner | epoch 013:     18 / 94 loss=8.206, nll_loss=5.324, ppl=40.05, wps=1566.7, ups=0.42, wpb=3721.5, bsz=140, num_updates=1146, lr=1.3752e-05, gnorm=1.858, train_wall=5, gb_free=12.7, wall=3600
2024-08-17 18:28:36 | INFO | train_inner | epoch 013:     20 / 94 loss=8.032, nll_loss=5.118, ppl=34.73, wps=903.4, ups=0.46, wpb=1983.5, bsz=72, num_updates=1148, lr=1.3776e-05, gnorm=2.385, train_wall=4, gb_free=15.1, wall=3605
2024-08-17 18:28:42 | INFO | train_inner | epoch 013:     22 / 94 loss=8.044, nll_loss=5.156, ppl=35.64, wps=1393.6, ups=0.39, wpb=3606, bsz=220, num_updates=1150, lr=1.38e-05, gnorm=1.786, train_wall=5, gb_free=12.3, wall=3610
2024-08-17 18:28:46 | INFO | train_inner | epoch 013:     24 / 94 loss=8.086, nll_loss=5.194, ppl=36.62, wps=1054.3, ups=0.43, wpb=2424, bsz=96, num_updates=1152, lr=1.3824e-05, gnorm=2.301, train_wall=5, gb_free=12.7, wall=3614
2024-08-17 18:28:50 | INFO | train_inner | epoch 013:     26 / 94 loss=8.321, nll_loss=5.498, ppl=45.19, wps=1632, ups=0.46, wpb=3569.5, bsz=100, num_updates=1154, lr=1.3848e-05, gnorm=2.209, train_wall=4, gb_free=17.3, wall=3619
2024-08-17 18:28:55 | INFO | train_inner | epoch 013:     28 / 94 loss=8.057, nll_loss=5.144, ppl=35.36, wps=1329.7, ups=0.42, wpb=3169.5, bsz=107, num_updates=1156, lr=1.3872e-05, gnorm=1.974, train_wall=5, gb_free=15.1, wall=3624
2024-08-17 18:29:00 | INFO | train_inner | epoch 013:     30 / 94 loss=8.19, nll_loss=5.313, ppl=39.76, wps=1296.9, ups=0.42, wpb=3052.5, bsz=136, num_updates=1158, lr=1.3896e-05, gnorm=2.095, train_wall=5, gb_free=16, wall=3628
2024-08-17 18:29:05 | INFO | train_inner | epoch 013:     32 / 94 loss=8.095, nll_loss=5.17, ppl=35.99, wps=1188.4, ups=0.43, wpb=2786, bsz=92, num_updates=1160, lr=1.392e-05, gnorm=2.063, train_wall=5, gb_free=18.8, wall=3633
2024-08-17 18:29:09 | INFO | train_inner | epoch 013:     34 / 94 loss=7.979, nll_loss=5.033, ppl=32.74, wps=847.4, ups=0.52, wpb=1636.5, bsz=79.5, num_updates=1162, lr=1.3944e-05, gnorm=3.089, train_wall=4, gb_free=20.3, wall=3637
2024-08-17 18:29:13 | INFO | train_inner | epoch 013:     36 / 94 loss=8.092, nll_loss=5.17, ppl=35.99, wps=790.3, ups=0.44, wpb=1798.5, bsz=68, num_updates=1164, lr=1.3968e-05, gnorm=2.602, train_wall=5, gb_free=13.5, wall=3641
2024-08-17 18:29:17 | INFO | train_inner | epoch 013:     38 / 94 loss=8.133, nll_loss=5.226, ppl=37.42, wps=1509.2, ups=0.56, wpb=2682.5, bsz=68, num_updates=1166, lr=1.3992e-05, gnorm=2.169, train_wall=4, gb_free=13.3, wall=3645
2024-08-17 18:29:22 | INFO | train_inner | epoch 013:     40 / 94 loss=8.176, nll_loss=5.3, ppl=39.38, wps=1562.4, ups=0.4, wpb=3897.5, bsz=164, num_updates=1168, lr=1.4016e-05, gnorm=1.788, train_wall=5, gb_free=12.8, wall=3650
2024-08-17 18:29:25 | INFO | train_inner | epoch 013:     42 / 94 loss=8.055, nll_loss=5.151, ppl=35.54, wps=1324, ups=0.53, wpb=2487, bsz=80, num_updates=1170, lr=1.404e-05, gnorm=2.39, train_wall=4, gb_free=15.1, wall=3654
2024-08-17 18:29:29 | INFO | train_inner | epoch 013:     44 / 94 loss=8.042, nll_loss=5.151, ppl=35.53, wps=1010.4, ups=0.5, wpb=2020, bsz=116, num_updates=1172, lr=1.4064e-05, gnorm=2.423, train_wall=4, gb_free=14, wall=3658
2024-08-17 18:29:34 | INFO | train_inner | epoch 013:     46 / 94 loss=8.11, nll_loss=5.213, ppl=37.08, wps=1591.9, ups=0.44, wpb=3658, bsz=132, num_updates=1174, lr=1.4088e-05, gnorm=1.867, train_wall=5, gb_free=14.7, wall=3662
2024-08-17 18:29:38 | INFO | train_inner | epoch 013:     48 / 94 loss=8.157, nll_loss=5.275, ppl=38.72, wps=1235.1, ups=0.44, wpb=2779.5, bsz=96, num_updates=1176, lr=1.4112e-05, gnorm=2.347, train_wall=4, gb_free=14.2, wall=3667
2024-08-17 18:29:43 | INFO | train_inner | epoch 013:     50 / 94 loss=8.199, nll_loss=5.322, ppl=40.01, wps=1485.6, ups=0.43, wpb=3495, bsz=132, num_updates=1178, lr=1.4136e-05, gnorm=2.068, train_wall=5, gb_free=18.3, wall=3671
2024-08-17 18:29:48 | INFO | train_inner | epoch 013:     52 / 94 loss=8.071, nll_loss=5.157, ppl=35.68, wps=1222.1, ups=0.42, wpb=2896.5, bsz=120, num_updates=1180, lr=1.416e-05, gnorm=2.036, train_wall=5, gb_free=16.3, wall=3676
2024-08-17 18:29:53 | INFO | train_inner | epoch 013:     54 / 94 loss=8.094, nll_loss=5.194, ppl=36.61, wps=993.4, ups=0.43, wpb=2305.5, bsz=120, num_updates=1182, lr=1.4184e-05, gnorm=2.399, train_wall=5, gb_free=14, wall=3681
2024-08-17 18:29:57 | INFO | train_inner | epoch 013:     56 / 94 loss=8.094, nll_loss=5.18, ppl=36.24, wps=1306.7, ups=0.43, wpb=3060.5, bsz=120, num_updates=1184, lr=1.4208e-05, gnorm=1.958, train_wall=5, gb_free=15, wall=3686
2024-08-17 18:30:02 | INFO | train_inner | epoch 013:     58 / 94 loss=8.041, nll_loss=5.104, ppl=34.4, wps=1642.7, ups=0.42, wpb=3935.5, bsz=136, num_updates=1186, lr=1.4232e-05, gnorm=1.743, train_wall=5, gb_free=14, wall=3690
2024-08-17 18:30:07 | INFO | train_inner | epoch 013:     60 / 94 loss=8.04, nll_loss=5.127, ppl=34.95, wps=934.5, ups=0.4, wpb=2318.5, bsz=128, num_updates=1188, lr=1.4256e-05, gnorm=2.2, train_wall=5, gb_free=13.9, wall=3695
2024-08-17 18:30:12 | INFO | train_inner | epoch 013:     62 / 94 loss=8.072, nll_loss=5.167, ppl=35.92, wps=1298.5, ups=0.42, wpb=3125.5, bsz=144, num_updates=1190, lr=1.428e-05, gnorm=1.996, train_wall=5, gb_free=15, wall=3700
2024-08-17 18:30:16 | INFO | train_inner | epoch 013:     64 / 94 loss=8.126, nll_loss=5.237, ppl=37.72, wps=1366.1, ups=0.47, wpb=2902, bsz=100, num_updates=1192, lr=1.4304e-05, gnorm=2.144, train_wall=4, gb_free=11.6, wall=3704
2024-08-17 18:30:25 | INFO | train_inner | epoch 013:     66 / 94 loss=8.031, nll_loss=5.128, ppl=34.96, wps=577.4, ups=0.21, wpb=2696, bsz=148, num_updates=1194, lr=1.4328e-05, gnorm=2.228, train_wall=9, gb_free=14.4, wall=3714
2024-08-17 18:30:30 | INFO | train_inner | epoch 013:     68 / 94 loss=8.157, nll_loss=5.275, ppl=38.71, wps=1530.2, ups=0.4, wpb=3795, bsz=136, num_updates=1196, lr=1.4352e-05, gnorm=1.83, train_wall=5, gb_free=8.5, wall=3719
2024-08-17 18:30:35 | INFO | train_inner | epoch 013:     70 / 94 loss=8.07, nll_loss=5.159, ppl=35.72, wps=843.4, ups=0.4, wpb=2110.5, bsz=68, num_updates=1198, lr=1.4376e-05, gnorm=2.531, train_wall=5, gb_free=13.8, wall=3724
2024-08-17 18:30:40 | INFO | train_inner | epoch 013:     72 / 94 loss=7.992, nll_loss=5.06, ppl=33.37, wps=1584.7, ups=0.42, wpb=3811, bsz=184, num_updates=1200, lr=1.44e-05, gnorm=1.722, train_wall=5, gb_free=15.6, wall=3728
2024-08-17 18:30:45 | INFO | train_inner | epoch 013:     74 / 94 loss=8.099, nll_loss=5.2, ppl=36.76, wps=843.9, ups=0.43, wpb=1969.5, bsz=92, num_updates=1202, lr=1.4424e-05, gnorm=2.345, train_wall=5, gb_free=14.3, wall=3733
2024-08-17 18:30:49 | INFO | train_inner | epoch 013:     76 / 94 loss=8.104, nll_loss=5.198, ppl=36.72, wps=1465, ups=0.53, wpb=2779, bsz=76, num_updates=1204, lr=1.4448e-05, gnorm=2.108, train_wall=4, gb_free=14.9, wall=3737
2024-08-17 18:30:53 | INFO | train_inner | epoch 013:     78 / 94 loss=8.205, nll_loss=5.341, ppl=40.52, wps=1394.3, ups=0.43, wpb=3213, bsz=140, num_updates=1206, lr=1.4472e-05, gnorm=1.965, train_wall=5, gb_free=11.9, wall=3742
2024-08-17 18:30:58 | INFO | train_inner | epoch 013:     80 / 94 loss=8.184, nll_loss=5.306, ppl=39.56, wps=1219, ups=0.39, wpb=3091, bsz=128, num_updates=1208, lr=1.4496e-05, gnorm=2.088, train_wall=5, gb_free=12.5, wall=3747
2024-08-17 18:31:03 | INFO | train_inner | epoch 013:     82 / 94 loss=7.98, nll_loss=5.061, ppl=33.37, wps=1038.2, ups=0.45, wpb=2291.5, bsz=116, num_updates=1210, lr=1.452e-05, gnorm=2.38, train_wall=4, gb_free=12.3, wall=3751
2024-08-17 18:31:08 | INFO | train_inner | epoch 013:     84 / 94 loss=8.036, nll_loss=5.128, ppl=34.98, wps=1335.8, ups=0.41, wpb=3288, bsz=188, num_updates=1212, lr=1.4544e-05, gnorm=1.775, train_wall=5, gb_free=15.4, wall=3756
2024-08-17 18:31:12 | INFO | train_inner | epoch 013:     86 / 94 loss=8.141, nll_loss=5.259, ppl=38.3, wps=1119.2, ups=0.41, wpb=2697.5, bsz=112, num_updates=1214, lr=1.4568e-05, gnorm=2.306, train_wall=5, gb_free=13.5, wall=3761
2024-08-17 18:31:17 | INFO | train_inner | epoch 013:     88 / 94 loss=8.169, nll_loss=5.289, ppl=39.09, wps=1743.8, ups=0.41, wpb=4295.5, bsz=212, num_updates=1216, lr=1.4592e-05, gnorm=1.832, train_wall=5, gb_free=10.6, wall=3766
2024-08-17 18:31:23 | INFO | train_inner | epoch 013:     90 / 94 loss=8.133, nll_loss=5.243, ppl=37.87, wps=1077.4, ups=0.37, wpb=2927, bsz=120, num_updates=1218, lr=1.4616e-05, gnorm=1.975, train_wall=5, gb_free=13.9, wall=3771
2024-08-17 18:31:27 | INFO | train_inner | epoch 013:     92 / 94 loss=8.063, nll_loss=5.159, ppl=35.72, wps=793.6, ups=0.45, wpb=1752, bsz=64, num_updates=1220, lr=1.464e-05, gnorm=2.554, train_wall=4, gb_free=11.6, wall=3776
2024-08-17 18:31:31 | INFO | train_inner | epoch 013:     94 / 94 loss=8.097, nll_loss=5.202, ppl=36.81, wps=1403.8, ups=0.52, wpb=2677, bsz=120, num_updates=1222, lr=1.4664e-05, gnorm=2.432, train_wall=4, gb_free=17, wall=3779
2024-08-17 18:31:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22147.6484375Mb; avail=232918.48828125Mb
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000543
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22147.6484375Mb; avail=232918.48828125Mb
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005323
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22147.6484375Mb; avail=232918.48828125Mb
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004778
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.010979
2024-08-17 18:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22147.6484375Mb; avail=232918.48828125Mb
2024-08-17 18:31:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.005 | nll_loss 4.93 | ppl 30.49 | wps 2427.2 | wpb 944.1 | bsz 40.1 | num_updates 1222 | best_loss 8.005
2024-08-17 18:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1222 updates
2024-08-17 18:31:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:32:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:32:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 13 @ 1222 updates, score 8.005) (writing took 62.45494402386248 seconds)
2024-08-17 18:32:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-08-17 18:32:47 | INFO | train | epoch 013 | loss 8.104 | nll_loss 5.206 | ppl 36.92 | wps 904.4 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 1222 | lr 1.4664e-05 | gnorm 2.184 | train_wall 218 | gb_free 17 | wall 3855
2024-08-17 18:32:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:32:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:32:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:32:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000789
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20760.51953125Mb; avail=234305.6171875Mb
2024-08-17 18:32:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000214
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001619
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20760.51953125Mb; avail=234305.6171875Mb
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000047
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20760.51953125Mb; avail=234305.6171875Mb
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000546
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002524
2024-08-17 18:32:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20760.51953125Mb; avail=234305.6171875Mb
2024-08-17 18:32:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:32:47 | INFO | fairseq.trainer | begin training epoch 14
2024-08-17 18:32:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:32:51 | INFO | train_inner | epoch 014:      2 / 94 loss=8.08, nll_loss=5.176, ppl=36.14, wps=81.9, ups=0.02, wpb=3285, bsz=116, num_updates=1224, lr=1.4688e-05, gnorm=2.082, train_wall=4, gb_free=12, wall=3860
2024-08-17 18:32:56 | INFO | train_inner | epoch 014:      4 / 94 loss=7.899, nll_loss=4.932, ppl=30.53, wps=920.5, ups=0.44, wpb=2104, bsz=84, num_updates=1226, lr=1.4712e-05, gnorm=2.169, train_wall=5, gb_free=15.7, wall=3864
2024-08-17 18:33:01 | INFO | train_inner | epoch 014:      6 / 94 loss=8.054, nll_loss=5.134, ppl=35.12, wps=1478, ups=0.37, wpb=4027, bsz=180, num_updates=1228, lr=1.4736e-05, gnorm=1.658, train_wall=5, gb_free=12.3, wall=3870
2024-08-17 18:33:06 | INFO | train_inner | epoch 014:      8 / 94 loss=8.068, nll_loss=5.154, ppl=35.61, wps=1006.2, ups=0.39, wpb=2602.5, bsz=120, num_updates=1230, lr=1.476e-05, gnorm=2.215, train_wall=5, gb_free=9.9, wall=3875
2024-08-17 18:33:11 | INFO | train_inner | epoch 014:     10 / 94 loss=8.017, nll_loss=5.091, ppl=34.09, wps=1186.4, ups=0.46, wpb=2554, bsz=108, num_updates=1232, lr=1.4784e-05, gnorm=2.22, train_wall=4, gb_free=18.3, wall=3879
2024-08-17 18:33:15 | INFO | train_inner | epoch 014:     12 / 94 loss=8.122, nll_loss=5.218, ppl=37.22, wps=1934.1, ups=0.45, wpb=4302, bsz=144, num_updates=1234, lr=1.4808e-05, gnorm=1.76, train_wall=4, gb_free=13.4, wall=3883
2024-08-17 18:33:20 | INFO | train_inner | epoch 014:     14 / 94 loss=8.009, nll_loss=5.091, ppl=34.09, wps=740.4, ups=0.46, wpb=1610, bsz=64, num_updates=1236, lr=1.4832e-05, gnorm=2.752, train_wall=4, gb_free=16.5, wall=3888
2024-08-17 18:33:24 | INFO | train_inner | epoch 014:     16 / 94 loss=8.046, nll_loss=5.128, ppl=34.97, wps=1280.2, ups=0.46, wpb=2789, bsz=100, num_updates=1238, lr=1.4856e-05, gnorm=2.109, train_wall=4, gb_free=15.1, wall=3892
2024-08-17 18:33:29 | INFO | train_inner | epoch 014:     18 / 94 loss=8.024, nll_loss=5.106, ppl=34.45, wps=1082.5, ups=0.41, wpb=2618.5, bsz=128, num_updates=1240, lr=1.488e-05, gnorm=2.213, train_wall=5, gb_free=15.9, wall=3897
2024-08-17 18:33:33 | INFO | train_inner | epoch 014:     20 / 94 loss=8.069, nll_loss=5.16, ppl=35.75, wps=1247.3, ups=0.45, wpb=2783, bsz=100, num_updates=1242, lr=1.4904e-05, gnorm=2.147, train_wall=4, gb_free=14, wall=3901
2024-08-17 18:33:38 | INFO | train_inner | epoch 014:     22 / 94 loss=8.143, nll_loss=5.249, ppl=38.02, wps=1374.3, ups=0.45, wpb=3067.5, bsz=120, num_updates=1244, lr=1.4928e-05, gnorm=2.045, train_wall=4, gb_free=14.9, wall=3906
2024-08-17 18:33:43 | INFO | train_inner | epoch 014:     24 / 94 loss=8.001, nll_loss=5.08, ppl=33.81, wps=1218.7, ups=0.37, wpb=3292.5, bsz=204, num_updates=1246, lr=1.4952e-05, gnorm=1.884, train_wall=5, gb_free=11.1, wall=3911
2024-08-17 18:33:48 | INFO | train_inner | epoch 014:     26 / 94 loss=8.154, nll_loss=5.263, ppl=38.4, wps=1522.6, ups=0.43, wpb=3552.5, bsz=136, num_updates=1248, lr=1.4976e-05, gnorm=1.939, train_wall=5, gb_free=13.4, wall=3916
2024-08-17 18:33:52 | INFO | train_inner | epoch 014:     28 / 94 loss=8.135, nll_loss=5.259, ppl=38.28, wps=1510.1, ups=0.52, wpb=2893.5, bsz=120, num_updates=1250, lr=1.5e-05, gnorm=2.315, train_wall=4, gb_free=24.4, wall=3920
2024-08-17 18:33:56 | INFO | train_inner | epoch 014:     30 / 94 loss=8.095, nll_loss=5.198, ppl=36.7, wps=1638, ups=0.41, wpb=4007, bsz=160, num_updates=1252, lr=1.5024e-05, gnorm=1.912, train_wall=5, gb_free=12.5, wall=3925
2024-08-17 18:34:07 | INFO | train_inner | epoch 014:     32 / 94 loss=8.005, nll_loss=5.089, ppl=34.03, wps=733.2, ups=0.19, wpb=3824, bsz=224, num_updates=1254, lr=1.5048e-05, gnorm=1.731, train_wall=10, gb_free=13.4, wall=3935
2024-08-17 18:34:11 | INFO | train_inner | epoch 014:     34 / 94 loss=8.055, nll_loss=5.146, ppl=35.4, wps=1572, ups=0.43, wpb=3633.5, bsz=188, num_updates=1256, lr=1.5072e-05, gnorm=1.861, train_wall=5, gb_free=11.9, wall=3940
2024-08-17 18:34:17 | INFO | train_inner | epoch 014:     36 / 94 loss=8.047, nll_loss=5.134, ppl=35.11, wps=1520, ups=0.4, wpb=3807, bsz=172, num_updates=1258, lr=1.5096e-05, gnorm=1.917, train_wall=5, gb_free=15.6, wall=3945
2024-08-17 18:34:20 | INFO | train_inner | epoch 014:     38 / 94 loss=7.85, nll_loss=4.878, ppl=29.41, wps=751.9, ups=0.51, wpb=1482.5, bsz=52, num_updates=1260, lr=1.512e-05, gnorm=2.791, train_wall=4, gb_free=15.5, wall=3949
2024-08-17 18:34:25 | INFO | train_inner | epoch 014:     40 / 94 loss=7.946, nll_loss=5.004, ppl=32.1, wps=1187.9, ups=0.44, wpb=2711, bsz=104, num_updates=1262, lr=1.5144e-05, gnorm=2.079, train_wall=5, gb_free=12.1, wall=3953
2024-08-17 18:34:30 | INFO | train_inner | epoch 014:     42 / 94 loss=8.072, nll_loss=5.16, ppl=35.75, wps=943.5, ups=0.41, wpb=2286.5, bsz=92, num_updates=1264, lr=1.5168e-05, gnorm=2.433, train_wall=5, gb_free=10.4, wall=3958
2024-08-17 18:34:35 | INFO | train_inner | epoch 014:     44 / 94 loss=7.982, nll_loss=5.054, ppl=33.22, wps=678.3, ups=0.43, wpb=1578, bsz=60, num_updates=1266, lr=1.5192e-05, gnorm=2.715, train_wall=5, gb_free=14.7, wall=3963
2024-08-17 18:34:39 | INFO | train_inner | epoch 014:     46 / 94 loss=8.063, nll_loss=5.149, ppl=35.48, wps=1154.2, ups=0.47, wpb=2444, bsz=84, num_updates=1268, lr=1.5216e-05, gnorm=2.228, train_wall=4, gb_free=16, wall=3967
2024-08-17 18:34:43 | INFO | train_inner | epoch 014:     48 / 94 loss=7.953, nll_loss=5.01, ppl=32.22, wps=1026.5, ups=0.51, wpb=2024, bsz=63.5, num_updates=1270, lr=1.524e-05, gnorm=2.474, train_wall=4, gb_free=14.9, wall=3971
2024-08-17 18:34:48 | INFO | train_inner | epoch 014:     50 / 94 loss=8.043, nll_loss=5.127, ppl=34.95, wps=1310.1, ups=0.38, wpb=3477, bsz=160, num_updates=1272, lr=1.5264e-05, gnorm=1.876, train_wall=5, gb_free=12, wall=3976
2024-08-17 18:34:53 | INFO | train_inner | epoch 014:     52 / 94 loss=8.092, nll_loss=5.184, ppl=36.37, wps=1321.8, ups=0.44, wpb=3037, bsz=108, num_updates=1274, lr=1.5288e-05, gnorm=2.01, train_wall=5, gb_free=10.7, wall=3981
2024-08-17 18:34:58 | INFO | train_inner | epoch 014:     54 / 94 loss=7.982, nll_loss=5.053, ppl=33.21, wps=1064.7, ups=0.4, wpb=2685.5, bsz=112, num_updates=1276, lr=1.5312e-05, gnorm=2.048, train_wall=5, gb_free=10.3, wall=3986
2024-08-17 18:35:02 | INFO | train_inner | epoch 014:     56 / 94 loss=7.969, nll_loss=5.049, ppl=33.11, wps=896.7, ups=0.52, wpb=1731.5, bsz=88, num_updates=1278, lr=1.5336e-05, gnorm=2.607, train_wall=4, gb_free=14.9, wall=3990
2024-08-17 18:35:06 | INFO | train_inner | epoch 014:     58 / 94 loss=8.053, nll_loss=5.137, ppl=35.18, wps=992.8, ups=0.45, wpb=2197.5, bsz=68, num_updates=1280, lr=1.536e-05, gnorm=2.295, train_wall=4, gb_free=10.8, wall=3994
2024-08-17 18:35:10 | INFO | train_inner | epoch 014:     60 / 94 loss=8.131, nll_loss=5.238, ppl=37.74, wps=1117.4, ups=0.47, wpb=2352.5, bsz=64, num_updates=1282, lr=1.5384e-05, gnorm=2.328, train_wall=4, gb_free=14.1, wall=3998
2024-08-17 18:35:15 | INFO | train_inner | epoch 014:     62 / 94 loss=8.009, nll_loss=5.087, ppl=33.99, wps=1270.5, ups=0.43, wpb=2981, bsz=116, num_updates=1284, lr=1.5408e-05, gnorm=1.923, train_wall=5, gb_free=11.3, wall=4003
2024-08-17 18:35:20 | INFO | train_inner | epoch 014:     64 / 94 loss=7.989, nll_loss=5.07, ppl=33.59, wps=1166.6, ups=0.4, wpb=2951.5, bsz=136, num_updates=1286, lr=1.5432e-05, gnorm=2.099, train_wall=5, gb_free=10.6, wall=4008
2024-08-17 18:35:23 | INFO | train_inner | epoch 014:     66 / 94 loss=8.201, nll_loss=5.33, ppl=40.22, wps=1587.6, ups=0.58, wpb=2741, bsz=76, num_updates=1288, lr=1.5456e-05, gnorm=2.288, train_wall=3, gb_free=13.9, wall=4012
2024-08-17 18:35:28 | INFO | train_inner | epoch 014:     68 / 94 loss=8.129, nll_loss=5.247, ppl=37.97, wps=1550.8, ups=0.47, wpb=3294.5, bsz=115, num_updates=1290, lr=1.548e-05, gnorm=2.147, train_wall=4, gb_free=15.1, wall=4016
2024-08-17 18:35:32 | INFO | train_inner | epoch 014:     70 / 94 loss=8.082, nll_loss=5.173, ppl=36.08, wps=1189.6, ups=0.45, wpb=2625.5, bsz=96, num_updates=1292, lr=1.5504e-05, gnorm=2.558, train_wall=4, gb_free=14.7, wall=4020
2024-08-17 18:35:36 | INFO | train_inner | epoch 014:     72 / 94 loss=8.145, nll_loss=5.245, ppl=37.92, wps=1468.1, ups=0.52, wpb=2796.5, bsz=104, num_updates=1294, lr=1.5528e-05, gnorm=2.115, train_wall=4, gb_free=15.4, wall=4024
2024-08-17 18:35:40 | INFO | train_inner | epoch 014:     74 / 94 loss=7.955, nll_loss=5.011, ppl=32.25, wps=1254.4, ups=0.47, wpb=2691, bsz=168, num_updates=1296, lr=1.5552e-05, gnorm=2.224, train_wall=4, gb_free=17.1, wall=4028
2024-08-17 18:35:45 | INFO | train_inner | epoch 014:     76 / 94 loss=7.991, nll_loss=5.045, ppl=33.02, wps=1218.6, ups=0.46, wpb=2676, bsz=100, num_updates=1298, lr=1.5576e-05, gnorm=2.249, train_wall=4, gb_free=15.6, wall=4033
2024-08-17 18:35:50 | INFO | train_inner | epoch 014:     78 / 94 loss=8.146, nll_loss=5.266, ppl=38.47, wps=1564.4, ups=0.37, wpb=4194, bsz=208, num_updates=1300, lr=1.56e-05, gnorm=1.849, train_wall=5, gb_free=9.3, wall=4038
2024-08-17 18:35:54 | INFO | train_inner | epoch 014:     80 / 94 loss=8.194, nll_loss=5.326, ppl=40.13, wps=1314.1, ups=0.47, wpb=2825.5, bsz=104, num_updates=1302, lr=1.5624e-05, gnorm=2.152, train_wall=4, gb_free=12.4, wall=4042
2024-08-17 18:35:59 | INFO | train_inner | epoch 014:     82 / 94 loss=8.162, nll_loss=5.292, ppl=39.19, wps=1325.5, ups=0.42, wpb=3187.5, bsz=108, num_updates=1304, lr=1.5648e-05, gnorm=2.163, train_wall=5, gb_free=12.7, wall=4047
2024-08-17 18:36:04 | INFO | train_inner | epoch 014:     84 / 94 loss=8.062, nll_loss=5.164, ppl=35.86, wps=1320.4, ups=0.43, wpb=3056.5, bsz=152, num_updates=1306, lr=1.5672e-05, gnorm=2.267, train_wall=5, gb_free=13.4, wall=4052
2024-08-17 18:36:08 | INFO | train_inner | epoch 014:     86 / 94 loss=8.074, nll_loss=5.169, ppl=35.98, wps=1182, ups=0.44, wpb=2675.5, bsz=116, num_updates=1308, lr=1.5696e-05, gnorm=2.307, train_wall=5, gb_free=14.9, wall=4056
2024-08-17 18:36:13 | INFO | train_inner | epoch 014:     88 / 94 loss=8.004, nll_loss=5.083, ppl=33.9, wps=1523.2, ups=0.4, wpb=3799.5, bsz=200, num_updates=1310, lr=1.572e-05, gnorm=1.71, train_wall=5, gb_free=15.8, wall=4061
2024-08-17 18:36:18 | INFO | train_inner | epoch 014:     90 / 94 loss=7.966, nll_loss=5.016, ppl=32.35, wps=902.5, ups=0.45, wpb=2017, bsz=76, num_updates=1312, lr=1.5744e-05, gnorm=2.453, train_wall=4, gb_free=14.6, wall=4066
2024-08-17 18:36:22 | INFO | train_inner | epoch 014:     92 / 94 loss=7.966, nll_loss=5.024, ppl=32.53, wps=829.7, ups=0.46, wpb=1813, bsz=92, num_updates=1314, lr=1.5768e-05, gnorm=2.542, train_wall=4, gb_free=14.2, wall=4070
2024-08-17 18:36:26 | INFO | train_inner | epoch 014:     94 / 94 loss=7.953, nll_loss=5.013, ppl=32.29, wps=1122.1, ups=0.47, wpb=2404, bsz=120, num_updates=1316, lr=1.5792e-05, gnorm=2.327, train_wall=4, gb_free=17.6, wall=4075
2024-08-17 18:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18388.234375Mb; avail=236677.82421875Mb
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000546
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18388.234375Mb; avail=236677.82421875Mb
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005388
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18388.234375Mb; avail=236677.82421875Mb
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004771
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011042
2024-08-17 18:36:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18388.234375Mb; avail=236677.82421875Mb
2024-08-17 18:36:40 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.997 | nll_loss 4.902 | ppl 29.9 | wps 2427.1 | wpb 944.1 | bsz 40.1 | num_updates 1316 | best_loss 7.997
2024-08-17 18:36:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1316 updates
2024-08-17 18:36:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:37:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:37:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 14 @ 1316 updates, score 7.997) (writing took 63.90156685188413 seconds)
2024-08-17 18:37:44 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-08-17 18:37:44 | INFO | train | epoch 014 | loss 8.056 | nll_loss 5.144 | ppl 35.36 | wps 899.8 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 1316 | lr 1.5792e-05 | gnorm 2.174 | train_wall 219 | gb_free 17.6 | wall 4152
2024-08-17 18:37:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:37:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:37:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:37:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000811
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25876.0859375Mb; avail=229190.05078125Mb
2024-08-17 18:37:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000183
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001481
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25876.796875Mb; avail=229189.33984375Mb
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25876.796875Mb; avail=229189.33984375Mb
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000554
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002389
2024-08-17 18:37:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25876.796875Mb; avail=229189.33984375Mb
2024-08-17 18:37:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:37:44 | INFO | fairseq.trainer | begin training epoch 15
2024-08-17 18:37:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:37:49 | INFO | train_inner | epoch 015:      2 / 94 loss=8.108, nll_loss=5.21, ppl=37.02, wps=109.1, ups=0.02, wpb=4511, bsz=188, num_updates=1318, lr=1.5816e-05, gnorm=1.95, train_wall=5, gb_free=12.7, wall=4157
2024-08-17 18:37:54 | INFO | train_inner | epoch 015:      4 / 94 loss=8.005, nll_loss=5.094, ppl=34.15, wps=1311.6, ups=0.38, wpb=3470, bsz=188, num_updates=1320, lr=1.584e-05, gnorm=1.881, train_wall=5, gb_free=12.1, wall=4163
2024-08-17 18:37:58 | INFO | train_inner | epoch 015:      6 / 94 loss=7.816, nll_loss=4.836, ppl=28.55, wps=618.4, ups=0.54, wpb=1152.5, bsz=32, num_updates=1322, lr=1.5864e-05, gnorm=3.232, train_wall=4, gb_free=15.6, wall=4166
2024-08-17 18:38:03 | INFO | train_inner | epoch 015:      8 / 94 loss=8.035, nll_loss=5.123, ppl=34.84, wps=1569.4, ups=0.37, wpb=4273.5, bsz=212, num_updates=1324, lr=1.5888e-05, gnorm=1.689, train_wall=5, gb_free=10.6, wall=4172
2024-08-17 18:38:08 | INFO | train_inner | epoch 015:     10 / 94 loss=7.959, nll_loss=5.026, ppl=32.57, wps=1320.6, ups=0.44, wpb=3001.5, bsz=120, num_updates=1326, lr=1.5912e-05, gnorm=1.975, train_wall=5, gb_free=14.3, wall=4176
2024-08-17 18:38:12 | INFO | train_inner | epoch 015:     12 / 94 loss=8.077, nll_loss=5.169, ppl=35.97, wps=1466.5, ups=0.52, wpb=2841.5, bsz=88, num_updates=1328, lr=1.5936e-05, gnorm=2.05, train_wall=4, gb_free=13.6, wall=4180
2024-08-17 18:38:16 | INFO | train_inner | epoch 015:     14 / 94 loss=7.977, nll_loss=5.034, ppl=32.75, wps=1033.3, ups=0.45, wpb=2285, bsz=80, num_updates=1330, lr=1.596e-05, gnorm=2.269, train_wall=4, gb_free=14.6, wall=4185
2024-08-17 18:38:21 | INFO | train_inner | epoch 015:     16 / 94 loss=7.939, nll_loss=5.002, ppl=32.05, wps=804.2, ups=0.39, wpb=2053.5, bsz=96, num_updates=1332, lr=1.5984e-05, gnorm=2.324, train_wall=5, gb_free=13.8, wall=4190
2024-08-17 18:38:26 | INFO | train_inner | epoch 015:     18 / 94 loss=7.999, nll_loss=5.068, ppl=33.56, wps=1171.5, ups=0.45, wpb=2625, bsz=108, num_updates=1334, lr=1.6008e-05, gnorm=2.195, train_wall=4, gb_free=14.9, wall=4194
2024-08-17 18:38:31 | INFO | train_inner | epoch 015:     20 / 94 loss=8.031, nll_loss=5.129, ppl=34.98, wps=1515.3, ups=0.41, wpb=3678, bsz=192, num_updates=1336, lr=1.6032e-05, gnorm=1.918, train_wall=5, gb_free=14.9, wall=4199
2024-08-17 18:38:35 | INFO | train_inner | epoch 015:     22 / 94 loss=8.033, nll_loss=5.115, ppl=34.65, wps=1419.8, ups=0.47, wpb=3023, bsz=88, num_updates=1338, lr=1.6056e-05, gnorm=2.154, train_wall=4, gb_free=12.6, wall=4203
2024-08-17 18:38:39 | INFO | train_inner | epoch 015:     24 / 94 loss=7.887, nll_loss=4.929, ppl=30.47, wps=749.9, ups=0.5, wpb=1501.5, bsz=56, num_updates=1340, lr=1.608e-05, gnorm=2.668, train_wall=4, gb_free=15, wall=4207
2024-08-17 18:38:44 | INFO | train_inner | epoch 015:     26 / 94 loss=8.034, nll_loss=5.113, ppl=34.61, wps=1361.8, ups=0.42, wpb=3257, bsz=172, num_updates=1342, lr=1.6104e-05, gnorm=1.969, train_wall=5, gb_free=14.8, wall=4212
2024-08-17 18:38:48 | INFO | train_inner | epoch 015:     28 / 94 loss=7.953, nll_loss=4.991, ppl=31.8, wps=1568.4, ups=0.47, wpb=3341, bsz=116, num_updates=1344, lr=1.6128e-05, gnorm=1.922, train_wall=4, gb_free=11.4, wall=4216
2024-08-17 18:38:52 | INFO | train_inner | epoch 015:     30 / 94 loss=7.998, nll_loss=5.061, ppl=33.38, wps=966.5, ups=0.45, wpb=2141, bsz=60, num_updates=1346, lr=1.6152e-05, gnorm=2.403, train_wall=4, gb_free=16.6, wall=4221
2024-08-17 18:38:57 | INFO | train_inner | epoch 015:     32 / 94 loss=8.027, nll_loss=5.108, ppl=34.49, wps=1036, ups=0.4, wpb=2594, bsz=108, num_updates=1348, lr=1.6176e-05, gnorm=2.216, train_wall=5, gb_free=10.4, wall=4226
2024-08-17 18:39:02 | INFO | train_inner | epoch 015:     34 / 94 loss=7.884, nll_loss=4.931, ppl=30.5, wps=1204, ups=0.4, wpb=2988.5, bsz=148, num_updates=1350, lr=1.62e-05, gnorm=2.11, train_wall=5, gb_free=14.6, wall=4231
2024-08-17 18:39:07 | INFO | train_inner | epoch 015:     36 / 94 loss=8.015, nll_loss=5.107, ppl=34.45, wps=1298.6, ups=0.46, wpb=2794, bsz=124, num_updates=1352, lr=1.6224e-05, gnorm=2.1, train_wall=4, gb_free=16.7, wall=4235
2024-08-17 18:39:11 | INFO | train_inner | epoch 015:     38 / 94 loss=7.983, nll_loss=5.053, ppl=33.19, wps=1266.4, ups=0.43, wpb=2917.5, bsz=112, num_updates=1354, lr=1.6248e-05, gnorm=2.07, train_wall=5, gb_free=12.4, wall=4240
2024-08-17 18:39:16 | INFO | train_inner | epoch 015:     40 / 94 loss=8.118, nll_loss=5.229, ppl=37.51, wps=1824.8, ups=0.41, wpb=4422, bsz=188, num_updates=1356, lr=1.6272e-05, gnorm=1.711, train_wall=5, gb_free=15.1, wall=4244
2024-08-17 18:39:21 | INFO | train_inner | epoch 015:     42 / 94 loss=7.912, nll_loss=4.963, ppl=31.18, wps=768.2, ups=0.45, wpb=1694, bsz=64, num_updates=1358, lr=1.6296e-05, gnorm=2.686, train_wall=4, gb_free=16.9, wall=4249
2024-08-17 18:39:25 | INFO | train_inner | epoch 015:     44 / 94 loss=8.054, nll_loss=5.139, ppl=35.23, wps=832.3, ups=0.41, wpb=2007, bsz=72, num_updates=1360, lr=1.632e-05, gnorm=2.385, train_wall=5, gb_free=14, wall=4254
2024-08-17 18:39:30 | INFO | train_inner | epoch 015:     46 / 94 loss=8.009, nll_loss=5.073, ppl=33.67, wps=969.3, ups=0.43, wpb=2238, bsz=104, num_updates=1362, lr=1.6344e-05, gnorm=2.252, train_wall=5, gb_free=14.9, wall=4258
2024-08-17 18:39:35 | INFO | train_inner | epoch 015:     48 / 94 loss=7.949, nll_loss=5.003, ppl=32.06, wps=1096.8, ups=0.39, wpb=2793.5, bsz=136, num_updates=1364, lr=1.6368e-05, gnorm=2.15, train_wall=5, gb_free=13, wall=4263
2024-08-17 18:39:39 | INFO | train_inner | epoch 015:     50 / 94 loss=8.057, nll_loss=5.141, ppl=35.28, wps=1525.1, ups=0.55, wpb=2783.5, bsz=92, num_updates=1366, lr=1.6392e-05, gnorm=2.15, train_wall=4, gb_free=11.1, wall=4267
2024-08-17 18:39:43 | INFO | train_inner | epoch 015:     52 / 94 loss=7.935, nll_loss=4.997, ppl=31.93, wps=1266.9, ups=0.43, wpb=2959.5, bsz=160, num_updates=1368, lr=1.6416e-05, gnorm=1.954, train_wall=5, gb_free=14.3, wall=4272
2024-08-17 18:39:48 | INFO | train_inner | epoch 015:     54 / 94 loss=8.013, nll_loss=5.086, ppl=33.97, wps=1524.2, ups=0.49, wpb=3126, bsz=100, num_updates=1370, lr=1.644e-05, gnorm=2.072, train_wall=4, gb_free=12, wall=4276
2024-08-17 18:39:52 | INFO | train_inner | epoch 015:     56 / 94 loss=7.979, nll_loss=5.058, ppl=33.3, wps=870.2, ups=0.45, wpb=1934, bsz=80, num_updates=1372, lr=1.6464e-05, gnorm=2.608, train_wall=4, gb_free=14.7, wall=4280
2024-08-17 18:39:56 | INFO | train_inner | epoch 015:     58 / 94 loss=7.946, nll_loss=5.025, ppl=32.57, wps=1528.3, ups=0.49, wpb=3089, bsz=112, num_updates=1374, lr=1.6488e-05, gnorm=2.036, train_wall=4, gb_free=16.4, wall=4284
2024-08-17 18:40:01 | INFO | train_inner | epoch 015:     60 / 94 loss=7.966, nll_loss=5.05, ppl=33.12, wps=1265.1, ups=0.37, wpb=3392, bsz=184, num_updates=1376, lr=1.6512e-05, gnorm=2.141, train_wall=5, gb_free=8.4, wall=4290
2024-08-17 18:40:05 | INFO | train_inner | epoch 015:     62 / 94 loss=7.846, nll_loss=4.867, ppl=29.18, wps=1156.4, ups=0.53, wpb=2193, bsz=68, num_updates=1378, lr=1.6536e-05, gnorm=2.263, train_wall=4, gb_free=14.1, wall=4293
2024-08-17 18:40:10 | INFO | train_inner | epoch 015:     64 / 94 loss=7.966, nll_loss=5.023, ppl=32.52, wps=1356.9, ups=0.39, wpb=3451, bsz=176, num_updates=1380, lr=1.656e-05, gnorm=1.862, train_wall=5, gb_free=14.2, wall=4299
2024-08-17 18:40:15 | INFO | train_inner | epoch 015:     66 / 94 loss=7.932, nll_loss=4.982, ppl=31.6, wps=1057.3, ups=0.43, wpb=2462, bsz=123, num_updates=1382, lr=1.6584e-05, gnorm=2.394, train_wall=5, gb_free=14.4, wall=4303
2024-08-17 18:40:19 | INFO | train_inner | epoch 015:     68 / 94 loss=7.964, nll_loss=5.015, ppl=32.34, wps=1297.9, ups=0.44, wpb=2939, bsz=140, num_updates=1384, lr=1.6608e-05, gnorm=2.041, train_wall=5, gb_free=10.4, wall=4308
2024-08-17 18:40:24 | INFO | train_inner | epoch 015:     70 / 94 loss=8.124, nll_loss=5.218, ppl=37.21, wps=1467.9, ups=0.43, wpb=3381, bsz=136, num_updates=1386, lr=1.6632e-05, gnorm=2.134, train_wall=5, gb_free=16.7, wall=4312
2024-08-17 18:40:28 | INFO | train_inner | epoch 015:     72 / 94 loss=8.119, nll_loss=5.214, ppl=37.11, wps=1274.3, ups=0.48, wpb=2656.5, bsz=68, num_updates=1388, lr=1.6656e-05, gnorm=2.331, train_wall=4, gb_free=15.9, wall=4317
2024-08-17 18:40:33 | INFO | train_inner | epoch 015:     74 / 94 loss=8.02, nll_loss=5.115, ppl=34.65, wps=1403.6, ups=0.39, wpb=3553.5, bsz=160, num_updates=1390, lr=1.668e-05, gnorm=1.881, train_wall=5, gb_free=14.2, wall=4322
2024-08-17 18:40:39 | INFO | train_inner | epoch 015:     76 / 94 loss=8.006, nll_loss=5.103, ppl=34.38, wps=1199.2, ups=0.38, wpb=3149.5, bsz=132, num_updates=1392, lr=1.6704e-05, gnorm=1.907, train_wall=5, gb_free=14.6, wall=4327
2024-08-17 18:40:43 | INFO | train_inner | epoch 015:     78 / 94 loss=8.097, nll_loss=5.214, ppl=37.12, wps=1390.4, ups=0.41, wpb=3350.5, bsz=136, num_updates=1394, lr=1.6728e-05, gnorm=1.958, train_wall=5, gb_free=10, wall=4332
2024-08-17 18:40:48 | INFO | train_inner | epoch 015:     80 / 94 loss=8.031, nll_loss=5.121, ppl=34.81, wps=1336.2, ups=0.47, wpb=2820, bsz=136, num_updates=1396, lr=1.6752e-05, gnorm=2.083, train_wall=4, gb_free=14.2, wall=4336
2024-08-17 18:40:53 | INFO | train_inner | epoch 015:     82 / 94 loss=7.988, nll_loss=5.052, ppl=33.17, wps=1056.1, ups=0.39, wpb=2710.5, bsz=128, num_updates=1398, lr=1.6776e-05, gnorm=2.11, train_wall=5, gb_free=14, wall=4341
2024-08-17 18:40:58 | INFO | train_inner | epoch 015:     84 / 94 loss=7.977, nll_loss=5.035, ppl=32.8, wps=701.4, ups=0.4, wpb=1733, bsz=68, num_updates=1400, lr=1.68e-05, gnorm=2.554, train_wall=5, gb_free=16.1, wall=4346
2024-08-17 18:41:02 | INFO | train_inner | epoch 015:     86 / 94 loss=8.078, nll_loss=5.159, ppl=35.73, wps=1687.5, ups=0.42, wpb=3989, bsz=132, num_updates=1402, lr=1.6824e-05, gnorm=1.833, train_wall=5, gb_free=10.1, wall=4351
2024-08-17 18:41:07 | INFO | train_inner | epoch 015:     88 / 94 loss=7.983, nll_loss=5.039, ppl=32.88, wps=1301.2, ups=0.42, wpb=3119, bsz=127.5, num_updates=1404, lr=1.6848e-05, gnorm=2.536, train_wall=5, gb_free=13.7, wall=4355
2024-08-17 18:41:12 | INFO | train_inner | epoch 015:     90 / 94 loss=7.946, nll_loss=5.012, ppl=32.26, wps=1129.5, ups=0.44, wpb=2544.5, bsz=120, num_updates=1406, lr=1.6872e-05, gnorm=2.292, train_wall=4, gb_free=13.3, wall=4360
2024-08-17 18:41:16 | INFO | train_inner | epoch 015:     92 / 94 loss=8, nll_loss=5.074, ppl=33.69, wps=1176.6, ups=0.48, wpb=2433, bsz=72, num_updates=1408, lr=1.6896e-05, gnorm=2.119, train_wall=4, gb_free=14.5, wall=4364
2024-08-17 18:41:19 | INFO | train_inner | epoch 015:     94 / 94 loss=7.854, nll_loss=4.89, ppl=29.66, wps=1380.2, ups=0.65, wpb=2116.5, bsz=108, num_updates=1410, lr=1.692e-05, gnorm=2.363, train_wall=3, gb_free=17.6, wall=4367
2024-08-17 18:41:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18596.09765625Mb; avail=236470.10546875Mb
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18596.09765625Mb; avail=236470.10546875Mb
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005434
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18596.09765625Mb; avail=236470.10546875Mb
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004897
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011226
2024-08-17 18:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18596.09765625Mb; avail=236470.10546875Mb
2024-08-17 18:41:32 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.977 | nll_loss 4.879 | ppl 29.42 | wps 2427.4 | wpb 944.1 | bsz 40.1 | num_updates 1410 | best_loss 7.977
2024-08-17 18:41:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1410 updates
2024-08-17 18:41:32 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:42:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:42:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 15 @ 1410 updates, score 7.977) (writing took 65.51095785014331 seconds)
2024-08-17 18:42:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-08-17 18:42:38 | INFO | train | epoch 015 | loss 8.003 | nll_loss 5.078 | ppl 33.77 | wps 907 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 1410 | lr 1.692e-05 | gnorm 2.168 | train_wall 215 | gb_free 17.6 | wall 4446
2024-08-17 18:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000822
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24448.9765625Mb; avail=230617.34375Mb
2024-08-17 18:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000220
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001636
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24449.46875Mb; avail=230616.8515625Mb
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000058
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24449.46875Mb; avail=230616.8515625Mb
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000565
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002696
2024-08-17 18:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24449.46875Mb; avail=230616.8515625Mb
2024-08-17 18:42:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:42:38 | INFO | fairseq.trainer | begin training epoch 16
2024-08-17 18:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:42:43 | INFO | train_inner | epoch 016:      2 / 94 loss=7.881, nll_loss=4.911, ppl=30.09, wps=49.8, ups=0.02, wpb=2082.5, bsz=56, num_updates=1412, lr=1.6944e-05, gnorm=2.419, train_wall=4, gb_free=13.2, wall=4451
2024-08-17 18:42:47 | INFO | train_inner | epoch 016:      4 / 94 loss=7.99, nll_loss=5.057, ppl=33.29, wps=1515.7, ups=0.43, wpb=3521.5, bsz=156, num_updates=1414, lr=1.6968e-05, gnorm=2.141, train_wall=5, gb_free=13.9, wall=4455
2024-08-17 18:42:52 | INFO | train_inner | epoch 016:      6 / 94 loss=7.942, nll_loss=4.996, ppl=31.91, wps=992.3, ups=0.42, wpb=2344, bsz=112, num_updates=1416, lr=1.6992e-05, gnorm=2.216, train_wall=5, gb_free=9.7, wall=4460
2024-08-17 18:42:56 | INFO | train_inner | epoch 016:      8 / 94 loss=7.905, nll_loss=4.94, ppl=30.7, wps=1668.9, ups=0.44, wpb=3790.5, bsz=156, num_updates=1418, lr=1.7016e-05, gnorm=1.804, train_wall=5, gb_free=12.2, wall=4465
2024-08-17 18:43:01 | INFO | train_inner | epoch 016:     10 / 94 loss=7.953, nll_loss=5.004, ppl=32.1, wps=985.1, ups=0.41, wpb=2404, bsz=88, num_updates=1420, lr=1.704e-05, gnorm=2.403, train_wall=5, gb_free=13.3, wall=4470
2024-08-17 18:43:06 | INFO | train_inner | epoch 016:     12 / 94 loss=8.034, nll_loss=5.123, ppl=34.84, wps=1752.4, ups=0.39, wpb=4460, bsz=200, num_updates=1422, lr=1.7064e-05, gnorm=1.769, train_wall=5, gb_free=10, wall=4475
2024-08-17 18:43:11 | INFO | train_inner | epoch 016:     14 / 94 loss=7.968, nll_loss=5.03, ppl=32.68, wps=1696.5, ups=0.42, wpb=4058, bsz=144, num_updates=1424, lr=1.7088e-05, gnorm=1.759, train_wall=5, gb_free=12.5, wall=4480
2024-08-17 18:43:16 | INFO | train_inner | epoch 016:     16 / 94 loss=7.853, nll_loss=4.887, ppl=29.6, wps=1017.4, ups=0.39, wpb=2597, bsz=112, num_updates=1426, lr=1.7112e-05, gnorm=2.124, train_wall=5, gb_free=9.2, wall=4485
2024-08-17 18:43:21 | INFO | train_inner | epoch 016:     18 / 94 loss=7.983, nll_loss=5.064, ppl=33.46, wps=1354.2, ups=0.46, wpb=2919.5, bsz=148, num_updates=1428, lr=1.7136e-05, gnorm=2.111, train_wall=4, gb_free=14.6, wall=4489
2024-08-17 18:43:25 | INFO | train_inner | epoch 016:     20 / 94 loss=7.957, nll_loss=5.009, ppl=32.2, wps=1325.3, ups=0.42, wpb=3141, bsz=108, num_updates=1430, lr=1.716e-05, gnorm=2.071, train_wall=5, gb_free=12.3, wall=4494
2024-08-17 18:43:29 | INFO | train_inner | epoch 016:     22 / 94 loss=7.903, nll_loss=4.934, ppl=30.57, wps=1043, ups=0.49, wpb=2122, bsz=64, num_updates=1432, lr=1.7184e-05, gnorm=2.431, train_wall=4, gb_free=11.7, wall=4498
2024-08-17 18:43:34 | INFO | train_inner | epoch 016:     24 / 94 loss=7.97, nll_loss=5.032, ppl=32.72, wps=1156.8, ups=0.42, wpb=2768.5, bsz=136, num_updates=1434, lr=1.7208e-05, gnorm=2.201, train_wall=5, gb_free=15.7, wall=4503
2024-08-17 18:43:40 | INFO | train_inner | epoch 016:     26 / 94 loss=8.022, nll_loss=5.101, ppl=34.33, wps=1301.7, ups=0.38, wpb=3428, bsz=180, num_updates=1436, lr=1.7232e-05, gnorm=1.977, train_wall=5, gb_free=14.8, wall=4508
2024-08-17 18:43:44 | INFO | train_inner | epoch 016:     28 / 94 loss=7.912, nll_loss=4.97, ppl=31.33, wps=883.4, ups=0.44, wpb=2016.5, bsz=104, num_updates=1438, lr=1.7256e-05, gnorm=2.387, train_wall=5, gb_free=16.6, wall=4512
2024-08-17 18:43:49 | INFO | train_inner | epoch 016:     30 / 94 loss=7.974, nll_loss=5.031, ppl=32.69, wps=1427, ups=0.4, wpb=3544.5, bsz=152, num_updates=1440, lr=1.728e-05, gnorm=2.004, train_wall=5, gb_free=13.9, wall=4517
2024-08-17 18:43:54 | INFO | train_inner | epoch 016:     32 / 94 loss=7.959, nll_loss=5.032, ppl=32.71, wps=1401.9, ups=0.38, wpb=3647.5, bsz=208, num_updates=1442, lr=1.7304e-05, gnorm=1.691, train_wall=5, gb_free=12.1, wall=4523
2024-08-17 18:43:59 | INFO | train_inner | epoch 016:     34 / 94 loss=7.962, nll_loss=5.04, ppl=32.89, wps=894, ups=0.45, wpb=1980.5, bsz=84, num_updates=1444, lr=1.7328e-05, gnorm=2.466, train_wall=4, gb_free=15.1, wall=4527
2024-08-17 18:44:03 | INFO | train_inner | epoch 016:     36 / 94 loss=8.03, nll_loss=5.11, ppl=34.54, wps=1430.9, ups=0.5, wpb=2877.5, bsz=80, num_updates=1446, lr=1.7352e-05, gnorm=2.267, train_wall=4, gb_free=15.4, wall=4531
2024-08-17 18:44:07 | INFO | train_inner | epoch 016:     38 / 94 loss=7.918, nll_loss=4.98, ppl=31.56, wps=1111.9, ups=0.46, wpb=2439, bsz=112, num_updates=1448, lr=1.7376e-05, gnorm=2.165, train_wall=4, gb_free=18.2, wall=4535
2024-08-17 18:44:12 | INFO | train_inner | epoch 016:     40 / 94 loss=7.893, nll_loss=4.943, ppl=30.75, wps=1164.8, ups=0.45, wpb=2586.5, bsz=128, num_updates=1450, lr=1.74e-05, gnorm=2.377, train_wall=4, gb_free=14.2, wall=4540
2024-08-17 18:44:15 | INFO | train_inner | epoch 016:     42 / 94 loss=7.927, nll_loss=4.959, ppl=31.11, wps=1270.5, ups=0.54, wpb=2368.5, bsz=59, num_updates=1452, lr=1.7424e-05, gnorm=2.415, train_wall=4, gb_free=15.2, wall=4544
2024-08-17 18:44:21 | INFO | train_inner | epoch 016:     44 / 94 loss=7.982, nll_loss=5.047, ppl=33.05, wps=1282.3, ups=0.38, wpb=3376, bsz=160, num_updates=1454, lr=1.7448e-05, gnorm=1.906, train_wall=5, gb_free=11.5, wall=4549
2024-08-17 18:44:25 | INFO | train_inner | epoch 016:     46 / 94 loss=8.076, nll_loss=5.165, ppl=35.89, wps=1657, ups=0.44, wpb=3759, bsz=140, num_updates=1456, lr=1.7472e-05, gnorm=1.945, train_wall=5, gb_free=12.7, wall=4553
2024-08-17 18:44:30 | INFO | train_inner | epoch 016:     48 / 94 loss=7.841, nll_loss=4.889, ppl=29.63, wps=1108.5, ups=0.39, wpb=2842, bsz=184, num_updates=1458, lr=1.7496e-05, gnorm=2.285, train_wall=5, gb_free=15, wall=4558
2024-08-17 18:44:34 | INFO | train_inner | epoch 016:     50 / 94 loss=7.776, nll_loss=4.787, ppl=27.6, wps=779.7, ups=0.49, wpb=1603.5, bsz=56, num_updates=1460, lr=1.752e-05, gnorm=2.775, train_wall=4, gb_free=14.7, wall=4563
2024-08-17 18:44:39 | INFO | train_inner | epoch 016:     52 / 94 loss=7.907, nll_loss=4.965, ppl=31.24, wps=1224.7, ups=0.41, wpb=2999, bsz=144, num_updates=1462, lr=1.7544e-05, gnorm=1.957, train_wall=5, gb_free=13.9, wall=4567
2024-08-17 18:44:44 | INFO | train_inner | epoch 016:     54 / 94 loss=7.769, nll_loss=4.787, ppl=27.61, wps=1096, ups=0.44, wpb=2506.5, bsz=127.5, num_updates=1464, lr=1.7568e-05, gnorm=2.281, train_wall=5, gb_free=12, wall=4572
2024-08-17 18:44:49 | INFO | train_inner | epoch 016:     56 / 94 loss=7.88, nll_loss=4.904, ppl=29.95, wps=1102.8, ups=0.42, wpb=2644, bsz=76, num_updates=1466, lr=1.7592e-05, gnorm=2.149, train_wall=5, gb_free=15, wall=4577
2024-08-17 18:44:58 | INFO | train_inner | epoch 016:     58 / 94 loss=7.91, nll_loss=4.948, ppl=30.86, wps=618.9, ups=0.21, wpb=2973.5, bsz=120, num_updates=1468, lr=1.7616e-05, gnorm=1.946, train_wall=10, gb_free=14, wall=4586
2024-08-17 18:45:02 | INFO | train_inner | epoch 016:     60 / 94 loss=8.126, nll_loss=5.239, ppl=37.76, wps=1436.7, ups=0.58, wpb=2471.5, bsz=48, num_updates=1470, lr=1.764e-05, gnorm=2.686, train_wall=3, gb_free=18.6, wall=4590
2024-08-17 18:45:06 | INFO | train_inner | epoch 016:     62 / 94 loss=7.781, nll_loss=4.806, ppl=27.96, wps=675.8, ups=0.43, wpb=1572.5, bsz=60, num_updates=1472, lr=1.7664e-05, gnorm=2.558, train_wall=5, gb_free=9.6, wall=4595
2024-08-17 18:45:11 | INFO | train_inner | epoch 016:     64 / 94 loss=7.959, nll_loss=5.014, ppl=32.31, wps=1226.4, ups=0.43, wpb=2857, bsz=96, num_updates=1474, lr=1.7688e-05, gnorm=2.015, train_wall=5, gb_free=9.4, wall=4599
2024-08-17 18:45:15 | INFO | train_inner | epoch 016:     66 / 94 loss=8.007, nll_loss=5.072, ppl=33.64, wps=1601.3, ups=0.48, wpb=3314, bsz=104, num_updates=1476, lr=1.7712e-05, gnorm=2, train_wall=4, gb_free=15.9, wall=4603
2024-08-17 18:45:20 | INFO | train_inner | epoch 016:     68 / 94 loss=8.008, nll_loss=5.073, ppl=33.67, wps=1496.4, ups=0.41, wpb=3614, bsz=136, num_updates=1478, lr=1.7736e-05, gnorm=1.978, train_wall=5, gb_free=13.2, wall=4608
2024-08-17 18:45:24 | INFO | train_inner | epoch 016:     70 / 94 loss=7.908, nll_loss=4.946, ppl=30.81, wps=1004.5, ups=0.51, wpb=1957, bsz=84, num_updates=1480, lr=1.776e-05, gnorm=2.423, train_wall=4, gb_free=9.6, wall=4612
2024-08-17 18:45:28 | INFO | train_inner | epoch 016:     72 / 94 loss=7.815, nll_loss=4.846, ppl=28.76, wps=897.2, ups=0.44, wpb=2017.5, bsz=116, num_updates=1482, lr=1.7784e-05, gnorm=2.63, train_wall=4, gb_free=15.2, wall=4617
2024-08-17 18:45:33 | INFO | train_inner | epoch 016:     74 / 94 loss=7.778, nll_loss=4.801, ppl=27.87, wps=983.4, ups=0.43, wpb=2284, bsz=148, num_updates=1484, lr=1.7808e-05, gnorm=2.218, train_wall=5, gb_free=14.3, wall=4621
2024-08-17 18:45:37 | INFO | train_inner | epoch 016:     76 / 94 loss=7.84, nll_loss=4.88, ppl=29.45, wps=1160.2, ups=0.47, wpb=2488, bsz=140, num_updates=1486, lr=1.7832e-05, gnorm=2.275, train_wall=4, gb_free=17.8, wall=4626
2024-08-17 18:45:42 | INFO | train_inner | epoch 016:     78 / 94 loss=8, nll_loss=5.065, ppl=33.48, wps=1148.2, ups=0.4, wpb=2889.5, bsz=116, num_updates=1488, lr=1.7856e-05, gnorm=2.328, train_wall=5, gb_free=13, wall=4631
2024-08-17 18:45:47 | INFO | train_inner | epoch 016:     80 / 94 loss=7.965, nll_loss=5.03, ppl=32.68, wps=1316.8, ups=0.46, wpb=2844, bsz=120, num_updates=1490, lr=1.788e-05, gnorm=2.3, train_wall=4, gb_free=13.3, wall=4635
2024-08-17 18:45:51 | INFO | train_inner | epoch 016:     82 / 94 loss=7.943, nll_loss=5.001, ppl=32.02, wps=1063.2, ups=0.46, wpb=2289.5, bsz=60, num_updates=1492, lr=1.7904e-05, gnorm=2.445, train_wall=4, gb_free=11.6, wall=4639
2024-08-17 18:45:55 | INFO | train_inner | epoch 016:     84 / 94 loss=8.007, nll_loss=5.089, ppl=34.04, wps=1523.4, ups=0.55, wpb=2765.5, bsz=56, num_updates=1494, lr=1.7928e-05, gnorm=2.257, train_wall=4, gb_free=15.1, wall=4643
2024-08-17 18:45:59 | INFO | train_inner | epoch 016:     86 / 94 loss=8.098, nll_loss=5.198, ppl=36.71, wps=1541.1, ups=0.44, wpb=3519, bsz=112, num_updates=1496, lr=1.7952e-05, gnorm=2.016, train_wall=5, gb_free=16.2, wall=4647
2024-08-17 18:46:04 | INFO | train_inner | epoch 016:     88 / 94 loss=7.974, nll_loss=5.032, ppl=32.72, wps=1036, ups=0.41, wpb=2541, bsz=100, num_updates=1498, lr=1.7976e-05, gnorm=2.199, train_wall=5, gb_free=12.5, wall=4652
2024-08-17 18:46:09 | INFO | train_inner | epoch 016:     90 / 94 loss=7.94, nll_loss=4.991, ppl=31.81, wps=1470.8, ups=0.4, wpb=3639, bsz=176, num_updates=1500, lr=1.8e-05, gnorm=1.84, train_wall=5, gb_free=14.4, wall=4657
2024-08-17 18:46:15 | INFO | train_inner | epoch 016:     92 / 94 loss=7.97, nll_loss=5.036, ppl=32.81, wps=1345, ups=0.36, wpb=3751.5, bsz=212, num_updates=1502, lr=1.8024e-05, gnorm=1.884, train_wall=6, gb_free=10.3, wall=4663
2024-08-17 18:46:18 | INFO | train_inner | epoch 016:     94 / 94 loss=8.099, nll_loss=5.204, ppl=36.85, wps=1640.4, ups=0.57, wpb=2874, bsz=132, num_updates=1504, lr=1.8048e-05, gnorm=2.381, train_wall=3, gb_free=18.4, wall=4666
2024-08-17 18:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22133.09765625Mb; avail=232933.02734375Mb
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000546
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22133.09765625Mb; avail=232933.02734375Mb
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005401
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22133.09765625Mb; avail=232933.02734375Mb
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004732
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011026
2024-08-17 18:46:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22133.01953125Mb; avail=232933.02734375Mb
2024-08-17 18:46:32 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.975 | nll_loss 4.884 | ppl 29.53 | wps 2428.5 | wpb 944.1 | bsz 40.1 | num_updates 1504 | best_loss 7.975
2024-08-17 18:46:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1504 updates
2024-08-17 18:46:32 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:47:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:47:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 16 @ 1504 updates, score 7.975) (writing took 56.32414946798235 seconds)
2024-08-17 18:47:28 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-08-17 18:47:28 | INFO | train | epoch 016 | loss 7.953 | nll_loss 5.013 | ppl 32.28 | wps 921.1 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 1504 | lr 1.8048e-05 | gnorm 2.189 | train_wall 219 | gb_free 18.4 | wall 4736
2024-08-17 18:47:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:47:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:47:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:47:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000920
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20939.8671875Mb; avail=234126.26171875Mb
2024-08-17 18:47:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000232
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001564
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20940.359375Mb; avail=234125.76953125Mb
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20940.359375Mb; avail=234125.76953125Mb
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000573
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002501
2024-08-17 18:47:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20940.359375Mb; avail=234125.76953125Mb
2024-08-17 18:47:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:47:28 | INFO | fairseq.trainer | begin training epoch 17
2024-08-17 18:47:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:47:33 | INFO | train_inner | epoch 017:      2 / 94 loss=7.904, nll_loss=4.952, ppl=30.96, wps=81.3, ups=0.03, wpb=3033, bsz=136, num_updates=1506, lr=1.8072e-05, gnorm=2.041, train_wall=5, gb_free=14.7, wall=4741
2024-08-17 18:47:38 | INFO | train_inner | epoch 017:      4 / 94 loss=7.913, nll_loss=4.967, ppl=31.28, wps=1150.9, ups=0.41, wpb=2820, bsz=124, num_updates=1508, lr=1.8096e-05, gnorm=2.063, train_wall=5, gb_free=10.8, wall=4746
2024-08-17 18:47:42 | INFO | train_inner | epoch 017:      6 / 94 loss=7.724, nll_loss=4.708, ppl=26.13, wps=864.3, ups=0.49, wpb=1764, bsz=47, num_updates=1510, lr=1.812e-05, gnorm=2.883, train_wall=4, gb_free=11.4, wall=4750
2024-08-17 18:47:45 | INFO | train_inner | epoch 017:      8 / 94 loss=7.879, nll_loss=4.915, ppl=30.16, wps=1114.1, ups=0.54, wpb=2071.5, bsz=60, num_updates=1512, lr=1.8144e-05, gnorm=2.402, train_wall=4, gb_free=16.7, wall=4754
2024-08-17 18:47:49 | INFO | train_inner | epoch 017:     10 / 94 loss=8.019, nll_loss=5.101, ppl=34.33, wps=1561.4, ups=0.52, wpb=3011.5, bsz=100, num_updates=1514, lr=1.8168e-05, gnorm=2.317, train_wall=4, gb_free=14.4, wall=4757
2024-08-17 18:47:55 | INFO | train_inner | epoch 017:     12 / 94 loss=7.907, nll_loss=4.95, ppl=30.92, wps=1644.6, ups=0.35, wpb=4644, bsz=224, num_updates=1516, lr=1.8192e-05, gnorm=1.63, train_wall=6, gb_free=10.3, wall=4763
2024-08-17 18:48:00 | INFO | train_inner | epoch 017:     14 / 94 loss=8.006, nll_loss=5.072, ppl=33.64, wps=1642.9, ups=0.41, wpb=4010, bsz=164, num_updates=1518, lr=1.8216e-05, gnorm=1.753, train_wall=5, gb_free=13, wall=4768
2024-08-17 18:48:05 | INFO | train_inner | epoch 017:     16 / 94 loss=7.791, nll_loss=4.814, ppl=28.14, wps=1255.7, ups=0.41, wpb=3064.5, bsz=176, num_updates=1520, lr=1.824e-05, gnorm=2, train_wall=5, gb_free=14.2, wall=4773
2024-08-17 18:48:09 | INFO | train_inner | epoch 017:     18 / 94 loss=7.974, nll_loss=5.04, ppl=32.89, wps=1547.5, ups=0.41, wpb=3742.5, bsz=168, num_updates=1522, lr=1.8264e-05, gnorm=1.858, train_wall=5, gb_free=11.9, wall=4778
2024-08-17 18:48:14 | INFO | train_inner | epoch 017:     20 / 94 loss=7.864, nll_loss=4.915, ppl=30.16, wps=934, ups=0.42, wpb=2237, bsz=108, num_updates=1524, lr=1.8288e-05, gnorm=2.434, train_wall=5, gb_free=14.8, wall=4782
2024-08-17 18:48:29 | INFO | train_inner | epoch 017:     22 / 94 loss=7.838, nll_loss=4.865, ppl=29.14, wps=467.5, ups=0.14, wpb=3440, bsz=136, num_updates=1526, lr=1.8312e-05, gnorm=1.89, train_wall=15, gb_free=11.8, wall=4797
2024-08-17 18:48:33 | INFO | train_inner | epoch 017:     24 / 94 loss=7.655, nll_loss=4.632, ppl=24.8, wps=890.7, ups=0.5, wpb=1772.5, bsz=64, num_updates=1528, lr=1.8336e-05, gnorm=2.363, train_wall=4, gb_free=12.4, wall=4801
2024-08-17 18:48:37 | INFO | train_inner | epoch 017:     26 / 94 loss=7.933, nll_loss=4.974, ppl=31.42, wps=975, ups=0.48, wpb=2023.5, bsz=52, num_updates=1530, lr=1.836e-05, gnorm=2.487, train_wall=4, gb_free=12.6, wall=4805
2024-08-17 18:48:42 | INFO | train_inner | epoch 017:     28 / 94 loss=7.964, nll_loss=5.028, ppl=32.63, wps=1545.6, ups=0.43, wpb=3572.5, bsz=184, num_updates=1532, lr=1.8384e-05, gnorm=1.941, train_wall=5, gb_free=14.6, wall=4810
2024-08-17 18:48:47 | INFO | train_inner | epoch 017:     30 / 94 loss=7.877, nll_loss=4.913, ppl=30.14, wps=1200, ups=0.41, wpb=2960.5, bsz=128, num_updates=1534, lr=1.8408e-05, gnorm=1.986, train_wall=5, gb_free=11.2, wall=4815
2024-08-17 18:48:51 | INFO | train_inner | epoch 017:     32 / 94 loss=7.826, nll_loss=4.863, ppl=29.09, wps=1562.3, ups=0.44, wpb=3530.5, bsz=192, num_updates=1536, lr=1.8432e-05, gnorm=1.93, train_wall=5, gb_free=17.7, wall=4819
2024-08-17 18:48:56 | INFO | train_inner | epoch 017:     34 / 94 loss=7.794, nll_loss=4.82, ppl=28.25, wps=1215.7, ups=0.44, wpb=2783, bsz=148, num_updates=1538, lr=1.8456e-05, gnorm=2.063, train_wall=5, gb_free=14.9, wall=4824
2024-08-17 18:49:00 | INFO | train_inner | epoch 017:     36 / 94 loss=7.92, nll_loss=4.964, ppl=31.22, wps=1417.3, ups=0.48, wpb=2962.5, bsz=103.5, num_updates=1540, lr=1.848e-05, gnorm=2.318, train_wall=4, gb_free=12.5, wall=4828
2024-08-17 18:49:05 | INFO | train_inner | epoch 017:     38 / 94 loss=7.986, nll_loss=5.053, ppl=33.2, wps=1612.2, ups=0.42, wpb=3836, bsz=168, num_updates=1542, lr=1.8504e-05, gnorm=1.762, train_wall=5, gb_free=12.1, wall=4833
2024-08-17 18:49:10 | INFO | train_inner | epoch 017:     40 / 94 loss=7.874, nll_loss=4.918, ppl=30.23, wps=1191.1, ups=0.4, wpb=2948.5, bsz=140, num_updates=1544, lr=1.8528e-05, gnorm=1.934, train_wall=5, gb_free=12.7, wall=4838
2024-08-17 18:49:14 | INFO | train_inner | epoch 017:     42 / 94 loss=7.864, nll_loss=4.882, ppl=29.48, wps=997.4, ups=0.42, wpb=2372, bsz=88, num_updates=1546, lr=1.8552e-05, gnorm=2.207, train_wall=5, gb_free=10.8, wall=4843
2024-08-17 18:49:19 | INFO | train_inner | epoch 017:     44 / 94 loss=7.914, nll_loss=4.957, ppl=31.06, wps=728.5, ups=0.41, wpb=1759, bsz=68, num_updates=1548, lr=1.8576e-05, gnorm=2.483, train_wall=5, gb_free=12.1, wall=4847
2024-08-17 18:49:24 | INFO | train_inner | epoch 017:     46 / 94 loss=7.903, nll_loss=4.963, ppl=31.19, wps=999.1, ups=0.45, wpb=2232, bsz=108, num_updates=1550, lr=1.86e-05, gnorm=2.249, train_wall=4, gb_free=14.1, wall=4852
2024-08-17 18:49:28 | INFO | train_inner | epoch 017:     48 / 94 loss=7.918, nll_loss=4.988, ppl=31.74, wps=1079.6, ups=0.47, wpb=2312.5, bsz=84, num_updates=1552, lr=1.8624e-05, gnorm=2.421, train_wall=4, gb_free=14.9, wall=4856
2024-08-17 18:49:31 | INFO | train_inner | epoch 017:     50 / 94 loss=7.861, nll_loss=4.892, ppl=29.69, wps=1266.3, ups=0.6, wpb=2093.5, bsz=76, num_updates=1554, lr=1.8648e-05, gnorm=2.601, train_wall=3, gb_free=12.1, wall=4860
2024-08-17 18:49:35 | INFO | train_inner | epoch 017:     52 / 94 loss=7.754, nll_loss=4.755, ppl=27, wps=833.9, ups=0.47, wpb=1768, bsz=84, num_updates=1556, lr=1.8672e-05, gnorm=2.47, train_wall=4, gb_free=9.8, wall=4864
2024-08-17 18:49:39 | INFO | train_inner | epoch 017:     54 / 94 loss=7.807, nll_loss=4.807, ppl=27.99, wps=1110.7, ups=0.63, wpb=1761.5, bsz=40, num_updates=1558, lr=1.8696e-05, gnorm=2.628, train_wall=3, gb_free=20.1, wall=4867
2024-08-17 18:49:43 | INFO | train_inner | epoch 017:     56 / 94 loss=7.945, nll_loss=5.002, ppl=32.05, wps=1193.1, ups=0.42, wpb=2865, bsz=168, num_updates=1560, lr=1.872e-05, gnorm=2.118, train_wall=5, gb_free=14.4, wall=4872
2024-08-17 18:49:49 | INFO | train_inner | epoch 017:     58 / 94 loss=7.875, nll_loss=4.913, ppl=30.13, wps=1150.7, ups=0.37, wpb=3089.5, bsz=172, num_updates=1562, lr=1.8744e-05, gnorm=1.995, train_wall=5, gb_free=12.8, wall=4877
2024-08-17 18:49:53 | INFO | train_inner | epoch 017:     60 / 94 loss=7.809, nll_loss=4.82, ppl=28.24, wps=943.2, ups=0.47, wpb=2017, bsz=60, num_updates=1564, lr=1.8768e-05, gnorm=2.478, train_wall=4, gb_free=11.9, wall=4881
2024-08-17 18:49:58 | INFO | train_inner | epoch 017:     62 / 94 loss=7.93, nll_loss=4.993, ppl=31.84, wps=941.8, ups=0.39, wpb=2420.5, bsz=80, num_updates=1566, lr=1.8792e-05, gnorm=2.408, train_wall=5, gb_free=11.4, wall=4887
2024-08-17 18:50:03 | INFO | train_inner | epoch 017:     64 / 94 loss=7.922, nll_loss=4.977, ppl=31.5, wps=1450.5, ups=0.45, wpb=3207.5, bsz=88, num_updates=1568, lr=1.8816e-05, gnorm=2.016, train_wall=4, gb_free=14.5, wall=4891
2024-08-17 18:50:08 | INFO | train_inner | epoch 017:     66 / 94 loss=7.924, nll_loss=4.999, ppl=31.98, wps=1573.9, ups=0.4, wpb=3936, bsz=204, num_updates=1570, lr=1.884e-05, gnorm=1.929, train_wall=5, gb_free=16.6, wall=4896
2024-08-17 18:50:12 | INFO | train_inner | epoch 017:     68 / 94 loss=7.855, nll_loss=4.893, ppl=29.7, wps=1444.7, ups=0.44, wpb=3298.5, bsz=140, num_updates=1572, lr=1.8864e-05, gnorm=1.861, train_wall=5, gb_free=15.4, wall=4901
2024-08-17 18:50:16 | INFO | train_inner | epoch 017:     70 / 94 loss=8.045, nll_loss=5.127, ppl=34.95, wps=1683.8, ups=0.48, wpb=3507, bsz=80, num_updates=1574, lr=1.8888e-05, gnorm=2.006, train_wall=4, gb_free=14.1, wall=4905
2024-08-17 18:50:22 | INFO | train_inner | epoch 017:     72 / 94 loss=7.986, nll_loss=5.049, ppl=33.11, wps=1578.4, ups=0.39, wpb=4052, bsz=176, num_updates=1576, lr=1.8912e-05, gnorm=1.799, train_wall=5, gb_free=12.8, wall=4910
2024-08-17 18:50:26 | INFO | train_inner | epoch 017:     74 / 94 loss=7.865, nll_loss=4.896, ppl=29.77, wps=1007.4, ups=0.43, wpb=2346, bsz=116, num_updates=1578, lr=1.8936e-05, gnorm=2.094, train_wall=5, gb_free=14.1, wall=4914
2024-08-17 18:50:31 | INFO | train_inner | epoch 017:     76 / 94 loss=7.897, nll_loss=4.943, ppl=30.76, wps=1587.1, ups=0.41, wpb=3883, bsz=168, num_updates=1580, lr=1.896e-05, gnorm=1.779, train_wall=5, gb_free=12.4, wall=4919
2024-08-17 18:50:36 | INFO | train_inner | epoch 017:     78 / 94 loss=7.827, nll_loss=4.853, ppl=28.91, wps=782.9, ups=0.4, wpb=1971.5, bsz=88, num_updates=1582, lr=1.8984e-05, gnorm=2.345, train_wall=5, gb_free=10.8, wall=4924
2024-08-17 18:50:40 | INFO | train_inner | epoch 017:     80 / 94 loss=7.901, nll_loss=4.944, ppl=30.77, wps=1284.9, ups=0.52, wpb=2449.5, bsz=60, num_updates=1584, lr=1.9008e-05, gnorm=2.41, train_wall=4, gb_free=8.9, wall=4928
2024-08-17 18:50:45 | INFO | train_inner | epoch 017:     82 / 94 loss=7.856, nll_loss=4.903, ppl=29.92, wps=1181.9, ups=0.39, wpb=3065.5, bsz=180, num_updates=1586, lr=1.9032e-05, gnorm=2.158, train_wall=5, gb_free=14.3, wall=4933
2024-08-17 18:50:50 | INFO | train_inner | epoch 017:     84 / 94 loss=7.958, nll_loss=5.016, ppl=32.35, wps=1229.2, ups=0.38, wpb=3253.5, bsz=128, num_updates=1588, lr=1.9056e-05, gnorm=1.932, train_wall=5, gb_free=10.6, wall=4939
2024-08-17 18:50:56 | INFO | train_inner | epoch 017:     86 / 94 loss=7.866, nll_loss=4.901, ppl=29.88, wps=1045.3, ups=0.37, wpb=2857.5, bsz=124, num_updates=1590, lr=1.908e-05, gnorm=2.171, train_wall=5, gb_free=10.1, wall=4944
2024-08-17 18:51:00 | INFO | train_inner | epoch 017:     88 / 94 loss=7.945, nll_loss=4.988, ppl=31.74, wps=1468.3, ups=0.47, wpb=3128, bsz=88, num_updates=1592, lr=1.9104e-05, gnorm=2.166, train_wall=4, gb_free=16.7, wall=4948
2024-08-17 18:51:04 | INFO | train_inner | epoch 017:     90 / 94 loss=7.865, nll_loss=4.883, ppl=29.5, wps=1147.7, ups=0.53, wpb=2167, bsz=60, num_updates=1594, lr=1.9128e-05, gnorm=2.221, train_wall=4, gb_free=14.8, wall=4952
2024-08-17 18:51:09 | INFO | train_inner | epoch 017:     92 / 94 loss=7.835, nll_loss=4.865, ppl=29.15, wps=1178.2, ups=0.36, wpb=3235, bsz=188, num_updates=1596, lr=1.9152e-05, gnorm=1.898, train_wall=5, gb_free=14.9, wall=4958
2024-08-17 18:51:13 | INFO | train_inner | epoch 017:     94 / 94 loss=7.855, nll_loss=4.892, ppl=29.7, wps=1178.8, ups=0.53, wpb=2213.5, bsz=72, num_updates=1598, lr=1.9176e-05, gnorm=2.381, train_wall=4, gb_free=13.4, wall=4961
2024-08-17 18:51:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18171.66015625Mb; avail=236894.42578125Mb
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18171.66015625Mb; avail=236894.42578125Mb
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005360
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18171.66015625Mb; avail=236894.42578125Mb
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004820
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011097
2024-08-17 18:51:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18171.66015625Mb; avail=236894.42578125Mb
2024-08-17 18:51:27 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.953 | nll_loss 4.862 | ppl 29.08 | wps 2423.9 | wpb 944.1 | bsz 40.1 | num_updates 1598 | best_loss 7.953
2024-08-17 18:51:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1598 updates
2024-08-17 18:51:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:52:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:52:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 17 @ 1598 updates, score 7.953) (writing took 65.9370881980285 seconds)
2024-08-17 18:52:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-08-17 18:52:33 | INFO | train | epoch 017 | loss 7.896 | nll_loss 4.941 | ppl 30.71 | wps 876 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 1598 | lr 1.9176e-05 | gnorm 2.155 | train_wall 225 | gb_free 13.4 | wall 5041
2024-08-17 18:52:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:52:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:52:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:52:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.003160
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17835.0625Mb; avail=237230.5234375Mb
2024-08-17 18:52:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000256
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.003487
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.0625Mb; avail=237231.015625Mb
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000100
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.0625Mb; avail=237231.015625Mb
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000878
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.010351
2024-08-17 18:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17835.0625Mb; avail=237231.015625Mb
2024-08-17 18:52:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:52:33 | INFO | fairseq.trainer | begin training epoch 18
2024-08-17 18:52:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:52:38 | INFO | train_inner | epoch 018:      2 / 94 loss=7.847, nll_loss=4.891, ppl=29.68, wps=61.2, ups=0.02, wpb=2591.5, bsz=132, num_updates=1600, lr=1.92e-05, gnorm=2.341, train_wall=5, gb_free=13.3, wall=5046
2024-08-17 18:52:42 | INFO | train_inner | epoch 018:      4 / 94 loss=7.769, nll_loss=4.783, ppl=27.54, wps=1317.7, ups=0.52, wpb=2537, bsz=60, num_updates=1602, lr=1.9224e-05, gnorm=2.163, train_wall=4, gb_free=17.3, wall=5050
2024-08-17 18:52:47 | INFO | train_inner | epoch 018:      6 / 94 loss=7.777, nll_loss=4.8, ppl=27.86, wps=1102.3, ups=0.42, wpb=2619, bsz=136, num_updates=1604, lr=1.9248e-05, gnorm=2.069, train_wall=5, gb_free=10.5, wall=5055
2024-08-17 18:52:51 | INFO | train_inner | epoch 018:      8 / 94 loss=7.895, nll_loss=4.925, ppl=30.39, wps=1481.8, ups=0.46, wpb=3217, bsz=128, num_updates=1606, lr=1.9272e-05, gnorm=2.488, train_wall=4, gb_free=13.3, wall=5059
2024-08-17 18:52:56 | INFO | train_inner | epoch 018:     10 / 94 loss=7.801, nll_loss=4.806, ppl=27.98, wps=1078.3, ups=0.38, wpb=2806.5, bsz=124, num_updates=1608, lr=1.9296e-05, gnorm=2.199, train_wall=5, gb_free=10.4, wall=5064
2024-08-17 18:53:01 | INFO | train_inner | epoch 018:     12 / 94 loss=7.783, nll_loss=4.772, ppl=27.32, wps=881.6, ups=0.4, wpb=2216.5, bsz=76, num_updates=1610, lr=1.932e-05, gnorm=2.349, train_wall=5, gb_free=13.8, wall=5069
2024-08-17 18:53:06 | INFO | train_inner | epoch 018:     14 / 94 loss=7.747, nll_loss=4.73, ppl=26.54, wps=1433.8, ups=0.43, wpb=3319, bsz=108, num_updates=1612, lr=1.9344e-05, gnorm=1.98, train_wall=5, gb_free=11.4, wall=5074
2024-08-17 18:53:10 | INFO | train_inner | epoch 018:     16 / 94 loss=7.901, nll_loss=4.943, ppl=30.75, wps=1461.3, ups=0.44, wpb=3288.5, bsz=120, num_updates=1614, lr=1.9368e-05, gnorm=1.888, train_wall=4, gb_free=14.8, wall=5078
2024-08-17 18:53:14 | INFO | train_inner | epoch 018:     18 / 94 loss=7.784, nll_loss=4.806, ppl=27.98, wps=1592.2, ups=0.5, wpb=3177, bsz=148, num_updates=1616, lr=1.9392e-05, gnorm=2.014, train_wall=4, gb_free=15.1, wall=5082
2024-08-17 18:53:18 | INFO | train_inner | epoch 018:     20 / 94 loss=7.839, nll_loss=4.883, ppl=29.51, wps=1720.7, ups=0.47, wpb=3640, bsz=176, num_updates=1618, lr=1.9416e-05, gnorm=1.876, train_wall=4, gb_free=16.2, wall=5087
2024-08-17 18:53:23 | INFO | train_inner | epoch 018:     22 / 94 loss=7.962, nll_loss=5.029, ppl=32.64, wps=1450.2, ups=0.46, wpb=3175.5, bsz=120, num_updates=1620, lr=1.944e-05, gnorm=2.069, train_wall=4, gb_free=14.7, wall=5091
2024-08-17 18:53:28 | INFO | train_inner | epoch 018:     24 / 94 loss=7.765, nll_loss=4.784, ppl=27.54, wps=1273.2, ups=0.37, wpb=3412.5, bsz=176, num_updates=1622, lr=1.9464e-05, gnorm=1.885, train_wall=5, gb_free=10.1, wall=5096
2024-08-17 18:53:33 | INFO | train_inner | epoch 018:     26 / 94 loss=7.841, nll_loss=4.863, ppl=29.11, wps=1342.8, ups=0.41, wpb=3276.5, bsz=124, num_updates=1624, lr=1.9488e-05, gnorm=1.882, train_wall=5, gb_free=14.5, wall=5101
2024-08-17 18:53:38 | INFO | train_inner | epoch 018:     28 / 94 loss=7.974, nll_loss=5.031, ppl=32.7, wps=1588, ups=0.45, wpb=3564.5, bsz=108, num_updates=1626, lr=1.9512e-05, gnorm=1.946, train_wall=4, gb_free=14.1, wall=5106
2024-08-17 18:53:42 | INFO | train_inner | epoch 018:     30 / 94 loss=7.868, nll_loss=4.887, ppl=29.59, wps=1206, ups=0.49, wpb=2441, bsz=60, num_updates=1628, lr=1.9536e-05, gnorm=2.339, train_wall=4, gb_free=13, wall=5110
2024-08-17 18:53:46 | INFO | train_inner | epoch 018:     32 / 94 loss=7.811, nll_loss=4.822, ppl=28.28, wps=1376.4, ups=0.47, wpb=2900.5, bsz=88, num_updates=1630, lr=1.956e-05, gnorm=1.994, train_wall=4, gb_free=11.8, wall=5114
2024-08-17 18:53:50 | INFO | train_inner | epoch 018:     34 / 94 loss=7.84, nll_loss=4.859, ppl=29.02, wps=1318.4, ups=0.51, wpb=2571.5, bsz=80, num_updates=1632, lr=1.9584e-05, gnorm=2.387, train_wall=4, gb_free=13.3, wall=5118
2024-08-17 18:53:54 | INFO | train_inner | epoch 018:     36 / 94 loss=7.841, nll_loss=4.872, ppl=29.29, wps=1262.7, ups=0.49, wpb=2600, bsz=112, num_updates=1634, lr=1.9608e-05, gnorm=2.339, train_wall=4, gb_free=15.7, wall=5122
2024-08-17 18:53:59 | INFO | train_inner | epoch 018:     38 / 94 loss=7.837, nll_loss=4.851, ppl=28.87, wps=1227.3, ups=0.41, wpb=3019.5, bsz=88, num_updates=1636, lr=1.9632e-05, gnorm=2, train_wall=5, gb_free=8.6, wall=5127
2024-08-17 18:54:03 | INFO | train_inner | epoch 018:     40 / 94 loss=7.854, nll_loss=4.891, ppl=29.66, wps=1244.5, ups=0.43, wpb=2916.5, bsz=156, num_updates=1638, lr=1.9656e-05, gnorm=2.032, train_wall=5, gb_free=15.4, wall=5132
2024-08-17 18:54:08 | INFO | train_inner | epoch 018:     42 / 94 loss=7.916, nll_loss=4.97, ppl=31.35, wps=1260.6, ups=0.47, wpb=2666, bsz=112, num_updates=1640, lr=1.968e-05, gnorm=2.32, train_wall=4, gb_free=11.2, wall=5136
2024-08-17 18:54:13 | INFO | train_inner | epoch 018:     44 / 94 loss=7.815, nll_loss=4.839, ppl=28.62, wps=1309.6, ups=0.38, wpb=3422.5, bsz=168, num_updates=1642, lr=1.9704e-05, gnorm=1.848, train_wall=5, gb_free=11.3, wall=5141
2024-08-17 18:54:18 | INFO | train_inner | epoch 018:     46 / 94 loss=7.776, nll_loss=4.803, ppl=27.92, wps=1093.9, ups=0.41, wpb=2685, bsz=156, num_updates=1644, lr=1.9728e-05, gnorm=2.044, train_wall=5, gb_free=14.1, wall=5146
2024-08-17 18:54:23 | INFO | train_inner | epoch 018:     48 / 94 loss=7.816, nll_loss=4.849, ppl=28.82, wps=1067.1, ups=0.41, wpb=2602, bsz=156, num_updates=1646, lr=1.9752e-05, gnorm=2.034, train_wall=5, gb_free=14.7, wall=5151
2024-08-17 18:54:27 | INFO | train_inner | epoch 018:     50 / 94 loss=7.772, nll_loss=4.785, ppl=27.58, wps=875.5, ups=0.43, wpb=2039.5, bsz=56, num_updates=1648, lr=1.9776e-05, gnorm=2.624, train_wall=5, gb_free=14.7, wall=5156
2024-08-17 18:54:31 | INFO | train_inner | epoch 018:     52 / 94 loss=7.892, nll_loss=4.942, ppl=30.73, wps=1466, ups=0.52, wpb=2842.5, bsz=144, num_updates=1650, lr=1.98e-05, gnorm=2.014, train_wall=4, gb_free=18, wall=5159
2024-08-17 18:54:36 | INFO | train_inner | epoch 018:     54 / 94 loss=7.896, nll_loss=4.944, ppl=30.79, wps=1215.1, ups=0.42, wpb=2926, bsz=136, num_updates=1652, lr=1.9824e-05, gnorm=1.974, train_wall=5, gb_free=11.1, wall=5164
2024-08-17 18:54:40 | INFO | train_inner | epoch 018:     56 / 94 loss=7.864, nll_loss=4.902, ppl=29.89, wps=1006.8, ups=0.49, wpb=2065.5, bsz=72, num_updates=1654, lr=1.9848e-05, gnorm=2.653, train_wall=4, gb_free=15.6, wall=5168
2024-08-17 18:54:44 | INFO | train_inner | epoch 018:     58 / 94 loss=7.916, nll_loss=4.955, ppl=31.02, wps=1714.1, ups=0.51, wpb=3334.5, bsz=92, num_updates=1656, lr=1.9872e-05, gnorm=2.175, train_wall=4, gb_free=23.4, wall=5172
2024-08-17 18:54:48 | INFO | train_inner | epoch 018:     60 / 94 loss=7.858, nll_loss=4.9, ppl=29.86, wps=1350.7, ups=0.46, wpb=2924, bsz=128, num_updates=1658, lr=1.9896e-05, gnorm=2.162, train_wall=4, gb_free=12.5, wall=5177
2024-08-17 18:54:53 | INFO | train_inner | epoch 018:     62 / 94 loss=7.867, nll_loss=4.903, ppl=29.93, wps=1175.9, ups=0.41, wpb=2891, bsz=147.5, num_updates=1660, lr=1.992e-05, gnorm=2.134, train_wall=5, gb_free=9.6, wall=5182
2024-08-17 18:54:58 | INFO | train_inner | epoch 018:     64 / 94 loss=7.74, nll_loss=4.739, ppl=26.7, wps=730.5, ups=0.43, wpb=1713.5, bsz=43, num_updates=1662, lr=1.9944e-05, gnorm=2.802, train_wall=5, gb_free=14.2, wall=5186
2024-08-17 18:55:03 | INFO | train_inner | epoch 018:     66 / 94 loss=7.971, nll_loss=5.045, ppl=33.01, wps=1099.4, ups=0.41, wpb=2714.5, bsz=156, num_updates=1664, lr=1.9968e-05, gnorm=2.144, train_wall=5, gb_free=15.6, wall=5191
2024-08-17 18:55:08 | INFO | train_inner | epoch 018:     68 / 94 loss=7.851, nll_loss=4.881, ppl=29.46, wps=1434.8, ups=0.39, wpb=3718.5, bsz=196, num_updates=1666, lr=1.9992e-05, gnorm=1.855, train_wall=5, gb_free=15.1, wall=5196
2024-08-17 18:55:13 | INFO | train_inner | epoch 018:     70 / 94 loss=7.806, nll_loss=4.821, ppl=28.27, wps=1131.3, ups=0.43, wpb=2648.5, bsz=124, num_updates=1668, lr=2.0016e-05, gnorm=2.132, train_wall=5, gb_free=14.2, wall=5201
2024-08-17 18:55:18 | INFO | train_inner | epoch 018:     72 / 94 loss=7.843, nll_loss=4.878, ppl=29.4, wps=1350.1, ups=0.42, wpb=3199, bsz=152, num_updates=1670, lr=2.004e-05, gnorm=2.111, train_wall=5, gb_free=10.1, wall=5206
2024-08-17 18:55:23 | INFO | train_inner | epoch 018:     74 / 94 loss=7.839, nll_loss=4.862, ppl=29.08, wps=951.1, ups=0.39, wpb=2431.5, bsz=76, num_updates=1672, lr=2.0064e-05, gnorm=2.406, train_wall=5, gb_free=9.7, wall=5211
2024-08-17 18:55:26 | INFO | train_inner | epoch 018:     76 / 94 loss=7.879, nll_loss=4.925, ppl=30.37, wps=1154.2, ups=0.58, wpb=1976.5, bsz=60, num_updates=1674, lr=2.0088e-05, gnorm=2.484, train_wall=3, gb_free=17.3, wall=5214
2024-08-17 18:55:31 | INFO | train_inner | epoch 018:     78 / 94 loss=7.88, nll_loss=4.916, ppl=30.19, wps=1362.8, ups=0.4, wpb=3412.5, bsz=120, num_updates=1676, lr=2.0112e-05, gnorm=2.094, train_wall=5, gb_free=12.4, wall=5219
2024-08-17 18:55:36 | INFO | train_inner | epoch 018:     80 / 94 loss=7.832, nll_loss=4.866, ppl=29.16, wps=937.5, ups=0.39, wpb=2392.5, bsz=120, num_updates=1678, lr=2.0136e-05, gnorm=2.371, train_wall=5, gb_free=12.7, wall=5224
2024-08-17 18:55:42 | INFO | train_inner | epoch 018:     82 / 94 loss=7.897, nll_loss=4.948, ppl=30.88, wps=1429.6, ups=0.36, wpb=3978, bsz=192, num_updates=1680, lr=2.016e-05, gnorm=1.931, train_wall=6, gb_free=10, wall=5230
2024-08-17 18:55:47 | INFO | train_inner | epoch 018:     84 / 94 loss=7.866, nll_loss=4.906, ppl=29.99, wps=1249, ups=0.41, wpb=3052.5, bsz=136, num_updates=1682, lr=2.0184e-05, gnorm=1.949, train_wall=5, gb_free=14, wall=5235
2024-08-17 18:55:56 | INFO | train_inner | epoch 018:     86 / 94 loss=7.865, nll_loss=4.894, ppl=29.73, wps=732.8, ups=0.2, wpb=3624.5, bsz=144, num_updates=1684, lr=2.0208e-05, gnorm=1.98, train_wall=10, gb_free=12.1, wall=5245
2024-08-17 18:56:00 | INFO | train_inner | epoch 018:     88 / 94 loss=7.825, nll_loss=4.861, ppl=29.06, wps=918.5, ups=0.51, wpb=1813, bsz=80, num_updates=1686, lr=2.0232e-05, gnorm=2.446, train_wall=4, gb_free=12.7, wall=5249
2024-08-17 18:56:05 | INFO | train_inner | epoch 018:     90 / 94 loss=7.818, nll_loss=4.836, ppl=28.57, wps=1078.6, ups=0.43, wpb=2482.5, bsz=96, num_updates=1688, lr=2.0256e-05, gnorm=2.15, train_wall=5, gb_free=9.3, wall=5253
2024-08-17 18:56:10 | INFO | train_inner | epoch 018:     92 / 94 loss=7.897, nll_loss=4.949, ppl=30.88, wps=1292.9, ups=0.4, wpb=3233, bsz=172, num_updates=1690, lr=2.028e-05, gnorm=2.04, train_wall=5, gb_free=12.9, wall=5258
2024-08-17 18:56:14 | INFO | train_inner | epoch 018:     94 / 94 loss=7.651, nll_loss=4.639, ppl=24.91, wps=796.4, ups=0.56, wpb=1410, bsz=56, num_updates=1692, lr=2.0304e-05, gnorm=3.107, train_wall=4, gb_free=19.8, wall=5262
2024-08-17 18:56:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14739.9375Mb; avail=240326.1875Mb
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14739.9375Mb; avail=240326.1875Mb
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005394
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14739.9375Mb; avail=240326.1875Mb
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004761
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011032
2024-08-17 18:56:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14739.9375Mb; avail=240326.1875Mb
2024-08-17 18:56:27 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.944 | nll_loss 4.843 | ppl 28.7 | wps 2435.3 | wpb 944.1 | bsz 40.1 | num_updates 1692 | best_loss 7.944
2024-08-17 18:56:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1692 updates
2024-08-17 18:56:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 18:57:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 18 @ 1692 updates, score 7.944) (writing took 66.4514646306634 seconds)
2024-08-17 18:57:33 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-08-17 18:57:33 | INFO | train | epoch 018 | loss 7.848 | nll_loss 4.879 | ppl 29.42 | wps 887.5 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 1692 | lr 2.0304e-05 | gnorm 2.175 | train_wall 220 | gb_free 19.8 | wall 5342
2024-08-17 18:57:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 18:57:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 18:57:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 18:57:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000825
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17278.3984375Mb; avail=237787.7265625Mb
2024-08-17 18:57:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000170
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001494
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17278.3984375Mb; avail=237787.7265625Mb
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17278.3984375Mb; avail=237787.7265625Mb
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000573
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002417
2024-08-17 18:57:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17278.3984375Mb; avail=237787.7265625Mb
2024-08-17 18:57:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 18:57:34 | INFO | fairseq.trainer | begin training epoch 19
2024-08-17 18:57:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 18:57:38 | INFO | train_inner | epoch 019:      2 / 94 loss=7.763, nll_loss=4.784, ppl=27.54, wps=64, ups=0.02, wpb=2715, bsz=152, num_updates=1694, lr=2.0328e-05, gnorm=2.034, train_wall=5, gb_free=12, wall=5347
2024-08-17 18:57:43 | INFO | train_inner | epoch 019:      4 / 94 loss=7.807, nll_loss=4.836, ppl=28.55, wps=1449.3, ups=0.42, wpb=3449, bsz=200, num_updates=1696, lr=2.0352e-05, gnorm=1.751, train_wall=5, gb_free=16.1, wall=5351
2024-08-17 18:57:47 | INFO | train_inner | epoch 019:      6 / 94 loss=7.627, nll_loss=4.578, ppl=23.88, wps=1227.1, ups=0.54, wpb=2275.5, bsz=96, num_updates=1698, lr=2.0376e-05, gnorm=2.337, train_wall=4, gb_free=10.7, wall=5355
2024-08-17 18:57:51 | INFO | train_inner | epoch 019:      8 / 94 loss=7.777, nll_loss=4.765, ppl=27.19, wps=1413, ups=0.44, wpb=3205.5, bsz=104, num_updates=1700, lr=2.04e-05, gnorm=2.163, train_wall=5, gb_free=12.7, wall=5360
2024-08-17 18:57:56 | INFO | train_inner | epoch 019:     10 / 94 loss=7.63, nll_loss=4.608, ppl=24.39, wps=1130.3, ups=0.42, wpb=2660, bsz=164, num_updates=1702, lr=2.0424e-05, gnorm=2.482, train_wall=5, gb_free=14.6, wall=5364
2024-08-17 18:58:01 | INFO | train_inner | epoch 019:     12 / 94 loss=7.854, nll_loss=4.897, ppl=29.8, wps=1101.5, ups=0.4, wpb=2754.5, bsz=144, num_updates=1704, lr=2.0448e-05, gnorm=2.371, train_wall=5, gb_free=14, wall=5369
2024-08-17 18:58:06 | INFO | train_inner | epoch 019:     14 / 94 loss=7.834, nll_loss=4.856, ppl=28.97, wps=1124.2, ups=0.41, wpb=2757.5, bsz=100, num_updates=1706, lr=2.0472e-05, gnorm=2.223, train_wall=5, gb_free=15.4, wall=5374
2024-08-17 18:58:11 | INFO | train_inner | epoch 019:     16 / 94 loss=7.779, nll_loss=4.805, ppl=27.95, wps=1189, ups=0.44, wpb=2715, bsz=160, num_updates=1708, lr=2.0496e-05, gnorm=2.024, train_wall=5, gb_free=13.8, wall=5379
2024-08-17 18:58:15 | INFO | train_inner | epoch 019:     18 / 94 loss=7.924, nll_loss=4.969, ppl=31.32, wps=1426, ups=0.46, wpb=3126.5, bsz=80, num_updates=1710, lr=2.052e-05, gnorm=2.134, train_wall=4, gb_free=17, wall=5383
2024-08-17 18:58:19 | INFO | train_inner | epoch 019:     20 / 94 loss=7.703, nll_loss=4.709, ppl=26.15, wps=1068.4, ups=0.5, wpb=2140.5, bsz=104, num_updates=1712, lr=2.0544e-05, gnorm=2.348, train_wall=4, gb_free=14.7, wall=5387
2024-08-17 18:58:24 | INFO | train_inner | epoch 019:     22 / 94 loss=7.874, nll_loss=4.91, ppl=30.06, wps=1271.5, ups=0.37, wpb=3424, bsz=160, num_updates=1714, lr=2.0568e-05, gnorm=2.179, train_wall=5, gb_free=10.4, wall=5393
2024-08-17 18:58:28 | INFO | train_inner | epoch 019:     24 / 94 loss=7.684, nll_loss=4.678, ppl=25.6, wps=918.3, ups=0.51, wpb=1785, bsz=60, num_updates=1716, lr=2.0592e-05, gnorm=2.501, train_wall=4, gb_free=17.7, wall=5397
2024-08-17 18:58:32 | INFO | train_inner | epoch 019:     26 / 94 loss=7.748, nll_loss=4.741, ppl=26.75, wps=1363, ups=0.48, wpb=2861, bsz=104, num_updates=1718, lr=2.0616e-05, gnorm=2.128, train_wall=4, gb_free=20.5, wall=5401
2024-08-17 18:58:37 | INFO | train_inner | epoch 019:     28 / 94 loss=7.87, nll_loss=4.89, ppl=29.66, wps=1410.3, ups=0.43, wpb=3270, bsz=116, num_updates=1720, lr=2.064e-05, gnorm=2.332, train_wall=5, gb_free=12, wall=5405
2024-08-17 18:58:41 | INFO | train_inner | epoch 019:     30 / 94 loss=7.747, nll_loss=4.736, ppl=26.65, wps=1005.5, ups=0.46, wpb=2191.5, bsz=96, num_updates=1722, lr=2.0664e-05, gnorm=2.436, train_wall=4, gb_free=8.6, wall=5410
2024-08-17 18:58:46 | INFO | train_inner | epoch 019:     32 / 94 loss=7.698, nll_loss=4.674, ppl=25.53, wps=986.8, ups=0.45, wpb=2202, bsz=68, num_updates=1724, lr=2.0688e-05, gnorm=2.386, train_wall=4, gb_free=15.4, wall=5414
2024-08-17 18:58:50 | INFO | train_inner | epoch 019:     34 / 94 loss=7.8, nll_loss=4.815, ppl=28.15, wps=1290.2, ups=0.47, wpb=2725.5, bsz=112, num_updates=1726, lr=2.0712e-05, gnorm=2.14, train_wall=4, gb_free=13.5, wall=5418
2024-08-17 18:58:54 | INFO | train_inner | epoch 019:     36 / 94 loss=7.796, nll_loss=4.819, ppl=28.24, wps=1247.6, ups=0.46, wpb=2696, bsz=88, num_updates=1728, lr=2.0736e-05, gnorm=2.107, train_wall=4, gb_free=15.5, wall=5423
2024-08-17 18:58:59 | INFO | train_inner | epoch 019:     38 / 94 loss=7.73, nll_loss=4.746, ppl=26.84, wps=934.4, ups=0.48, wpb=1966.5, bsz=60, num_updates=1730, lr=2.076e-05, gnorm=2.436, train_wall=4, gb_free=14.4, wall=5427
2024-08-17 18:59:03 | INFO | train_inner | epoch 019:     40 / 94 loss=7.809, nll_loss=4.824, ppl=28.33, wps=1322.6, ups=0.43, wpb=3091.5, bsz=96, num_updates=1732, lr=2.0784e-05, gnorm=2.113, train_wall=5, gb_free=13, wall=5432
2024-08-17 18:59:09 | INFO | train_inner | epoch 019:     42 / 94 loss=7.87, nll_loss=4.917, ppl=30.2, wps=1267.5, ups=0.37, wpb=3450.5, bsz=172, num_updates=1734, lr=2.0808e-05, gnorm=2.083, train_wall=5, gb_free=9.7, wall=5437
2024-08-17 18:59:13 | INFO | train_inner | epoch 019:     44 / 94 loss=7.647, nll_loss=4.631, ppl=24.77, wps=904.5, ups=0.46, wpb=1979, bsz=120, num_updates=1736, lr=2.0832e-05, gnorm=2.848, train_wall=4, gb_free=15, wall=5441
2024-08-17 18:59:18 | INFO | train_inner | epoch 019:     46 / 94 loss=7.84, nll_loss=4.854, ppl=28.92, wps=1523.7, ups=0.45, wpb=3391.5, bsz=152, num_updates=1738, lr=2.0856e-05, gnorm=1.899, train_wall=4, gb_free=18, wall=5446
2024-08-17 18:59:22 | INFO | train_inner | epoch 019:     48 / 94 loss=7.813, nll_loss=4.815, ppl=28.14, wps=1308.8, ups=0.42, wpb=3128.5, bsz=124, num_updates=1740, lr=2.088e-05, gnorm=1.984, train_wall=5, gb_free=10.3, wall=5451
2024-08-17 18:59:28 | INFO | train_inner | epoch 019:     50 / 94 loss=7.833, nll_loss=4.846, ppl=28.76, wps=1301.9, ups=0.35, wpb=3671.5, bsz=128, num_updates=1742, lr=2.0904e-05, gnorm=1.948, train_wall=6, gb_free=12.2, wall=5456
2024-08-17 18:59:33 | INFO | train_inner | epoch 019:     52 / 94 loss=7.825, nll_loss=4.855, ppl=28.93, wps=1146.4, ups=0.41, wpb=2777, bsz=112, num_updates=1744, lr=2.0928e-05, gnorm=1.995, train_wall=5, gb_free=15.7, wall=5461
2024-08-17 18:59:38 | INFO | train_inner | epoch 019:     54 / 94 loss=7.733, nll_loss=4.749, ppl=26.89, wps=910.1, ups=0.43, wpb=2123.5, bsz=76, num_updates=1746, lr=2.0952e-05, gnorm=2.366, train_wall=5, gb_free=13.5, wall=5466
2024-08-17 18:59:42 | INFO | train_inner | epoch 019:     56 / 94 loss=7.681, nll_loss=4.673, ppl=25.51, wps=912.1, ups=0.46, wpb=1995, bsz=72, num_updates=1748, lr=2.0976e-05, gnorm=2.397, train_wall=4, gb_free=12.4, wall=5470
2024-08-17 18:59:47 | INFO | train_inner | epoch 019:     58 / 94 loss=7.783, nll_loss=4.812, ppl=28.1, wps=1705.7, ups=0.39, wpb=4345.5, bsz=232, num_updates=1750, lr=2.1e-05, gnorm=1.727, train_wall=5, gb_free=11.9, wall=5475
2024-08-17 18:59:51 | INFO | train_inner | epoch 019:     60 / 94 loss=7.807, nll_loss=4.811, ppl=28.06, wps=1701.5, ups=0.56, wpb=3015, bsz=88, num_updates=1752, lr=2.1024e-05, gnorm=2.058, train_wall=4, gb_free=17.2, wall=5479
2024-08-17 18:59:55 | INFO | train_inner | epoch 019:     62 / 94 loss=7.836, nll_loss=4.853, ppl=28.91, wps=942.8, ups=0.47, wpb=2018.5, bsz=56, num_updates=1754, lr=2.1048e-05, gnorm=2.531, train_wall=4, gb_free=13.5, wall=5483
2024-08-17 19:00:05 | INFO | train_inner | epoch 019:     64 / 94 loss=7.76, nll_loss=4.768, ppl=27.24, wps=717.6, ups=0.2, wpb=3615.5, bsz=224, num_updates=1756, lr=2.1072e-05, gnorm=1.826, train_wall=10, gb_free=10.5, wall=5493
2024-08-17 19:00:10 | INFO | train_inner | epoch 019:     66 / 94 loss=7.848, nll_loss=4.872, ppl=29.29, wps=1735.9, ups=0.39, wpb=4466, bsz=180, num_updates=1758, lr=2.1096e-05, gnorm=1.708, train_wall=5, gb_free=13.3, wall=5498
2024-08-17 19:00:14 | INFO | train_inner | epoch 019:     68 / 94 loss=7.811, nll_loss=4.835, ppl=28.54, wps=1178.2, ups=0.47, wpb=2512, bsz=80, num_updates=1760, lr=2.112e-05, gnorm=2.163, train_wall=4, gb_free=12.1, wall=5503
2024-08-17 19:00:19 | INFO | train_inner | epoch 019:     70 / 94 loss=7.839, nll_loss=4.89, ppl=29.66, wps=1283.5, ups=0.44, wpb=2921, bsz=132, num_updates=1762, lr=2.1144e-05, gnorm=2.041, train_wall=5, gb_free=15.7, wall=5507
2024-08-17 19:00:23 | INFO | train_inner | epoch 019:     72 / 94 loss=7.889, nll_loss=4.952, ppl=30.96, wps=1782.7, ups=0.45, wpb=3937.5, bsz=172, num_updates=1764, lr=2.1168e-05, gnorm=1.892, train_wall=4, gb_free=19, wall=5512
2024-08-17 19:00:28 | INFO | train_inner | epoch 019:     74 / 94 loss=7.717, nll_loss=4.727, ppl=26.49, wps=951.6, ups=0.44, wpb=2184.5, bsz=143.5, num_updates=1766, lr=2.1192e-05, gnorm=2.603, train_wall=5, gb_free=14, wall=5516
2024-08-17 19:00:33 | INFO | train_inner | epoch 019:     76 / 94 loss=7.878, nll_loss=4.898, ppl=29.82, wps=1402.3, ups=0.42, wpb=3311.5, bsz=116, num_updates=1768, lr=2.1216e-05, gnorm=1.976, train_wall=5, gb_free=14.5, wall=5521
2024-08-17 19:00:37 | INFO | train_inner | epoch 019:     78 / 94 loss=7.675, nll_loss=4.637, ppl=24.88, wps=802.2, ups=0.46, wpb=1745.5, bsz=56, num_updates=1770, lr=2.124e-05, gnorm=2.591, train_wall=4, gb_free=13.8, wall=5525
2024-08-17 19:00:41 | INFO | train_inner | epoch 019:     80 / 94 loss=7.812, nll_loss=4.814, ppl=28.12, wps=1074.7, ups=0.51, wpb=2104, bsz=64, num_updates=1772, lr=2.1264e-05, gnorm=2.401, train_wall=4, gb_free=14.6, wall=5529
2024-08-17 19:00:46 | INFO | train_inner | epoch 019:     82 / 94 loss=7.745, nll_loss=4.736, ppl=26.65, wps=1062.4, ups=0.42, wpb=2548, bsz=104, num_updates=1774, lr=2.1288e-05, gnorm=2.156, train_wall=5, gb_free=9, wall=5534
2024-08-17 19:00:51 | INFO | train_inner | epoch 019:     84 / 94 loss=7.853, nll_loss=4.884, ppl=29.52, wps=1244.1, ups=0.39, wpb=3225, bsz=180, num_updates=1776, lr=2.1312e-05, gnorm=2.093, train_wall=5, gb_free=11.1, wall=5539
2024-08-17 19:00:56 | INFO | train_inner | epoch 019:     86 / 94 loss=7.78, nll_loss=4.805, ppl=27.95, wps=1214.6, ups=0.41, wpb=2999, bsz=144, num_updates=1778, lr=2.1336e-05, gnorm=1.981, train_wall=5, gb_free=10, wall=5544
2024-08-17 19:01:00 | INFO | train_inner | epoch 019:     88 / 94 loss=7.905, nll_loss=4.956, ppl=31.04, wps=1655.8, ups=0.47, wpb=3550.5, bsz=100, num_updates=1780, lr=2.136e-05, gnorm=2.194, train_wall=4, gb_free=13.7, wall=5548
2024-08-17 19:01:05 | INFO | train_inner | epoch 019:     90 / 94 loss=7.804, nll_loss=4.839, ppl=28.62, wps=1504.6, ups=0.44, wpb=3439, bsz=135, num_updates=1782, lr=2.1384e-05, gnorm=2.296, train_wall=5, gb_free=11.9, wall=5553
2024-08-17 19:01:09 | INFO | train_inner | epoch 019:     92 / 94 loss=7.615, nll_loss=4.595, ppl=24.17, wps=909.3, ups=0.43, wpb=2123, bsz=88, num_updates=1784, lr=2.1408e-05, gnorm=2.311, train_wall=5, gb_free=15.1, wall=5558
2024-08-17 19:01:13 | INFO | train_inner | epoch 019:     94 / 94 loss=7.905, nll_loss=4.956, ppl=31.04, wps=1491.9, ups=0.51, wpb=2899.5, bsz=96, num_updates=1786, lr=2.1432e-05, gnorm=2.51, train_wall=4, gb_free=15.2, wall=5562
2024-08-17 19:01:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15346.08203125Mb; avail=239720.05078125Mb
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15346.08203125Mb; avail=239720.05078125Mb
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005378
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15346.08203125Mb; avail=239720.05078125Mb
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004771
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011053
2024-08-17 19:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15346.08203125Mb; avail=239720.05078125Mb
2024-08-17 19:01:27 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.959 | nll_loss 4.835 | ppl 28.54 | wps 2426 | wpb 944.1 | bsz 40.1 | num_updates 1786 | best_loss 7.944
2024-08-17 19:01:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1786 updates
2024-08-17 19:01:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:02:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:02:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 19 @ 1786 updates, score 7.959) (writing took 40.56315876310691 seconds)
2024-08-17 19:02:07 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-08-17 19:02:07 | INFO | train | epoch 019 | loss 7.798 | nll_loss 4.815 | ppl 28.15 | wps 975 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 1786 | lr 2.1432e-05 | gnorm 2.196 | train_wall 219 | gb_free 15.2 | wall 5616
2024-08-17 19:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000783
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17702.50390625Mb; avail=237363.625Mb
2024-08-17 19:02:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000212
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001521
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17702.50390625Mb; avail=237363.625Mb
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17702.5Mb; avail=237363.625Mb
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000531
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002382
2024-08-17 19:02:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17702.5Mb; avail=237363.625Mb
2024-08-17 19:02:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:02:07 | INFO | fairseq.trainer | begin training epoch 20
2024-08-17 19:02:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:02:12 | INFO | train_inner | epoch 020:      2 / 94 loss=7.592, nll_loss=4.549, ppl=23.41, wps=87.4, ups=0.03, wpb=2559.5, bsz=152, num_updates=1788, lr=2.1456e-05, gnorm=2.485, train_wall=4, gb_free=17, wall=5620
2024-08-17 19:02:17 | INFO | train_inner | epoch 020:      4 / 94 loss=7.803, nll_loss=4.799, ppl=27.84, wps=1425.9, ups=0.39, wpb=3638, bsz=164, num_updates=1790, lr=2.148e-05, gnorm=1.943, train_wall=5, gb_free=13.7, wall=5625
2024-08-17 19:02:22 | INFO | train_inner | epoch 020:      6 / 94 loss=7.693, nll_loss=4.657, ppl=25.23, wps=922.5, ups=0.42, wpb=2171.5, bsz=80, num_updates=1792, lr=2.1504e-05, gnorm=2.428, train_wall=5, gb_free=15.6, wall=5630
2024-08-17 19:02:26 | INFO | train_inner | epoch 020:      8 / 94 loss=7.692, nll_loss=4.67, ppl=25.46, wps=1070.9, ups=0.46, wpb=2318.5, bsz=84, num_updates=1794, lr=2.1528e-05, gnorm=2.405, train_wall=4, gb_free=16.1, wall=5634
2024-08-17 19:02:30 | INFO | train_inner | epoch 020:     10 / 94 loss=7.618, nll_loss=4.594, ppl=24.15, wps=820.6, ups=0.46, wpb=1801, bsz=72, num_updates=1796, lr=2.1552e-05, gnorm=2.705, train_wall=4, gb_free=14.6, wall=5639
2024-08-17 19:02:34 | INFO | train_inner | epoch 020:     12 / 94 loss=7.662, nll_loss=4.654, ppl=25.18, wps=926.2, ups=0.49, wpb=1903.5, bsz=60, num_updates=1798, lr=2.1576e-05, gnorm=2.423, train_wall=4, gb_free=11.3, wall=5643
2024-08-17 19:02:39 | INFO | train_inner | epoch 020:     14 / 94 loss=7.565, nll_loss=4.533, ppl=23.16, wps=787.2, ups=0.44, wpb=1793, bsz=96, num_updates=1800, lr=2.16e-05, gnorm=2.536, train_wall=5, gb_free=9.8, wall=5647
2024-08-17 19:02:44 | INFO | train_inner | epoch 020:     16 / 94 loss=7.747, nll_loss=4.731, ppl=26.56, wps=1153.1, ups=0.4, wpb=2884, bsz=112, num_updates=1802, lr=2.1624e-05, gnorm=2.27, train_wall=5, gb_free=13.7, wall=5652
2024-08-17 19:02:48 | INFO | train_inner | epoch 020:     18 / 94 loss=7.738, nll_loss=4.71, ppl=26.17, wps=1257.5, ups=0.49, wpb=2585, bsz=91, num_updates=1804, lr=2.1648e-05, gnorm=2.551, train_wall=4, gb_free=14.5, wall=5656
2024-08-17 19:02:52 | INFO | train_inner | epoch 020:     20 / 94 loss=7.551, nll_loss=4.514, ppl=22.84, wps=846.6, ups=0.47, wpb=1786.5, bsz=92, num_updates=1806, lr=2.1672e-05, gnorm=2.51, train_wall=4, gb_free=13.6, wall=5661
2024-08-17 19:02:56 | INFO | train_inner | epoch 020:     22 / 94 loss=7.73, nll_loss=4.734, ppl=26.61, wps=1578, ups=0.53, wpb=2962, bsz=144, num_updates=1808, lr=2.1696e-05, gnorm=2.114, train_wall=4, gb_free=17, wall=5664
2024-08-17 19:03:01 | INFO | train_inner | epoch 020:     24 / 94 loss=7.764, nll_loss=4.766, ppl=27.21, wps=1421.5, ups=0.39, wpb=3635.5, bsz=140, num_updates=1810, lr=2.172e-05, gnorm=1.952, train_wall=5, gb_free=12.1, wall=5670
2024-08-17 19:03:07 | INFO | train_inner | epoch 020:     26 / 94 loss=7.81, nll_loss=4.823, ppl=28.3, wps=1556.5, ups=0.34, wpb=4552, bsz=184, num_updates=1812, lr=2.1744e-05, gnorm=1.688, train_wall=6, gb_free=12.3, wall=5675
2024-08-17 19:03:12 | INFO | train_inner | epoch 020:     28 / 94 loss=7.691, nll_loss=4.691, ppl=25.84, wps=660.7, ups=0.39, wpb=1709.5, bsz=68, num_updates=1814, lr=2.1768e-05, gnorm=2.575, train_wall=5, gb_free=11.2, wall=5681
2024-08-17 19:03:17 | INFO | train_inner | epoch 020:     30 / 94 loss=7.723, nll_loss=4.733, ppl=26.6, wps=1056.5, ups=0.43, wpb=2446.5, bsz=144, num_updates=1816, lr=2.1792e-05, gnorm=2.354, train_wall=5, gb_free=14.6, wall=5685
2024-08-17 19:03:22 | INFO | train_inner | epoch 020:     32 / 94 loss=7.839, nll_loss=4.863, ppl=29.11, wps=1465.9, ups=0.42, wpb=3524, bsz=148, num_updates=1818, lr=2.1816e-05, gnorm=2.006, train_wall=5, gb_free=13.6, wall=5690
2024-08-17 19:03:27 | INFO | train_inner | epoch 020:     34 / 94 loss=7.786, nll_loss=4.791, ppl=27.69, wps=1211.8, ups=0.39, wpb=3120.5, bsz=151.5, num_updates=1820, lr=2.184e-05, gnorm=2.503, train_wall=5, gb_free=8, wall=5695
2024-08-17 19:03:32 | INFO | train_inner | epoch 020:     36 / 94 loss=7.773, nll_loss=4.764, ppl=27.17, wps=1432.5, ups=0.39, wpb=3683.5, bsz=184, num_updates=1822, lr=2.1864e-05, gnorm=1.802, train_wall=5, gb_free=12.3, wall=5700
2024-08-17 19:03:37 | INFO | train_inner | epoch 020:     38 / 94 loss=7.801, nll_loss=4.815, ppl=28.15, wps=1569.9, ups=0.4, wpb=3954, bsz=180, num_updates=1824, lr=2.1888e-05, gnorm=1.801, train_wall=5, gb_free=10.8, wall=5705
2024-08-17 19:03:46 | INFO | train_inner | epoch 020:     40 / 94 loss=7.889, nll_loss=4.926, ppl=30.4, wps=676.7, ups=0.22, wpb=3138, bsz=92, num_updates=1826, lr=2.1912e-05, gnorm=2.102, train_wall=9, gb_free=14.8, wall=5715
2024-08-17 19:03:51 | INFO | train_inner | epoch 020:     42 / 94 loss=7.536, nll_loss=4.487, ppl=22.42, wps=755.5, ups=0.45, wpb=1697, bsz=80, num_updates=1828, lr=2.1936e-05, gnorm=2.549, train_wall=4, gb_free=14.1, wall=5719
2024-08-17 19:03:55 | INFO | train_inner | epoch 020:     44 / 94 loss=7.688, nll_loss=4.677, ppl=25.59, wps=914.2, ups=0.45, wpb=2032, bsz=64, num_updates=1830, lr=2.196e-05, gnorm=2.501, train_wall=4, gb_free=12, wall=5724
2024-08-17 19:04:00 | INFO | train_inner | epoch 020:     46 / 94 loss=7.831, nll_loss=4.869, ppl=29.22, wps=1286.2, ups=0.47, wpb=2741.5, bsz=120, num_updates=1832, lr=2.1984e-05, gnorm=2.336, train_wall=4, gb_free=14.3, wall=5728
2024-08-17 19:04:04 | INFO | train_inner | epoch 020:     48 / 94 loss=7.825, nll_loss=4.839, ppl=28.62, wps=1744.3, ups=0.43, wpb=4029.5, bsz=136, num_updates=1834, lr=2.2008e-05, gnorm=1.791, train_wall=5, gb_free=12, wall=5732
2024-08-17 19:04:08 | INFO | train_inner | epoch 020:     50 / 94 loss=7.72, nll_loss=4.715, ppl=26.27, wps=1192.5, ups=0.51, wpb=2349, bsz=96, num_updates=1836, lr=2.2032e-05, gnorm=2.384, train_wall=4, gb_free=13.7, wall=5736
2024-08-17 19:04:13 | INFO | train_inner | epoch 020:     52 / 94 loss=7.791, nll_loss=4.78, ppl=27.48, wps=1254.4, ups=0.41, wpb=3070, bsz=104, num_updates=1838, lr=2.2056e-05, gnorm=2.019, train_wall=5, gb_free=8.1, wall=5741
2024-08-17 19:04:18 | INFO | train_inner | epoch 020:     54 / 94 loss=7.835, nll_loss=4.839, ppl=28.63, wps=1377.4, ups=0.41, wpb=3329.5, bsz=92, num_updates=1840, lr=2.208e-05, gnorm=2.189, train_wall=5, gb_free=15.7, wall=5746
2024-08-17 19:04:23 | INFO | train_inner | epoch 020:     56 / 94 loss=7.84, nll_loss=4.874, ppl=29.32, wps=1489.6, ups=0.37, wpb=3980, bsz=236, num_updates=1842, lr=2.2104e-05, gnorm=1.832, train_wall=5, gb_free=11.5, wall=5751
2024-08-17 19:04:27 | INFO | train_inner | epoch 020:     58 / 94 loss=7.741, nll_loss=4.743, ppl=26.79, wps=1301, ups=0.46, wpb=2812.5, bsz=92, num_updates=1844, lr=2.2128e-05, gnorm=2.129, train_wall=4, gb_free=14.6, wall=5756
2024-08-17 19:04:33 | INFO | train_inner | epoch 020:     60 / 94 loss=7.605, nll_loss=4.58, ppl=23.91, wps=802.8, ups=0.39, wpb=2045.5, bsz=108, num_updates=1846, lr=2.2152e-05, gnorm=2.161, train_wall=5, gb_free=10.3, wall=5761
2024-08-17 19:04:36 | INFO | train_inner | epoch 020:     62 / 94 loss=7.814, nll_loss=4.831, ppl=28.46, wps=1684.9, ups=0.52, wpb=3221.5, bsz=96, num_updates=1848, lr=2.2176e-05, gnorm=2.071, train_wall=4, gb_free=21.3, wall=5765
2024-08-17 19:04:40 | INFO | train_inner | epoch 020:     64 / 94 loss=7.664, nll_loss=4.646, ppl=25.03, wps=1510.8, ups=0.51, wpb=2987, bsz=112, num_updates=1850, lr=2.22e-05, gnorm=1.985, train_wall=4, gb_free=12.6, wall=5769
2024-08-17 19:04:45 | INFO | train_inner | epoch 020:     66 / 94 loss=7.643, nll_loss=4.627, ppl=24.71, wps=1099, ups=0.44, wpb=2525, bsz=160, num_updates=1852, lr=2.2224e-05, gnorm=2.278, train_wall=5, gb_free=13.3, wall=5773
2024-08-17 19:04:49 | INFO | train_inner | epoch 020:     68 / 94 loss=7.845, nll_loss=4.874, ppl=29.32, wps=1484.1, ups=0.47, wpb=3159, bsz=128, num_updates=1854, lr=2.2248e-05, gnorm=2.158, train_wall=4, gb_free=13.7, wall=5777
2024-08-17 19:04:53 | INFO | train_inner | epoch 020:     70 / 94 loss=7.729, nll_loss=4.713, ppl=26.24, wps=1353.4, ups=0.51, wpb=2641.5, bsz=88, num_updates=1856, lr=2.2272e-05, gnorm=2.044, train_wall=4, gb_free=14.6, wall=5781
2024-08-17 19:04:58 | INFO | train_inner | epoch 020:     72 / 94 loss=7.577, nll_loss=4.539, ppl=23.25, wps=698, ups=0.43, wpb=1630.5, bsz=88, num_updates=1858, lr=2.2296e-05, gnorm=2.621, train_wall=5, gb_free=12.9, wall=5786
2024-08-17 19:05:03 | INFO | train_inner | epoch 020:     74 / 94 loss=7.782, nll_loss=4.8, ppl=27.85, wps=1542.7, ups=0.41, wpb=3751, bsz=208, num_updates=1860, lr=2.232e-05, gnorm=1.737, train_wall=5, gb_free=14.6, wall=5791
2024-08-17 19:05:08 | INFO | train_inner | epoch 020:     76 / 94 loss=7.819, nll_loss=4.833, ppl=28.51, wps=1423.3, ups=0.4, wpb=3589, bsz=132, num_updates=1862, lr=2.2344e-05, gnorm=2.026, train_wall=5, gb_free=15.2, wall=5796
2024-08-17 19:05:12 | INFO | train_inner | epoch 020:     78 / 94 loss=7.662, nll_loss=4.64, ppl=24.93, wps=924.7, ups=0.44, wpb=2080.5, bsz=76, num_updates=1864, lr=2.2368e-05, gnorm=2.54, train_wall=4, gb_free=12.6, wall=5800
2024-08-17 19:05:17 | INFO | train_inner | epoch 020:     80 / 94 loss=7.73, nll_loss=4.724, ppl=26.42, wps=1362.9, ups=0.43, wpb=3188, bsz=140, num_updates=1866, lr=2.2392e-05, gnorm=2.084, train_wall=5, gb_free=14, wall=5805
2024-08-17 19:05:21 | INFO | train_inner | epoch 020:     82 / 94 loss=7.733, nll_loss=4.727, ppl=26.48, wps=1306.8, ups=0.45, wpb=2932, bsz=112, num_updates=1868, lr=2.2416e-05, gnorm=2.001, train_wall=4, gb_free=13.1, wall=5810
2024-08-17 19:05:26 | INFO | train_inner | epoch 020:     84 / 94 loss=7.717, nll_loss=4.697, ppl=25.94, wps=1440.1, ups=0.47, wpb=3038, bsz=76, num_updates=1870, lr=2.244e-05, gnorm=2.016, train_wall=4, gb_free=17.7, wall=5814
2024-08-17 19:05:30 | INFO | train_inner | epoch 020:     86 / 94 loss=7.837, nll_loss=4.855, ppl=28.95, wps=1837.6, ups=0.51, wpb=3625.5, bsz=124, num_updates=1872, lr=2.2464e-05, gnorm=1.956, train_wall=4, gb_free=17.1, wall=5818
2024-08-17 19:05:34 | INFO | train_inner | epoch 020:     88 / 94 loss=7.722, nll_loss=4.732, ppl=26.57, wps=1268.9, ups=0.4, wpb=3144.5, bsz=192, num_updates=1874, lr=2.2488e-05, gnorm=2.094, train_wall=5, gb_free=12, wall=5823
2024-08-17 19:05:39 | INFO | train_inner | epoch 020:     90 / 94 loss=7.803, nll_loss=4.822, ppl=28.28, wps=1291.5, ups=0.46, wpb=2822.5, bsz=104, num_updates=1876, lr=2.2512e-05, gnorm=2.192, train_wall=4, gb_free=15, wall=5827
2024-08-17 19:05:43 | INFO | train_inner | epoch 020:     92 / 94 loss=7.793, nll_loss=4.806, ppl=27.98, wps=1204.4, ups=0.44, wpb=2738, bsz=112, num_updates=1878, lr=2.2536e-05, gnorm=2.264, train_wall=5, gb_free=14.3, wall=5832
2024-08-17 19:05:47 | INFO | train_inner | epoch 020:     94 / 94 loss=7.722, nll_loss=4.711, ppl=26.2, wps=1242.1, ups=0.57, wpb=2163, bsz=96, num_updates=1880, lr=2.256e-05, gnorm=2.318, train_wall=3, gb_free=16.2, wall=5835
2024-08-17 19:05:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18030.4375Mb; avail=237035.69140625Mb
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000556
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18030.4375Mb; avail=237035.69140625Mb
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005403
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18030.4375Mb; avail=237035.69140625Mb
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004792
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011082
2024-08-17 19:05:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18030.4375Mb; avail=237035.69140625Mb
2024-08-17 19:06:00 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.941 | nll_loss 4.833 | ppl 28.5 | wps 2422.8 | wpb 944.1 | bsz 40.1 | num_updates 1880 | best_loss 7.941
2024-08-17 19:06:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1880 updates
2024-08-17 19:06:00 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 19:06:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 19:07:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 20 @ 1880 updates, score 7.941) (writing took 61.61069100210443 seconds)
2024-08-17 19:07:02 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-08-17 19:07:02 | INFO | train | epoch 020 | loss 7.751 | nll_loss 4.751 | ppl 26.92 | wps 906 | ups 0.32 | wpb 2840.2 | bsz 119.4 | num_updates 1880 | lr 2.256e-05 | gnorm 2.201 | train_wall 219 | gb_free 16.2 | wall 5910
2024-08-17 19:07:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:07:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:07:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:07:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000828
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37984.1015625Mb; avail=217082.05859375Mb
2024-08-17 19:07:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000162
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001537
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37984.59375Mb; avail=217081.56640625Mb
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37984.1015625Mb; avail=217082.05859375Mb
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000561
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002455
2024-08-17 19:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37984.59375Mb; avail=217081.56640625Mb
2024-08-17 19:07:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:07:02 | INFO | fairseq.trainer | begin training epoch 21
2024-08-17 19:07:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:07:06 | INFO | train_inner | epoch 021:      2 / 94 loss=7.765, nll_loss=4.782, ppl=27.51, wps=71.2, ups=0.03, wpb=2821, bsz=100, num_updates=1882, lr=2.2584e-05, gnorm=2.157, train_wall=4, gb_free=9.6, wall=5914
2024-08-17 19:07:10 | INFO | train_inner | epoch 021:      4 / 94 loss=7.729, nll_loss=4.737, ppl=26.67, wps=1207, ups=0.47, wpb=2548.5, bsz=96, num_updates=1884, lr=2.2608e-05, gnorm=2.197, train_wall=4, gb_free=14.5, wall=5919
2024-08-17 19:07:15 | INFO | train_inner | epoch 021:      6 / 94 loss=7.749, nll_loss=4.744, ppl=26.8, wps=1454.3, ups=0.39, wpb=3726, bsz=148, num_updates=1886, lr=2.2632e-05, gnorm=1.682, train_wall=5, gb_free=10.9, wall=5924
2024-08-17 19:07:20 | INFO | train_inner | epoch 021:      8 / 94 loss=7.715, nll_loss=4.709, ppl=26.16, wps=1726.8, ups=0.42, wpb=4070.5, bsz=184, num_updates=1888, lr=2.2656e-05, gnorm=1.717, train_wall=5, gb_free=14.4, wall=5928
2024-08-17 19:07:24 | INFO | train_inner | epoch 021:     10 / 94 loss=7.836, nll_loss=4.872, ppl=29.29, wps=1634.8, ups=0.47, wpb=3462.5, bsz=156, num_updates=1890, lr=2.268e-05, gnorm=2.05, train_wall=4, gb_free=14.8, wall=5933
2024-08-17 19:07:29 | INFO | train_inner | epoch 021:     12 / 94 loss=7.637, nll_loss=4.605, ppl=24.34, wps=1039.1, ups=0.4, wpb=2601, bsz=120, num_updates=1892, lr=2.2704e-05, gnorm=2.323, train_wall=5, gb_free=9.5, wall=5938
2024-08-17 19:07:35 | INFO | train_inner | epoch 021:     14 / 94 loss=7.582, nll_loss=4.533, ppl=23.15, wps=850.9, ups=0.38, wpb=2256.5, bsz=104, num_updates=1894, lr=2.2728e-05, gnorm=2.245, train_wall=5, gb_free=10.2, wall=5943
2024-08-17 19:07:39 | INFO | train_inner | epoch 021:     16 / 94 loss=7.642, nll_loss=4.619, ppl=24.57, wps=1186.1, ups=0.43, wpb=2730, bsz=148, num_updates=1896, lr=2.2752e-05, gnorm=2.055, train_wall=5, gb_free=14.4, wall=5948
2024-08-17 19:07:49 | INFO | train_inner | epoch 021:     18 / 94 loss=7.626, nll_loss=4.598, ppl=24.21, wps=640.2, ups=0.2, wpb=3217, bsz=164, num_updates=1898, lr=2.2776e-05, gnorm=1.896, train_wall=10, gb_free=14.5, wall=5958
2024-08-17 19:07:54 | INFO | train_inner | epoch 021:     20 / 94 loss=7.717, nll_loss=4.72, ppl=26.36, wps=1223.5, ups=0.45, wpb=2725.5, bsz=136, num_updates=1900, lr=2.28e-05, gnorm=2.206, train_wall=4, gb_free=18, wall=5962
2024-08-17 19:07:58 | INFO | train_inner | epoch 021:     22 / 94 loss=7.59, nll_loss=4.553, ppl=23.48, wps=1043.8, ups=0.43, wpb=2404.5, bsz=116, num_updates=1902, lr=2.2824e-05, gnorm=2.235, train_wall=5, gb_free=12.4, wall=5967
2024-08-17 19:08:03 | INFO | train_inner | epoch 021:     24 / 94 loss=7.484, nll_loss=4.409, ppl=21.24, wps=885.2, ups=0.48, wpb=1835, bsz=84, num_updates=1904, lr=2.2848e-05, gnorm=2.425, train_wall=4, gb_free=18.1, wall=5971
2024-08-17 19:08:07 | INFO | train_inner | epoch 021:     26 / 94 loss=7.731, nll_loss=4.733, ppl=26.6, wps=1136.7, ups=0.43, wpb=2624, bsz=132, num_updates=1906, lr=2.2872e-05, gnorm=2.351, train_wall=5, gb_free=14.3, wall=5976
2024-08-17 19:08:12 | INFO | train_inner | epoch 021:     28 / 94 loss=7.713, nll_loss=4.698, ppl=25.95, wps=1074.7, ups=0.43, wpb=2485, bsz=131.5, num_updates=1908, lr=2.2896e-05, gnorm=2.133, train_wall=5, gb_free=16.2, wall=5980
2024-08-17 19:08:17 | INFO | train_inner | epoch 021:     30 / 94 loss=7.604, nll_loss=4.572, ppl=23.79, wps=805.7, ups=0.41, wpb=1965.5, bsz=108, num_updates=1910, lr=2.292e-05, gnorm=2.49, train_wall=5, gb_free=14.7, wall=5985
2024-08-17 19:08:22 | INFO | train_inner | epoch 021:     32 / 94 loss=7.801, nll_loss=4.816, ppl=28.17, wps=1463.7, ups=0.4, wpb=3698, bsz=200, num_updates=1912, lr=2.2944e-05, gnorm=1.773, train_wall=5, gb_free=13.5, wall=5990
2024-08-17 19:08:26 | INFO | train_inner | epoch 021:     34 / 94 loss=7.707, nll_loss=4.687, ppl=25.76, wps=1537.3, ups=0.5, wpb=3050.5, bsz=140, num_updates=1914, lr=2.2968e-05, gnorm=2.003, train_wall=4, gb_free=12.8, wall=5994
2024-08-17 19:08:31 | INFO | train_inner | epoch 021:     36 / 94 loss=7.636, nll_loss=4.6, ppl=24.25, wps=1104, ups=0.41, wpb=2712.5, bsz=132, num_updates=1916, lr=2.2992e-05, gnorm=2.117, train_wall=5, gb_free=13.7, wall=5999
2024-08-17 19:08:35 | INFO | train_inner | epoch 021:     38 / 94 loss=7.702, nll_loss=4.679, ppl=25.61, wps=1410.9, ups=0.43, wpb=3260.5, bsz=136, num_updates=1918, lr=2.3016e-05, gnorm=2.118, train_wall=5, gb_free=12.6, wall=6004
2024-08-17 19:08:39 | INFO | train_inner | epoch 021:     40 / 94 loss=7.799, nll_loss=4.824, ppl=28.33, wps=1184.2, ups=0.49, wpb=2403.5, bsz=92, num_updates=1920, lr=2.304e-05, gnorm=2.556, train_wall=4, gb_free=18.7, wall=6008
2024-08-17 19:08:44 | INFO | train_inner | epoch 021:     42 / 94 loss=7.577, nll_loss=4.535, ppl=23.18, wps=1012.7, ups=0.44, wpb=2276, bsz=72, num_updates=1922, lr=2.3064e-05, gnorm=2.535, train_wall=4, gb_free=13.5, wall=6012
2024-08-17 19:08:49 | INFO | train_inner | epoch 021:     44 / 94 loss=7.802, nll_loss=4.83, ppl=28.45, wps=1116.5, ups=0.42, wpb=2688.5, bsz=136, num_updates=1924, lr=2.3088e-05, gnorm=2.151, train_wall=5, gb_free=15.8, wall=6017
2024-08-17 19:08:53 | INFO | train_inner | epoch 021:     46 / 94 loss=7.624, nll_loss=4.567, ppl=23.71, wps=1399.4, ups=0.45, wpb=3096, bsz=76, num_updates=1926, lr=2.3112e-05, gnorm=2.079, train_wall=4, gb_free=14.8, wall=6021
2024-08-17 19:08:58 | INFO | train_inner | epoch 021:     48 / 94 loss=7.678, nll_loss=4.642, ppl=24.97, wps=1480.2, ups=0.39, wpb=3768.5, bsz=156, num_updates=1928, lr=2.3136e-05, gnorm=1.773, train_wall=5, gb_free=13.3, wall=6026
2024-08-17 19:09:03 | INFO | train_inner | epoch 021:     50 / 94 loss=7.763, nll_loss=4.754, ppl=26.99, wps=1566.3, ups=0.44, wpb=3559, bsz=104, num_updates=1930, lr=2.316e-05, gnorm=1.901, train_wall=5, gb_free=13.2, wall=6031
2024-08-17 19:09:07 | INFO | train_inner | epoch 021:     52 / 94 loss=7.599, nll_loss=4.562, ppl=23.63, wps=1450.5, ups=0.47, wpb=3064, bsz=80, num_updates=1932, lr=2.3184e-05, gnorm=1.94, train_wall=4, gb_free=12.4, wall=6035
2024-08-17 19:09:11 | INFO | train_inner | epoch 021:     54 / 94 loss=7.776, nll_loss=4.801, ppl=27.88, wps=1228.3, ups=0.49, wpb=2498.5, bsz=56, num_updates=1934, lr=2.3208e-05, gnorm=2.374, train_wall=4, gb_free=14, wall=6039
2024-08-17 19:09:16 | INFO | train_inner | epoch 021:     56 / 94 loss=7.816, nll_loss=4.835, ppl=28.54, wps=1377.3, ups=0.37, wpb=3695.5, bsz=140, num_updates=1936, lr=2.3232e-05, gnorm=2.044, train_wall=5, gb_free=11, wall=6045
2024-08-17 19:09:20 | INFO | train_inner | epoch 021:     58 / 94 loss=7.793, nll_loss=4.805, ppl=27.95, wps=1600.1, ups=0.51, wpb=3147, bsz=108, num_updates=1938, lr=2.3256e-05, gnorm=2.232, train_wall=4, gb_free=17.3, wall=6049
2024-08-17 19:09:25 | INFO | train_inner | epoch 021:     60 / 94 loss=7.582, nll_loss=4.532, ppl=23.13, wps=1120.6, ups=0.46, wpb=2445, bsz=116, num_updates=1940, lr=2.328e-05, gnorm=2.313, train_wall=4, gb_free=18.8, wall=6053
2024-08-17 19:09:29 | INFO | train_inner | epoch 021:     62 / 94 loss=7.752, nll_loss=4.765, ppl=27.18, wps=1206.3, ups=0.52, wpb=2322.5, bsz=124, num_updates=1942, lr=2.3304e-05, gnorm=2.397, train_wall=4, gb_free=20.8, wall=6057
2024-08-17 19:09:32 | INFO | train_inner | epoch 021:     64 / 94 loss=7.775, nll_loss=4.785, ppl=27.58, wps=1537.7, ups=0.54, wpb=2831.5, bsz=52, num_updates=1944, lr=2.3328e-05, gnorm=2.423, train_wall=4, gb_free=16.7, wall=6060
2024-08-17 19:09:37 | INFO | train_inner | epoch 021:     66 / 94 loss=7.813, nll_loss=4.833, ppl=28.51, wps=1499.3, ups=0.41, wpb=3673, bsz=176, num_updates=1946, lr=2.3352e-05, gnorm=1.924, train_wall=5, gb_free=13.6, wall=6065
2024-08-17 19:09:42 | INFO | train_inner | epoch 021:     68 / 94 loss=7.529, nll_loss=4.467, ppl=22.12, wps=748.4, ups=0.44, wpb=1709, bsz=51, num_updates=1948, lr=2.3376e-05, gnorm=2.833, train_wall=5, gb_free=11.6, wall=6070
2024-08-17 19:09:47 | INFO | train_inner | epoch 021:     70 / 94 loss=7.698, nll_loss=4.678, ppl=25.6, wps=1429.3, ups=0.41, wpb=3508, bsz=200, num_updates=1950, lr=2.34e-05, gnorm=1.903, train_wall=5, gb_free=14.2, wall=6075
2024-08-17 19:09:52 | INFO | train_inner | epoch 021:     72 / 94 loss=7.673, nll_loss=4.638, ppl=24.91, wps=1022.8, ups=0.4, wpb=2533, bsz=108, num_updates=1952, lr=2.3424e-05, gnorm=2.133, train_wall=5, gb_free=15, wall=6080
2024-08-17 19:09:56 | INFO | train_inner | epoch 021:     74 / 94 loss=7.603, nll_loss=4.563, ppl=23.64, wps=990, ups=0.46, wpb=2173, bsz=88, num_updates=1954, lr=2.3448e-05, gnorm=2.343, train_wall=4, gb_free=14.3, wall=6084
2024-08-17 19:10:00 | INFO | train_inner | epoch 021:     76 / 94 loss=7.661, nll_loss=4.635, ppl=24.84, wps=890.9, ups=0.48, wpb=1855, bsz=56, num_updates=1956, lr=2.3472e-05, gnorm=2.638, train_wall=4, gb_free=13.1, wall=6088
2024-08-17 19:10:05 | INFO | train_inner | epoch 021:     78 / 94 loss=7.762, nll_loss=4.783, ppl=27.54, wps=960.8, ups=0.38, wpb=2512, bsz=80, num_updates=1958, lr=2.3496e-05, gnorm=2.293, train_wall=5, gb_free=10.9, wall=6094
2024-08-17 19:10:10 | INFO | train_inner | epoch 021:     80 / 94 loss=7.705, nll_loss=4.714, ppl=26.24, wps=1190.5, ups=0.43, wpb=2786, bsz=136, num_updates=1960, lr=2.352e-05, gnorm=2.678, train_wall=5, gb_free=11.6, wall=6098
2024-08-17 19:10:15 | INFO | train_inner | epoch 021:     82 / 94 loss=7.676, nll_loss=4.655, ppl=25.19, wps=1437.2, ups=0.42, wpb=3444.5, bsz=124, num_updates=1962, lr=2.3544e-05, gnorm=1.894, train_wall=5, gb_free=17.1, wall=6103
2024-08-17 19:10:19 | INFO | train_inner | epoch 021:     84 / 94 loss=7.7, nll_loss=4.681, ppl=25.66, wps=1434.1, ups=0.47, wpb=3041, bsz=84, num_updates=1964, lr=2.3568e-05, gnorm=1.961, train_wall=4, gb_free=17.7, wall=6107
2024-08-17 19:10:24 | INFO | train_inner | epoch 021:     86 / 94 loss=7.777, nll_loss=4.788, ppl=27.63, wps=1184.1, ups=0.38, wpb=3145, bsz=116, num_updates=1966, lr=2.3592e-05, gnorm=1.898, train_wall=5, gb_free=8.6, wall=6113
2024-08-17 19:10:29 | INFO | train_inner | epoch 021:     88 / 94 loss=7.723, nll_loss=4.728, ppl=26.51, wps=1594.9, ups=0.43, wpb=3689.5, bsz=200, num_updates=1968, lr=2.3616e-05, gnorm=1.914, train_wall=5, gb_free=17.4, wall=6117
2024-08-17 19:10:34 | INFO | train_inner | epoch 021:     90 / 94 loss=7.799, nll_loss=4.821, ppl=28.27, wps=1183.9, ups=0.43, wpb=2751, bsz=108, num_updates=1970, lr=2.364e-05, gnorm=2.175, train_wall=5, gb_free=16.2, wall=6122
2024-08-17 19:10:39 | INFO | train_inner | epoch 021:     92 / 94 loss=7.676, nll_loss=4.649, ppl=25.09, wps=903.9, ups=0.4, wpb=2280, bsz=112, num_updates=1972, lr=2.3664e-05, gnorm=2.181, train_wall=5, gb_free=13.5, wall=6127
2024-08-17 19:10:42 | INFO | train_inner | epoch 021:     94 / 94 loss=7.72, nll_loss=4.716, ppl=26.28, wps=1291.1, ups=0.54, wpb=2399.5, bsz=124, num_updates=1974, lr=2.3688e-05, gnorm=2.28, train_wall=4, gb_free=19, wall=6131
2024-08-17 19:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40112.87109375Mb; avail=214953.26171875Mb
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000543
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40112.87109375Mb; avail=214953.26171875Mb
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005331
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40112.87109375Mb; avail=214953.26171875Mb
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004738
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.010948
2024-08-17 19:10:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40112.87109375Mb; avail=214953.26171875Mb
2024-08-17 19:10:56 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.938 | nll_loss 4.815 | ppl 28.14 | wps 2424 | wpb 944.1 | bsz 40.1 | num_updates 1974 | best_loss 7.938
2024-08-17 19:10:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1974 updates
2024-08-17 19:10:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 19:11:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 19:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 21 @ 1974 updates, score 7.938) (writing took 68.43812824878842 seconds)
2024-08-17 19:12:04 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-08-17 19:12:04 | INFO | train | epoch 021 | loss 7.707 | nll_loss 4.697 | ppl 25.94 | wps 883 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 1974 | lr 2.3688e-05 | gnorm 2.169 | train_wall 220 | gb_free 19 | wall 6213
2024-08-17 19:12:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:12:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:12:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:12:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000721
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30251.4296875Mb; avail=224814.6953125Mb
2024-08-17 19:12:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000164
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001530
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30251.4296875Mb; avail=224814.6953125Mb
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000046
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30251.4296875Mb; avail=224814.6953125Mb
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000551
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002436
2024-08-17 19:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30251.4296875Mb; avail=224814.6953125Mb
2024-08-17 19:12:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:12:04 | INFO | fairseq.trainer | begin training epoch 22
2024-08-17 19:12:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:12:08 | INFO | train_inner | epoch 022:      2 / 94 loss=7.627, nll_loss=4.598, ppl=24.23, wps=64, ups=0.02, wpb=2755.5, bsz=112, num_updates=1976, lr=2.3712e-05, gnorm=1.961, train_wall=4, gb_free=15.4, wall=6217
2024-08-17 19:12:13 | INFO | train_inner | epoch 022:      4 / 94 loss=7.589, nll_loss=4.551, ppl=23.44, wps=1192, ups=0.41, wpb=2903.5, bsz=100, num_updates=1978, lr=2.3736e-05, gnorm=1.993, train_wall=5, gb_free=14, wall=6222
2024-08-17 19:12:18 | INFO | train_inner | epoch 022:      6 / 94 loss=7.527, nll_loss=4.458, ppl=21.97, wps=1014.5, ups=0.43, wpb=2333.5, bsz=100, num_updates=1980, lr=2.376e-05, gnorm=2.33, train_wall=5, gb_free=14.8, wall=6226
2024-08-17 19:12:22 | INFO | train_inner | epoch 022:      8 / 94 loss=7.43, nll_loss=4.359, ppl=20.53, wps=839.6, ups=0.47, wpb=1790.5, bsz=100, num_updates=1982, lr=2.3784e-05, gnorm=2.503, train_wall=4, gb_free=13.5, wall=6230
2024-08-17 19:12:28 | INFO | train_inner | epoch 022:     10 / 94 loss=7.782, nll_loss=4.79, ppl=27.67, wps=1538.4, ups=0.37, wpb=4155.5, bsz=176, num_updates=1984, lr=2.3808e-05, gnorm=1.746, train_wall=5, gb_free=11, wall=6236
2024-08-17 19:12:33 | INFO | train_inner | epoch 022:     12 / 94 loss=7.792, nll_loss=4.801, ppl=27.88, wps=1756.9, ups=0.36, wpb=4930, bsz=244, num_updates=1986, lr=2.3832e-05, gnorm=1.673, train_wall=6, gb_free=13.6, wall=6241
2024-08-17 19:12:38 | INFO | train_inner | epoch 022:     14 / 94 loss=7.642, nll_loss=4.62, ppl=24.59, wps=1142.1, ups=0.41, wpb=2773, bsz=168, num_updates=1988, lr=2.3856e-05, gnorm=1.982, train_wall=5, gb_free=14.7, wall=6246
2024-08-17 19:12:42 | INFO | train_inner | epoch 022:     16 / 94 loss=7.6, nll_loss=4.545, ppl=23.34, wps=1012.2, ups=0.51, wpb=1974.5, bsz=64, num_updates=1990, lr=2.388e-05, gnorm=2.456, train_wall=4, gb_free=16.9, wall=6250
2024-08-17 19:12:46 | INFO | train_inner | epoch 022:     18 / 94 loss=7.658, nll_loss=4.628, ppl=24.73, wps=1327, ups=0.47, wpb=2846.5, bsz=84, num_updates=1992, lr=2.3904e-05, gnorm=2.214, train_wall=4, gb_free=15.6, wall=6255
2024-08-17 19:12:51 | INFO | train_inner | epoch 022:     20 / 94 loss=7.611, nll_loss=4.597, ppl=24.2, wps=1241.6, ups=0.39, wpb=3224, bsz=204, num_updates=1994, lr=2.3928e-05, gnorm=1.996, train_wall=5, gb_free=10.8, wall=6260
2024-08-17 19:12:57 | INFO | train_inner | epoch 022:     22 / 94 loss=7.687, nll_loss=4.67, ppl=25.46, wps=1223.3, ups=0.38, wpb=3232.5, bsz=152, num_updates=1996, lr=2.3952e-05, gnorm=1.947, train_wall=5, gb_free=13.1, wall=6265
2024-08-17 19:13:01 | INFO | train_inner | epoch 022:     24 / 94 loss=7.456, nll_loss=4.386, ppl=20.91, wps=1015, ups=0.53, wpb=1932, bsz=60, num_updates=1998, lr=2.3976e-05, gnorm=2.395, train_wall=4, gb_free=17.4, wall=6269
2024-08-17 19:13:06 | INFO | train_inner | epoch 022:     26 / 94 loss=7.659, nll_loss=4.626, ppl=24.69, wps=1451, ups=0.4, wpb=3623.5, bsz=128, num_updates=2000, lr=2.4e-05, gnorm=1.924, train_wall=5, gb_free=12.7, wall=6274
2024-08-17 19:13:09 | INFO | train_inner | epoch 022:     28 / 94 loss=7.565, nll_loss=4.515, ppl=22.86, wps=1155.4, ups=0.52, wpb=2241, bsz=56, num_updates=2002, lr=2.4024e-05, gnorm=2.531, train_wall=4, gb_free=15, wall=6278
2024-08-17 19:13:14 | INFO | train_inner | epoch 022:     30 / 94 loss=7.584, nll_loss=4.531, ppl=23.12, wps=1114.7, ups=0.44, wpb=2530.5, bsz=92, num_updates=2004, lr=2.4048e-05, gnorm=2.145, train_wall=5, gb_free=11.4, wall=6282
2024-08-17 19:13:19 | INFO | train_inner | epoch 022:     32 / 94 loss=7.669, nll_loss=4.636, ppl=24.86, wps=1085.9, ups=0.43, wpb=2514, bsz=72, num_updates=2006, lr=2.4072e-05, gnorm=2.272, train_wall=5, gb_free=13.7, wall=6287
2024-08-17 19:13:23 | INFO | train_inner | epoch 022:     34 / 94 loss=7.741, nll_loss=4.721, ppl=26.37, wps=1864.2, ups=0.43, wpb=4291.5, bsz=132, num_updates=2008, lr=2.4096e-05, gnorm=1.741, train_wall=5, gb_free=12.5, wall=6291
2024-08-17 19:13:28 | INFO | train_inner | epoch 022:     36 / 94 loss=7.551, nll_loss=4.482, ppl=22.34, wps=1083.2, ups=0.46, wpb=2360.5, bsz=96, num_updates=2010, lr=2.412e-05, gnorm=2.203, train_wall=4, gb_free=17.3, wall=6296
2024-08-17 19:13:33 | INFO | train_inner | epoch 022:     38 / 94 loss=7.7, nll_loss=4.699, ppl=25.97, wps=1336.7, ups=0.39, wpb=3390.5, bsz=136, num_updates=2012, lr=2.4144e-05, gnorm=2.01, train_wall=5, gb_free=10.1, wall=6301
2024-08-17 19:13:36 | INFO | train_inner | epoch 022:     40 / 94 loss=7.482, nll_loss=4.412, ppl=21.28, wps=1109.5, ups=0.58, wpb=1920, bsz=60, num_updates=2014, lr=2.4168e-05, gnorm=2.391, train_wall=3, gb_free=17.1, wall=6304
2024-08-17 19:13:40 | INFO | train_inner | epoch 022:     42 / 94 loss=7.613, nll_loss=4.59, ppl=24.09, wps=1030.8, ups=0.55, wpb=1889.5, bsz=52, num_updates=2016, lr=2.4192e-05, gnorm=2.683, train_wall=4, gb_free=18.5, wall=6308
2024-08-17 19:13:44 | INFO | train_inner | epoch 022:     44 / 94 loss=7.489, nll_loss=4.434, ppl=21.61, wps=945.4, ups=0.45, wpb=2096.5, bsz=112, num_updates=2018, lr=2.4216e-05, gnorm=2.439, train_wall=4, gb_free=12.5, wall=6312
2024-08-17 19:13:50 | INFO | train_inner | epoch 022:     46 / 94 loss=7.839, nll_loss=4.878, ppl=29.4, wps=1663.6, ups=0.37, wpb=4540, bsz=240, num_updates=2020, lr=2.424e-05, gnorm=1.803, train_wall=5, gb_free=12.7, wall=6318
2024-08-17 19:13:53 | INFO | train_inner | epoch 022:     48 / 94 loss=7.651, nll_loss=4.608, ppl=24.39, wps=1591.1, ups=0.58, wpb=2720.5, bsz=60, num_updates=2022, lr=2.4264e-05, gnorm=2.156, train_wall=3, gb_free=18.5, wall=6321
2024-08-17 19:13:58 | INFO | train_inner | epoch 022:     50 / 94 loss=7.646, nll_loss=4.616, ppl=24.52, wps=1173.4, ups=0.42, wpb=2804, bsz=148, num_updates=2024, lr=2.4288e-05, gnorm=1.957, train_wall=5, gb_free=12.1, wall=6326
2024-08-17 19:14:02 | INFO | train_inner | epoch 022:     52 / 94 loss=7.636, nll_loss=4.606, ppl=24.34, wps=1380.3, ups=0.47, wpb=2959.5, bsz=112, num_updates=2026, lr=2.4312e-05, gnorm=2.063, train_wall=4, gb_free=13.2, wall=6330
2024-08-17 19:14:06 | INFO | train_inner | epoch 022:     54 / 94 loss=7.676, nll_loss=4.655, ppl=25.2, wps=1158.7, ups=0.48, wpb=2407.5, bsz=56, num_updates=2028, lr=2.4336e-05, gnorm=2.327, train_wall=4, gb_free=10.3, wall=6335
2024-08-17 19:14:12 | INFO | train_inner | epoch 022:     56 / 94 loss=7.727, nll_loss=4.717, ppl=26.3, wps=1445.7, ups=0.35, wpb=4166.5, bsz=172, num_updates=2030, lr=2.436e-05, gnorm=1.857, train_wall=6, gb_free=9.5, wall=6340
2024-08-17 19:14:17 | INFO | train_inner | epoch 022:     58 / 94 loss=7.705, nll_loss=4.707, ppl=26.12, wps=1371.6, ups=0.41, wpb=3308, bsz=164, num_updates=2032, lr=2.4384e-05, gnorm=1.992, train_wall=5, gb_free=10, wall=6345
2024-08-17 19:14:21 | INFO | train_inner | epoch 022:     60 / 94 loss=7.388, nll_loss=4.288, ppl=19.54, wps=664.6, ups=0.43, wpb=1532, bsz=60, num_updates=2034, lr=2.4408e-05, gnorm=2.641, train_wall=5, gb_free=14, wall=6350
2024-08-17 19:14:27 | INFO | train_inner | epoch 022:     62 / 94 loss=7.725, nll_loss=4.708, ppl=26.14, wps=1467.9, ups=0.38, wpb=3834.5, bsz=164, num_updates=2036, lr=2.4432e-05, gnorm=1.813, train_wall=5, gb_free=13.4, wall=6355
2024-08-17 19:14:32 | INFO | train_inner | epoch 022:     64 / 94 loss=7.657, nll_loss=4.63, ppl=24.77, wps=1523.5, ups=0.38, wpb=4038.5, bsz=200, num_updates=2038, lr=2.4456e-05, gnorm=1.757, train_wall=5, gb_free=10.4, wall=6360
2024-08-17 19:14:37 | INFO | train_inner | epoch 022:     66 / 94 loss=7.669, nll_loss=4.652, ppl=25.15, wps=1099, ups=0.39, wpb=2800.5, bsz=112, num_updates=2040, lr=2.448e-05, gnorm=2.089, train_wall=5, gb_free=11.8, wall=6365
2024-08-17 19:14:41 | INFO | train_inner | epoch 022:     68 / 94 loss=7.653, nll_loss=4.613, ppl=24.47, wps=1719.4, ups=0.52, wpb=3309.5, bsz=76, num_updates=2042, lr=2.4504e-05, gnorm=2.158, train_wall=4, gb_free=16.6, wall=6369
2024-08-17 19:14:46 | INFO | train_inner | epoch 022:     70 / 94 loss=7.674, nll_loss=4.672, ppl=25.49, wps=1414.1, ups=0.36, wpb=3886.5, bsz=232, num_updates=2044, lr=2.4528e-05, gnorm=1.713, train_wall=5, gb_free=11.9, wall=6375
2024-08-17 19:14:51 | INFO | train_inner | epoch 022:     72 / 94 loss=7.644, nll_loss=4.624, ppl=24.65, wps=1370.6, ups=0.46, wpb=2990, bsz=152, num_updates=2046, lr=2.4552e-05, gnorm=1.872, train_wall=4, gb_free=14.2, wall=6379
2024-08-17 19:14:55 | INFO | train_inner | epoch 022:     74 / 94 loss=7.621, nll_loss=4.601, ppl=24.26, wps=1077.6, ups=0.45, wpb=2395, bsz=120, num_updates=2048, lr=2.4576e-05, gnorm=2.21, train_wall=4, gb_free=10.5, wall=6384
2024-08-17 19:15:00 | INFO | train_inner | epoch 022:     76 / 94 loss=7.506, nll_loss=4.453, ppl=21.91, wps=1090.5, ups=0.43, wpb=2510.5, bsz=119, num_updates=2050, lr=2.46e-05, gnorm=2.11, train_wall=5, gb_free=12, wall=6388
2024-08-17 19:15:05 | INFO | train_inner | epoch 022:     78 / 94 loss=7.631, nll_loss=4.608, ppl=24.39, wps=1238.5, ups=0.42, wpb=2928, bsz=112, num_updates=2052, lr=2.4624e-05, gnorm=2.085, train_wall=5, gb_free=14.8, wall=6393
2024-08-17 19:15:09 | INFO | train_inner | epoch 022:     80 / 94 loss=7.513, nll_loss=4.46, ppl=22.01, wps=964.9, ups=0.48, wpb=1996.5, bsz=88, num_updates=2054, lr=2.4648e-05, gnorm=2.404, train_wall=4, gb_free=15.4, wall=6397
2024-08-17 19:15:13 | INFO | train_inner | epoch 022:     82 / 94 loss=7.613, nll_loss=4.573, ppl=23.8, wps=1182.1, ups=0.43, wpb=2741, bsz=104, num_updates=2056, lr=2.4672e-05, gnorm=2.079, train_wall=5, gb_free=18.3, wall=6402
2024-08-17 19:15:19 | INFO | train_inner | epoch 022:     84 / 94 loss=7.656, nll_loss=4.628, ppl=24.73, wps=1231.2, ups=0.38, wpb=3276.5, bsz=196, num_updates=2058, lr=2.4696e-05, gnorm=1.942, train_wall=5, gb_free=14.3, wall=6407
2024-08-17 19:15:24 | INFO | train_inner | epoch 022:     86 / 94 loss=7.329, nll_loss=4.185, ppl=18.19, wps=784.9, ups=0.39, wpb=1997, bsz=68, num_updates=2060, lr=2.472e-05, gnorm=2.332, train_wall=5, gb_free=13, wall=6412
2024-08-17 19:15:28 | INFO | train_inner | epoch 022:     88 / 94 loss=7.539, nll_loss=4.473, ppl=22.21, wps=651.7, ups=0.45, wpb=1439.5, bsz=71.5, num_updates=2062, lr=2.4744e-05, gnorm=3.15, train_wall=4, gb_free=16.1, wall=6416
2024-08-17 19:15:33 | INFO | train_inner | epoch 022:     90 / 94 loss=7.654, nll_loss=4.625, ppl=24.68, wps=960.7, ups=0.44, wpb=2200.5, bsz=92, num_updates=2064, lr=2.4768e-05, gnorm=2.688, train_wall=5, gb_free=11.4, wall=6421
2024-08-17 19:15:37 | INFO | train_inner | epoch 022:     92 / 94 loss=7.632, nll_loss=4.597, ppl=24.21, wps=1203.9, ups=0.49, wpb=2452.5, bsz=88, num_updates=2066, lr=2.4792e-05, gnorm=2.375, train_wall=4, gb_free=15.1, wall=6425
2024-08-17 19:15:40 | INFO | train_inner | epoch 022:     94 / 94 loss=7.61, nll_loss=4.57, ppl=23.75, wps=1418.6, ups=0.56, wpb=2546, bsz=104, num_updates=2068, lr=2.4816e-05, gnorm=2.119, train_wall=4, gb_free=16.4, wall=6429
2024-08-17 19:15:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30599.23046875Mb; avail=224466.859375Mb
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000637
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30599.23046875Mb; avail=224466.859375Mb
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005384
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30599.23046875Mb; avail=224466.859375Mb
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004848
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011216
2024-08-17 19:15:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30599.72265625Mb; avail=224466.3671875Mb
2024-08-17 19:15:54 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.945 | nll_loss 4.837 | ppl 28.58 | wps 2431.1 | wpb 944.1 | bsz 40.1 | num_updates 2068 | best_loss 7.938
2024-08-17 19:15:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2068 updates
2024-08-17 19:15:54 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:16:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:16:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 22 @ 2068 updates, score 7.945) (writing took 44.21951531013474 seconds)
2024-08-17 19:16:38 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-08-17 19:16:38 | INFO | train | epoch 022 | loss 7.641 | nll_loss 4.612 | ppl 24.46 | wps 975 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2068 | lr 2.4816e-05 | gnorm 2.154 | train_wall 216 | gb_free 16.4 | wall 6486
2024-08-17 19:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000727
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31625.96484375Mb; avail=223440.16796875Mb
2024-08-17 19:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000157
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001488
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31625.96484375Mb; avail=223440.16796875Mb
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31625.9609375Mb; avail=223440.16796875Mb
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000521
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002388
2024-08-17 19:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31625.9609375Mb; avail=223440.16796875Mb
2024-08-17 19:16:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:16:38 | INFO | fairseq.trainer | begin training epoch 23
2024-08-17 19:16:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:16:43 | INFO | train_inner | epoch 023:      2 / 94 loss=7.509, nll_loss=4.449, ppl=21.84, wps=75, ups=0.03, wpb=2343.5, bsz=100, num_updates=2070, lr=2.484e-05, gnorm=2.368, train_wall=5, gb_free=8.6, wall=6491
2024-08-17 19:16:47 | INFO | train_inner | epoch 023:      4 / 94 loss=7.47, nll_loss=4.398, ppl=21.09, wps=1046.9, ups=0.45, wpb=2312.5, bsz=99.5, num_updates=2072, lr=2.4864e-05, gnorm=2.222, train_wall=4, gb_free=14.7, wall=6496
2024-08-17 19:16:52 | INFO | train_inner | epoch 023:      6 / 94 loss=7.622, nll_loss=4.589, ppl=24.07, wps=1803.5, ups=0.45, wpb=4027.5, bsz=184, num_updates=2074, lr=2.4888e-05, gnorm=1.898, train_wall=4, gb_free=9.9, wall=6500
2024-08-17 19:16:57 | INFO | train_inner | epoch 023:      8 / 94 loss=7.622, nll_loss=4.59, ppl=24.08, wps=1525.6, ups=0.39, wpb=3873, bsz=204, num_updates=2076, lr=2.4912e-05, gnorm=1.746, train_wall=5, gb_free=12.4, wall=6505
2024-08-17 19:17:02 | INFO | train_inner | epoch 023:     10 / 94 loss=7.548, nll_loss=4.491, ppl=22.49, wps=1004.6, ups=0.43, wpb=2330, bsz=88, num_updates=2078, lr=2.4936e-05, gnorm=2.435, train_wall=5, gb_free=12, wall=6510
2024-08-17 19:17:07 | INFO | train_inner | epoch 023:     12 / 94 loss=7.552, nll_loss=4.479, ppl=22.3, wps=1488.9, ups=0.38, wpb=3880.5, bsz=204, num_updates=2080, lr=2.496e-05, gnorm=1.735, train_wall=5, gb_free=11.4, wall=6515
2024-08-17 19:17:11 | INFO | train_inner | epoch 023:     14 / 94 loss=7.565, nll_loss=4.495, ppl=22.55, wps=1377.8, ups=0.44, wpb=3101, bsz=136, num_updates=2082, lr=2.4984e-05, gnorm=1.972, train_wall=4, gb_free=12.3, wall=6520
2024-08-17 19:17:17 | INFO | train_inner | epoch 023:     16 / 94 loss=7.564, nll_loss=4.508, ppl=22.75, wps=1135.1, ups=0.37, wpb=3102.5, bsz=168, num_updates=2084, lr=2.5008e-05, gnorm=2.084, train_wall=5, gb_free=10.7, wall=6525
2024-08-17 19:17:21 | INFO | train_inner | epoch 023:     18 / 94 loss=7.484, nll_loss=4.416, ppl=21.34, wps=1251.2, ups=0.47, wpb=2689, bsz=88, num_updates=2086, lr=2.5032e-05, gnorm=2.082, train_wall=4, gb_free=12.1, wall=6529
2024-08-17 19:17:26 | INFO | train_inner | epoch 023:     20 / 94 loss=7.639, nll_loss=4.628, ppl=24.73, wps=1500.4, ups=0.43, wpb=3450.5, bsz=188, num_updates=2088, lr=2.5056e-05, gnorm=1.807, train_wall=5, gb_free=10.2, wall=6534
2024-08-17 19:17:30 | INFO | train_inner | epoch 023:     22 / 94 loss=7.603, nll_loss=4.582, ppl=23.94, wps=1469.7, ups=0.41, wpb=3544, bsz=148, num_updates=2090, lr=2.508e-05, gnorm=1.849, train_wall=5, gb_free=16.3, wall=6539
2024-08-17 19:17:34 | INFO | train_inner | epoch 023:     24 / 94 loss=7.636, nll_loss=4.605, ppl=24.33, wps=1619.1, ups=0.5, wpb=3207, bsz=84, num_updates=2092, lr=2.5104e-05, gnorm=2.011, train_wall=4, gb_free=14.4, wall=6543
2024-08-17 19:17:39 | INFO | train_inner | epoch 023:     26 / 94 loss=7.633, nll_loss=4.599, ppl=24.24, wps=1381.9, ups=0.43, wpb=3235, bsz=100, num_updates=2094, lr=2.5128e-05, gnorm=1.953, train_wall=5, gb_free=16.1, wall=6547
2024-08-17 19:17:43 | INFO | train_inner | epoch 023:     28 / 94 loss=7.519, nll_loss=4.451, ppl=21.87, wps=1132.5, ups=0.57, wpb=1991.5, bsz=52, num_updates=2096, lr=2.5152e-05, gnorm=2.499, train_wall=4, gb_free=18.6, wall=6551
2024-08-17 19:17:47 | INFO | train_inner | epoch 023:     30 / 94 loss=7.567, nll_loss=4.484, ppl=22.38, wps=1223.8, ups=0.44, wpb=2783.5, bsz=88, num_updates=2098, lr=2.5176e-05, gnorm=2.11, train_wall=5, gb_free=12.4, wall=6555
2024-08-17 19:17:52 | INFO | train_inner | epoch 023:     32 / 94 loss=7.563, nll_loss=4.49, ppl=22.47, wps=1523, ups=0.46, wpb=3304.5, bsz=84, num_updates=2100, lr=2.52e-05, gnorm=1.997, train_wall=4, gb_free=15.2, wall=6560
2024-08-17 19:17:57 | INFO | train_inner | epoch 023:     34 / 94 loss=7.526, nll_loss=4.456, ppl=21.95, wps=1008.3, ups=0.37, wpb=2747.5, bsz=100, num_updates=2102, lr=2.5224e-05, gnorm=2.417, train_wall=5, gb_free=13.3, wall=6565
2024-08-17 19:18:02 | INFO | train_inner | epoch 023:     36 / 94 loss=7.585, nll_loss=4.53, ppl=23.1, wps=1389.2, ups=0.4, wpb=3453, bsz=108, num_updates=2104, lr=2.5248e-05, gnorm=1.905, train_wall=5, gb_free=14.9, wall=6570
2024-08-17 19:18:07 | INFO | train_inner | epoch 023:     38 / 94 loss=7.675, nll_loss=4.66, ppl=25.28, wps=1685.1, ups=0.43, wpb=3939, bsz=124, num_updates=2106, lr=2.5272e-05, gnorm=1.917, train_wall=5, gb_free=10.1, wall=6575
2024-08-17 19:18:12 | INFO | train_inner | epoch 023:     40 / 94 loss=7.494, nll_loss=4.437, ppl=21.66, wps=982.7, ups=0.41, wpb=2418.5, bsz=116, num_updates=2108, lr=2.5296e-05, gnorm=2.241, train_wall=5, gb_free=12.1, wall=6580
2024-08-17 19:18:15 | INFO | train_inner | epoch 023:     42 / 94 loss=7.519, nll_loss=4.473, ppl=22.21, wps=1119.3, ups=0.63, wpb=1781, bsz=56, num_updates=2110, lr=2.532e-05, gnorm=2.758, train_wall=3, gb_free=14.8, wall=6583
2024-08-17 19:18:20 | INFO | train_inner | epoch 023:     44 / 94 loss=7.745, nll_loss=4.742, ppl=26.75, wps=1396.9, ups=0.41, wpb=3374, bsz=128, num_updates=2112, lr=2.5344e-05, gnorm=2.052, train_wall=5, gb_free=10.9, wall=6588
2024-08-17 19:18:24 | INFO | train_inner | epoch 023:     46 / 94 loss=7.64, nll_loss=4.605, ppl=24.33, wps=1394.5, ups=0.41, wpb=3400.5, bsz=172, num_updates=2114, lr=2.5368e-05, gnorm=1.956, train_wall=5, gb_free=15.9, wall=6593
2024-08-17 19:18:29 | INFO | train_inner | epoch 023:     48 / 94 loss=7.673, nll_loss=4.635, ppl=24.85, wps=1463.3, ups=0.47, wpb=3140.5, bsz=104, num_updates=2116, lr=2.5392e-05, gnorm=2.092, train_wall=4, gb_free=18.3, wall=6597
2024-08-17 19:18:33 | INFO | train_inner | epoch 023:     50 / 94 loss=7.461, nll_loss=4.372, ppl=20.71, wps=656, ups=0.43, wpb=1541.5, bsz=60, num_updates=2118, lr=2.5416e-05, gnorm=2.714, train_wall=5, gb_free=13.8, wall=6602
2024-08-17 19:18:38 | INFO | train_inner | epoch 023:     52 / 94 loss=7.604, nll_loss=4.562, ppl=23.62, wps=1530.1, ups=0.46, wpb=3300.5, bsz=92, num_updates=2120, lr=2.544e-05, gnorm=1.948, train_wall=4, gb_free=15.3, wall=6606
2024-08-17 19:18:42 | INFO | train_inner | epoch 023:     54 / 94 loss=7.481, nll_loss=4.411, ppl=21.27, wps=1159.6, ups=0.46, wpb=2508, bsz=111, num_updates=2122, lr=2.5464e-05, gnorm=2.159, train_wall=4, gb_free=13.8, wall=6610
2024-08-17 19:18:47 | INFO | train_inner | epoch 023:     56 / 94 loss=7.548, nll_loss=4.505, ppl=22.71, wps=1139.4, ups=0.41, wpb=2746, bsz=140, num_updates=2124, lr=2.5488e-05, gnorm=2.043, train_wall=5, gb_free=14.2, wall=6615
2024-08-17 19:18:52 | INFO | train_inner | epoch 023:     58 / 94 loss=7.632, nll_loss=4.612, ppl=24.45, wps=1095.6, ups=0.43, wpb=2573, bsz=108, num_updates=2126, lr=2.5512e-05, gnorm=2.184, train_wall=5, gb_free=12.4, wall=6620
2024-08-17 19:19:02 | INFO | train_inner | epoch 023:     60 / 94 loss=7.683, nll_loss=4.664, ppl=25.35, wps=774.2, ups=0.19, wpb=4026.5, bsz=192, num_updates=2128, lr=2.5536e-05, gnorm=1.691, train_wall=10, gb_free=12, wall=6630
2024-08-17 19:19:06 | INFO | train_inner | epoch 023:     62 / 94 loss=7.58, nll_loss=4.545, ppl=23.34, wps=1222.3, ups=0.46, wpb=2655.5, bsz=152, num_updates=2130, lr=2.556e-05, gnorm=2.011, train_wall=4, gb_free=11.2, wall=6635
2024-08-17 19:19:10 | INFO | train_inner | epoch 023:     64 / 94 loss=7.389, nll_loss=4.302, ppl=19.73, wps=730.5, ups=0.48, wpb=1519, bsz=88, num_updates=2132, lr=2.5584e-05, gnorm=2.659, train_wall=4, gb_free=14.2, wall=6639
2024-08-17 19:19:15 | INFO | train_inner | epoch 023:     66 / 94 loss=7.678, nll_loss=4.666, ppl=25.39, wps=1353.5, ups=0.43, wpb=3144.5, bsz=156, num_updates=2134, lr=2.5608e-05, gnorm=2.034, train_wall=5, gb_free=10, wall=6643
2024-08-17 19:19:20 | INFO | train_inner | epoch 023:     68 / 94 loss=7.56, nll_loss=4.505, ppl=22.7, wps=1037.8, ups=0.44, wpb=2381, bsz=100, num_updates=2136, lr=2.5632e-05, gnorm=2.365, train_wall=5, gb_free=17.1, wall=6648
2024-08-17 19:19:24 | INFO | train_inner | epoch 023:     70 / 94 loss=7.622, nll_loss=4.589, ppl=24.07, wps=1346.8, ups=0.44, wpb=3091, bsz=144, num_updates=2138, lr=2.5656e-05, gnorm=2.219, train_wall=5, gb_free=12.7, wall=6653
2024-08-17 19:19:30 | INFO | train_inner | epoch 023:     72 / 94 loss=7.673, nll_loss=4.651, ppl=25.12, wps=1369.4, ups=0.38, wpb=3607, bsz=188, num_updates=2140, lr=2.568e-05, gnorm=1.987, train_wall=5, gb_free=9.8, wall=6658
2024-08-17 19:19:34 | INFO | train_inner | epoch 023:     74 / 94 loss=7.514, nll_loss=4.438, ppl=21.68, wps=1016.4, ups=0.44, wpb=2303, bsz=64, num_updates=2142, lr=2.5704e-05, gnorm=2.456, train_wall=5, gb_free=11.7, wall=6662
2024-08-17 19:19:38 | INFO | train_inner | epoch 023:     76 / 94 loss=7.744, nll_loss=4.739, ppl=26.71, wps=1703.2, ups=0.46, wpb=3691, bsz=104, num_updates=2144, lr=2.5728e-05, gnorm=2.001, train_wall=4, gb_free=12.4, wall=6667
2024-08-17 19:19:43 | INFO | train_inner | epoch 023:     78 / 94 loss=7.538, nll_loss=4.495, ppl=22.54, wps=1077.1, ups=0.43, wpb=2499.5, bsz=168, num_updates=2146, lr=2.5752e-05, gnorm=2.014, train_wall=5, gb_free=14.1, wall=6671
2024-08-17 19:19:48 | INFO | train_inner | epoch 023:     80 / 94 loss=7.618, nll_loss=4.58, ppl=23.92, wps=1102.5, ups=0.39, wpb=2819, bsz=104, num_updates=2148, lr=2.5776e-05, gnorm=2.128, train_wall=5, gb_free=14.4, wall=6676
2024-08-17 19:19:52 | INFO | train_inner | epoch 023:     82 / 94 loss=7.504, nll_loss=4.456, ppl=21.95, wps=810.2, ups=0.47, wpb=1732, bsz=96, num_updates=2150, lr=2.58e-05, gnorm=2.814, train_wall=4, gb_free=10.1, wall=6681
2024-08-17 19:19:56 | INFO | train_inner | epoch 023:     84 / 94 loss=7.396, nll_loss=4.293, ppl=19.6, wps=880.2, ups=0.56, wpb=1579.5, bsz=44, num_updates=2152, lr=2.5824e-05, gnorm=2.881, train_wall=4, gb_free=14.3, wall=6684
2024-08-17 19:20:01 | INFO | train_inner | epoch 023:     86 / 94 loss=7.598, nll_loss=4.559, ppl=23.57, wps=1030.7, ups=0.39, wpb=2621.5, bsz=112, num_updates=2154, lr=2.5848e-05, gnorm=2.241, train_wall=5, gb_free=12.6, wall=6689
2024-08-17 19:20:06 | INFO | train_inner | epoch 023:     88 / 94 loss=7.591, nll_loss=4.546, ppl=23.36, wps=1472.9, ups=0.45, wpb=3256.5, bsz=144, num_updates=2156, lr=2.5872e-05, gnorm=1.941, train_wall=4, gb_free=12.7, wall=6694
2024-08-17 19:20:10 | INFO | train_inner | epoch 023:     90 / 94 loss=7.427, nll_loss=4.347, ppl=20.34, wps=736.2, ups=0.49, wpb=1506, bsz=48, num_updates=2158, lr=2.5896e-05, gnorm=2.746, train_wall=4, gb_free=16.8, wall=6698
2024-08-17 19:20:14 | INFO | train_inner | epoch 023:     92 / 94 loss=7.336, nll_loss=4.209, ppl=18.49, wps=708.7, ups=0.44, wpb=1611, bsz=48, num_updates=2160, lr=2.592e-05, gnorm=2.695, train_wall=5, gb_free=8.9, wall=6702
2024-08-17 19:20:19 | INFO | train_inner | epoch 023:     94 / 94 loss=7.7, nll_loss=4.701, ppl=26.01, wps=1553.5, ups=0.46, wpb=3347, bsz=224, num_updates=2162, lr=2.5944e-05, gnorm=1.846, train_wall=4, gb_free=17, wall=6707
2024-08-17 19:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16864.77734375Mb; avail=238201.3515625Mb
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16864.77734375Mb; avail=238201.3515625Mb
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005403
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16864.7734375Mb; avail=238201.3515625Mb
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004795
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011087
2024-08-17 19:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16864.7734375Mb; avail=238201.3515625Mb
2024-08-17 19:20:32 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.952 | nll_loss 4.839 | ppl 28.63 | wps 2428.6 | wpb 944.1 | bsz 40.1 | num_updates 2162 | best_loss 7.938
2024-08-17 19:20:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2162 updates
2024-08-17 19:20:32 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:21:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:21:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 23 @ 2162 updates, score 7.952) (writing took 38.68249530205503 seconds)
2024-08-17 19:21:11 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-08-17 19:21:11 | INFO | train | epoch 023 | loss 7.589 | nll_loss 4.545 | ppl 23.34 | wps 979.7 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2162 | lr 2.5944e-05 | gnorm 2.168 | train_wall 220 | gb_free 17 | wall 6759
2024-08-17 19:21:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:21:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:21:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:21:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000737
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19229.140625Mb; avail=235836.984375Mb
2024-08-17 19:21:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000160
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001476
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19229.140625Mb; avail=235836.984375Mb
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19229.140625Mb; avail=235836.984375Mb
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000525
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002329
2024-08-17 19:21:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19229.140625Mb; avail=235836.984375Mb
2024-08-17 19:21:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:21:11 | INFO | fairseq.trainer | begin training epoch 24
2024-08-17 19:21:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:21:15 | INFO | train_inner | epoch 024:      2 / 94 loss=7.496, nll_loss=4.433, ppl=21.6, wps=107, ups=0.04, wpb=3045.5, bsz=124, num_updates=2164, lr=2.5968e-05, gnorm=2.004, train_wall=5, gb_free=14.2, wall=6764
2024-08-17 19:21:20 | INFO | train_inner | epoch 024:      4 / 94 loss=7.676, nll_loss=4.663, ppl=25.34, wps=1677.4, ups=0.41, wpb=4134, bsz=220, num_updates=2166, lr=2.5992e-05, gnorm=1.976, train_wall=5, gb_free=14.3, wall=6769
2024-08-17 19:21:26 | INFO | train_inner | epoch 024:      6 / 94 loss=7.568, nll_loss=4.505, ppl=22.71, wps=1170.6, ups=0.39, wpb=3040, bsz=100, num_updates=2168, lr=2.6016e-05, gnorm=2.004, train_wall=5, gb_free=10.7, wall=6774
2024-08-17 19:21:31 | INFO | train_inner | epoch 024:      8 / 94 loss=7.654, nll_loss=4.617, ppl=24.55, wps=1416, ups=0.38, wpb=3684, bsz=172, num_updates=2170, lr=2.604e-05, gnorm=1.779, train_wall=5, gb_free=13, wall=6779
2024-08-17 19:21:35 | INFO | train_inner | epoch 024:     10 / 94 loss=7.578, nll_loss=4.523, ppl=22.98, wps=1328.7, ups=0.47, wpb=2828, bsz=116, num_updates=2172, lr=2.6064e-05, gnorm=2.141, train_wall=4, gb_free=14.9, wall=6783
2024-08-17 19:21:40 | INFO | train_inner | epoch 024:     12 / 94 loss=7.551, nll_loss=4.486, ppl=22.42, wps=1324.2, ups=0.44, wpb=2992.5, bsz=116, num_updates=2174, lr=2.6088e-05, gnorm=2.06, train_wall=5, gb_free=12.9, wall=6788
2024-08-17 19:21:44 | INFO | train_inner | epoch 024:     14 / 94 loss=7.572, nll_loss=4.513, ppl=22.83, wps=952.3, ups=0.42, wpb=2262.5, bsz=103.5, num_updates=2176, lr=2.6112e-05, gnorm=2.434, train_wall=5, gb_free=15.6, wall=6793
2024-08-17 19:21:48 | INFO | train_inner | epoch 024:     16 / 94 loss=7.428, nll_loss=4.321, ppl=19.99, wps=1235.5, ups=0.49, wpb=2542.5, bsz=72, num_updates=2178, lr=2.6136e-05, gnorm=2.372, train_wall=4, gb_free=17.9, wall=6797
2024-08-17 19:21:53 | INFO | train_inner | epoch 024:     18 / 94 loss=7.444, nll_loss=4.366, ppl=20.63, wps=1213.1, ups=0.39, wpb=3079.5, bsz=180, num_updates=2180, lr=2.616e-05, gnorm=1.908, train_wall=5, gb_free=13.3, wall=6802
2024-08-17 19:21:57 | INFO | train_inner | epoch 024:     20 / 94 loss=7.372, nll_loss=4.263, ppl=19.2, wps=927.9, ups=0.52, wpb=1791, bsz=60, num_updates=2182, lr=2.6184e-05, gnorm=2.595, train_wall=4, gb_free=18.1, wall=6806
2024-08-17 19:22:01 | INFO | train_inner | epoch 024:     22 / 94 loss=7.473, nll_loss=4.394, ppl=21.02, wps=1275.9, ups=0.54, wpb=2348.5, bsz=76, num_updates=2184, lr=2.6208e-05, gnorm=2.309, train_wall=4, gb_free=15.4, wall=6809
2024-08-17 19:22:06 | INFO | train_inner | epoch 024:     24 / 94 loss=7.32, nll_loss=4.204, ppl=18.43, wps=683, ups=0.44, wpb=1566.5, bsz=56, num_updates=2186, lr=2.6232e-05, gnorm=2.84, train_wall=5, gb_free=16.1, wall=6814
2024-08-17 19:22:11 | INFO | train_inner | epoch 024:     26 / 94 loss=7.589, nll_loss=4.54, ppl=23.27, wps=942.2, ups=0.41, wpb=2316, bsz=88, num_updates=2188, lr=2.6256e-05, gnorm=2.365, train_wall=5, gb_free=10, wall=6819
2024-08-17 19:22:15 | INFO | train_inner | epoch 024:     28 / 94 loss=7.455, nll_loss=4.361, ppl=20.55, wps=874.1, ups=0.43, wpb=2036, bsz=72, num_updates=2190, lr=2.628e-05, gnorm=2.962, train_wall=5, gb_free=15.1, wall=6823
2024-08-17 19:22:25 | INFO | train_inner | epoch 024:     30 / 94 loss=7.548, nll_loss=4.495, ppl=22.56, wps=778.4, ups=0.2, wpb=3856, bsz=204, num_updates=2192, lr=2.6304e-05, gnorm=1.782, train_wall=10, gb_free=12.6, wall=6833
2024-08-17 19:22:29 | INFO | train_inner | epoch 024:     32 / 94 loss=7.587, nll_loss=4.536, ppl=23.2, wps=1963.6, ups=0.53, wpb=3717.5, bsz=68, num_updates=2194, lr=2.6328e-05, gnorm=2.007, train_wall=4, gb_free=17.2, wall=6837
2024-08-17 19:22:34 | INFO | train_inner | epoch 024:     34 / 94 loss=7.619, nll_loss=4.572, ppl=23.79, wps=1404.4, ups=0.42, wpb=3371.5, bsz=112, num_updates=2196, lr=2.6352e-05, gnorm=2.004, train_wall=5, gb_free=13.1, wall=6842
2024-08-17 19:22:38 | INFO | train_inner | epoch 024:     36 / 94 loss=7.529, nll_loss=4.46, ppl=22.01, wps=1328.5, ups=0.42, wpb=3198.5, bsz=104, num_updates=2198, lr=2.6376e-05, gnorm=1.828, train_wall=5, gb_free=12.3, wall=6847
2024-08-17 19:22:42 | INFO | train_inner | epoch 024:     38 / 94 loss=7.59, nll_loss=4.561, ppl=23.6, wps=1188.9, ups=0.55, wpb=2174.5, bsz=60, num_updates=2200, lr=2.64e-05, gnorm=2.409, train_wall=4, gb_free=16.6, wall=6850
2024-08-17 19:22:46 | INFO | train_inner | epoch 024:     40 / 94 loss=7.558, nll_loss=4.5, ppl=22.63, wps=1635.2, ups=0.58, wpb=2820.5, bsz=80, num_updates=2202, lr=2.6424e-05, gnorm=2.209, train_wall=3, gb_free=13.1, wall=6854
2024-08-17 19:22:50 | INFO | train_inner | epoch 024:     42 / 94 loss=7.494, nll_loss=4.419, ppl=21.39, wps=1029.5, ups=0.44, wpb=2364, bsz=96, num_updates=2204, lr=2.6448e-05, gnorm=2.565, train_wall=5, gb_free=9.7, wall=6858
2024-08-17 19:22:55 | INFO | train_inner | epoch 024:     44 / 94 loss=7.586, nll_loss=4.534, ppl=23.17, wps=1561, ups=0.46, wpb=3423.5, bsz=132, num_updates=2206, lr=2.6472e-05, gnorm=1.987, train_wall=4, gb_free=13.5, wall=6863
2024-08-17 19:23:00 | INFO | train_inner | epoch 024:     46 / 94 loss=7.685, nll_loss=4.654, ppl=25.18, wps=1590.8, ups=0.38, wpb=4241, bsz=192, num_updates=2208, lr=2.6496e-05, gnorm=1.896, train_wall=5, gb_free=12, wall=6868
2024-08-17 19:23:05 | INFO | train_inner | epoch 024:     48 / 94 loss=7.617, nll_loss=4.563, ppl=23.64, wps=1451.8, ups=0.43, wpb=3381.5, bsz=120, num_updates=2210, lr=2.652e-05, gnorm=2.036, train_wall=5, gb_free=15.2, wall=6873
2024-08-17 19:23:09 | INFO | train_inner | epoch 024:     50 / 94 loss=7.476, nll_loss=4.387, ppl=20.92, wps=1256.3, ups=0.41, wpb=3055.5, bsz=128, num_updates=2212, lr=2.6544e-05, gnorm=1.993, train_wall=5, gb_free=13.9, wall=6878
2024-08-17 19:23:14 | INFO | train_inner | epoch 024:     52 / 94 loss=7.474, nll_loss=4.389, ppl=20.96, wps=1253.9, ups=0.47, wpb=2663, bsz=92, num_updates=2214, lr=2.6568e-05, gnorm=2.259, train_wall=4, gb_free=13.2, wall=6882
2024-08-17 19:23:18 | INFO | train_inner | epoch 024:     54 / 94 loss=7.477, nll_loss=4.432, ppl=21.59, wps=1003.8, ups=0.42, wpb=2363.5, bsz=140, num_updates=2216, lr=2.6592e-05, gnorm=2.324, train_wall=5, gb_free=13.5, wall=6887
2024-08-17 19:23:22 | INFO | train_inner | epoch 024:     56 / 94 loss=7.38, nll_loss=4.311, ppl=19.85, wps=946.8, ups=0.55, wpb=1707, bsz=48, num_updates=2218, lr=2.6616e-05, gnorm=2.672, train_wall=4, gb_free=18.1, wall=6890
2024-08-17 19:23:26 | INFO | train_inner | epoch 024:     58 / 94 loss=7.517, nll_loss=4.455, ppl=21.94, wps=1249.3, ups=0.46, wpb=2694, bsz=88, num_updates=2220, lr=2.664e-05, gnorm=2.516, train_wall=4, gb_free=14.4, wall=6895
2024-08-17 19:23:31 | INFO | train_inner | epoch 024:     60 / 94 loss=7.498, nll_loss=4.437, ppl=21.66, wps=1286.9, ups=0.43, wpb=2988, bsz=167, num_updates=2222, lr=2.6664e-05, gnorm=1.912, train_wall=5, gb_free=11.6, wall=6899
2024-08-17 19:23:36 | INFO | train_inner | epoch 024:     62 / 94 loss=7.689, nll_loss=4.673, ppl=25.52, wps=1555.3, ups=0.39, wpb=3995, bsz=192, num_updates=2224, lr=2.6688e-05, gnorm=2.013, train_wall=5, gb_free=12.3, wall=6904
2024-08-17 19:23:41 | INFO | train_inner | epoch 024:     64 / 94 loss=7.586, nll_loss=4.533, ppl=23.15, wps=1106.1, ups=0.42, wpb=2650.5, bsz=136, num_updates=2226, lr=2.6712e-05, gnorm=2.099, train_wall=5, gb_free=14.4, wall=6909
2024-08-17 19:23:46 | INFO | train_inner | epoch 024:     66 / 94 loss=7.542, nll_loss=4.487, ppl=22.43, wps=1152, ups=0.42, wpb=2722, bsz=136, num_updates=2228, lr=2.6736e-05, gnorm=2.142, train_wall=5, gb_free=11.5, wall=6914
2024-08-17 19:23:50 | INFO | train_inner | epoch 024:     68 / 94 loss=7.365, nll_loss=4.267, ppl=19.25, wps=1002.1, ups=0.48, wpb=2088.5, bsz=112, num_updates=2230, lr=2.676e-05, gnorm=2.317, train_wall=4, gb_free=16.8, wall=6918
2024-08-17 19:23:55 | INFO | train_inner | epoch 024:     70 / 94 loss=7.63, nll_loss=4.596, ppl=24.18, wps=1562.9, ups=0.42, wpb=3713.5, bsz=172, num_updates=2232, lr=2.6784e-05, gnorm=1.935, train_wall=5, gb_free=15.7, wall=6923
2024-08-17 19:23:59 | INFO | train_inner | epoch 024:     72 / 94 loss=7.357, nll_loss=4.259, ppl=19.15, wps=639.3, ups=0.44, wpb=1460, bsz=60, num_updates=2234, lr=2.6808e-05, gnorm=2.857, train_wall=5, gb_free=13.7, wall=6927
2024-08-17 19:24:04 | INFO | train_inner | epoch 024:     74 / 94 loss=7.566, nll_loss=4.52, ppl=22.95, wps=1031, ups=0.43, wpb=2422, bsz=104, num_updates=2236, lr=2.6832e-05, gnorm=2.225, train_wall=5, gb_free=14.5, wall=6932
2024-08-17 19:24:08 | INFO | train_inner | epoch 024:     76 / 94 loss=7.628, nll_loss=4.588, ppl=24.05, wps=1326.5, ups=0.43, wpb=3076, bsz=148, num_updates=2238, lr=2.6856e-05, gnorm=2.14, train_wall=5, gb_free=18.2, wall=6937
2024-08-17 19:24:13 | INFO | train_inner | epoch 024:     78 / 94 loss=7.479, nll_loss=4.382, ppl=20.85, wps=1288.9, ups=0.48, wpb=2674, bsz=84, num_updates=2240, lr=2.688e-05, gnorm=2.191, train_wall=4, gb_free=16.1, wall=6941
2024-08-17 19:24:17 | INFO | train_inner | epoch 024:     80 / 94 loss=7.531, nll_loss=4.474, ppl=22.22, wps=1339.5, ups=0.42, wpb=3182.5, bsz=228, num_updates=2242, lr=2.6904e-05, gnorm=1.98, train_wall=5, gb_free=17.6, wall=6946
2024-08-17 19:24:22 | INFO | train_inner | epoch 024:     82 / 94 loss=7.527, nll_loss=4.451, ppl=21.87, wps=1237.2, ups=0.45, wpb=2767.5, bsz=88, num_updates=2244, lr=2.6928e-05, gnorm=2.316, train_wall=4, gb_free=14.7, wall=6950
2024-08-17 19:24:27 | INFO | train_inner | epoch 024:     84 / 94 loss=7.41, nll_loss=4.313, ppl=19.87, wps=840.6, ups=0.41, wpb=2055.5, bsz=104, num_updates=2246, lr=2.6952e-05, gnorm=2.323, train_wall=5, gb_free=14.9, wall=6955
2024-08-17 19:24:32 | INFO | train_inner | epoch 024:     86 / 94 loss=7.62, nll_loss=4.595, ppl=24.17, wps=1367.4, ups=0.36, wpb=3817, bsz=176, num_updates=2248, lr=2.6976e-05, gnorm=1.815, train_wall=6, gb_free=12.8, wall=6961
2024-08-17 19:24:37 | INFO | train_inner | epoch 024:     88 / 94 loss=7.596, nll_loss=4.55, ppl=23.43, wps=1324.5, ups=0.41, wpb=3198.5, bsz=136, num_updates=2250, lr=2.7e-05, gnorm=1.958, train_wall=5, gb_free=14.1, wall=6965
2024-08-17 19:24:42 | INFO | train_inner | epoch 024:     90 / 94 loss=7.563, nll_loss=4.519, ppl=22.93, wps=1068.6, ups=0.41, wpb=2634, bsz=140, num_updates=2252, lr=2.7024e-05, gnorm=2.049, train_wall=5, gb_free=11.5, wall=6970
2024-08-17 19:24:47 | INFO | train_inner | epoch 024:     92 / 94 loss=7.6, nll_loss=4.564, ppl=23.66, wps=1480.4, ups=0.41, wpb=3572.5, bsz=140, num_updates=2254, lr=2.7048e-05, gnorm=1.827, train_wall=5, gb_free=14.8, wall=6975
2024-08-17 19:24:51 | INFO | train_inner | epoch 024:     94 / 94 loss=7.458, nll_loss=4.379, ppl=20.81, wps=882.3, ups=0.5, wpb=1774, bsz=68, num_updates=2256, lr=2.7072e-05, gnorm=2.909, train_wall=4, gb_free=17, wall=6979
2024-08-17 19:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12434.546875Mb; avail=242631.5859375Mb
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12434.546875Mb; avail=242631.5859375Mb
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005364
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12434.546875Mb; avail=242631.5859375Mb
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004792
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011042
2024-08-17 19:24:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12434.546875Mb; avail=242631.5859375Mb
2024-08-17 19:25:04 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.948 | nll_loss 4.816 | ppl 28.16 | wps 2423.3 | wpb 944.1 | bsz 40.1 | num_updates 2256 | best_loss 7.938
2024-08-17 19:25:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2256 updates
2024-08-17 19:25:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:25:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:25:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 24 @ 2256 updates, score 7.948) (writing took 43.26807108614594 seconds)
2024-08-17 19:25:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-08-17 19:25:48 | INFO | train | epoch 024 | loss 7.548 | nll_loss 4.491 | ppl 22.48 | wps 963.8 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2256 | lr 2.7072e-05 | gnorm 2.197 | train_wall 220 | gb_free 17 | wall 7036
2024-08-17 19:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000669
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20623.30078125Mb; avail=234442.87109375Mb
2024-08-17 19:25:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000161
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001459
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20623.30078125Mb; avail=234442.87109375Mb
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20623.30078125Mb; avail=234442.87109375Mb
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000530
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002317
2024-08-17 19:25:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20623.30078125Mb; avail=234442.87109375Mb
2024-08-17 19:25:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:25:48 | INFO | fairseq.trainer | begin training epoch 25
2024-08-17 19:25:48 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:25:51 | INFO | train_inner | epoch 025:      2 / 94 loss=7.197, nll_loss=4.031, ppl=16.35, wps=49.2, ups=0.03, wpb=1489, bsz=40, num_updates=2258, lr=2.7096e-05, gnorm=2.924, train_wall=4, gb_free=18.3, wall=7040
2024-08-17 19:25:55 | INFO | train_inner | epoch 025:      4 / 94 loss=7.462, nll_loss=4.361, ppl=20.55, wps=1398.6, ups=0.62, wpb=2246.5, bsz=60, num_updates=2260, lr=2.712e-05, gnorm=2.585, train_wall=3, gb_free=13.1, wall=7043
2024-08-17 19:25:58 | INFO | train_inner | epoch 025:      6 / 94 loss=7.417, nll_loss=4.309, ppl=19.82, wps=1099.4, ups=0.54, wpb=2038, bsz=84, num_updates=2262, lr=2.7144e-05, gnorm=2.818, train_wall=4, gb_free=19.3, wall=7047
2024-08-17 19:26:03 | INFO | train_inner | epoch 025:      8 / 94 loss=7.545, nll_loss=4.47, ppl=22.16, wps=1681.9, ups=0.45, wpb=3735, bsz=116, num_updates=2264, lr=2.7168e-05, gnorm=2.005, train_wall=4, gb_free=9.5, wall=7051
2024-08-17 19:26:08 | INFO | train_inner | epoch 025:     10 / 94 loss=7.396, nll_loss=4.273, ppl=19.33, wps=1230.7, ups=0.42, wpb=2953, bsz=96, num_updates=2266, lr=2.7192e-05, gnorm=2.063, train_wall=5, gb_free=11, wall=7056
2024-08-17 19:26:12 | INFO | train_inner | epoch 025:     12 / 94 loss=7.392, nll_loss=4.276, ppl=19.38, wps=986.3, ups=0.42, wpb=2372.5, bsz=92, num_updates=2268, lr=2.7216e-05, gnorm=2.262, train_wall=5, gb_free=15.3, wall=7061
2024-08-17 19:26:16 | INFO | train_inner | epoch 025:     14 / 94 loss=7.05, nll_loss=3.854, ppl=14.46, wps=801.7, ups=0.54, wpb=1484, bsz=43, num_updates=2270, lr=2.724e-05, gnorm=2.921, train_wall=4, gb_free=19.7, wall=7064
2024-08-17 19:26:22 | INFO | train_inner | epoch 025:     16 / 94 loss=7.434, nll_loss=4.348, ppl=20.36, wps=994.3, ups=0.37, wpb=2703.5, bsz=108, num_updates=2272, lr=2.7264e-05, gnorm=2.138, train_wall=5, gb_free=9.5, wall=7070
2024-08-17 19:26:26 | INFO | train_inner | epoch 025:     18 / 94 loss=7.477, nll_loss=4.439, ppl=21.69, wps=1136.2, ups=0.44, wpb=2585.5, bsz=156, num_updates=2274, lr=2.7288e-05, gnorm=2.284, train_wall=5, gb_free=14.6, wall=7074
2024-08-17 19:26:30 | INFO | train_inner | epoch 025:     20 / 94 loss=7.303, nll_loss=4.175, ppl=18.06, wps=991.7, ups=0.48, wpb=2081, bsz=56, num_updates=2276, lr=2.7312e-05, gnorm=2.409, train_wall=4, gb_free=14.7, wall=7079
2024-08-17 19:26:35 | INFO | train_inner | epoch 025:     22 / 94 loss=7.377, nll_loss=4.257, ppl=19.13, wps=1141.2, ups=0.46, wpb=2478, bsz=76, num_updates=2278, lr=2.7336e-05, gnorm=2.332, train_wall=4, gb_free=15, wall=7083
2024-08-17 19:26:39 | INFO | train_inner | epoch 025:     24 / 94 loss=7.391, nll_loss=4.29, ppl=19.56, wps=1137.4, ups=0.44, wpb=2558.5, bsz=128, num_updates=2280, lr=2.736e-05, gnorm=2.104, train_wall=4, gb_free=11.1, wall=7087
2024-08-17 19:26:44 | INFO | train_inner | epoch 025:     26 / 94 loss=7.51, nll_loss=4.435, ppl=21.64, wps=1217.2, ups=0.39, wpb=3159, bsz=168, num_updates=2282, lr=2.7384e-05, gnorm=2.004, train_wall=5, gb_free=13.3, wall=7093
2024-08-17 19:26:49 | INFO | train_inner | epoch 025:     28 / 94 loss=7.525, nll_loss=4.45, ppl=21.85, wps=1645, ups=0.45, wpb=3668.5, bsz=108, num_updates=2284, lr=2.7408e-05, gnorm=2.054, train_wall=4, gb_free=12.4, wall=7097
2024-08-17 19:26:54 | INFO | train_inner | epoch 025:     30 / 94 loss=7.598, nll_loss=4.558, ppl=23.56, wps=1399.5, ups=0.38, wpb=3654.5, bsz=148, num_updates=2286, lr=2.7432e-05, gnorm=1.857, train_wall=5, gb_free=11.5, wall=7102
2024-08-17 19:26:59 | INFO | train_inner | epoch 025:     32 / 94 loss=7.505, nll_loss=4.448, ppl=21.82, wps=1472.7, ups=0.45, wpb=3308.5, bsz=136, num_updates=2288, lr=2.7456e-05, gnorm=1.905, train_wall=4, gb_free=15.9, wall=7107
2024-08-17 19:27:03 | INFO | train_inner | epoch 025:     34 / 94 loss=7.431, nll_loss=4.361, ppl=20.55, wps=894.4, ups=0.41, wpb=2193.5, bsz=124, num_updates=2290, lr=2.748e-05, gnorm=2.352, train_wall=5, gb_free=13.4, wall=7112
2024-08-17 19:27:08 | INFO | train_inner | epoch 025:     36 / 94 loss=7.437, nll_loss=4.349, ppl=20.37, wps=1191, ups=0.44, wpb=2709, bsz=123.5, num_updates=2292, lr=2.7504e-05, gnorm=2.211, train_wall=5, gb_free=14, wall=7116
2024-08-17 19:27:12 | INFO | train_inner | epoch 025:     38 / 94 loss=7.286, nll_loss=4.149, ppl=17.74, wps=858, ups=0.46, wpb=1866, bsz=80, num_updates=2294, lr=2.7528e-05, gnorm=2.431, train_wall=4, gb_free=15.1, wall=7121
2024-08-17 19:27:16 | INFO | train_inner | epoch 025:     40 / 94 loss=7.47, nll_loss=4.363, ppl=20.58, wps=1277.7, ups=0.56, wpb=2294.5, bsz=52, num_updates=2296, lr=2.7552e-05, gnorm=2.409, train_wall=4, gb_free=17.4, wall=7124
2024-08-17 19:27:21 | INFO | train_inner | epoch 025:     42 / 94 loss=7.314, nll_loss=4.181, ppl=18.13, wps=801.8, ups=0.39, wpb=2033, bsz=76, num_updates=2298, lr=2.7576e-05, gnorm=2.365, train_wall=5, gb_free=10.4, wall=7129
2024-08-17 19:27:26 | INFO | train_inner | epoch 025:     44 / 94 loss=7.477, nll_loss=4.396, ppl=21.05, wps=1509.6, ups=0.42, wpb=3633.5, bsz=132, num_updates=2300, lr=2.76e-05, gnorm=1.889, train_wall=5, gb_free=13.5, wall=7134
2024-08-17 19:27:31 | INFO | train_inner | epoch 025:     46 / 94 loss=7.38, nll_loss=4.279, ppl=19.42, wps=843.4, ups=0.39, wpb=2148.5, bsz=92, num_updates=2302, lr=2.7624e-05, gnorm=2.313, train_wall=5, gb_free=13.5, wall=7139
2024-08-17 19:27:35 | INFO | train_inner | epoch 025:     48 / 94 loss=7.183, nll_loss=4.052, ppl=16.58, wps=897.5, ups=0.48, wpb=1864, bsz=104, num_updates=2304, lr=2.7648e-05, gnorm=2.558, train_wall=4, gb_free=9.5, wall=7143
2024-08-17 19:27:40 | INFO | train_inner | epoch 025:     50 / 94 loss=7.47, nll_loss=4.41, ppl=21.26, wps=1271.7, ups=0.39, wpb=3285.5, bsz=228, num_updates=2306, lr=2.7672e-05, gnorm=1.872, train_wall=5, gb_free=10.8, wall=7148
2024-08-17 19:27:45 | INFO | train_inner | epoch 025:     52 / 94 loss=7.459, nll_loss=4.366, ppl=20.63, wps=1331.5, ups=0.46, wpb=2871.5, bsz=100, num_updates=2308, lr=2.7696e-05, gnorm=2.089, train_wall=4, gb_free=15.4, wall=7153
2024-08-17 19:27:49 | INFO | train_inner | epoch 025:     54 / 94 loss=7.431, nll_loss=4.327, ppl=20.07, wps=1088.8, ups=0.46, wpb=2354, bsz=68, num_updates=2310, lr=2.772e-05, gnorm=2.578, train_wall=4, gb_free=13.9, wall=7157
2024-08-17 19:27:54 | INFO | train_inner | epoch 025:     56 / 94 loss=7.569, nll_loss=4.506, ppl=22.72, wps=1286.3, ups=0.38, wpb=3414.5, bsz=152, num_updates=2312, lr=2.7744e-05, gnorm=2.184, train_wall=5, gb_free=13.4, wall=7162
2024-08-17 19:27:58 | INFO | train_inner | epoch 025:     58 / 94 loss=7.402, nll_loss=4.314, ppl=19.89, wps=1160.4, ups=0.52, wpb=2242, bsz=120, num_updates=2314, lr=2.7768e-05, gnorm=2.47, train_wall=4, gb_free=18, wall=7166
2024-08-17 19:28:03 | INFO | train_inner | epoch 025:     60 / 94 loss=7.519, nll_loss=4.447, ppl=21.81, wps=1229, ups=0.38, wpb=3261.5, bsz=144, num_updates=2316, lr=2.7792e-05, gnorm=1.896, train_wall=5, gb_free=12.5, wall=7172
2024-08-17 19:28:09 | INFO | train_inner | epoch 025:     62 / 94 loss=7.578, nll_loss=4.528, ppl=23.07, wps=1429, ups=0.37, wpb=3812, bsz=136, num_updates=2318, lr=2.7816e-05, gnorm=1.877, train_wall=5, gb_free=12.1, wall=7177
2024-08-17 19:28:13 | INFO | train_inner | epoch 025:     64 / 94 loss=7.523, nll_loss=4.465, ppl=22.08, wps=1415.3, ups=0.42, wpb=3355.5, bsz=144, num_updates=2320, lr=2.784e-05, gnorm=1.846, train_wall=5, gb_free=13, wall=7182
2024-08-17 19:28:18 | INFO | train_inner | epoch 025:     66 / 94 loss=7.48, nll_loss=4.418, ppl=21.37, wps=1283.4, ups=0.45, wpb=2872.5, bsz=140, num_updates=2322, lr=2.7864e-05, gnorm=2.218, train_wall=4, gb_free=15.2, wall=7186
2024-08-17 19:28:23 | INFO | train_inner | epoch 025:     68 / 94 loss=7.496, nll_loss=4.429, ppl=21.55, wps=1114, ups=0.42, wpb=2657, bsz=104, num_updates=2324, lr=2.7888e-05, gnorm=2.091, train_wall=5, gb_free=14.7, wall=7191
2024-08-17 19:28:28 | INFO | train_inner | epoch 025:     70 / 94 loss=7.584, nll_loss=4.553, ppl=23.48, wps=1538.9, ups=0.38, wpb=4029.5, bsz=244, num_updates=2326, lr=2.7912e-05, gnorm=1.85, train_wall=5, gb_free=13.8, wall=7196
2024-08-17 19:28:32 | INFO | train_inner | epoch 025:     72 / 94 loss=7.458, nll_loss=4.357, ppl=20.49, wps=1432.3, ups=0.48, wpb=2979, bsz=92, num_updates=2328, lr=2.7936e-05, gnorm=1.971, train_wall=4, gb_free=14.9, wall=7200
2024-08-17 19:28:37 | INFO | train_inner | epoch 025:     74 / 94 loss=7.6, nll_loss=4.556, ppl=23.53, wps=1301.9, ups=0.38, wpb=3390.5, bsz=164, num_updates=2330, lr=2.796e-05, gnorm=2.129, train_wall=5, gb_free=12, wall=7206
2024-08-17 19:28:42 | INFO | train_inner | epoch 025:     76 / 94 loss=7.558, nll_loss=4.513, ppl=22.84, wps=1467.2, ups=0.39, wpb=3715, bsz=220, num_updates=2332, lr=2.7984e-05, gnorm=1.734, train_wall=5, gb_free=14.9, wall=7211
2024-08-17 19:28:48 | INFO | train_inner | epoch 025:     78 / 94 loss=7.64, nll_loss=4.611, ppl=24.43, wps=1508.4, ups=0.37, wpb=4056, bsz=180, num_updates=2334, lr=2.8008e-05, gnorm=1.826, train_wall=5, gb_free=9.9, wall=7216
2024-08-17 19:28:53 | INFO | train_inner | epoch 025:     80 / 94 loss=7.463, nll_loss=4.4, ppl=21.12, wps=1118, ups=0.39, wpb=2833, bsz=140, num_updates=2336, lr=2.8032e-05, gnorm=2.102, train_wall=5, gb_free=14.6, wall=7221
2024-08-17 19:28:57 | INFO | train_inner | epoch 025:     82 / 94 loss=7.533, nll_loss=4.48, ppl=22.31, wps=1306.6, ups=0.45, wpb=2872.5, bsz=116, num_updates=2338, lr=2.8056e-05, gnorm=2.183, train_wall=4, gb_free=14.1, wall=7225
2024-08-17 19:29:02 | INFO | train_inner | epoch 025:     84 / 94 loss=7.547, nll_loss=4.496, ppl=22.56, wps=1561, ups=0.42, wpb=3702.5, bsz=156, num_updates=2340, lr=2.808e-05, gnorm=2.063, train_wall=5, gb_free=13.5, wall=7230
2024-08-17 19:29:06 | INFO | train_inner | epoch 025:     86 / 94 loss=7.445, nll_loss=4.342, ppl=20.28, wps=1397.8, ups=0.46, wpb=3063.5, bsz=88, num_updates=2342, lr=2.8104e-05, gnorm=2.062, train_wall=4, gb_free=16.4, wall=7235
2024-08-17 19:29:11 | INFO | train_inner | epoch 025:     88 / 94 loss=7.448, nll_loss=4.348, ppl=20.36, wps=1224, ups=0.41, wpb=2980, bsz=136, num_updates=2344, lr=2.8128e-05, gnorm=1.94, train_wall=5, gb_free=14.6, wall=7239
2024-08-17 19:29:16 | INFO | train_inner | epoch 025:     90 / 94 loss=7.444, nll_loss=4.346, ppl=20.34, wps=1044.9, ups=0.43, wpb=2449, bsz=80, num_updates=2346, lr=2.8152e-05, gnorm=2.409, train_wall=5, gb_free=10.7, wall=7244
2024-08-17 19:29:20 | INFO | train_inner | epoch 025:     92 / 94 loss=7.584, nll_loss=4.542, ppl=23.29, wps=1510.6, ups=0.44, wpb=3445.5, bsz=144, num_updates=2348, lr=2.8176e-05, gnorm=2.001, train_wall=5, gb_free=11.3, wall=7249
2024-08-17 19:29:24 | INFO | train_inner | epoch 025:     94 / 94 loss=7.544, nll_loss=4.478, ppl=22.29, wps=1274.3, ups=0.49, wpb=2590, bsz=116, num_updates=2350, lr=2.82e-05, gnorm=2.246, train_wall=4, gb_free=17.4, wall=7253
2024-08-17 19:29:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21525.9765625Mb; avail=233540.15625Mb
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000652
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21525.9765625Mb; avail=233540.15625Mb
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005465
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21526.46875Mb; avail=233539.6640625Mb
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004783
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011240
2024-08-17 19:29:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21525.9765625Mb; avail=233540.15625Mb
2024-08-17 19:29:38 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.925 | nll_loss 4.807 | ppl 28 | wps 2428.5 | wpb 944.1 | bsz 40.1 | num_updates 2350 | best_loss 7.925
2024-08-17 19:29:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2350 updates
2024-08-17 19:29:38 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 19:30:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt
2024-08-17 19:30:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_best.pt (epoch 25 @ 2350 updates, score 7.925) (writing took 72.01506367418915 seconds)
2024-08-17 19:30:50 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-08-17 19:30:50 | INFO | train | epoch 025 | loss 7.476 | nll_loss 4.398 | ppl 21.08 | wps 883.1 | ups 0.31 | wpb 2840.2 | bsz 119.4 | num_updates 2350 | lr 2.82e-05 | gnorm 2.188 | train_wall 216 | gb_free 17.4 | wall 7338
2024-08-17 19:30:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:30:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:30:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:30:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000761
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23663.6796875Mb; avail=231402.48828125Mb
2024-08-17 19:30:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000160
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001497
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23663.6796875Mb; avail=231402.48828125Mb
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23663.6796875Mb; avail=231402.48828125Mb
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000584
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002427
2024-08-17 19:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23663.6796875Mb; avail=231402.48828125Mb
2024-08-17 19:30:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:30:50 | INFO | fairseq.trainer | begin training epoch 26
2024-08-17 19:30:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:30:54 | INFO | train_inner | epoch 026:      2 / 94 loss=7.259, nll_loss=4.127, ppl=17.47, wps=53.8, ups=0.02, wpb=2418.5, bsz=68, num_updates=2352, lr=2.8224e-05, gnorm=2.235, train_wall=4, gb_free=15.1, wall=7343
2024-08-17 19:30:58 | INFO | train_inner | epoch 026:      4 / 94 loss=7.406, nll_loss=4.321, ppl=19.99, wps=1474.2, ups=0.56, wpb=2625.5, bsz=100, num_updates=2354, lr=2.8248e-05, gnorm=2.179, train_wall=4, gb_free=15.5, wall=7346
2024-08-17 19:31:03 | INFO | train_inner | epoch 026:      6 / 94 loss=7.322, nll_loss=4.21, ppl=18.5, wps=1191.2, ups=0.43, wpb=2802.5, bsz=72, num_updates=2356, lr=2.8272e-05, gnorm=2.332, train_wall=5, gb_free=16.7, wall=7351
2024-08-17 19:31:07 | INFO | train_inner | epoch 026:      8 / 94 loss=7.278, nll_loss=4.154, ppl=17.81, wps=1151.5, ups=0.47, wpb=2431.5, bsz=112, num_updates=2358, lr=2.8296e-05, gnorm=2.386, train_wall=4, gb_free=14.8, wall=7355
2024-08-17 19:31:11 | INFO | train_inner | epoch 026:     10 / 94 loss=7.454, nll_loss=4.362, ppl=20.56, wps=1533.6, ups=0.53, wpb=2880, bsz=100, num_updates=2360, lr=2.832e-05, gnorm=2.101, train_wall=4, gb_free=12.3, wall=7359
2024-08-17 19:31:16 | INFO | train_inner | epoch 026:     12 / 94 loss=7.502, nll_loss=4.422, ppl=21.43, wps=1304.7, ups=0.36, wpb=3671, bsz=188, num_updates=2362, lr=2.8344e-05, gnorm=1.917, train_wall=6, gb_free=9.1, wall=7365
2024-08-17 19:31:21 | INFO | train_inner | epoch 026:     14 / 94 loss=7.386, nll_loss=4.26, ppl=19.16, wps=1340.8, ups=0.46, wpb=2944.5, bsz=88, num_updates=2364, lr=2.8368e-05, gnorm=2.092, train_wall=4, gb_free=14.5, wall=7369
2024-08-17 19:31:25 | INFO | train_inner | epoch 026:     16 / 94 loss=7.278, nll_loss=4.142, ppl=17.66, wps=861.1, ups=0.45, wpb=1924.5, bsz=76, num_updates=2366, lr=2.8392e-05, gnorm=2.498, train_wall=4, gb_free=14.7, wall=7373
2024-08-17 19:31:30 | INFO | train_inner | epoch 026:     18 / 94 loss=7.422, nll_loss=4.321, ppl=19.98, wps=1411.9, ups=0.39, wpb=3616.5, bsz=120, num_updates=2368, lr=2.8416e-05, gnorm=1.827, train_wall=5, gb_free=12.2, wall=7379
2024-08-17 19:31:35 | INFO | train_inner | epoch 026:     20 / 94 loss=7.415, nll_loss=4.346, ppl=20.33, wps=1066.5, ups=0.4, wpb=2669.5, bsz=176, num_updates=2370, lr=2.844e-05, gnorm=2.119, train_wall=5, gb_free=14.4, wall=7384
2024-08-17 19:31:40 | INFO | train_inner | epoch 026:     22 / 94 loss=7.466, nll_loss=4.393, ppl=21.01, wps=1391.1, ups=0.41, wpb=3398.5, bsz=200, num_updates=2372, lr=2.8464e-05, gnorm=1.827, train_wall=5, gb_free=14.4, wall=7388
2024-08-17 19:31:44 | INFO | train_inner | epoch 026:     24 / 94 loss=7.55, nll_loss=4.497, ppl=22.58, wps=1847.7, ups=0.46, wpb=4027.5, bsz=87, num_updates=2374, lr=2.8488e-05, gnorm=2.281, train_wall=4, gb_free=16.9, wall=7393
2024-08-17 19:31:49 | INFO | train_inner | epoch 026:     26 / 94 loss=7.397, nll_loss=4.287, ppl=19.52, wps=1206.4, ups=0.49, wpb=2463.5, bsz=88, num_updates=2376, lr=2.8512e-05, gnorm=2.285, train_wall=4, gb_free=18.6, wall=7397
2024-08-17 19:31:53 | INFO | train_inner | epoch 026:     28 / 94 loss=7.369, nll_loss=4.258, ppl=19.13, wps=1230.9, ups=0.45, wpb=2733.5, bsz=132, num_updates=2378, lr=2.8536e-05, gnorm=2.037, train_wall=4, gb_free=14.8, wall=7401
2024-08-17 19:31:58 | INFO | train_inner | epoch 026:     30 / 94 loss=7.433, nll_loss=4.325, ppl=20.04, wps=1226.5, ups=0.42, wpb=2905, bsz=92, num_updates=2380, lr=2.856e-05, gnorm=2.049, train_wall=5, gb_free=12.1, wall=7406
2024-08-17 19:32:02 | INFO | train_inner | epoch 026:     32 / 94 loss=7.153, nll_loss=3.988, ppl=15.87, wps=731.6, ups=0.43, wpb=1714, bsz=91.5, num_updates=2382, lr=2.8584e-05, gnorm=2.706, train_wall=5, gb_free=16, wall=7411
2024-08-17 19:32:07 | INFO | train_inner | epoch 026:     34 / 94 loss=7.28, nll_loss=4.157, ppl=17.84, wps=825.7, ups=0.43, wpb=1918.5, bsz=52, num_updates=2384, lr=2.8608e-05, gnorm=2.561, train_wall=5, gb_free=13.9, wall=7415
2024-08-17 19:32:11 | INFO | train_inner | epoch 026:     36 / 94 loss=7.517, nll_loss=4.443, ppl=21.75, wps=1453.2, ups=0.47, wpb=3076, bsz=96, num_updates=2386, lr=2.8632e-05, gnorm=2.341, train_wall=4, gb_free=15.1, wall=7420
2024-08-17 19:32:17 | INFO | train_inner | epoch 026:     38 / 94 loss=7.407, nll_loss=4.316, ppl=19.92, wps=985, ups=0.38, wpb=2580.5, bsz=148, num_updates=2388, lr=2.8656e-05, gnorm=2.335, train_wall=5, gb_free=14.3, wall=7425
2024-08-17 19:32:21 | INFO | train_inner | epoch 026:     40 / 94 loss=7.481, nll_loss=4.391, ppl=20.99, wps=1443.8, ups=0.45, wpb=3208.5, bsz=140, num_updates=2390, lr=2.868e-05, gnorm=2.004, train_wall=4, gb_free=16, wall=7429
2024-08-17 19:32:26 | INFO | train_inner | epoch 026:     42 / 94 loss=7.317, nll_loss=4.172, ppl=18.03, wps=1198.6, ups=0.44, wpb=2697.5, bsz=84, num_updates=2392, lr=2.8704e-05, gnorm=2.09, train_wall=4, gb_free=11.6, wall=7434
2024-08-17 19:32:31 | INFO | train_inner | epoch 026:     44 / 94 loss=7.476, nll_loss=4.406, ppl=21.2, wps=1307.3, ups=0.37, wpb=3487, bsz=176, num_updates=2394, lr=2.8728e-05, gnorm=1.783, train_wall=5, gb_free=15.2, wall=7439
2024-08-17 19:32:35 | INFO | train_inner | epoch 026:     46 / 94 loss=7.386, nll_loss=4.294, ppl=19.62, wps=1183.9, ups=0.5, wpb=2360, bsz=76, num_updates=2396, lr=2.8752e-05, gnorm=2.298, train_wall=4, gb_free=10.5, wall=7443
2024-08-17 19:32:40 | INFO | train_inner | epoch 026:     48 / 94 loss=7.494, nll_loss=4.437, ppl=21.66, wps=1308.7, ups=0.37, wpb=3516.5, bsz=176, num_updates=2398, lr=2.8776e-05, gnorm=2.001, train_wall=5, gb_free=10.4, wall=7448
2024-08-17 19:32:45 | INFO | train_inner | epoch 026:     50 / 94 loss=7.425, nll_loss=4.349, ppl=20.38, wps=1379, ups=0.43, wpb=3232, bsz=144, num_updates=2400, lr=2.88e-05, gnorm=1.91, train_wall=5, gb_free=14.7, wall=7453
2024-08-17 19:32:49 | INFO | train_inner | epoch 026:     52 / 94 loss=7.514, nll_loss=4.457, ppl=21.96, wps=1381.4, ups=0.45, wpb=3100.5, bsz=140, num_updates=2402, lr=2.8824e-05, gnorm=2.116, train_wall=4, gb_free=15.7, wall=7458
2024-08-17 19:32:54 | INFO | train_inner | epoch 026:     54 / 94 loss=7.4, nll_loss=4.307, ppl=19.79, wps=923.6, ups=0.43, wpb=2128, bsz=92, num_updates=2404, lr=2.8848e-05, gnorm=2.669, train_wall=5, gb_free=14.7, wall=7462
2024-08-17 19:32:58 | INFO | train_inner | epoch 026:     56 / 94 loss=7.42, nll_loss=4.308, ppl=19.8, wps=1661.7, ups=0.49, wpb=3409.5, bsz=120, num_updates=2406, lr=2.8872e-05, gnorm=2.113, train_wall=4, gb_free=11.4, wall=7466
2024-08-17 19:33:03 | INFO | train_inner | epoch 026:     58 / 94 loss=7.619, nll_loss=4.561, ppl=23.61, wps=1473.2, ups=0.38, wpb=3864, bsz=188, num_updates=2408, lr=2.8896e-05, gnorm=1.935, train_wall=5, gb_free=13.9, wall=7472
2024-08-17 19:33:08 | INFO | train_inner | epoch 026:     60 / 94 loss=7.417, nll_loss=4.309, ppl=19.82, wps=1062.7, ups=0.4, wpb=2633, bsz=136, num_updates=2410, lr=2.892e-05, gnorm=2.144, train_wall=5, gb_free=13.3, wall=7477
2024-08-17 19:33:13 | INFO | train_inner | epoch 026:     62 / 94 loss=7.406, nll_loss=4.297, ppl=19.65, wps=1059.8, ups=0.4, wpb=2631, bsz=116, num_updates=2412, lr=2.8944e-05, gnorm=2.273, train_wall=5, gb_free=11.2, wall=7482
2024-08-17 19:33:18 | INFO | train_inner | epoch 026:     64 / 94 loss=7.474, nll_loss=4.392, ppl=20.99, wps=1440.5, ups=0.43, wpb=3358, bsz=120, num_updates=2414, lr=2.8968e-05, gnorm=2.07, train_wall=5, gb_free=11.1, wall=7486
2024-08-17 19:33:23 | INFO | train_inner | epoch 026:     66 / 94 loss=7.498, nll_loss=4.46, ppl=22.01, wps=1184.3, ups=0.38, wpb=3118.5, bsz=216, num_updates=2416, lr=2.8992e-05, gnorm=1.795, train_wall=5, gb_free=10.6, wall=7491
2024-08-17 19:33:27 | INFO | train_inner | epoch 026:     68 / 94 loss=7.334, nll_loss=4.22, ppl=18.64, wps=878.8, ups=0.49, wpb=1785.5, bsz=52, num_updates=2418, lr=2.9016e-05, gnorm=2.686, train_wall=4, gb_free=12.6, wall=7496
2024-08-17 19:33:32 | INFO | train_inner | epoch 026:     70 / 94 loss=7.356, nll_loss=4.255, ppl=19.09, wps=1093.2, ups=0.4, wpb=2747, bsz=132, num_updates=2420, lr=2.904e-05, gnorm=1.961, train_wall=5, gb_free=10.4, wall=7501
2024-08-17 19:33:37 | INFO | train_inner | epoch 026:     72 / 94 loss=7.415, nll_loss=4.329, ppl=20.1, wps=1105.3, ups=0.45, wpb=2431, bsz=124, num_updates=2422, lr=2.9064e-05, gnorm=2.242, train_wall=4, gb_free=13.4, wall=7505
2024-08-17 19:33:41 | INFO | train_inner | epoch 026:     74 / 94 loss=7.559, nll_loss=4.503, ppl=22.68, wps=1471.5, ups=0.43, wpb=3408, bsz=172, num_updates=2424, lr=2.9088e-05, gnorm=1.952, train_wall=5, gb_free=13.5, wall=7510
2024-08-17 19:33:50 | INFO | train_inner | epoch 026:     76 / 94 loss=7.171, nll_loss=3.997, ppl=15.97, wps=426.7, ups=0.24, wpb=1757.5, bsz=48, num_updates=2426, lr=2.9112e-05, gnorm=2.604, train_wall=8, gb_free=18.6, wall=7518
2024-08-17 19:33:54 | INFO | train_inner | epoch 026:     78 / 94 loss=7.547, nll_loss=4.495, ppl=22.55, wps=1886.7, ups=0.43, wpb=4354.5, bsz=196, num_updates=2428, lr=2.9136e-05, gnorm=1.752, train_wall=5, gb_free=11.4, wall=7522
2024-08-17 19:33:58 | INFO | train_inner | epoch 026:     80 / 94 loss=7.287, nll_loss=4.152, ppl=17.77, wps=1162, ups=0.5, wpb=2332.5, bsz=72, num_updates=2430, lr=2.916e-05, gnorm=2.399, train_wall=4, gb_free=16.8, wall=7526
2024-08-17 19:34:04 | INFO | train_inner | epoch 026:     82 / 94 loss=7.492, nll_loss=4.416, ppl=21.34, wps=1118.5, ups=0.38, wpb=2978.5, bsz=160, num_updates=2432, lr=2.9184e-05, gnorm=2.07, train_wall=5, gb_free=14.8, wall=7532
2024-08-17 19:34:08 | INFO | train_inner | epoch 026:     84 / 94 loss=7.308, nll_loss=4.166, ppl=17.95, wps=914.1, ups=0.42, wpb=2201.5, bsz=88, num_updates=2434, lr=2.9208e-05, gnorm=2.293, train_wall=5, gb_free=15.5, wall=7537
2024-08-17 19:34:13 | INFO | train_inner | epoch 026:     86 / 94 loss=7.518, nll_loss=4.436, ppl=21.65, wps=1720.2, ups=0.44, wpb=3915, bsz=128, num_updates=2436, lr=2.9232e-05, gnorm=1.96, train_wall=5, gb_free=12.9, wall=7541
2024-08-17 19:34:17 | INFO | train_inner | epoch 026:     88 / 94 loss=7.374, nll_loss=4.282, ppl=19.45, wps=1211.1, ups=0.45, wpb=2706.5, bsz=124, num_updates=2438, lr=2.9256e-05, gnorm=2.087, train_wall=4, gb_free=13.4, wall=7546
2024-08-17 19:34:22 | INFO | train_inner | epoch 026:     90 / 94 loss=7.393, nll_loss=4.296, ppl=19.65, wps=864.8, ups=0.42, wpb=2067, bsz=84, num_updates=2440, lr=2.928e-05, gnorm=2.531, train_wall=5, gb_free=14.4, wall=7550
2024-08-17 19:34:27 | INFO | train_inner | epoch 026:     92 / 94 loss=7.385, nll_loss=4.299, ppl=19.68, wps=1023.4, ups=0.38, wpb=2665, bsz=140, num_updates=2442, lr=2.9304e-05, gnorm=2.266, train_wall=5, gb_free=11.1, wall=7556
2024-08-17 19:34:31 | INFO | train_inner | epoch 026:     94 / 94 loss=7.482, nll_loss=4.411, ppl=21.28, wps=1570.4, ups=0.61, wpb=2594.5, bsz=80, num_updates=2444, lr=2.9328e-05, gnorm=2.376, train_wall=3, gb_free=19.7, wall=7559
2024-08-17 19:34:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21027.05859375Mb; avail=234039.1171875Mb
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21027.05859375Mb; avail=234039.1171875Mb
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005331
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21027.05859375Mb; avail=234039.1171875Mb
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004809
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011037
2024-08-17 19:34:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21027.05859375Mb; avail=234039.1171875Mb
2024-08-17 19:34:44 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.969 | nll_loss 4.841 | ppl 28.67 | wps 2428.2 | wpb 944.1 | bsz 40.1 | num_updates 2444 | best_loss 7.925
2024-08-17 19:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2444 updates
2024-08-17 19:34:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:35:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 26 @ 2444 updates, score 7.969) (writing took 39.37856666184962 seconds)
2024-08-17 19:35:23 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-08-17 19:35:24 | INFO | train | epoch 026 | loss 7.427 | nll_loss 4.335 | ppl 20.19 | wps 976 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2444 | lr 2.9328e-05 | gnorm 2.181 | train_wall 220 | gb_free 19.7 | wall 7612
2024-08-17 19:35:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:35:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:35:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:35:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000812
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28632.11328125Mb; avail=226434.0625Mb
2024-08-17 19:35:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000163
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001490
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28632.11328125Mb; avail=226434.0625Mb
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000058
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28632.11328125Mb; avail=226434.0625Mb
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000557
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002452
2024-08-17 19:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28632.11328125Mb; avail=226434.0625Mb
2024-08-17 19:35:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:35:24 | INFO | fairseq.trainer | begin training epoch 27
2024-08-17 19:35:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:35:28 | INFO | train_inner | epoch 027:      2 / 94 loss=7.153, nll_loss=3.994, ppl=15.93, wps=61.5, ups=0.03, wpb=1763, bsz=68, num_updates=2446, lr=2.9352e-05, gnorm=2.75, train_wall=4, gb_free=13.3, wall=7616
2024-08-17 19:35:32 | INFO | train_inner | epoch 027:      4 / 94 loss=7.356, nll_loss=4.219, ppl=18.62, wps=1640.2, ups=0.45, wpb=3661, bsz=127, num_updates=2448, lr=2.9376e-05, gnorm=2.074, train_wall=4, gb_free=12.5, wall=7621
2024-08-17 19:35:37 | INFO | train_inner | epoch 027:      6 / 94 loss=7.33, nll_loss=4.206, ppl=18.46, wps=1252.3, ups=0.42, wpb=3002, bsz=168, num_updates=2450, lr=2.94e-05, gnorm=1.972, train_wall=5, gb_free=15.3, wall=7626
2024-08-17 19:35:42 | INFO | train_inner | epoch 027:      8 / 94 loss=7.48, nll_loss=4.402, ppl=21.14, wps=1714.4, ups=0.42, wpb=4078, bsz=188, num_updates=2452, lr=2.9424e-05, gnorm=1.799, train_wall=5, gb_free=15.1, wall=7630
2024-08-17 19:35:46 | INFO | train_inner | epoch 027:     10 / 94 loss=7.111, nll_loss=3.931, ppl=15.25, wps=908.7, ups=0.47, wpb=1945.5, bsz=72, num_updates=2454, lr=2.9448e-05, gnorm=2.409, train_wall=4, gb_free=11.2, wall=7635
2024-08-17 19:35:50 | INFO | train_inner | epoch 027:     12 / 94 loss=7.522, nll_loss=4.481, ppl=22.33, wps=1358.8, ups=0.48, wpb=2810.5, bsz=144, num_updates=2456, lr=2.9472e-05, gnorm=2.253, train_wall=4, gb_free=13.6, wall=7639
2024-08-17 19:35:54 | INFO | train_inner | epoch 027:     14 / 94 loss=7.106, nll_loss=3.91, ppl=15.03, wps=1015.7, ups=0.5, wpb=2023, bsz=72, num_updates=2458, lr=2.9496e-05, gnorm=2.369, train_wall=4, gb_free=14.6, wall=7643
2024-08-17 19:35:59 | INFO | train_inner | epoch 027:     16 / 94 loss=7.441, nll_loss=4.328, ppl=20.08, wps=1781.3, ups=0.41, wpb=4346.5, bsz=164, num_updates=2460, lr=2.952e-05, gnorm=1.738, train_wall=5, gb_free=13.4, wall=7648
2024-08-17 19:36:04 | INFO | train_inner | epoch 027:     18 / 94 loss=7.425, nll_loss=4.317, ppl=19.93, wps=1472, ups=0.39, wpb=3788, bsz=124, num_updates=2462, lr=2.9544e-05, gnorm=1.919, train_wall=5, gb_free=14.7, wall=7653
2024-08-17 19:36:09 | INFO | train_inner | epoch 027:     20 / 94 loss=7.138, nll_loss=3.97, ppl=15.67, wps=695, ups=0.41, wpb=1694, bsz=68, num_updates=2464, lr=2.9568e-05, gnorm=2.673, train_wall=5, gb_free=10.9, wall=7658
2024-08-17 19:36:14 | INFO | train_inner | epoch 027:     22 / 94 loss=7.275, nll_loss=4.127, ppl=17.48, wps=1245.1, ups=0.41, wpb=3069, bsz=112, num_updates=2466, lr=2.9592e-05, gnorm=1.977, train_wall=5, gb_free=15.6, wall=7663
2024-08-17 19:36:19 | INFO | train_inner | epoch 027:     24 / 94 loss=7.461, nll_loss=4.39, ppl=20.97, wps=1281.1, ups=0.42, wpb=3030, bsz=140, num_updates=2468, lr=2.9616e-05, gnorm=2.209, train_wall=5, gb_free=13.3, wall=7667
2024-08-17 19:36:24 | INFO | train_inner | epoch 027:     26 / 94 loss=7.53, nll_loss=4.487, ppl=22.43, wps=1399.1, ups=0.4, wpb=3506, bsz=184, num_updates=2470, lr=2.964e-05, gnorm=1.95, train_wall=5, gb_free=14, wall=7672
2024-08-17 19:36:28 | INFO | train_inner | epoch 027:     28 / 94 loss=7.221, nll_loss=4.081, ppl=16.92, wps=981.5, ups=0.47, wpb=2107.5, bsz=120, num_updates=2472, lr=2.9664e-05, gnorm=2.417, train_wall=4, gb_free=15, wall=7677
2024-08-17 19:36:34 | INFO | train_inner | epoch 027:     30 / 94 loss=7.481, nll_loss=4.384, ppl=20.89, wps=1477.7, ups=0.37, wpb=3947, bsz=188, num_updates=2474, lr=2.9688e-05, gnorm=1.809, train_wall=5, gb_free=10.2, wall=7682
2024-08-17 19:36:39 | INFO | train_inner | epoch 027:     32 / 94 loss=7.49, nll_loss=4.417, ppl=21.36, wps=1388, ups=0.39, wpb=3559, bsz=223.5, num_updates=2476, lr=2.9712e-05, gnorm=1.849, train_wall=5, gb_free=9.2, wall=7687
2024-08-17 19:36:43 | INFO | train_inner | epoch 027:     34 / 94 loss=7.392, nll_loss=4.282, ppl=19.46, wps=1571.8, ups=0.43, wpb=3650.5, bsz=132, num_updates=2478, lr=2.9736e-05, gnorm=1.797, train_wall=5, gb_free=12.1, wall=7692
2024-08-17 19:36:48 | INFO | train_inner | epoch 027:     36 / 94 loss=7.214, nll_loss=4.072, ppl=16.82, wps=920.4, ups=0.4, wpb=2322.5, bsz=108, num_updates=2480, lr=2.976e-05, gnorm=2.335, train_wall=5, gb_free=11.5, wall=7697
2024-08-17 19:36:53 | INFO | train_inner | epoch 027:     38 / 94 loss=7.515, nll_loss=4.467, ppl=22.12, wps=1570.2, ups=0.43, wpb=3689.5, bsz=144, num_updates=2482, lr=2.9784e-05, gnorm=1.878, train_wall=5, gb_free=10.8, wall=7701
2024-08-17 19:36:57 | INFO | train_inner | epoch 027:     40 / 94 loss=7.247, nll_loss=4.092, ppl=17.05, wps=1106.1, ups=0.49, wpb=2248.5, bsz=56, num_updates=2484, lr=2.9808e-05, gnorm=2.264, train_wall=4, gb_free=13.6, wall=7705
2024-08-17 19:37:02 | INFO | train_inner | epoch 027:     42 / 94 loss=7.4, nll_loss=4.297, ppl=19.66, wps=847.9, ups=0.39, wpb=2180, bsz=104, num_updates=2486, lr=2.9832e-05, gnorm=2.354, train_wall=5, gb_free=14.3, wall=7711
2024-08-17 19:37:07 | INFO | train_inner | epoch 027:     44 / 94 loss=7.112, nll_loss=3.911, ppl=15.05, wps=886.4, ups=0.43, wpb=2041, bsz=64, num_updates=2488, lr=2.9856e-05, gnorm=2.498, train_wall=5, gb_free=17.3, wall=7715
2024-08-17 19:37:12 | INFO | train_inner | epoch 027:     46 / 94 loss=7.35, nll_loss=4.21, ppl=18.5, wps=1102.4, ups=0.43, wpb=2581.5, bsz=80, num_updates=2490, lr=2.988e-05, gnorm=2.261, train_wall=5, gb_free=16, wall=7720
2024-08-17 19:37:16 | INFO | train_inner | epoch 027:     48 / 94 loss=7.275, nll_loss=4.126, ppl=17.46, wps=1111.8, ups=0.48, wpb=2297.5, bsz=68, num_updates=2492, lr=2.9904e-05, gnorm=2.487, train_wall=4, gb_free=18.2, wall=7724
2024-08-17 19:37:25 | INFO | train_inner | epoch 027:     50 / 94 loss=7.45, nll_loss=4.368, ppl=20.65, wps=589, ups=0.22, wpb=2680.5, bsz=116, num_updates=2494, lr=2.9928e-05, gnorm=2.322, train_wall=9, gb_free=13.7, wall=7733
2024-08-17 19:37:29 | INFO | train_inner | epoch 027:     52 / 94 loss=7.41, nll_loss=4.324, ppl=20.03, wps=1293, ups=0.48, wpb=2673.5, bsz=124, num_updates=2496, lr=2.9952e-05, gnorm=2.194, train_wall=4, gb_free=14.1, wall=7737
2024-08-17 19:37:34 | INFO | train_inner | epoch 027:     54 / 94 loss=7.515, nll_loss=4.457, ppl=21.97, wps=1630.1, ups=0.41, wpb=3975.5, bsz=168, num_updates=2498, lr=2.9976e-05, gnorm=1.849, train_wall=5, gb_free=13, wall=7742
2024-08-17 19:37:39 | INFO | train_inner | epoch 027:     56 / 94 loss=7.195, nll_loss=4.02, ppl=16.22, wps=934.5, ups=0.41, wpb=2305, bsz=88, num_updates=2500, lr=3e-05, gnorm=2.393, train_wall=5, gb_free=15.6, wall=7747
2024-08-17 19:37:43 | INFO | train_inner | epoch 027:     58 / 94 loss=7.155, nll_loss=3.983, ppl=15.81, wps=968.3, ups=0.5, wpb=1935.5, bsz=68, num_updates=2502, lr=2.9988e-05, gnorm=2.732, train_wall=4, gb_free=12.7, wall=7751
2024-08-17 19:37:47 | INFO | train_inner | epoch 027:     60 / 94 loss=7.457, nll_loss=4.389, ppl=20.96, wps=1315.6, ups=0.46, wpb=2846, bsz=164, num_updates=2504, lr=2.9976e-05, gnorm=2.405, train_wall=4, gb_free=12.6, wall=7755
2024-08-17 19:37:51 | INFO | train_inner | epoch 027:     62 / 94 loss=7.247, nll_loss=4.113, ppl=17.31, wps=891.7, ups=0.46, wpb=1930.5, bsz=68, num_updates=2506, lr=2.99641e-05, gnorm=2.644, train_wall=4, gb_free=15.3, wall=7760
2024-08-17 19:37:56 | INFO | train_inner | epoch 027:     64 / 94 loss=7.353, nll_loss=4.238, ppl=18.87, wps=1252.8, ups=0.46, wpb=2746.5, bsz=148, num_updates=2508, lr=2.99521e-05, gnorm=2.232, train_wall=4, gb_free=13.1, wall=7764
2024-08-17 19:38:00 | INFO | train_inner | epoch 027:     66 / 94 loss=7.37, nll_loss=4.249, ppl=19.01, wps=1481.2, ups=0.43, wpb=3422, bsz=84, num_updates=2510, lr=2.99402e-05, gnorm=2.209, train_wall=5, gb_free=15.5, wall=7769
2024-08-17 19:38:05 | INFO | train_inner | epoch 027:     68 / 94 loss=7.475, nll_loss=4.384, ppl=20.88, wps=1532.8, ups=0.49, wpb=3120.5, bsz=92, num_updates=2512, lr=2.99283e-05, gnorm=2.329, train_wall=4, gb_free=12.4, wall=7773
2024-08-17 19:38:09 | INFO | train_inner | epoch 027:     70 / 94 loss=7.415, nll_loss=4.318, ppl=19.95, wps=1364.8, ups=0.5, wpb=2706.5, bsz=88, num_updates=2514, lr=2.99164e-05, gnorm=2.156, train_wall=4, gb_free=13.2, wall=7777
2024-08-17 19:38:13 | INFO | train_inner | epoch 027:     72 / 94 loss=7.418, nll_loss=4.326, ppl=20.06, wps=1452.5, ups=0.48, wpb=3014.5, bsz=88, num_updates=2516, lr=2.99045e-05, gnorm=2.171, train_wall=4, gb_free=13.5, wall=7781
2024-08-17 19:38:18 | INFO | train_inner | epoch 027:     74 / 94 loss=7.124, nll_loss=3.953, ppl=15.49, wps=867.2, ups=0.41, wpb=2117.5, bsz=104, num_updates=2518, lr=2.98926e-05, gnorm=2.299, train_wall=5, gb_free=13.2, wall=7786
2024-08-17 19:38:22 | INFO | train_inner | epoch 027:     76 / 94 loss=7.443, nll_loss=4.364, ppl=20.59, wps=1262.7, ups=0.48, wpb=2652.5, bsz=144, num_updates=2520, lr=2.98807e-05, gnorm=2.137, train_wall=4, gb_free=14.4, wall=7790
2024-08-17 19:38:27 | INFO | train_inner | epoch 027:     78 / 94 loss=7.431, nll_loss=4.342, ppl=20.28, wps=1154.3, ups=0.41, wpb=2813, bsz=136, num_updates=2522, lr=2.98689e-05, gnorm=2.074, train_wall=5, gb_free=9.7, wall=7795
2024-08-17 19:38:32 | INFO | train_inner | epoch 027:     80 / 94 loss=7.494, nll_loss=4.424, ppl=21.46, wps=1563.3, ups=0.37, wpb=4259.5, bsz=216, num_updates=2524, lr=2.9857e-05, gnorm=1.622, train_wall=5, gb_free=10.9, wall=7800
2024-08-17 19:38:38 | INFO | train_inner | epoch 027:     82 / 94 loss=7.447, nll_loss=4.359, ppl=20.52, wps=1181.7, ups=0.35, wpb=3329, bsz=168, num_updates=2526, lr=2.98452e-05, gnorm=1.89, train_wall=6, gb_free=10.8, wall=7806
2024-08-17 19:38:42 | INFO | train_inner | epoch 027:     84 / 94 loss=7.368, nll_loss=4.265, ppl=19.22, wps=1123.8, ups=0.5, wpb=2232.5, bsz=96, num_updates=2528, lr=2.98334e-05, gnorm=2.411, train_wall=4, gb_free=11.3, wall=7810
2024-08-17 19:38:46 | INFO | train_inner | epoch 027:     86 / 94 loss=7.37, nll_loss=4.26, ppl=19.16, wps=1288.5, ups=0.44, wpb=2904.5, bsz=140, num_updates=2530, lr=2.98216e-05, gnorm=2.004, train_wall=4, gb_free=15.2, wall=7814
2024-08-17 19:38:51 | INFO | train_inner | epoch 027:     88 / 94 loss=7.428, nll_loss=4.333, ppl=20.15, wps=990.2, ups=0.45, wpb=2219, bsz=76, num_updates=2532, lr=2.98098e-05, gnorm=2.409, train_wall=4, gb_free=9.3, wall=7819
2024-08-17 19:38:55 | INFO | train_inner | epoch 027:     90 / 94 loss=7.573, nll_loss=4.529, ppl=23.09, wps=1564.2, ups=0.43, wpb=3653.5, bsz=128, num_updates=2534, lr=2.97981e-05, gnorm=2.141, train_wall=5, gb_free=14, wall=7824
2024-08-17 19:39:01 | INFO | train_inner | epoch 027:     92 / 94 loss=7.515, nll_loss=4.433, ppl=21.6, wps=1361.8, ups=0.38, wpb=3566.5, bsz=168, num_updates=2536, lr=2.97863e-05, gnorm=1.894, train_wall=5, gb_free=14.5, wall=7829
2024-08-17 19:39:03 | INFO | train_inner | epoch 027:     94 / 94 loss=6.981, nll_loss=3.773, ppl=13.67, wps=842, ups=0.81, wpb=1044, bsz=20, num_updates=2538, lr=2.97746e-05, gnorm=3.264, train_wall=2, gb_free=21.7, wall=7831
2024-08-17 19:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21018.64453125Mb; avail=234047.484375Mb
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21018.64453125Mb; avail=234047.484375Mb
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005402
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21018.64453125Mb; avail=234047.484375Mb
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004793
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011091
2024-08-17 19:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21018.64453125Mb; avail=234047.484375Mb
2024-08-17 19:39:17 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.979 | nll_loss 4.836 | ppl 28.56 | wps 2417.4 | wpb 944.1 | bsz 40.1 | num_updates 2538 | best_loss 7.925
2024-08-17 19:39:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2538 updates
2024-08-17 19:39:17 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:40:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 27 @ 2538 updates, score 7.979) (writing took 44.377540840767324 seconds)
2024-08-17 19:40:01 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-08-17 19:40:01 | INFO | train | epoch 027 | loss 7.385 | nll_loss 4.279 | ppl 19.42 | wps 962.3 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2538 | lr 2.97746e-05 | gnorm 2.209 | train_wall 219 | gb_free 21.7 | wall 7889
2024-08-17 19:40:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:40:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:40:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:40:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000797
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31944.05859375Mb; avail=223122.0859375Mb
2024-08-17 19:40:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000190
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001501
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31944.05859375Mb; avail=223122.0859375Mb
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31944.05859375Mb; avail=223122.0859375Mb
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000536
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002366
2024-08-17 19:40:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31944.05859375Mb; avail=223122.0859375Mb
2024-08-17 19:40:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:40:01 | INFO | fairseq.trainer | begin training epoch 28
2024-08-17 19:40:01 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:40:06 | INFO | train_inner | epoch 028:      2 / 94 loss=7.308, nll_loss=4.171, ppl=18.01, wps=99.8, ups=0.03, wpb=3119, bsz=144, num_updates=2540, lr=2.97628e-05, gnorm=2.104, train_wall=5, gb_free=12.7, wall=7894
2024-08-17 19:40:10 | INFO | train_inner | epoch 028:      4 / 94 loss=7.344, nll_loss=4.217, ppl=18.6, wps=1440.2, ups=0.45, wpb=3232, bsz=103, num_updates=2542, lr=2.97511e-05, gnorm=2.224, train_wall=4, gb_free=12.9, wall=7898
2024-08-17 19:40:15 | INFO | train_inner | epoch 028:      6 / 94 loss=7.251, nll_loss=4.098, ppl=17.12, wps=1371.6, ups=0.43, wpb=3207, bsz=100, num_updates=2544, lr=2.97394e-05, gnorm=2.122, train_wall=5, gb_free=11.6, wall=7903
2024-08-17 19:40:19 | INFO | train_inner | epoch 028:      8 / 94 loss=7.341, nll_loss=4.229, ppl=18.75, wps=1545.9, ups=0.49, wpb=3131.5, bsz=120, num_updates=2546, lr=2.97278e-05, gnorm=2.055, train_wall=4, gb_free=12.7, wall=7907
2024-08-17 19:40:24 | INFO | train_inner | epoch 028:     10 / 94 loss=7.22, nll_loss=4.07, ppl=16.8, wps=1233.7, ups=0.42, wpb=2937, bsz=132, num_updates=2548, lr=2.97161e-05, gnorm=2.085, train_wall=5, gb_free=17.6, wall=7912
2024-08-17 19:40:28 | INFO | train_inner | epoch 028:     12 / 94 loss=7.377, nll_loss=4.262, ppl=19.18, wps=1524.7, ups=0.4, wpb=3766.5, bsz=160, num_updates=2550, lr=2.97044e-05, gnorm=1.902, train_wall=5, gb_free=14.7, wall=7917
2024-08-17 19:40:33 | INFO | train_inner | epoch 028:     14 / 94 loss=7.104, nll_loss=3.916, ppl=15.09, wps=986, ups=0.47, wpb=2114.5, bsz=80, num_updates=2552, lr=2.96928e-05, gnorm=2.226, train_wall=4, gb_free=13.4, wall=7921
2024-08-17 19:40:37 | INFO | train_inner | epoch 028:     16 / 94 loss=7.336, nll_loss=4.208, ppl=18.48, wps=1317.4, ups=0.52, wpb=2534, bsz=96, num_updates=2554, lr=2.96812e-05, gnorm=2.327, train_wall=4, gb_free=13.5, wall=7925
2024-08-17 19:40:42 | INFO | train_inner | epoch 028:     18 / 94 loss=7.454, nll_loss=4.365, ppl=20.61, wps=1342.6, ups=0.38, wpb=3540, bsz=132, num_updates=2556, lr=2.96695e-05, gnorm=1.989, train_wall=5, gb_free=11.3, wall=7930
2024-08-17 19:40:47 | INFO | train_inner | epoch 028:     20 / 94 loss=7.315, nll_loss=4.205, ppl=18.44, wps=1216.7, ups=0.37, wpb=3267.5, bsz=172, num_updates=2558, lr=2.96579e-05, gnorm=1.955, train_wall=5, gb_free=10.1, wall=7936
2024-08-17 19:40:52 | INFO | train_inner | epoch 028:     22 / 94 loss=7.222, nll_loss=4.074, ppl=16.84, wps=1325.1, ups=0.41, wpb=3219, bsz=136, num_updates=2560, lr=2.96464e-05, gnorm=1.926, train_wall=5, gb_free=12.8, wall=7940
2024-08-17 19:40:57 | INFO | train_inner | epoch 028:     24 / 94 loss=7.429, nll_loss=4.321, ppl=19.99, wps=1721.7, ups=0.45, wpb=3832, bsz=120, num_updates=2562, lr=2.96348e-05, gnorm=1.954, train_wall=4, gb_free=16.7, wall=7945
2024-08-17 19:41:06 | INFO | train_inner | epoch 028:     26 / 94 loss=6.637, nll_loss=3.325, ppl=10.02, wps=290.7, ups=0.22, wpb=1301, bsz=48, num_updates=2564, lr=2.96232e-05, gnorm=2.759, train_wall=9, gb_free=17, wall=7954
2024-08-17 19:41:10 | INFO | train_inner | epoch 028:     28 / 94 loss=7.331, nll_loss=4.194, ppl=18.31, wps=1469.2, ups=0.5, wpb=2949, bsz=104, num_updates=2566, lr=2.96117e-05, gnorm=2.073, train_wall=4, gb_free=10.9, wall=7958
2024-08-17 19:41:14 | INFO | train_inner | epoch 028:     30 / 94 loss=7.314, nll_loss=4.189, ppl=18.24, wps=1224.5, ups=0.42, wpb=2944.5, bsz=144, num_updates=2568, lr=2.96001e-05, gnorm=2.126, train_wall=5, gb_free=12.8, wall=7963
2024-08-17 19:41:19 | INFO | train_inner | epoch 028:     32 / 94 loss=6.719, nll_loss=3.427, ppl=10.75, wps=576.4, ups=0.46, wpb=1252, bsz=56, num_updates=2570, lr=2.95886e-05, gnorm=2.873, train_wall=4, gb_free=14.9, wall=7967
2024-08-17 19:41:24 | INFO | train_inner | epoch 028:     34 / 94 loss=7.317, nll_loss=4.195, ppl=18.32, wps=1027.9, ups=0.41, wpb=2498, bsz=111.5, num_updates=2572, lr=2.95771e-05, gnorm=2.293, train_wall=5, gb_free=15.1, wall=7972
2024-08-17 19:41:28 | INFO | train_inner | epoch 028:     36 / 94 loss=7.348, nll_loss=4.211, ppl=18.52, wps=1533.2, ups=0.46, wpb=3358.5, bsz=96, num_updates=2574, lr=2.95656e-05, gnorm=2.049, train_wall=4, gb_free=14, wall=7976
2024-08-17 19:41:34 | INFO | train_inner | epoch 028:     38 / 94 loss=7.239, nll_loss=4.097, ppl=17.12, wps=1015, ups=0.35, wpb=2878, bsz=152, num_updates=2576, lr=2.95541e-05, gnorm=1.946, train_wall=6, gb_free=11.8, wall=7982
2024-08-17 19:41:38 | INFO | train_inner | epoch 028:     40 / 94 loss=7.381, nll_loss=4.275, ppl=19.36, wps=1594.8, ups=0.49, wpb=3279.5, bsz=76, num_updates=2578, lr=2.95427e-05, gnorm=2.123, train_wall=4, gb_free=15.5, wall=7986
2024-08-17 19:41:42 | INFO | train_inner | epoch 028:     42 / 94 loss=7.193, nll_loss=4.042, ppl=16.47, wps=1037.8, ups=0.43, wpb=2428.5, bsz=92, num_updates=2580, lr=2.95312e-05, gnorm=2.219, train_wall=5, gb_free=14.8, wall=7991
2024-08-17 19:41:47 | INFO | train_inner | epoch 028:     44 / 94 loss=7.29, nll_loss=4.172, ppl=18.02, wps=1081.3, ups=0.45, wpb=2413.5, bsz=124, num_updates=2582, lr=2.95198e-05, gnorm=2.405, train_wall=4, gb_free=18.4, wall=7995
2024-08-17 19:41:52 | INFO | train_inner | epoch 028:     46 / 94 loss=7.151, nll_loss=3.983, ppl=15.81, wps=1083.7, ups=0.42, wpb=2582, bsz=108, num_updates=2584, lr=2.95084e-05, gnorm=2.107, train_wall=5, gb_free=14.3, wall=8000
2024-08-17 19:41:56 | INFO | train_inner | epoch 028:     48 / 94 loss=7.415, nll_loss=4.33, ppl=20.12, wps=1531, ups=0.44, wpb=3493, bsz=152, num_updates=2586, lr=2.94969e-05, gnorm=2.057, train_wall=5, gb_free=10.8, wall=8004
2024-08-17 19:42:01 | INFO | train_inner | epoch 028:     50 / 94 loss=7.285, nll_loss=4.143, ppl=17.67, wps=1341.2, ups=0.43, wpb=3089, bsz=128, num_updates=2588, lr=2.94855e-05, gnorm=1.924, train_wall=5, gb_free=9.5, wall=8009
2024-08-17 19:42:05 | INFO | train_inner | epoch 028:     52 / 94 loss=7.23, nll_loss=4.082, ppl=16.93, wps=1248.3, ups=0.46, wpb=2735, bsz=108, num_updates=2590, lr=2.94742e-05, gnorm=2.142, train_wall=4, gb_free=13.5, wall=8013
2024-08-17 19:42:10 | INFO | train_inner | epoch 028:     54 / 94 loss=7.294, nll_loss=4.164, ppl=17.93, wps=1281.4, ups=0.46, wpb=2776.5, bsz=136, num_updates=2592, lr=2.94628e-05, gnorm=2.172, train_wall=4, gb_free=14.7, wall=8018
2024-08-17 19:42:15 | INFO | train_inner | epoch 028:     56 / 94 loss=7.227, nll_loss=4.073, ppl=16.83, wps=949.5, ups=0.39, wpb=2431, bsz=124, num_updates=2594, lr=2.94514e-05, gnorm=2.193, train_wall=5, gb_free=14.5, wall=8023
2024-08-17 19:42:19 | INFO | train_inner | epoch 028:     58 / 94 loss=7.199, nll_loss=4.043, ppl=16.49, wps=1113.4, ups=0.44, wpb=2540, bsz=72, num_updates=2596, lr=2.94401e-05, gnorm=2.23, train_wall=5, gb_free=11.6, wall=8027
2024-08-17 19:42:23 | INFO | train_inner | epoch 028:     60 / 94 loss=7.012, nll_loss=3.819, ppl=14.11, wps=967.9, ups=0.53, wpb=1809.5, bsz=60, num_updates=2598, lr=2.94287e-05, gnorm=2.632, train_wall=4, gb_free=18.8, wall=8031
2024-08-17 19:42:28 | INFO | train_inner | epoch 028:     62 / 94 loss=7.475, nll_loss=4.405, ppl=21.19, wps=1274.2, ups=0.39, wpb=3232.5, bsz=184, num_updates=2600, lr=2.94174e-05, gnorm=2.019, train_wall=5, gb_free=11.1, wall=8036
2024-08-17 19:42:33 | INFO | train_inner | epoch 028:     64 / 94 loss=7.411, nll_loss=4.315, ppl=19.9, wps=1219.3, ups=0.4, wpb=3044, bsz=156, num_updates=2602, lr=2.94061e-05, gnorm=2.066, train_wall=5, gb_free=10.2, wall=8041
2024-08-17 19:42:38 | INFO | train_inner | epoch 028:     66 / 94 loss=7.327, nll_loss=4.195, ppl=18.31, wps=1204.5, ups=0.43, wpb=2772.5, bsz=88, num_updates=2604, lr=2.93948e-05, gnorm=2.301, train_wall=5, gb_free=14.1, wall=8046
2024-08-17 19:42:42 | INFO | train_inner | epoch 028:     68 / 94 loss=7.077, nll_loss=3.861, ppl=14.53, wps=836.6, ups=0.42, wpb=1983, bsz=88, num_updates=2606, lr=2.93835e-05, gnorm=2.261, train_wall=5, gb_free=14.7, wall=8051
2024-08-17 19:42:47 | INFO | train_inner | epoch 028:     70 / 94 loss=7.448, nll_loss=4.346, ppl=20.34, wps=1408.3, ups=0.44, wpb=3205, bsz=136, num_updates=2608, lr=2.93723e-05, gnorm=1.995, train_wall=5, gb_free=9.6, wall=8055
2024-08-17 19:42:51 | INFO | train_inner | epoch 028:     72 / 94 loss=7.237, nll_loss=4.107, ppl=17.23, wps=1243.7, ups=0.48, wpb=2583.5, bsz=116, num_updates=2610, lr=2.9361e-05, gnorm=2.109, train_wall=4, gb_free=14.9, wall=8059
2024-08-17 19:42:56 | INFO | train_inner | epoch 028:     74 / 94 loss=7.218, nll_loss=4.075, ppl=16.86, wps=869.5, ups=0.39, wpb=2216, bsz=104, num_updates=2612, lr=2.93498e-05, gnorm=2.279, train_wall=5, gb_free=13.3, wall=8064
2024-08-17 19:43:00 | INFO | train_inner | epoch 028:     76 / 94 loss=7.358, nll_loss=4.268, ppl=19.27, wps=1295.3, ups=0.48, wpb=2680.5, bsz=120, num_updates=2614, lr=2.93385e-05, gnorm=2.254, train_wall=4, gb_free=14.8, wall=8069
2024-08-17 19:43:05 | INFO | train_inner | epoch 028:     78 / 94 loss=7.282, nll_loss=4.143, ppl=17.67, wps=1250.1, ups=0.44, wpb=2814.5, bsz=116, num_updates=2616, lr=2.93273e-05, gnorm=2.074, train_wall=4, gb_free=15.3, wall=8073
2024-08-17 19:43:10 | INFO | train_inner | epoch 028:     80 / 94 loss=7.485, nll_loss=4.404, ppl=21.16, wps=1557.6, ups=0.37, wpb=4245.5, bsz=224, num_updates=2618, lr=2.93161e-05, gnorm=1.845, train_wall=5, gb_free=12.5, wall=8079
2024-08-17 19:43:13 | INFO | train_inner | epoch 028:     82 / 94 loss=7.185, nll_loss=4.03, ppl=16.34, wps=1225.2, ups=0.65, wpb=1899, bsz=76, num_updates=2620, lr=2.93049e-05, gnorm=2.58, train_wall=3, gb_free=14.7, wall=8082
2024-08-17 19:43:18 | INFO | train_inner | epoch 028:     84 / 94 loss=7.161, nll_loss=3.986, ppl=15.85, wps=759.5, ups=0.41, wpb=1840, bsz=76, num_updates=2622, lr=2.92937e-05, gnorm=2.618, train_wall=5, gb_free=15.5, wall=8086
2024-08-17 19:43:23 | INFO | train_inner | epoch 028:     86 / 94 loss=7.319, nll_loss=4.184, ppl=18.17, wps=1213, ups=0.43, wpb=2829.5, bsz=108, num_updates=2624, lr=2.92826e-05, gnorm=2.186, train_wall=5, gb_free=14.2, wall=8091
2024-08-17 19:43:28 | INFO | train_inner | epoch 028:     88 / 94 loss=7.427, nll_loss=4.326, ppl=20.06, wps=1336, ups=0.37, wpb=3635, bsz=156, num_updates=2626, lr=2.92714e-05, gnorm=1.893, train_wall=5, gb_free=10.5, wall=8097
2024-08-17 19:43:33 | INFO | train_inner | epoch 028:     90 / 94 loss=7.355, nll_loss=4.251, ppl=19.04, wps=985.6, ups=0.4, wpb=2469.5, bsz=112, num_updates=2628, lr=2.92603e-05, gnorm=2.222, train_wall=5, gb_free=9.7, wall=8102
2024-08-17 19:43:38 | INFO | train_inner | epoch 028:     92 / 94 loss=7.492, nll_loss=4.433, ppl=21.6, wps=1525.1, ups=0.39, wpb=3941, bsz=208, num_updates=2630, lr=2.92492e-05, gnorm=1.886, train_wall=5, gb_free=12.6, wall=8107
2024-08-17 19:43:42 | INFO | train_inner | epoch 028:     94 / 94 loss=7.513, nll_loss=4.448, ppl=21.83, wps=1728.6, ups=0.5, wpb=3439.5, bsz=156, num_updates=2632, lr=2.9238e-05, gnorm=2.287, train_wall=4, gb_free=19.5, wall=8111
2024-08-17 19:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21839.3671875Mb; avail=233226.7734375Mb
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21839.3671875Mb; avail=233226.7734375Mb
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005378
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21839.3671875Mb; avail=233226.7734375Mb
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004768
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011074
2024-08-17 19:43:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21839.3671875Mb; avail=233226.7734375Mb
2024-08-17 19:43:56 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.967 | nll_loss 4.838 | ppl 28.6 | wps 2425.7 | wpb 944.1 | bsz 40.1 | num_updates 2632 | best_loss 7.925
2024-08-17 19:43:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2632 updates
2024-08-17 19:43:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:44:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:44:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 28 @ 2632 updates, score 7.967) (writing took 39.08522132318467 seconds)
2024-08-17 19:44:35 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-08-17 19:44:35 | INFO | train | epoch 028 | loss 7.31 | nll_loss 4.183 | ppl 18.16 | wps 974.1 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2632 | lr 2.9238e-05 | gnorm 2.172 | train_wall 221 | gb_free 19.5 | wall 8163
2024-08-17 19:44:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:44:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:44:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:44:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000714
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29524.109375Mb; avail=225542.03125Mb
2024-08-17 19:44:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000157
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001449
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29524.109375Mb; avail=225542.03125Mb
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29524.109375Mb; avail=225542.03125Mb
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000535
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002322
2024-08-17 19:44:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29524.109375Mb; avail=225542.03125Mb
2024-08-17 19:44:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:44:35 | INFO | fairseq.trainer | begin training epoch 29
2024-08-17 19:44:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:44:40 | INFO | train_inner | epoch 029:      2 / 94 loss=7.126, nll_loss=3.942, ppl=15.37, wps=89, ups=0.03, wpb=2557, bsz=96, num_updates=2634, lr=2.92269e-05, gnorm=2.143, train_wall=5, gb_free=10, wall=8168
2024-08-17 19:44:44 | INFO | train_inner | epoch 029:      4 / 94 loss=7.326, nll_loss=4.202, ppl=18.4, wps=1715.1, ups=0.47, wpb=3641, bsz=112, num_updates=2636, lr=2.92159e-05, gnorm=1.993, train_wall=4, gb_free=15.1, wall=8172
2024-08-17 19:44:49 | INFO | train_inner | epoch 029:      6 / 94 loss=7.155, nll_loss=3.966, ppl=15.63, wps=1413.7, ups=0.42, wpb=3330.5, bsz=140, num_updates=2638, lr=2.92048e-05, gnorm=1.91, train_wall=5, gb_free=13, wall=8177
2024-08-17 19:44:54 | INFO | train_inner | epoch 029:      8 / 94 loss=7.103, nll_loss=3.929, ppl=15.23, wps=894.7, ups=0.41, wpb=2176.5, bsz=116, num_updates=2640, lr=2.91937e-05, gnorm=2.36, train_wall=5, gb_free=10.8, wall=8182
2024-08-17 19:44:59 | INFO | train_inner | epoch 029:     10 / 94 loss=7.251, nll_loss=4.099, ppl=17.13, wps=1113, ups=0.4, wpb=2793.5, bsz=84, num_updates=2642, lr=2.91827e-05, gnorm=2.235, train_wall=5, gb_free=15.5, wall=8187
2024-08-17 19:45:02 | INFO | train_inner | epoch 029:     12 / 94 loss=6.961, nll_loss=3.731, ppl=13.28, wps=1024.5, ups=0.66, wpb=1556.5, bsz=44, num_updates=2644, lr=2.91716e-05, gnorm=2.812, train_wall=3, gb_free=14.4, wall=8190
2024-08-17 19:45:07 | INFO | train_inner | epoch 029:     14 / 94 loss=7.432, nll_loss=4.338, ppl=20.22, wps=1726.8, ups=0.39, wpb=4421, bsz=168, num_updates=2646, lr=2.91606e-05, gnorm=1.912, train_wall=5, gb_free=13.8, wall=8195
2024-08-17 19:45:11 | INFO | train_inner | epoch 029:     16 / 94 loss=7.141, nll_loss=3.98, ppl=15.78, wps=905, ups=0.44, wpb=2043, bsz=96, num_updates=2648, lr=2.91496e-05, gnorm=2.395, train_wall=5, gb_free=11.3, wall=8200
2024-08-17 19:45:16 | INFO | train_inner | epoch 029:     18 / 94 loss=7.05, nll_loss=3.851, ppl=14.43, wps=872, ups=0.46, wpb=1915.5, bsz=55.5, num_updates=2650, lr=2.91386e-05, gnorm=2.472, train_wall=4, gb_free=11.4, wall=8204
2024-08-17 19:45:21 | INFO | train_inner | epoch 029:     20 / 94 loss=7.051, nll_loss=3.839, ppl=14.31, wps=1264.5, ups=0.43, wpb=2925.5, bsz=83, num_updates=2652, lr=2.91276e-05, gnorm=2.102, train_wall=5, gb_free=12.1, wall=8209
2024-08-17 19:45:25 | INFO | train_inner | epoch 029:     22 / 94 loss=7.243, nll_loss=4.09, ppl=17.03, wps=1513.9, ups=0.48, wpb=3168, bsz=84, num_updates=2654, lr=2.91166e-05, gnorm=2.221, train_wall=4, gb_free=15.8, wall=8213
2024-08-17 19:45:30 | INFO | train_inner | epoch 029:     24 / 94 loss=7.241, nll_loss=4.094, ppl=17.07, wps=1153.2, ups=0.39, wpb=2951, bsz=144, num_updates=2656, lr=2.91056e-05, gnorm=2.102, train_wall=5, gb_free=14.9, wall=8218
2024-08-17 19:45:35 | INFO | train_inner | epoch 029:     26 / 94 loss=7.402, nll_loss=4.314, ppl=19.89, wps=1405.1, ups=0.42, wpb=3311, bsz=204, num_updates=2658, lr=2.90947e-05, gnorm=1.991, train_wall=5, gb_free=10.9, wall=8223
2024-08-17 19:45:40 | INFO | train_inner | epoch 029:     28 / 94 loss=7.179, nll_loss=3.998, ppl=15.98, wps=1149.1, ups=0.39, wpb=2972.5, bsz=132, num_updates=2660, lr=2.90838e-05, gnorm=2.059, train_wall=5, gb_free=14.2, wall=8228
2024-08-17 19:45:44 | INFO | train_inner | epoch 029:     30 / 94 loss=7.274, nll_loss=4.126, ppl=17.45, wps=1394.5, ups=0.42, wpb=3289, bsz=116, num_updates=2662, lr=2.90728e-05, gnorm=2.233, train_wall=5, gb_free=11.6, wall=8233
2024-08-17 19:45:49 | INFO | train_inner | epoch 029:     32 / 94 loss=7.339, nll_loss=4.217, ppl=18.59, wps=1506.9, ups=0.41, wpb=3710, bsz=160, num_updates=2664, lr=2.90619e-05, gnorm=1.872, train_wall=5, gb_free=12.1, wall=8238
2024-08-17 19:45:55 | INFO | train_inner | epoch 029:     34 / 94 loss=7.366, nll_loss=4.259, ppl=19.15, wps=1286.7, ups=0.38, wpb=3389, bsz=144, num_updates=2666, lr=2.9051e-05, gnorm=2.203, train_wall=5, gb_free=9, wall=8243
2024-08-17 19:45:59 | INFO | train_inner | epoch 029:     36 / 94 loss=7.445, nll_loss=4.371, ppl=20.69, wps=1265.5, ups=0.45, wpb=2805, bsz=136, num_updates=2668, lr=2.90401e-05, gnorm=2.305, train_wall=4, gb_free=16.7, wall=8247
2024-08-17 19:46:03 | INFO | train_inner | epoch 029:     38 / 94 loss=7.137, nll_loss=3.957, ppl=15.53, wps=1405.3, ups=0.57, wpb=2463.5, bsz=52, num_updates=2670, lr=2.90292e-05, gnorm=2.296, train_wall=3, gb_free=14.7, wall=8251
2024-08-17 19:46:07 | INFO | train_inner | epoch 029:     40 / 94 loss=7.306, nll_loss=4.18, ppl=18.12, wps=1302, ups=0.43, wpb=3053, bsz=144, num_updates=2672, lr=2.90184e-05, gnorm=1.953, train_wall=5, gb_free=13.9, wall=8256
2024-08-17 19:46:12 | INFO | train_inner | epoch 029:     42 / 94 loss=7.332, nll_loss=4.221, ppl=18.65, wps=1421, ups=0.4, wpb=3576, bsz=208, num_updates=2674, lr=2.90075e-05, gnorm=1.835, train_wall=5, gb_free=14.3, wall=8261
2024-08-17 19:46:17 | INFO | train_inner | epoch 029:     44 / 94 loss=7.088, nll_loss=3.896, ppl=14.89, wps=1008, ups=0.46, wpb=2186, bsz=60, num_updates=2676, lr=2.89967e-05, gnorm=2.541, train_wall=4, gb_free=12.5, wall=8265
2024-08-17 19:46:22 | INFO | train_inner | epoch 029:     46 / 94 loss=7.04, nll_loss=3.821, ppl=14.13, wps=789.1, ups=0.4, wpb=1980, bsz=92, num_updates=2678, lr=2.89858e-05, gnorm=2.425, train_wall=5, gb_free=12.2, wall=8270
2024-08-17 19:46:27 | INFO | train_inner | epoch 029:     48 / 94 loss=7.142, nll_loss=3.96, ppl=15.56, wps=1029.9, ups=0.4, wpb=2567.5, bsz=108, num_updates=2680, lr=2.8975e-05, gnorm=2.335, train_wall=5, gb_free=13.3, wall=8275
2024-08-17 19:46:31 | INFO | train_inner | epoch 029:     50 / 94 loss=7.228, nll_loss=4.088, ppl=17.01, wps=952.1, ups=0.45, wpb=2096.5, bsz=64, num_updates=2682, lr=2.89642e-05, gnorm=2.706, train_wall=4, gb_free=13.8, wall=8279
2024-08-17 19:46:34 | INFO | train_inner | epoch 029:     52 / 94 loss=6.919, nll_loss=3.691, ppl=12.92, wps=765.8, ups=0.63, wpb=1218.5, bsz=40, num_updates=2684, lr=2.89534e-05, gnorm=3.163, train_wall=3, gb_free=18.8, wall=8282
2024-08-17 19:46:38 | INFO | train_inner | epoch 029:     54 / 94 loss=7.058, nll_loss=3.856, ppl=14.48, wps=1164.4, ups=0.55, wpb=2129.5, bsz=72, num_updates=2686, lr=2.89426e-05, gnorm=2.391, train_wall=4, gb_free=15.9, wall=8286
2024-08-17 19:46:43 | INFO | train_inner | epoch 029:     56 / 94 loss=7.408, nll_loss=4.312, ppl=19.86, wps=1287.3, ups=0.37, wpb=3461, bsz=208, num_updates=2688, lr=2.89319e-05, gnorm=1.981, train_wall=5, gb_free=13.4, wall=8292
2024-08-17 19:46:48 | INFO | train_inner | epoch 029:     58 / 94 loss=7.37, nll_loss=4.249, ppl=19.01, wps=1390.8, ups=0.39, wpb=3556, bsz=136, num_updates=2690, lr=2.89211e-05, gnorm=1.982, train_wall=5, gb_free=15.3, wall=8297
2024-08-17 19:46:53 | INFO | train_inner | epoch 029:     60 / 94 loss=7.278, nll_loss=4.127, ppl=17.48, wps=1308.3, ups=0.44, wpb=2946.5, bsz=144, num_updates=2692, lr=2.89104e-05, gnorm=1.961, train_wall=4, gb_free=17.7, wall=8301
2024-08-17 19:46:57 | INFO | train_inner | epoch 029:     62 / 94 loss=7.141, nll_loss=3.956, ppl=15.51, wps=1181.3, ups=0.47, wpb=2498, bsz=80, num_updates=2694, lr=2.88996e-05, gnorm=2.284, train_wall=4, gb_free=14.6, wall=8305
2024-08-17 19:47:02 | INFO | train_inner | epoch 029:     64 / 94 loss=7.267, nll_loss=4.117, ppl=17.35, wps=1286.9, ups=0.37, wpb=3448, bsz=132, num_updates=2696, lr=2.88889e-05, gnorm=1.811, train_wall=5, gb_free=10.6, wall=8311
2024-08-17 19:47:08 | INFO | train_inner | epoch 029:     66 / 94 loss=7.264, nll_loss=4.137, ppl=17.59, wps=1030.5, ups=0.38, wpb=2743.5, bsz=156, num_updates=2698, lr=2.88782e-05, gnorm=2.178, train_wall=5, gb_free=14.7, wall=8316
2024-08-17 19:47:12 | INFO | train_inner | epoch 029:     68 / 94 loss=7.394, nll_loss=4.308, ppl=19.81, wps=1636.8, ups=0.44, wpb=3697.5, bsz=168, num_updates=2700, lr=2.88675e-05, gnorm=1.829, train_wall=5, gb_free=11.1, wall=8321
2024-08-17 19:47:17 | INFO | train_inner | epoch 029:     70 / 94 loss=7.159, nll_loss=3.996, ppl=15.95, wps=1421.5, ups=0.43, wpb=3343, bsz=172, num_updates=2702, lr=2.88568e-05, gnorm=1.806, train_wall=5, gb_free=12.7, wall=8325
2024-08-17 19:47:22 | INFO | train_inner | epoch 029:     72 / 94 loss=7.308, nll_loss=4.181, ppl=18.14, wps=1164, ups=0.38, wpb=3024, bsz=168, num_updates=2704, lr=2.88462e-05, gnorm=1.995, train_wall=5, gb_free=14.2, wall=8330
2024-08-17 19:47:28 | INFO | train_inner | epoch 029:     74 / 94 loss=7.35, nll_loss=4.235, ppl=18.84, wps=1342.3, ups=0.36, wpb=3708, bsz=208, num_updates=2706, lr=2.88355e-05, gnorm=1.821, train_wall=6, gb_free=10.3, wall=8336
2024-08-17 19:47:32 | INFO | train_inner | epoch 029:     76 / 94 loss=7.281, nll_loss=4.135, ppl=17.57, wps=1570.7, ups=0.45, wpb=3488.5, bsz=116, num_updates=2708, lr=2.88248e-05, gnorm=1.859, train_wall=4, gb_free=16, wall=8340
2024-08-17 19:47:37 | INFO | train_inner | epoch 029:     78 / 94 loss=7.12, nll_loss=3.954, ppl=15.49, wps=1066.5, ups=0.4, wpb=2683, bsz=108, num_updates=2710, lr=2.88142e-05, gnorm=2.135, train_wall=5, gb_free=14.1, wall=8345
2024-08-17 19:47:42 | INFO | train_inner | epoch 029:     80 / 94 loss=7.102, nll_loss=3.932, ppl=15.27, wps=957, ups=0.45, wpb=2104, bsz=64, num_updates=2712, lr=2.88036e-05, gnorm=2.438, train_wall=4, gb_free=9.8, wall=8350
2024-08-17 19:47:46 | INFO | train_inner | epoch 029:     82 / 94 loss=7.211, nll_loss=4.065, ppl=16.74, wps=996.2, ups=0.41, wpb=2438, bsz=104, num_updates=2714, lr=2.8793e-05, gnorm=2.255, train_wall=5, gb_free=10, wall=8355
2024-08-17 19:47:51 | INFO | train_inner | epoch 029:     84 / 94 loss=7.194, nll_loss=4.038, ppl=16.43, wps=1204.4, ups=0.44, wpb=2715.5, bsz=136, num_updates=2716, lr=2.87824e-05, gnorm=2.012, train_wall=5, gb_free=13.3, wall=8359
2024-08-17 19:47:54 | INFO | train_inner | epoch 029:     86 / 94 loss=7.401, nll_loss=4.288, ppl=19.54, wps=1732.8, ups=0.57, wpb=3037, bsz=92, num_updates=2718, lr=2.87718e-05, gnorm=2.434, train_wall=3, gb_free=15.8, wall=8363
2024-08-17 19:48:00 | INFO | train_inner | epoch 029:     88 / 94 loss=7.28, nll_loss=4.127, ppl=17.47, wps=1344.7, ups=0.39, wpb=3439, bsz=132, num_updates=2720, lr=2.87612e-05, gnorm=1.979, train_wall=5, gb_free=9.6, wall=8368
2024-08-17 19:48:03 | INFO | train_inner | epoch 029:     90 / 94 loss=7.069, nll_loss=3.873, ppl=14.65, wps=974.4, ups=0.52, wpb=1857, bsz=92, num_updates=2722, lr=2.87506e-05, gnorm=2.457, train_wall=4, gb_free=19.2, wall=8372
2024-08-17 19:48:08 | INFO | train_inner | epoch 029:     92 / 94 loss=6.992, nll_loss=3.784, ppl=13.77, wps=826.2, ups=0.47, wpb=1756, bsz=60, num_updates=2724, lr=2.87401e-05, gnorm=2.604, train_wall=4, gb_free=14.2, wall=8376
2024-08-17 19:48:12 | INFO | train_inner | epoch 029:     94 / 94 loss=7.457, nll_loss=4.376, ppl=20.77, wps=1561.9, ups=0.47, wpb=3319.5, bsz=180, num_updates=2726, lr=2.87295e-05, gnorm=1.931, train_wall=4, gb_free=13.4, wall=8380
2024-08-17 19:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27994.6171875Mb; avail=227070.984375Mb
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000657
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27995.109375Mb; avail=227070.984375Mb
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005385
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27995.109375Mb; avail=227070.984375Mb
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004804
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011188
2024-08-17 19:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27995.109375Mb; avail=227070.984375Mb
2024-08-17 19:48:25 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.985 | nll_loss 4.862 | ppl 29.08 | wps 2426 | wpb 944.1 | bsz 40.1 | num_updates 2726 | best_loss 7.925
2024-08-17 19:48:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2726 updates
2024-08-17 19:48:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:49:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:49:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 29 @ 2726 updates, score 7.985) (writing took 44.214523897040635 seconds)
2024-08-17 19:49:10 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-08-17 19:49:10 | INFO | train | epoch 029 | loss 7.248 | nll_loss 4.102 | ppl 17.17 | wps 972.2 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2726 | lr 2.87295e-05 | gnorm 2.186 | train_wall 216 | gb_free 13.4 | wall 8438
2024-08-17 19:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000721
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31915.79296875Mb; avail=223150.34765625Mb
2024-08-17 19:49:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000209
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001463
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31915.7890625Mb; avail=223150.34765625Mb
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31915.7890625Mb; avail=223150.34765625Mb
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000534
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002334
2024-08-17 19:49:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31915.7890625Mb; avail=223150.34765625Mb
2024-08-17 19:49:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:49:10 | INFO | fairseq.trainer | begin training epoch 30
2024-08-17 19:49:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:49:15 | INFO | train_inner | epoch 030:      2 / 94 loss=7.155, nll_loss=3.992, ppl=15.91, wps=112.1, ups=0.03, wpb=3528, bsz=160, num_updates=2728, lr=2.8719e-05, gnorm=1.743, train_wall=5, gb_free=14, wall=8443
2024-08-17 19:49:19 | INFO | train_inner | epoch 030:      4 / 94 loss=7.211, nll_loss=4.07, ppl=16.8, wps=1633.5, ups=0.5, wpb=3272, bsz=84, num_updates=2730, lr=2.87085e-05, gnorm=1.929, train_wall=4, gb_free=16.6, wall=8447
2024-08-17 19:49:23 | INFO | train_inner | epoch 030:      6 / 94 loss=6.91, nll_loss=3.664, ppl=12.68, wps=1196.9, ups=0.5, wpb=2380.5, bsz=84, num_updates=2732, lr=2.8698e-05, gnorm=2.25, train_wall=4, gb_free=12.5, wall=8451
2024-08-17 19:49:28 | INFO | train_inner | epoch 030:      8 / 94 loss=7.378, nll_loss=4.277, ppl=19.39, wps=1566.2, ups=0.41, wpb=3832, bsz=240, num_updates=2734, lr=2.86875e-05, gnorm=2.045, train_wall=5, gb_free=13.9, wall=8456
2024-08-17 19:49:31 | INFO | train_inner | epoch 030:     10 / 94 loss=7.215, nll_loss=4.048, ppl=16.54, wps=1313, ups=0.53, wpb=2465.5, bsz=76, num_updates=2736, lr=2.8677e-05, gnorm=2.454, train_wall=4, gb_free=13.5, wall=8460
2024-08-17 19:49:36 | INFO | train_inner | epoch 030:     12 / 94 loss=7.258, nll_loss=4.114, ppl=17.31, wps=1300.8, ups=0.41, wpb=3147, bsz=164, num_updates=2738, lr=2.86665e-05, gnorm=1.901, train_wall=5, gb_free=15.1, wall=8465
2024-08-17 19:49:41 | INFO | train_inner | epoch 030:     14 / 94 loss=7.144, nll_loss=3.96, ppl=15.56, wps=1062, ups=0.41, wpb=2603, bsz=112, num_updates=2740, lr=2.8656e-05, gnorm=2.408, train_wall=5, gb_free=12.3, wall=8470
2024-08-17 19:49:46 | INFO | train_inner | epoch 030:     16 / 94 loss=6.981, nll_loss=3.756, ppl=13.51, wps=1021.1, ups=0.45, wpb=2262, bsz=108, num_updates=2742, lr=2.86456e-05, gnorm=2.235, train_wall=4, gb_free=14.7, wall=8474
2024-08-17 19:49:50 | INFO | train_inner | epoch 030:     18 / 94 loss=7.252, nll_loss=4.111, ppl=17.28, wps=1303.1, ups=0.43, wpb=3019, bsz=96, num_updates=2744, lr=2.86351e-05, gnorm=2.165, train_wall=5, gb_free=13.9, wall=8479
2024-08-17 19:49:55 | INFO | train_inner | epoch 030:     20 / 94 loss=7.223, nll_loss=4.072, ppl=16.82, wps=1427.4, ups=0.46, wpb=3075, bsz=136, num_updates=2746, lr=2.86247e-05, gnorm=2.129, train_wall=4, gb_free=12.1, wall=8483
2024-08-17 19:50:00 | INFO | train_inner | epoch 030:     22 / 94 loss=7.192, nll_loss=4.034, ppl=16.38, wps=1339.7, ups=0.41, wpb=3305, bsz=116, num_updates=2748, lr=2.86143e-05, gnorm=1.948, train_wall=5, gb_free=11.8, wall=8488
2024-08-17 19:50:04 | INFO | train_inner | epoch 030:     24 / 94 loss=6.815, nll_loss=3.537, ppl=11.61, wps=914.6, ups=0.47, wpb=1937.5, bsz=72, num_updates=2750, lr=2.86039e-05, gnorm=2.499, train_wall=4, gb_free=15, wall=8492
2024-08-17 19:50:08 | INFO | train_inner | epoch 030:     26 / 94 loss=6.621, nll_loss=3.286, ppl=9.76, wps=782.6, ups=0.47, wpb=1663, bsz=52, num_updates=2752, lr=2.85935e-05, gnorm=2.671, train_wall=4, gb_free=11.7, wall=8496
2024-08-17 19:50:13 | INFO | train_inner | epoch 030:     28 / 94 loss=7.241, nll_loss=4.085, ppl=16.97, wps=1392.2, ups=0.41, wpb=3369, bsz=124, num_updates=2754, lr=2.85831e-05, gnorm=1.995, train_wall=5, gb_free=15.3, wall=8501
2024-08-17 19:50:18 | INFO | train_inner | epoch 030:     30 / 94 loss=7.155, nll_loss=3.99, ppl=15.89, wps=1113.9, ups=0.39, wpb=2823, bsz=136, num_updates=2756, lr=2.85727e-05, gnorm=1.965, train_wall=5, gb_free=12.8, wall=8506
2024-08-17 19:50:22 | INFO | train_inner | epoch 030:     32 / 94 loss=6.996, nll_loss=3.771, ppl=13.65, wps=1143.3, ups=0.47, wpb=2449.5, bsz=56, num_updates=2758, lr=2.85624e-05, gnorm=2.375, train_wall=4, gb_free=14.4, wall=8511
2024-08-17 19:50:26 | INFO | train_inner | epoch 030:     34 / 94 loss=6.936, nll_loss=3.679, ppl=12.81, wps=1110.3, ups=0.51, wpb=2196, bsz=68, num_updates=2760, lr=2.8552e-05, gnorm=2.33, train_wall=4, gb_free=15.9, wall=8514
2024-08-17 19:50:31 | INFO | train_inner | epoch 030:     36 / 94 loss=7.296, nll_loss=4.164, ppl=17.92, wps=1519.7, ups=0.44, wpb=3466.5, bsz=160, num_updates=2762, lr=2.85417e-05, gnorm=1.815, train_wall=5, gb_free=10.6, wall=8519
2024-08-17 19:50:37 | INFO | train_inner | epoch 030:     38 / 94 loss=7.317, nll_loss=4.19, ppl=18.25, wps=1159.3, ups=0.34, wpb=3441.5, bsz=152, num_updates=2764, lr=2.85313e-05, gnorm=2.107, train_wall=6, gb_free=9.5, wall=8525
2024-08-17 19:50:41 | INFO | train_inner | epoch 030:     40 / 94 loss=7.199, nll_loss=4.046, ppl=16.52, wps=959, ups=0.47, wpb=2038.5, bsz=108, num_updates=2766, lr=2.8521e-05, gnorm=2.805, train_wall=4, gb_free=11.9, wall=8529
2024-08-17 19:50:46 | INFO | train_inner | epoch 030:     42 / 94 loss=7.253, nll_loss=4.107, ppl=17.23, wps=1210.4, ups=0.39, wpb=3136, bsz=148, num_updates=2768, lr=2.85107e-05, gnorm=2.126, train_wall=5, gb_free=15.4, wall=8534
2024-08-17 19:50:50 | INFO | train_inner | epoch 030:     44 / 94 loss=7.193, nll_loss=4.038, ppl=16.43, wps=1162.1, ups=0.5, wpb=2310.5, bsz=124, num_updates=2770, lr=2.85004e-05, gnorm=2.434, train_wall=4, gb_free=9.4, wall=8538
2024-08-17 19:50:54 | INFO | train_inner | epoch 030:     46 / 94 loss=7.142, nll_loss=3.968, ppl=15.65, wps=1565.4, ups=0.46, wpb=3433.5, bsz=92, num_updates=2772, lr=2.84901e-05, gnorm=1.998, train_wall=4, gb_free=14, wall=8543
2024-08-17 19:51:00 | INFO | train_inner | epoch 030:     48 / 94 loss=7.436, nll_loss=4.361, ppl=20.55, wps=1548.1, ups=0.37, wpb=4140.5, bsz=248, num_updates=2774, lr=2.84799e-05, gnorm=1.794, train_wall=5, gb_free=10.2, wall=8548
2024-08-17 19:51:04 | INFO | train_inner | epoch 030:     50 / 94 loss=6.989, nll_loss=3.771, ppl=13.65, wps=924.2, ups=0.5, wpb=1830.5, bsz=91.5, num_updates=2776, lr=2.84696e-05, gnorm=2.527, train_wall=4, gb_free=14, wall=8552
2024-08-17 19:51:08 | INFO | train_inner | epoch 030:     52 / 94 loss=7.158, nll_loss=3.981, ppl=15.8, wps=1399.8, ups=0.5, wpb=2809.5, bsz=108, num_updates=2778, lr=2.84594e-05, gnorm=2.085, train_wall=4, gb_free=9.1, wall=8556
2024-08-17 19:51:13 | INFO | train_inner | epoch 030:     54 / 94 loss=7.199, nll_loss=4.046, ppl=16.52, wps=1226.1, ups=0.37, wpb=3343.5, bsz=208, num_updates=2780, lr=2.84491e-05, gnorm=1.999, train_wall=5, gb_free=8.5, wall=8562
2024-08-17 19:51:18 | INFO | train_inner | epoch 030:     56 / 94 loss=7.332, nll_loss=4.199, ppl=18.37, wps=1791.1, ups=0.4, wpb=4527, bsz=152, num_updates=2782, lr=2.84389e-05, gnorm=1.734, train_wall=5, gb_free=10.4, wall=8567
2024-08-17 19:51:23 | INFO | train_inner | epoch 030:     58 / 94 loss=7.091, nll_loss=3.92, ppl=15.14, wps=1042.9, ups=0.39, wpb=2668.5, bsz=172, num_updates=2784, lr=2.84287e-05, gnorm=2.052, train_wall=5, gb_free=12.2, wall=8572
2024-08-17 19:51:29 | INFO | train_inner | epoch 030:     60 / 94 loss=7.171, nll_loss=3.999, ppl=15.99, wps=1023, ups=0.34, wpb=3039.5, bsz=132, num_updates=2786, lr=2.84185e-05, gnorm=2.271, train_wall=6, gb_free=10.4, wall=8578
2024-08-17 19:51:34 | INFO | train_inner | epoch 030:     62 / 94 loss=7.068, nll_loss=3.875, ppl=14.67, wps=873.3, ups=0.4, wpb=2194.5, bsz=96, num_updates=2788, lr=2.84083e-05, gnorm=2.321, train_wall=5, gb_free=14, wall=8583
2024-08-17 19:51:38 | INFO | train_inner | epoch 030:     64 / 94 loss=7.268, nll_loss=4.123, ppl=17.42, wps=1715.2, ups=0.5, wpb=3452.5, bsz=112, num_updates=2790, lr=2.83981e-05, gnorm=1.975, train_wall=4, gb_free=20.7, wall=8587
2024-08-17 19:51:43 | INFO | train_inner | epoch 030:     66 / 94 loss=7.323, nll_loss=4.209, ppl=18.49, wps=1221.4, ups=0.4, wpb=3057, bsz=152, num_updates=2792, lr=2.83879e-05, gnorm=2.359, train_wall=5, gb_free=13, wall=8592
2024-08-17 19:51:48 | INFO | train_inner | epoch 030:     68 / 94 loss=7.114, nll_loss=3.923, ppl=15.17, wps=1199.2, ups=0.46, wpb=2622, bsz=60, num_updates=2794, lr=2.83778e-05, gnorm=2.202, train_wall=4, gb_free=18.1, wall=8596
2024-08-17 19:51:52 | INFO | train_inner | epoch 030:     70 / 94 loss=6.611, nll_loss=3.305, ppl=9.88, wps=596.9, ups=0.51, wpb=1169, bsz=40, num_updates=2796, lr=2.83676e-05, gnorm=3.174, train_wall=4, gb_free=15.1, wall=8600
2024-08-17 19:51:57 | INFO | train_inner | epoch 030:     72 / 94 loss=7.372, nll_loss=4.263, ppl=19.2, wps=1525.7, ups=0.36, wpb=4230.5, bsz=228, num_updates=2798, lr=2.83575e-05, gnorm=1.814, train_wall=6, gb_free=11.1, wall=8606
2024-08-17 19:52:02 | INFO | train_inner | epoch 030:     74 / 94 loss=7.27, nll_loss=4.138, ppl=17.61, wps=1371.9, ups=0.42, wpb=3259.5, bsz=152, num_updates=2800, lr=2.83473e-05, gnorm=2.153, train_wall=5, gb_free=13.1, wall=8610
2024-08-17 19:52:06 | INFO | train_inner | epoch 030:     76 / 94 loss=7.304, nll_loss=4.18, ppl=18.13, wps=1661.2, ups=0.49, wpb=3404, bsz=164, num_updates=2802, lr=2.83372e-05, gnorm=1.98, train_wall=4, gb_free=18.4, wall=8614
2024-08-17 19:52:11 | INFO | train_inner | epoch 030:     78 / 94 loss=7.186, nll_loss=4.017, ppl=16.19, wps=1111.8, ups=0.4, wpb=2810, bsz=156, num_updates=2804, lr=2.83271e-05, gnorm=2.163, train_wall=5, gb_free=14.4, wall=8619
2024-08-17 19:52:20 | INFO | train_inner | epoch 030:     80 / 94 loss=7.025, nll_loss=3.804, ppl=13.97, wps=466.4, ups=0.22, wpb=2146.5, bsz=68, num_updates=2806, lr=2.8317e-05, gnorm=2.394, train_wall=9, gb_free=15.1, wall=8629
2024-08-17 19:52:25 | INFO | train_inner | epoch 030:     82 / 94 loss=6.888, nll_loss=3.622, ppl=12.31, wps=914.1, ups=0.48, wpb=1885.5, bsz=64, num_updates=2808, lr=2.83069e-05, gnorm=2.484, train_wall=4, gb_free=12.9, wall=8633
2024-08-17 19:52:29 | INFO | train_inner | epoch 030:     84 / 94 loss=7.094, nll_loss=3.911, ppl=15.04, wps=1090.4, ups=0.43, wpb=2534, bsz=67, num_updates=2810, lr=2.82969e-05, gnorm=2.27, train_wall=5, gb_free=11.4, wall=8637
2024-08-17 19:52:33 | INFO | train_inner | epoch 030:     86 / 94 loss=7.304, nll_loss=4.184, ppl=18.17, wps=1592.8, ups=0.46, wpb=3428.5, bsz=120, num_updates=2812, lr=2.82868e-05, gnorm=2.082, train_wall=4, gb_free=15.1, wall=8642
2024-08-17 19:52:39 | INFO | train_inner | epoch 030:     88 / 94 loss=6.95, nll_loss=3.733, ppl=13.3, wps=916.3, ups=0.39, wpb=2347, bsz=116, num_updates=2814, lr=2.82767e-05, gnorm=2.221, train_wall=5, gb_free=12.8, wall=8647
2024-08-17 19:52:43 | INFO | train_inner | epoch 030:     90 / 94 loss=7.267, nll_loss=4.122, ppl=17.41, wps=1676.1, ups=0.48, wpb=3510.5, bsz=100, num_updates=2816, lr=2.82667e-05, gnorm=1.973, train_wall=4, gb_free=13.5, wall=8651
2024-08-17 19:52:47 | INFO | train_inner | epoch 030:     92 / 94 loss=7.21, nll_loss=4.054, ppl=16.61, wps=928.6, ups=0.46, wpb=2017, bsz=84, num_updates=2818, lr=2.82567e-05, gnorm=2.585, train_wall=4, gb_free=10.7, wall=8655
2024-08-17 19:52:50 | INFO | train_inner | epoch 030:     94 / 94 loss=6.977, nll_loss=3.734, ppl=13.31, wps=1129.9, ups=0.59, wpb=1908.5, bsz=52, num_updates=2820, lr=2.82466e-05, gnorm=2.983, train_wall=3, gb_free=16.5, wall=8659
2024-08-17 19:52:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19155.60546875Mb; avail=235910.53515625Mb
2024-08-17 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2024-08-17 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19155.60546875Mb; avail=235910.53515625Mb
2024-08-17 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005472
2024-08-17 19:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19155.60546875Mb; avail=235910.53515625Mb
2024-08-17 19:52:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004885
2024-08-17 19:52:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011266
2024-08-17 19:52:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19155.60546875Mb; avail=235910.53515625Mb
2024-08-17 19:53:04 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.007 | nll_loss 4.867 | ppl 29.17 | wps 2426.4 | wpb 944.1 | bsz 40.1 | num_updates 2820 | best_loss 7.925
2024-08-17 19:53:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2820 updates
2024-08-17 19:53:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:53:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:53:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 30 @ 2820 updates, score 8.007) (writing took 41.22416645986959 seconds)
2024-08-17 19:53:45 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-08-17 19:53:45 | INFO | train | epoch 030 | loss 7.18 | nll_loss 4.015 | ppl 16.17 | wps 968.7 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2820 | lr 2.82466e-05 | gnorm 2.211 | train_wall 220 | gb_free 16.5 | wall 8714
2024-08-17 19:53:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:53:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:53:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:53:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000771
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47694.91796875Mb; avail=207369.44921875Mb
2024-08-17 19:53:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000206
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001566
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47687.55859375Mb; avail=207377.79296875Mb
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000046
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47689.60546875Mb; avail=207374.76171875Mb
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000532
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002436
2024-08-17 19:53:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47689.60546875Mb; avail=207374.76171875Mb
2024-08-17 19:53:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:53:45 | INFO | fairseq.trainer | begin training epoch 31
2024-08-17 19:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:53:49 | INFO | train_inner | epoch 031:      2 / 94 loss=7.05, nll_loss=3.837, ppl=14.29, wps=83.1, ups=0.03, wpb=2441, bsz=80, num_updates=2822, lr=2.82366e-05, gnorm=2.21, train_wall=4, gb_free=12.4, wall=8718
2024-08-17 19:53:53 | INFO | train_inner | epoch 031:      4 / 94 loss=6.973, nll_loss=3.761, ppl=13.55, wps=1268.9, ups=0.6, wpb=2125, bsz=84, num_updates=2824, lr=2.82266e-05, gnorm=2.385, train_wall=3, gb_free=24.1, wall=8721
2024-08-17 19:53:57 | INFO | train_inner | epoch 031:      6 / 94 loss=7.004, nll_loss=3.806, ppl=13.98, wps=1163.3, ups=0.43, wpb=2731.5, bsz=132, num_updates=2826, lr=2.82166e-05, gnorm=2.204, train_wall=5, gb_free=12.8, wall=8726
2024-08-17 19:54:02 | INFO | train_inner | epoch 031:      8 / 94 loss=7.12, nll_loss=3.942, ppl=15.37, wps=1393.4, ups=0.44, wpb=3174, bsz=132, num_updates=2828, lr=2.82067e-05, gnorm=2.115, train_wall=5, gb_free=12.6, wall=8730
2024-08-17 19:54:06 | INFO | train_inner | epoch 031:     10 / 94 loss=7.228, nll_loss=4.073, ppl=16.83, wps=1397.2, ups=0.49, wpb=2869, bsz=160, num_updates=2830, lr=2.81967e-05, gnorm=2.059, train_wall=4, gb_free=13.9, wall=8734
2024-08-17 19:54:10 | INFO | train_inner | epoch 031:     12 / 94 loss=7.126, nll_loss=3.929, ppl=15.23, wps=974.6, ups=0.5, wpb=1963.5, bsz=55.5, num_updates=2832, lr=2.81867e-05, gnorm=2.79, train_wall=4, gb_free=12.5, wall=8738
2024-08-17 19:54:15 | INFO | train_inner | epoch 031:     14 / 94 loss=7.025, nll_loss=3.83, ppl=14.23, wps=1048.1, ups=0.42, wpb=2495.5, bsz=168, num_updates=2834, lr=2.81768e-05, gnorm=2.195, train_wall=5, gb_free=14, wall=8743
2024-08-17 19:54:19 | INFO | train_inner | epoch 031:     16 / 94 loss=6.9, nll_loss=3.655, ppl=12.6, wps=870.7, ups=0.44, wpb=1984, bsz=76, num_updates=2836, lr=2.81668e-05, gnorm=2.687, train_wall=5, gb_free=18.4, wall=8748
2024-08-17 19:54:24 | INFO | train_inner | epoch 031:     18 / 94 loss=7.118, nll_loss=3.952, ppl=15.48, wps=1439.8, ups=0.43, wpb=3381, bsz=184, num_updates=2838, lr=2.81569e-05, gnorm=1.943, train_wall=5, gb_free=13.4, wall=8752
2024-08-17 19:54:28 | INFO | train_inner | epoch 031:     20 / 94 loss=7.075, nll_loss=3.891, ppl=14.84, wps=1340.4, ups=0.51, wpb=2630.5, bsz=84, num_updates=2840, lr=2.8147e-05, gnorm=2.185, train_wall=4, gb_free=18.3, wall=8756
2024-08-17 19:54:32 | INFO | train_inner | epoch 031:     22 / 94 loss=7.035, nll_loss=3.831, ppl=14.23, wps=1329.3, ups=0.45, wpb=2941.5, bsz=112, num_updates=2842, lr=2.81371e-05, gnorm=2.066, train_wall=4, gb_free=16.1, wall=8761
2024-08-17 19:54:38 | INFO | train_inner | epoch 031:     24 / 94 loss=7.342, nll_loss=4.207, ppl=18.47, wps=1351.5, ups=0.38, wpb=3548, bsz=144, num_updates=2844, lr=2.81272e-05, gnorm=2.086, train_wall=5, gb_free=10.3, wall=8766
2024-08-17 19:54:43 | INFO | train_inner | epoch 031:     26 / 94 loss=7.056, nll_loss=3.843, ppl=14.36, wps=1126.8, ups=0.38, wpb=2948.5, bsz=168, num_updates=2846, lr=2.81173e-05, gnorm=2.059, train_wall=5, gb_free=11.5, wall=8771
2024-08-17 19:54:47 | INFO | train_inner | epoch 031:     28 / 94 loss=7.031, nll_loss=3.804, ppl=13.97, wps=1386.6, ups=0.44, wpb=3130, bsz=112, num_updates=2848, lr=2.81074e-05, gnorm=1.902, train_wall=5, gb_free=13.6, wall=8776
2024-08-17 19:54:53 | INFO | train_inner | epoch 031:     30 / 94 loss=7.244, nll_loss=4.097, ppl=17.11, wps=1275.5, ups=0.38, wpb=3340.5, bsz=184, num_updates=2850, lr=2.80976e-05, gnorm=1.822, train_wall=5, gb_free=14.1, wall=8781
2024-08-17 19:54:56 | INFO | train_inner | epoch 031:     32 / 94 loss=6.865, nll_loss=3.597, ppl=12.1, wps=1248.1, ups=0.55, wpb=2272.5, bsz=68, num_updates=2852, lr=2.80877e-05, gnorm=2.277, train_wall=4, gb_free=14.9, wall=8784
2024-08-17 19:55:01 | INFO | train_inner | epoch 031:     34 / 94 loss=7.114, nll_loss=3.947, ppl=15.42, wps=1277.5, ups=0.46, wpb=2757.5, bsz=120, num_updates=2854, lr=2.80779e-05, gnorm=2.372, train_wall=4, gb_free=18.4, wall=8789
2024-08-17 19:55:05 | INFO | train_inner | epoch 031:     36 / 94 loss=7.17, nll_loss=4.029, ppl=16.32, wps=979.7, ups=0.42, wpb=2355.5, bsz=108, num_updates=2856, lr=2.8068e-05, gnorm=2.478, train_wall=5, gb_free=10.4, wall=8794
2024-08-17 19:55:10 | INFO | train_inner | epoch 031:     38 / 94 loss=7.063, nll_loss=3.878, ppl=14.7, wps=1175.8, ups=0.45, wpb=2629.5, bsz=120, num_updates=2858, lr=2.80582e-05, gnorm=2.174, train_wall=4, gb_free=16.6, wall=8798
2024-08-17 19:55:15 | INFO | train_inner | epoch 031:     40 / 94 loss=6.777, nll_loss=3.485, ppl=11.19, wps=606.5, ups=0.4, wpb=1528, bsz=64, num_updates=2860, lr=2.80484e-05, gnorm=3.079, train_wall=5, gb_free=13.4, wall=8803
2024-08-17 19:55:20 | INFO | train_inner | epoch 031:     42 / 94 loss=7.209, nll_loss=4.041, ppl=16.46, wps=1212.1, ups=0.39, wpb=3099, bsz=176, num_updates=2862, lr=2.80386e-05, gnorm=1.982, train_wall=5, gb_free=13.2, wall=8808
2024-08-17 19:55:24 | INFO | train_inner | epoch 031:     44 / 94 loss=7.108, nll_loss=3.906, ppl=14.99, wps=1243.1, ups=0.45, wpb=2781, bsz=112, num_updates=2864, lr=2.80288e-05, gnorm=2.07, train_wall=4, gb_free=18.3, wall=8813
2024-08-17 19:55:30 | INFO | train_inner | epoch 031:     46 / 94 loss=7.366, nll_loss=4.252, ppl=19.06, wps=1605.6, ups=0.36, wpb=4518.5, bsz=204, num_updates=2866, lr=2.8019e-05, gnorm=1.721, train_wall=6, gb_free=10.6, wall=8818
2024-08-17 19:55:38 | INFO | train_inner | epoch 031:     48 / 94 loss=7.059, nll_loss=3.852, ppl=14.44, wps=675.5, ups=0.24, wpb=2784.5, bsz=56, num_updates=2868, lr=2.80093e-05, gnorm=2.182, train_wall=8, gb_free=17.9, wall=8827
2024-08-17 19:55:42 | INFO | train_inner | epoch 031:     50 / 94 loss=7.254, nll_loss=4.115, ppl=17.32, wps=1897, ups=0.48, wpb=3958, bsz=100, num_updates=2870, lr=2.79995e-05, gnorm=1.816, train_wall=4, gb_free=17.3, wall=8831
2024-08-17 19:55:47 | INFO | train_inner | epoch 031:     52 / 94 loss=7.111, nll_loss=3.945, ppl=15.4, wps=1172.1, ups=0.42, wpb=2794, bsz=116, num_updates=2872, lr=2.79898e-05, gnorm=2.116, train_wall=5, gb_free=14.9, wall=8836
2024-08-17 19:55:52 | INFO | train_inner | epoch 031:     54 / 94 loss=7.233, nll_loss=4.08, ppl=16.92, wps=1333.1, ups=0.39, wpb=3406.5, bsz=120, num_updates=2874, lr=2.798e-05, gnorm=1.952, train_wall=5, gb_free=10.4, wall=8841
2024-08-17 19:55:57 | INFO | train_inner | epoch 031:     56 / 94 loss=7.249, nll_loss=4.101, ppl=17.16, wps=1679.8, ups=0.46, wpb=3627.5, bsz=168, num_updates=2876, lr=2.79703e-05, gnorm=1.799, train_wall=4, gb_free=11.3, wall=8845
2024-08-17 19:56:02 | INFO | train_inner | epoch 031:     58 / 94 loss=7.208, nll_loss=4.058, ppl=16.66, wps=1361.9, ups=0.41, wpb=3311.5, bsz=192, num_updates=2878, lr=2.79606e-05, gnorm=2.004, train_wall=5, gb_free=12.5, wall=8850
2024-08-17 19:56:06 | INFO | train_inner | epoch 031:     60 / 94 loss=7.297, nll_loss=4.156, ppl=17.83, wps=1286.8, ups=0.43, wpb=2983, bsz=108, num_updates=2880, lr=2.79508e-05, gnorm=2.465, train_wall=5, gb_free=13.3, wall=8854
2024-08-17 19:56:11 | INFO | train_inner | epoch 031:     62 / 94 loss=7.141, nll_loss=3.964, ppl=15.61, wps=1324.6, ups=0.43, wpb=3086.5, bsz=120, num_updates=2882, lr=2.79411e-05, gnorm=2.228, train_wall=5, gb_free=9.9, wall=8859
2024-08-17 19:56:16 | INFO | train_inner | epoch 031:     64 / 94 loss=7.144, nll_loss=3.972, ppl=15.69, wps=1360.8, ups=0.41, wpb=3283, bsz=132, num_updates=2884, lr=2.79315e-05, gnorm=2.016, train_wall=5, gb_free=10.1, wall=8864
2024-08-17 19:56:21 | INFO | train_inner | epoch 031:     66 / 94 loss=7.152, nll_loss=3.983, ppl=15.81, wps=1283.2, ups=0.4, wpb=3235, bsz=112, num_updates=2886, lr=2.79218e-05, gnorm=2.031, train_wall=5, gb_free=15.1, wall=8869
2024-08-17 19:56:25 | INFO | train_inner | epoch 031:     68 / 94 loss=7.119, nll_loss=3.939, ppl=15.33, wps=1068.8, ups=0.43, wpb=2502, bsz=108, num_updates=2888, lr=2.79121e-05, gnorm=2.272, train_wall=5, gb_free=14.9, wall=8874
2024-08-17 19:56:29 | INFO | train_inner | epoch 031:     70 / 94 loss=6.785, nll_loss=3.507, ppl=11.37, wps=737.5, ups=0.5, wpb=1481, bsz=48, num_updates=2890, lr=2.79024e-05, gnorm=2.911, train_wall=4, gb_free=13.5, wall=8878
2024-08-17 19:56:34 | INFO | train_inner | epoch 031:     72 / 94 loss=6.827, nll_loss=3.565, ppl=11.84, wps=930.9, ups=0.46, wpb=2044, bsz=104, num_updates=2892, lr=2.78928e-05, gnorm=2.425, train_wall=4, gb_free=14.6, wall=8882
2024-08-17 19:56:39 | INFO | train_inner | epoch 031:     74 / 94 loss=7.034, nll_loss=3.811, ppl=14.03, wps=1023.5, ups=0.4, wpb=2558.5, bsz=84, num_updates=2894, lr=2.78832e-05, gnorm=2.287, train_wall=5, gb_free=13.5, wall=8887
2024-08-17 19:56:43 | INFO | train_inner | epoch 031:     76 / 94 loss=7.253, nll_loss=4.114, ppl=17.31, wps=1231.2, ups=0.44, wpb=2804.5, bsz=128, num_updates=2896, lr=2.78735e-05, gnorm=2.225, train_wall=5, gb_free=16.7, wall=8892
2024-08-17 19:56:49 | INFO | train_inner | epoch 031:     78 / 94 loss=7.09, nll_loss=3.897, ppl=14.89, wps=1068.5, ups=0.37, wpb=2886, bsz=164, num_updates=2898, lr=2.78639e-05, gnorm=2.034, train_wall=5, gb_free=13.9, wall=8897
2024-08-17 19:56:54 | INFO | train_inner | epoch 031:     80 / 94 loss=7.222, nll_loss=4.067, ppl=16.76, wps=1437.4, ups=0.38, wpb=3829.5, bsz=163, num_updates=2900, lr=2.78543e-05, gnorm=1.855, train_wall=5, gb_free=11.3, wall=8902
2024-08-17 19:56:59 | INFO | train_inner | epoch 031:     82 / 94 loss=7.103, nll_loss=3.914, ppl=15.08, wps=1471.5, ups=0.45, wpb=3304, bsz=136, num_updates=2902, lr=2.78447e-05, gnorm=1.93, train_wall=4, gb_free=16.8, wall=8907
2024-08-17 19:57:04 | INFO | train_inner | epoch 031:     84 / 94 loss=7.148, nll_loss=3.959, ppl=15.55, wps=1276.1, ups=0.4, wpb=3153, bsz=104, num_updates=2904, lr=2.78351e-05, gnorm=2.365, train_wall=5, gb_free=12.8, wall=8912
2024-08-17 19:57:09 | INFO | train_inner | epoch 031:     86 / 94 loss=7.202, nll_loss=4.05, ppl=16.56, wps=1272.1, ups=0.39, wpb=3226, bsz=104, num_updates=2906, lr=2.78255e-05, gnorm=2.042, train_wall=5, gb_free=8.6, wall=8917
2024-08-17 19:57:13 | INFO | train_inner | epoch 031:     88 / 94 loss=7.125, nll_loss=3.949, ppl=15.44, wps=1185, ups=0.47, wpb=2516, bsz=100, num_updates=2908, lr=2.7816e-05, gnorm=2.211, train_wall=4, gb_free=13.5, wall=8921
2024-08-17 19:57:18 | INFO | train_inner | epoch 031:     90 / 94 loss=6.991, nll_loss=3.763, ppl=13.57, wps=1117.9, ups=0.43, wpb=2605, bsz=88, num_updates=2910, lr=2.78064e-05, gnorm=2.303, train_wall=5, gb_free=16.9, wall=8926
2024-08-17 19:57:23 | INFO | train_inner | epoch 031:     92 / 94 loss=7.091, nll_loss=3.893, ppl=14.86, wps=1058.9, ups=0.4, wpb=2670, bsz=120, num_updates=2912, lr=2.77968e-05, gnorm=2.275, train_wall=5, gb_free=11.1, wall=8931
2024-08-17 19:57:27 | INFO | train_inner | epoch 031:     94 / 94 loss=6.925, nll_loss=3.692, ppl=12.92, wps=900.6, ups=0.5, wpb=1795, bsz=88, num_updates=2914, lr=2.77873e-05, gnorm=2.569, train_wall=4, gb_free=18.4, wall=8935
2024-08-17 19:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25394.38671875Mb; avail=229671.265625Mb
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000547
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25394.38671875Mb; avail=229671.7578125Mb
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005371
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25394.38671875Mb; avail=229671.7578125Mb
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004778
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011051
2024-08-17 19:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25394.87890625Mb; avail=229671.265625Mb
2024-08-17 19:57:40 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.029 | nll_loss 4.892 | ppl 29.7 | wps 2426.3 | wpb 944.1 | bsz 40.1 | num_updates 2914 | best_loss 7.925
2024-08-17 19:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2914 updates
2024-08-17 19:57:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:58:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 19:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 31 @ 2914 updates, score 8.029) (writing took 43.99700919305906 seconds)
2024-08-17 19:58:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-08-17 19:58:24 | INFO | train | epoch 031 | loss 7.124 | nll_loss 3.943 | ppl 15.38 | wps 957.4 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 2914 | lr 2.77873e-05 | gnorm 2.195 | train_wall 221 | gb_free 18.4 | wall 8992
2024-08-17 19:58:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 19:58:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 19:58:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 19:58:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000803
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32634.3359375Mb; avail=222431.84375Mb
2024-08-17 19:58:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000161
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001431
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32634.3359375Mb; avail=222431.84375Mb
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000053
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32634.3359375Mb; avail=222431.84375Mb
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000548
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002322
2024-08-17 19:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32634.3359375Mb; avail=222431.84375Mb
2024-08-17 19:58:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 19:58:24 | INFO | fairseq.trainer | begin training epoch 32
2024-08-17 19:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 19:58:28 | INFO | train_inner | epoch 032:      2 / 94 loss=7.154, nll_loss=3.986, ppl=15.85, wps=109.3, ups=0.03, wpb=3379.5, bsz=152, num_updates=2916, lr=2.77778e-05, gnorm=2.035, train_wall=4, gb_free=19.8, wall=8997
2024-08-17 19:58:32 | INFO | train_inner | epoch 032:      4 / 94 loss=7.056, nll_loss=3.849, ppl=14.41, wps=1473.2, ups=0.52, wpb=2858, bsz=104, num_updates=2918, lr=2.77683e-05, gnorm=2.167, train_wall=4, gb_free=13.9, wall=9001
2024-08-17 19:58:37 | INFO | train_inner | epoch 032:      6 / 94 loss=7.138, nll_loss=3.965, ppl=15.61, wps=1372.3, ups=0.42, wpb=3283.5, bsz=196, num_updates=2920, lr=2.77587e-05, gnorm=1.957, train_wall=5, gb_free=13.4, wall=9005
2024-08-17 19:58:41 | INFO | train_inner | epoch 032:      8 / 94 loss=7.274, nll_loss=4.164, ppl=17.93, wps=1480.9, ups=0.49, wpb=3021.5, bsz=192, num_updates=2922, lr=2.77492e-05, gnorm=2.362, train_wall=4, gb_free=21.3, wall=9009
2024-08-17 19:58:45 | INFO | train_inner | epoch 032:     10 / 94 loss=7.01, nll_loss=3.793, ppl=13.86, wps=1124.2, ups=0.49, wpb=2296, bsz=96, num_updates=2924, lr=2.77398e-05, gnorm=2.597, train_wall=4, gb_free=11.7, wall=9013
2024-08-17 19:58:51 | INFO | train_inner | epoch 032:     12 / 94 loss=7.099, nll_loss=3.898, ppl=14.91, wps=1257.6, ups=0.37, wpb=3367, bsz=148, num_updates=2926, lr=2.77303e-05, gnorm=2.056, train_wall=5, gb_free=9.6, wall=9019
2024-08-17 19:59:00 | INFO | train_inner | epoch 032:     14 / 94 loss=6.887, nll_loss=3.639, ppl=12.46, wps=542.6, ups=0.21, wpb=2594, bsz=80, num_updates=2928, lr=2.77208e-05, gnorm=2.193, train_wall=10, gb_free=10.2, wall=9028
2024-08-17 19:59:05 | INFO | train_inner | epoch 032:     16 / 94 loss=7.141, nll_loss=3.975, ppl=15.73, wps=1527.5, ups=0.42, wpb=3676, bsz=160, num_updates=2930, lr=2.77113e-05, gnorm=1.92, train_wall=5, gb_free=13.8, wall=9033
2024-08-17 19:59:09 | INFO | train_inner | epoch 032:     18 / 94 loss=6.893, nll_loss=3.669, ppl=12.72, wps=1121.2, ups=0.46, wpb=2429.5, bsz=120, num_updates=2932, lr=2.77019e-05, gnorm=2.292, train_wall=4, gb_free=13.9, wall=9038
2024-08-17 19:59:15 | INFO | train_inner | epoch 032:     20 / 94 loss=7.047, nll_loss=3.85, ppl=14.42, wps=1131.7, ups=0.37, wpb=3077.5, bsz=148, num_updates=2934, lr=2.76924e-05, gnorm=2.194, train_wall=5, gb_free=12.5, wall=9043
2024-08-17 19:59:19 | INFO | train_inner | epoch 032:     22 / 94 loss=6.936, nll_loss=3.698, ppl=12.98, wps=1174.7, ups=0.44, wpb=2684, bsz=132, num_updates=2936, lr=2.7683e-05, gnorm=2.168, train_wall=5, gb_free=14.9, wall=9048
2024-08-17 19:59:24 | INFO | train_inner | epoch 032:     24 / 94 loss=7.071, nll_loss=3.879, ppl=14.71, wps=1215.8, ups=0.4, wpb=3015.5, bsz=143, num_updates=2938, lr=2.76736e-05, gnorm=2.111, train_wall=5, gb_free=9.4, wall=9053
2024-08-17 19:59:28 | INFO | train_inner | epoch 032:     26 / 94 loss=6.817, nll_loss=3.536, ppl=11.6, wps=1197.9, ups=0.47, wpb=2532.5, bsz=68, num_updates=2940, lr=2.76642e-05, gnorm=2.282, train_wall=4, gb_free=15, wall=9057
2024-08-17 19:59:33 | INFO | train_inner | epoch 032:     28 / 94 loss=7.133, nll_loss=3.961, ppl=15.58, wps=1130.4, ups=0.44, wpb=2544.5, bsz=112, num_updates=2942, lr=2.76548e-05, gnorm=2.243, train_wall=4, gb_free=11.5, wall=9061
2024-08-17 19:59:38 | INFO | train_inner | epoch 032:     30 / 94 loss=6.7, nll_loss=3.403, ppl=10.58, wps=771, ups=0.4, wpb=1931.5, bsz=108, num_updates=2944, lr=2.76454e-05, gnorm=2.433, train_wall=5, gb_free=14.5, wall=9066
2024-08-17 19:59:42 | INFO | train_inner | epoch 032:     32 / 94 loss=6.743, nll_loss=3.444, ppl=10.88, wps=918.9, ups=0.51, wpb=1793, bsz=60, num_updates=2946, lr=2.7636e-05, gnorm=2.669, train_wall=4, gb_free=13.4, wall=9070
2024-08-17 19:59:46 | INFO | train_inner | epoch 032:     34 / 94 loss=6.853, nll_loss=3.597, ppl=12.1, wps=1099.7, ups=0.48, wpb=2292.5, bsz=68, num_updates=2948, lr=2.76266e-05, gnorm=2.352, train_wall=4, gb_free=16.8, wall=9074
2024-08-17 19:59:51 | INFO | train_inner | epoch 032:     36 / 94 loss=7.151, nll_loss=3.989, ppl=15.88, wps=1150.3, ups=0.38, wpb=3031.5, bsz=164, num_updates=2950, lr=2.76172e-05, gnorm=2.13, train_wall=5, gb_free=10.2, wall=9080
2024-08-17 19:59:56 | INFO | train_inner | epoch 032:     38 / 94 loss=7.188, nll_loss=4, ppl=16, wps=1560, ups=0.39, wpb=4020.5, bsz=124, num_updates=2952, lr=2.76079e-05, gnorm=1.927, train_wall=5, gb_free=11.7, wall=9085
2024-08-17 20:00:02 | INFO | train_inner | epoch 032:     40 / 94 loss=7.236, nll_loss=4.062, ppl=16.71, wps=1566.7, ups=0.38, wpb=4159.5, bsz=212, num_updates=2954, lr=2.75985e-05, gnorm=1.783, train_wall=5, gb_free=13.5, wall=9090
2024-08-17 20:00:07 | INFO | train_inner | epoch 032:     42 / 94 loss=7.076, nll_loss=3.866, ppl=14.58, wps=891, ups=0.4, wpb=2200.5, bsz=92, num_updates=2956, lr=2.75892e-05, gnorm=2.495, train_wall=5, gb_free=15, wall=9095
2024-08-17 20:00:11 | INFO | train_inner | epoch 032:     44 / 94 loss=7.064, nll_loss=3.861, ppl=14.53, wps=1273.1, ups=0.43, wpb=2969, bsz=112, num_updates=2958, lr=2.75799e-05, gnorm=2.062, train_wall=5, gb_free=11.9, wall=9100
2024-08-17 20:00:16 | INFO | train_inner | epoch 032:     46 / 94 loss=6.863, nll_loss=3.607, ppl=12.18, wps=1259.4, ups=0.44, wpb=2852, bsz=84, num_updates=2960, lr=2.75705e-05, gnorm=1.993, train_wall=5, gb_free=9.3, wall=9104
2024-08-17 20:00:21 | INFO | train_inner | epoch 032:     48 / 94 loss=7.132, nll_loss=3.97, ppl=15.67, wps=1227.4, ups=0.43, wpb=2824.5, bsz=92, num_updates=2962, lr=2.75612e-05, gnorm=2.269, train_wall=5, gb_free=11.3, wall=9109
2024-08-17 20:00:26 | INFO | train_inner | epoch 032:     50 / 94 loss=7.141, nll_loss=3.974, ppl=15.71, wps=1315.8, ups=0.38, wpb=3451.5, bsz=160, num_updates=2964, lr=2.75519e-05, gnorm=2.106, train_wall=5, gb_free=9.3, wall=9114
2024-08-17 20:00:31 | INFO | train_inner | epoch 032:     52 / 94 loss=7.227, nll_loss=4.07, ppl=16.79, wps=1728, ups=0.38, wpb=4544, bsz=184, num_updates=2966, lr=2.75426e-05, gnorm=1.728, train_wall=5, gb_free=11.9, wall=9119
2024-08-17 20:00:35 | INFO | train_inner | epoch 032:     54 / 94 loss=6.912, nll_loss=3.65, ppl=12.56, wps=1100.9, ups=0.48, wpb=2289.5, bsz=76, num_updates=2968, lr=2.75334e-05, gnorm=2.349, train_wall=4, gb_free=12, wall=9123
2024-08-17 20:00:40 | INFO | train_inner | epoch 032:     56 / 94 loss=7.247, nll_loss=4.081, ppl=16.93, wps=1597.4, ups=0.4, wpb=3974, bsz=140, num_updates=2970, lr=2.75241e-05, gnorm=1.888, train_wall=5, gb_free=12.9, wall=9128
2024-08-17 20:00:46 | INFO | train_inner | epoch 032:     58 / 94 loss=7.03, nll_loss=3.814, ppl=14.07, wps=1284.7, ups=0.37, wpb=3464.5, bsz=140, num_updates=2972, lr=2.75148e-05, gnorm=1.943, train_wall=5, gb_free=8.9, wall=9134
2024-08-17 20:00:51 | INFO | train_inner | epoch 032:     60 / 94 loss=7.18, nll_loss=4.032, ppl=16.36, wps=1402.9, ups=0.38, wpb=3693.5, bsz=192, num_updates=2974, lr=2.75056e-05, gnorm=1.839, train_wall=5, gb_free=14.6, wall=9139
2024-08-17 20:00:56 | INFO | train_inner | epoch 032:     62 / 94 loss=6.986, nll_loss=3.774, ppl=13.68, wps=1036.9, ups=0.42, wpb=2495, bsz=76, num_updates=2976, lr=2.74963e-05, gnorm=2.29, train_wall=5, gb_free=12.4, wall=9144
2024-08-17 20:01:00 | INFO | train_inner | epoch 032:     64 / 94 loss=6.926, nll_loss=3.709, ppl=13.07, wps=866, ups=0.42, wpb=2048.5, bsz=72, num_updates=2978, lr=2.74871e-05, gnorm=2.761, train_wall=5, gb_free=13.7, wall=9149
2024-08-17 20:01:04 | INFO | train_inner | epoch 032:     66 / 94 loss=6.879, nll_loss=3.631, ppl=12.39, wps=1417, ups=0.6, wpb=2349, bsz=84, num_updates=2980, lr=2.74779e-05, gnorm=2.185, train_wall=3, gb_free=19.9, wall=9152
2024-08-17 20:01:07 | INFO | train_inner | epoch 032:     68 / 94 loss=7.048, nll_loss=3.835, ppl=14.27, wps=1396, ups=0.54, wpb=2579, bsz=100, num_updates=2982, lr=2.74687e-05, gnorm=2.168, train_wall=4, gb_free=15.3, wall=9156
2024-08-17 20:01:12 | INFO | train_inner | epoch 032:     70 / 94 loss=7.114, nll_loss=3.934, ppl=15.28, wps=1400.2, ups=0.46, wpb=3045, bsz=168, num_updates=2984, lr=2.74595e-05, gnorm=2.035, train_wall=4, gb_free=14.2, wall=9160
2024-08-17 20:01:16 | INFO | train_inner | epoch 032:     72 / 94 loss=6.931, nll_loss=3.692, ppl=12.92, wps=1046.9, ups=0.44, wpb=2369.5, bsz=84, num_updates=2986, lr=2.74503e-05, gnorm=2.365, train_wall=5, gb_free=13.7, wall=9165
2024-08-17 20:01:21 | INFO | train_inner | epoch 032:     74 / 94 loss=7.195, nll_loss=4.036, ppl=16.4, wps=1361.2, ups=0.45, wpb=3029.5, bsz=152, num_updates=2988, lr=2.74411e-05, gnorm=2.66, train_wall=4, gb_free=20.2, wall=9169
2024-08-17 20:01:25 | INFO | train_inner | epoch 032:     76 / 94 loss=6.819, nll_loss=3.539, ppl=11.62, wps=1014.1, ups=0.48, wpb=2129.5, bsz=72, num_updates=2990, lr=2.74319e-05, gnorm=2.501, train_wall=4, gb_free=18.7, wall=9173
2024-08-17 20:01:29 | INFO | train_inner | epoch 032:     78 / 94 loss=6.865, nll_loss=3.598, ppl=12.11, wps=1196.9, ups=0.51, wpb=2326.5, bsz=64, num_updates=2992, lr=2.74227e-05, gnorm=2.486, train_wall=4, gb_free=20.6, wall=9177
2024-08-17 20:01:34 | INFO | train_inner | epoch 032:     80 / 94 loss=7, nll_loss=3.788, ppl=13.81, wps=960.3, ups=0.41, wpb=2344, bsz=84, num_updates=2994, lr=2.74136e-05, gnorm=2.415, train_wall=5, gb_free=12.9, wall=9182
2024-08-17 20:01:38 | INFO | train_inner | epoch 032:     82 / 94 loss=7.028, nll_loss=3.81, ppl=14.02, wps=1258.2, ups=0.42, wpb=2969.5, bsz=108, num_updates=2996, lr=2.74044e-05, gnorm=2.117, train_wall=5, gb_free=12.2, wall=9187
2024-08-17 20:01:43 | INFO | train_inner | epoch 032:     84 / 94 loss=7.075, nll_loss=3.873, ppl=14.65, wps=1246.4, ups=0.41, wpb=3009.5, bsz=148, num_updates=2998, lr=2.73953e-05, gnorm=2.004, train_wall=5, gb_free=11.1, wall=9191
2024-08-17 20:01:47 | INFO | train_inner | epoch 032:     86 / 94 loss=6.93, nll_loss=3.695, ppl=12.95, wps=1178.3, ups=0.49, wpb=2424.5, bsz=64, num_updates=3000, lr=2.73861e-05, gnorm=2.278, train_wall=4, gb_free=14.7, wall=9196
2024-08-17 20:01:52 | INFO | train_inner | epoch 032:     88 / 94 loss=6.922, nll_loss=3.68, ppl=12.81, wps=867.1, ups=0.4, wpb=2187, bsz=64, num_updates=3002, lr=2.7377e-05, gnorm=2.511, train_wall=5, gb_free=8.2, wall=9201
2024-08-17 20:01:57 | INFO | train_inner | epoch 032:     90 / 94 loss=7.113, nll_loss=3.926, ppl=15.2, wps=1129.4, ups=0.4, wpb=2797.5, bsz=148, num_updates=3004, lr=2.73679e-05, gnorm=2.074, train_wall=5, gb_free=13, wall=9206
2024-08-17 20:02:02 | INFO | train_inner | epoch 032:     92 / 94 loss=7.067, nll_loss=3.877, ppl=14.69, wps=1039.4, ups=0.46, wpb=2274, bsz=131.5, num_updates=3006, lr=2.73588e-05, gnorm=2.497, train_wall=4, gb_free=18.9, wall=9210
2024-08-17 20:02:05 | INFO | train_inner | epoch 032:     94 / 94 loss=7.205, nll_loss=4.042, ppl=16.48, wps=1580.8, ups=0.55, wpb=2861, bsz=132, num_updates=3008, lr=2.73497e-05, gnorm=2.036, train_wall=4, gb_free=14.3, wall=9214
2024-08-17 20:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22980.39453125Mb; avail=232085.7890625Mb
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22980.39453125Mb; avail=232085.7890625Mb
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005476
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22980.39453125Mb; avail=232085.7890625Mb
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004789
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011181
2024-08-17 20:02:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22980.39453125Mb; avail=232085.7890625Mb
2024-08-17 20:02:19 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.041 | nll_loss 4.915 | ppl 30.17 | wps 2427.8 | wpb 944.1 | bsz 40.1 | num_updates 3008 | best_loss 7.925
2024-08-17 20:02:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3008 updates
2024-08-17 20:02:19 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:02:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:02:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 32 @ 3008 updates, score 8.041) (writing took 39.21673568803817 seconds)
2024-08-17 20:02:58 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-08-17 20:02:58 | INFO | train | epoch 032 | loss 7.055 | nll_loss 3.853 | ppl 14.45 | wps 974.7 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 3008 | lr 2.73497e-05 | gnorm 2.211 | train_wall 221 | gb_free 14.3 | wall 9266
2024-08-17 20:02:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 20:02:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 20:02:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 20:02:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000722
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25350.58984375Mb; avail=229715.59375Mb
2024-08-17 20:02:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000155
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001453
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25350.58984375Mb; avail=229715.59375Mb
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25350.58984375Mb; avail=229715.59375Mb
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000519
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002300
2024-08-17 20:02:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25350.58984375Mb; avail=229715.59375Mb
2024-08-17 20:02:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 20:02:58 | INFO | fairseq.trainer | begin training epoch 33
2024-08-17 20:02:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 20:03:03 | INFO | train_inner | epoch 033:      2 / 94 loss=6.978, nll_loss=3.762, ppl=13.57, wps=103.6, ups=0.03, wpb=2981.5, bsz=148, num_updates=3010, lr=2.73406e-05, gnorm=2.106, train_wall=5, gb_free=14.6, wall=9271
2024-08-17 20:03:07 | INFO | train_inner | epoch 033:      4 / 94 loss=6.84, nll_loss=3.579, ppl=11.95, wps=1004.6, ups=0.49, wpb=2064, bsz=75.5, num_updates=3012, lr=2.73315e-05, gnorm=3.001, train_wall=4, gb_free=16.1, wall=9275
2024-08-17 20:03:12 | INFO | train_inner | epoch 033:      6 / 94 loss=7.047, nll_loss=3.849, ppl=14.41, wps=1556.8, ups=0.44, wpb=3562.5, bsz=160, num_updates=3014, lr=2.73224e-05, gnorm=1.904, train_wall=5, gb_free=15.4, wall=9280
2024-08-17 20:03:16 | INFO | train_inner | epoch 033:      8 / 94 loss=7.037, nll_loss=3.831, ppl=14.23, wps=1355.9, ups=0.41, wpb=3280, bsz=168, num_updates=3016, lr=2.73134e-05, gnorm=2.014, train_wall=5, gb_free=11.1, wall=9285
2024-08-17 20:03:22 | INFO | train_inner | epoch 033:     10 / 94 loss=6.987, nll_loss=3.767, ppl=13.61, wps=1184, ups=0.36, wpb=3312, bsz=152, num_updates=3018, lr=2.73043e-05, gnorm=2.104, train_wall=6, gb_free=12.1, wall=9290
2024-08-17 20:03:27 | INFO | train_inner | epoch 033:     12 / 94 loss=6.962, nll_loss=3.718, ppl=13.16, wps=1204.1, ups=0.41, wpb=2962, bsz=112, num_updates=3020, lr=2.72953e-05, gnorm=2.021, train_wall=5, gb_free=8.9, wall=9295
2024-08-17 20:03:31 | INFO | train_inner | epoch 033:     14 / 94 loss=6.975, nll_loss=3.732, ppl=13.29, wps=1420.4, ups=0.55, wpb=2581.5, bsz=80, num_updates=3022, lr=2.72863e-05, gnorm=2.48, train_wall=4, gb_free=23.6, wall=9299
2024-08-17 20:03:35 | INFO | train_inner | epoch 033:     16 / 94 loss=6.589, nll_loss=3.245, ppl=9.48, wps=1059.9, ups=0.5, wpb=2123, bsz=64, num_updates=3024, lr=2.72772e-05, gnorm=2.245, train_wall=4, gb_free=13.2, wall=9303
2024-08-17 20:03:39 | INFO | train_inner | epoch 033:     18 / 94 loss=7.112, nll_loss=3.944, ppl=15.39, wps=1306.1, ups=0.41, wpb=3151, bsz=156, num_updates=3026, lr=2.72682e-05, gnorm=2.071, train_wall=5, gb_free=16.3, wall=9308
2024-08-17 20:03:44 | INFO | train_inner | epoch 033:     20 / 94 loss=6.834, nll_loss=3.598, ppl=12.11, wps=874.9, ups=0.45, wpb=1965.5, bsz=116, num_updates=3028, lr=2.72592e-05, gnorm=2.518, train_wall=4, gb_free=14.7, wall=9312
2024-08-17 20:03:49 | INFO | train_inner | epoch 033:     22 / 94 loss=7.158, nll_loss=3.996, ppl=15.96, wps=1523.8, ups=0.42, wpb=3588.5, bsz=188, num_updates=3030, lr=2.72502e-05, gnorm=1.998, train_wall=5, gb_free=14, wall=9317
2024-08-17 20:03:54 | INFO | train_inner | epoch 033:     24 / 94 loss=7.233, nll_loss=4.092, ppl=17.05, wps=1482.3, ups=0.36, wpb=4117.5, bsz=252, num_updates=3032, lr=2.72412e-05, gnorm=1.709, train_wall=6, gb_free=11.3, wall=9322
2024-08-17 20:03:59 | INFO | train_inner | epoch 033:     26 / 94 loss=6.594, nll_loss=3.249, ppl=9.5, wps=803, ups=0.42, wpb=1903.5, bsz=64, num_updates=3034, lr=2.72322e-05, gnorm=2.524, train_wall=5, gb_free=13.8, wall=9327
2024-08-17 20:04:03 | INFO | train_inner | epoch 033:     28 / 94 loss=6.781, nll_loss=3.489, ppl=11.23, wps=1182.1, ups=0.51, wpb=2320, bsz=72, num_updates=3036, lr=2.72233e-05, gnorm=2.405, train_wall=4, gb_free=13.1, wall=9331
2024-08-17 20:04:07 | INFO | train_inner | epoch 033:     30 / 94 loss=6.975, nll_loss=3.744, ppl=13.4, wps=1164, ups=0.49, wpb=2366.5, bsz=76, num_updates=3038, lr=2.72143e-05, gnorm=2.365, train_wall=4, gb_free=11.3, wall=9335
2024-08-17 20:04:10 | INFO | train_inner | epoch 033:     32 / 94 loss=6.838, nll_loss=3.587, ppl=12.01, wps=1241.4, ups=0.56, wpb=2211.5, bsz=64, num_updates=3040, lr=2.72054e-05, gnorm=2.403, train_wall=4, gb_free=17.1, wall=9339
2024-08-17 20:04:15 | INFO | train_inner | epoch 033:     34 / 94 loss=7.242, nll_loss=4.104, ppl=17.19, wps=1292.6, ups=0.42, wpb=3100.5, bsz=168, num_updates=3042, lr=2.71964e-05, gnorm=2.124, train_wall=5, gb_free=14.7, wall=9344
2024-08-17 20:04:20 | INFO | train_inner | epoch 033:     36 / 94 loss=6.845, nll_loss=3.598, ppl=12.11, wps=1056.7, ups=0.45, wpb=2348, bsz=108, num_updates=3044, lr=2.71875e-05, gnorm=2.237, train_wall=4, gb_free=14.5, wall=9348
2024-08-17 20:04:25 | INFO | train_inner | epoch 033:     38 / 94 loss=6.936, nll_loss=3.699, ppl=12.99, wps=1038.6, ups=0.4, wpb=2567.5, bsz=156, num_updates=3046, lr=2.71786e-05, gnorm=2.283, train_wall=5, gb_free=12.2, wall=9353
2024-08-17 20:04:29 | INFO | train_inner | epoch 033:     40 / 94 loss=6.926, nll_loss=3.667, ppl=12.7, wps=1170.2, ups=0.44, wpb=2639, bsz=100, num_updates=3048, lr=2.71696e-05, gnorm=2.317, train_wall=5, gb_free=13.2, wall=9357
2024-08-17 20:04:33 | INFO | train_inner | epoch 033:     42 / 94 loss=7.029, nll_loss=3.814, ppl=14.07, wps=1378.4, ups=0.48, wpb=2895.5, bsz=132, num_updates=3050, lr=2.71607e-05, gnorm=2.09, train_wall=4, gb_free=12.5, wall=9362
2024-08-17 20:04:39 | INFO | train_inner | epoch 033:     44 / 94 loss=7.319, nll_loss=4.184, ppl=18.18, wps=1911.4, ups=0.38, wpb=5082.5, bsz=212, num_updates=3052, lr=2.71518e-05, gnorm=1.766, train_wall=5, gb_free=14, wall=9367
2024-08-17 20:04:44 | INFO | train_inner | epoch 033:     46 / 94 loss=7.141, nll_loss=3.989, ppl=15.88, wps=1240.9, ups=0.39, wpb=3180, bsz=180, num_updates=3054, lr=2.71429e-05, gnorm=2.165, train_wall=5, gb_free=13.1, wall=9372
2024-08-17 20:04:49 | INFO | train_inner | epoch 033:     48 / 94 loss=7.194, nll_loss=4.036, ppl=16.4, wps=1479.3, ups=0.39, wpb=3747.5, bsz=220, num_updates=3056, lr=2.7134e-05, gnorm=1.851, train_wall=5, gb_free=14.4, wall=9377
2024-08-17 20:04:53 | INFO | train_inner | epoch 033:     50 / 94 loss=6.776, nll_loss=3.485, ppl=11.2, wps=982.9, ups=0.48, wpb=2057.5, bsz=64, num_updates=3058, lr=2.71252e-05, gnorm=2.543, train_wall=4, gb_free=15.8, wall=9381
2024-08-17 20:04:58 | INFO | train_inner | epoch 033:     52 / 94 loss=7.02, nll_loss=3.799, ppl=13.92, wps=1145.4, ups=0.41, wpb=2791.5, bsz=100, num_updates=3060, lr=2.71163e-05, gnorm=2.459, train_wall=5, gb_free=13.1, wall=9386
2024-08-17 20:05:03 | INFO | train_inner | epoch 033:     54 / 94 loss=7.053, nll_loss=3.842, ppl=14.34, wps=1349.4, ups=0.43, wpb=3130, bsz=132, num_updates=3062, lr=2.71075e-05, gnorm=2.198, train_wall=5, gb_free=12.7, wall=9391
2024-08-17 20:05:07 | INFO | train_inner | epoch 033:     56 / 94 loss=6.86, nll_loss=3.599, ppl=12.12, wps=1088.1, ups=0.5, wpb=2193, bsz=80, num_updates=3064, lr=2.70986e-05, gnorm=2.434, train_wall=4, gb_free=14.7, wall=9395
2024-08-17 20:05:11 | INFO | train_inner | epoch 033:     58 / 94 loss=6.906, nll_loss=3.667, ppl=12.7, wps=1270, ups=0.51, wpb=2478, bsz=68, num_updates=3066, lr=2.70898e-05, gnorm=2.384, train_wall=4, gb_free=17.7, wall=9399
2024-08-17 20:05:14 | INFO | train_inner | epoch 033:     60 / 94 loss=6.977, nll_loss=3.752, ppl=13.47, wps=1371.5, ups=0.51, wpb=2695, bsz=64, num_updates=3068, lr=2.70809e-05, gnorm=2.239, train_wall=4, gb_free=11, wall=9403
2024-08-17 20:05:20 | INFO | train_inner | epoch 033:     62 / 94 loss=7.115, nll_loss=3.929, ppl=15.23, wps=1381.7, ups=0.38, wpb=3616, bsz=200, num_updates=3070, lr=2.70721e-05, gnorm=1.997, train_wall=5, gb_free=15.4, wall=9408
2024-08-17 20:05:24 | INFO | train_inner | epoch 033:     64 / 94 loss=7.071, nll_loss=3.868, ppl=14.6, wps=1364.2, ups=0.43, wpb=3154, bsz=108, num_updates=3072, lr=2.70633e-05, gnorm=2.131, train_wall=5, gb_free=12.1, wall=9413
2024-08-17 20:05:29 | INFO | train_inner | epoch 033:     66 / 94 loss=7.054, nll_loss=3.848, ppl=14.4, wps=1236.6, ups=0.45, wpb=2728, bsz=100, num_updates=3074, lr=2.70545e-05, gnorm=2.112, train_wall=4, gb_free=13, wall=9417
2024-08-17 20:05:33 | INFO | train_inner | epoch 033:     68 / 94 loss=6.936, nll_loss=3.709, ppl=13.08, wps=1103.8, ups=0.48, wpb=2298, bsz=60, num_updates=3076, lr=2.70457e-05, gnorm=2.374, train_wall=4, gb_free=13.6, wall=9421
2024-08-17 20:05:37 | INFO | train_inner | epoch 033:     70 / 94 loss=6.889, nll_loss=3.629, ppl=12.37, wps=1300.6, ups=0.45, wpb=2905.5, bsz=116, num_updates=3078, lr=2.70369e-05, gnorm=2.044, train_wall=4, gb_free=15.3, wall=9426
2024-08-17 20:05:42 | INFO | train_inner | epoch 033:     72 / 94 loss=7.036, nll_loss=3.821, ppl=14.13, wps=1272.5, ups=0.42, wpb=3013.5, bsz=96, num_updates=3080, lr=2.70281e-05, gnorm=2.066, train_wall=5, gb_free=8.8, wall=9430
2024-08-17 20:05:47 | INFO | train_inner | epoch 033:     74 / 94 loss=6.828, nll_loss=3.562, ppl=11.81, wps=1192.3, ups=0.44, wpb=2725.5, bsz=79, num_updates=3082, lr=2.70194e-05, gnorm=2.276, train_wall=5, gb_free=11.8, wall=9435
2024-08-17 20:05:52 | INFO | train_inner | epoch 033:     76 / 94 loss=7.302, nll_loss=4.184, ppl=18.18, wps=1431.9, ups=0.36, wpb=4019, bsz=248, num_updates=3084, lr=2.70106e-05, gnorm=1.935, train_wall=6, gb_free=8.9, wall=9441
2024-08-17 20:05:56 | INFO | train_inner | epoch 033:     78 / 94 loss=7.015, nll_loss=3.799, ppl=13.92, wps=1406.3, ups=0.47, wpb=2970, bsz=84, num_updates=3086, lr=2.70018e-05, gnorm=2.184, train_wall=4, gb_free=14.6, wall=9445
2024-08-17 20:06:01 | INFO | train_inner | epoch 033:     80 / 94 loss=6.612, nll_loss=3.28, ppl=9.71, wps=827.3, ups=0.44, wpb=1873.5, bsz=68, num_updates=3088, lr=2.69931e-05, gnorm=2.403, train_wall=5, gb_free=10.8, wall=9449
2024-08-17 20:06:06 | INFO | train_inner | epoch 033:     82 / 94 loss=6.958, nll_loss=3.721, ppl=13.18, wps=1161.5, ups=0.42, wpb=2778, bsz=100, num_updates=3090, lr=2.69844e-05, gnorm=2.185, train_wall=5, gb_free=15.9, wall=9454
2024-08-17 20:06:11 | INFO | train_inner | epoch 033:     84 / 94 loss=6.938, nll_loss=3.68, ppl=12.81, wps=1019, ups=0.37, wpb=2718.5, bsz=92, num_updates=3092, lr=2.69756e-05, gnorm=2.145, train_wall=5, gb_free=11.1, wall=9459
2024-08-17 20:06:16 | INFO | train_inner | epoch 033:     86 / 94 loss=7.188, nll_loss=4.021, ppl=16.24, wps=1606.2, ups=0.38, wpb=4229.5, bsz=184, num_updates=3094, lr=2.69669e-05, gnorm=1.754, train_wall=5, gb_free=8.5, wall=9465
2024-08-17 20:06:21 | INFO | train_inner | epoch 033:     88 / 94 loss=6.952, nll_loss=3.731, ppl=13.28, wps=1155.5, ups=0.41, wpb=2834, bsz=100, num_updates=3096, lr=2.69582e-05, gnorm=2.105, train_wall=5, gb_free=13, wall=9470
2024-08-17 20:06:30 | INFO | train_inner | epoch 033:     90 / 94 loss=6.702, nll_loss=3.407, ppl=10.61, wps=422.7, ups=0.24, wpb=1777, bsz=40, num_updates=3098, lr=2.69495e-05, gnorm=2.716, train_wall=8, gb_free=17.9, wall=9478
2024-08-17 20:06:34 | INFO | train_inner | epoch 033:     92 / 94 loss=6.892, nll_loss=3.649, ppl=12.55, wps=1098, ups=0.49, wpb=2256, bsz=124, num_updates=3100, lr=2.69408e-05, gnorm=2.428, train_wall=4, gb_free=14.9, wall=9482
2024-08-17 20:06:37 | INFO | train_inner | epoch 033:     94 / 94 loss=7.132, nll_loss=3.944, ppl=15.39, wps=1251, ups=0.57, wpb=2197, bsz=80, num_updates=3102, lr=2.69321e-05, gnorm=2.528, train_wall=4, gb_free=18.4, wall=9486
2024-08-17 20:06:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12315.07421875Mb; avail=242751.109375Mb
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000554
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12315.07421875Mb; avail=242751.109375Mb
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005368
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12315.07421875Mb; avail=242751.109375Mb
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004738
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011000
2024-08-17 20:06:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12315.07421875Mb; avail=242751.109375Mb
2024-08-17 20:06:51 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.042 | nll_loss 4.908 | ppl 30.03 | wps 2432.3 | wpb 944.1 | bsz 40.1 | num_updates 3102 | best_loss 7.925
2024-08-17 20:06:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3102 updates
2024-08-17 20:06:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:07:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 33 @ 3102 updates, score 8.042) (writing took 39.11645253095776 seconds)
2024-08-17 20:07:30 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-08-17 20:07:30 | INFO | train | epoch 033 | loss 7.008 | nll_loss 3.793 | ppl 13.86 | wps 982 | ups 0.35 | wpb 2840.2 | bsz 119.4 | num_updates 3102 | lr 2.69321e-05 | gnorm 2.22 | train_wall 219 | gb_free 18.4 | wall 9538
2024-08-17 20:07:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 20:07:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 20:07:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 20:07:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000788
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33609.5546875Mb; avail=221457.07421875Mb
2024-08-17 20:07:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000214
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001543
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33609.0625Mb; avail=221457.07421875Mb
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33609.5546875Mb; avail=221456.58203125Mb
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000527
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002406
2024-08-17 20:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33609.0625Mb; avail=221457.07421875Mb
2024-08-17 20:07:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 20:07:30 | INFO | fairseq.trainer | begin training epoch 34
2024-08-17 20:07:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 20:07:35 | INFO | train_inner | epoch 034:      2 / 94 loss=7.024, nll_loss=3.813, ppl=14.06, wps=102.5, ups=0.03, wpb=2936, bsz=164, num_updates=3104, lr=2.69234e-05, gnorm=1.872, train_wall=5, gb_free=14, wall=9543
2024-08-17 20:07:39 | INFO | train_inner | epoch 034:      4 / 94 loss=6.785, nll_loss=3.507, ppl=11.37, wps=962, ups=0.42, wpb=2318, bsz=116, num_updates=3106, lr=2.69148e-05, gnorm=2.24, train_wall=5, gb_free=12.4, wall=9548
2024-08-17 20:07:43 | INFO | train_inner | epoch 034:      6 / 94 loss=6.427, nll_loss=3.049, ppl=8.27, wps=833.6, ups=0.53, wpb=1561, bsz=60, num_updates=3108, lr=2.69061e-05, gnorm=2.748, train_wall=4, gb_free=14.3, wall=9551
2024-08-17 20:07:47 | INFO | train_inner | epoch 034:      8 / 94 loss=6.653, nll_loss=3.337, ppl=10.1, wps=970.2, ups=0.47, wpb=2072.5, bsz=84, num_updates=3110, lr=2.68974e-05, gnorm=2.533, train_wall=4, gb_free=14.5, wall=9556
2024-08-17 20:07:52 | INFO | train_inner | epoch 034:     10 / 94 loss=6.924, nll_loss=3.698, ppl=12.98, wps=1215.8, ups=0.43, wpb=2826.5, bsz=128, num_updates=3112, lr=2.68888e-05, gnorm=2.549, train_wall=5, gb_free=10, wall=9560
2024-08-17 20:07:58 | INFO | train_inner | epoch 034:     12 / 94 loss=7.095, nll_loss=3.914, ppl=15.07, wps=1636.3, ups=0.37, wpb=4420, bsz=232, num_updates=3114, lr=2.68802e-05, gnorm=1.734, train_wall=5, gb_free=10.5, wall=9566
2024-08-17 20:08:01 | INFO | train_inner | epoch 034:     14 / 94 loss=6.957, nll_loss=3.741, ppl=13.37, wps=1275.8, ups=0.53, wpb=2411, bsz=116, num_updates=3116, lr=2.68715e-05, gnorm=2.402, train_wall=4, gb_free=11.9, wall=9570
2024-08-17 20:08:04 | INFO | train_inner | epoch 034:     16 / 94 loss=6.867, nll_loss=3.596, ppl=12.1, wps=1339.4, ups=0.63, wpb=2116.5, bsz=55.5, num_updates=3118, lr=2.68629e-05, gnorm=3.326, train_wall=3, gb_free=11.7, wall=9573
2024-08-17 20:08:09 | INFO | train_inner | epoch 034:     18 / 94 loss=6.509, nll_loss=3.141, ppl=8.82, wps=800.6, ups=0.45, wpb=1784.5, bsz=68, num_updates=3120, lr=2.68543e-05, gnorm=2.6, train_wall=4, gb_free=9.9, wall=9577
2024-08-17 20:08:14 | INFO | train_inner | epoch 034:     20 / 94 loss=7.016, nll_loss=3.802, ppl=13.94, wps=1134.7, ups=0.38, wpb=2954, bsz=164, num_updates=3122, lr=2.68457e-05, gnorm=2.152, train_wall=5, gb_free=11.5, wall=9582
2024-08-17 20:08:19 | INFO | train_inner | epoch 034:     22 / 94 loss=7.201, nll_loss=4.042, ppl=16.47, wps=1726.2, ups=0.41, wpb=4214.5, bsz=200, num_updates=3124, lr=2.68371e-05, gnorm=1.771, train_wall=5, gb_free=9.9, wall=9587
2024-08-17 20:08:24 | INFO | train_inner | epoch 034:     24 / 94 loss=7.089, nll_loss=3.885, ppl=14.78, wps=1600.8, ups=0.41, wpb=3882, bsz=116, num_updates=3126, lr=2.68285e-05, gnorm=1.922, train_wall=5, gb_free=9.6, wall=9592
2024-08-17 20:08:29 | INFO | train_inner | epoch 034:     26 / 94 loss=7.157, nll_loss=3.982, ppl=15.8, wps=1428.3, ups=0.38, wpb=3765.5, bsz=176, num_updates=3128, lr=2.68199e-05, gnorm=1.826, train_wall=5, gb_free=14, wall=9597
2024-08-17 20:08:34 | INFO | train_inner | epoch 034:     28 / 94 loss=7.045, nll_loss=3.839, ppl=14.31, wps=1392, ups=0.39, wpb=3547.5, bsz=128, num_updates=3130, lr=2.68114e-05, gnorm=1.942, train_wall=5, gb_free=11.1, wall=9602
2024-08-17 20:08:39 | INFO | train_inner | epoch 034:     30 / 94 loss=7.109, nll_loss=3.931, ppl=15.25, wps=1477.9, ups=0.39, wpb=3793, bsz=136, num_updates=3132, lr=2.68028e-05, gnorm=1.96, train_wall=5, gb_free=15.2, wall=9608
2024-08-17 20:08:44 | INFO | train_inner | epoch 034:     32 / 94 loss=6.877, nll_loss=3.622, ppl=12.31, wps=1025.7, ups=0.4, wpb=2569, bsz=92, num_updates=3134, lr=2.67943e-05, gnorm=2.12, train_wall=5, gb_free=13.7, wall=9613
2024-08-17 20:08:50 | INFO | train_inner | epoch 034:     34 / 94 loss=6.944, nll_loss=3.704, ppl=13.04, wps=1074.5, ups=0.38, wpb=2839, bsz=124, num_updates=3136, lr=2.67857e-05, gnorm=2.121, train_wall=5, gb_free=13.8, wall=9618
2024-08-17 20:08:53 | INFO | train_inner | epoch 034:     36 / 94 loss=7.068, nll_loss=3.862, ppl=14.54, wps=1315, ups=0.53, wpb=2464, bsz=100, num_updates=3138, lr=2.67772e-05, gnorm=2.352, train_wall=4, gb_free=15.8, wall=9622
2024-08-17 20:08:58 | INFO | train_inner | epoch 034:     38 / 94 loss=7.068, nll_loss=3.878, ppl=14.7, wps=1312.3, ups=0.4, wpb=3305.5, bsz=156, num_updates=3140, lr=2.67686e-05, gnorm=2.063, train_wall=5, gb_free=13.1, wall=9627
2024-08-17 20:09:02 | INFO | train_inner | epoch 034:     40 / 94 loss=6.534, nll_loss=3.186, ppl=9.1, wps=873.9, ups=0.52, wpb=1685, bsz=52, num_updates=3142, lr=2.67601e-05, gnorm=2.712, train_wall=4, gb_free=15.3, wall=9631
2024-08-17 20:09:07 | INFO | train_inner | epoch 034:     42 / 94 loss=6.607, nll_loss=3.259, ppl=9.57, wps=1017.7, ups=0.46, wpb=2208, bsz=72, num_updates=3144, lr=2.67516e-05, gnorm=2.364, train_wall=4, gb_free=12.5, wall=9635
2024-08-17 20:09:12 | INFO | train_inner | epoch 034:     44 / 94 loss=7.014, nll_loss=3.798, ppl=13.91, wps=1164.9, ups=0.36, wpb=3192, bsz=148, num_updates=3146, lr=2.67431e-05, gnorm=2.116, train_wall=5, gb_free=11.7, wall=9640
2024-08-17 20:09:17 | INFO | train_inner | epoch 034:     46 / 94 loss=6.942, nll_loss=3.717, ppl=13.15, wps=1144.6, ups=0.45, wpb=2544.5, bsz=140, num_updates=3148, lr=2.67346e-05, gnorm=2.237, train_wall=4, gb_free=13.6, wall=9645
2024-08-17 20:09:21 | INFO | train_inner | epoch 034:     48 / 94 loss=6.154, nll_loss=2.689, ppl=6.45, wps=538.8, ups=0.47, wpb=1139.5, bsz=52, num_updates=3150, lr=2.67261e-05, gnorm=2.954, train_wall=4, gb_free=18, wall=9649
2024-08-17 20:09:25 | INFO | train_inner | epoch 034:     50 / 94 loss=6.891, nll_loss=3.637, ppl=12.44, wps=1498.8, ups=0.44, wpb=3398, bsz=132, num_updates=3152, lr=2.67176e-05, gnorm=2.019, train_wall=5, gb_free=11.6, wall=9654
2024-08-17 20:09:29 | INFO | train_inner | epoch 034:     52 / 94 loss=6.723, nll_loss=3.423, ppl=10.72, wps=1538.5, ups=0.53, wpb=2894, bsz=72, num_updates=3154, lr=2.67092e-05, gnorm=2.131, train_wall=4, gb_free=14.8, wall=9657
2024-08-17 20:09:39 | INFO | train_inner | epoch 034:     54 / 94 loss=7, nll_loss=3.785, ppl=13.79, wps=713.8, ups=0.2, wpb=3621.5, bsz=200, num_updates=3156, lr=2.67007e-05, gnorm=1.852, train_wall=10, gb_free=15.1, wall=9668
2024-08-17 20:09:44 | INFO | train_inner | epoch 034:     56 / 94 loss=6.916, nll_loss=3.661, ppl=12.65, wps=1115.4, ups=0.43, wpb=2608, bsz=92, num_updates=3158, lr=2.66923e-05, gnorm=2.376, train_wall=5, gb_free=13.3, wall=9672
2024-08-17 20:09:49 | INFO | train_inner | epoch 034:     58 / 94 loss=7.026, nll_loss=3.816, ppl=14.08, wps=1489.3, ups=0.43, wpb=3438, bsz=148, num_updates=3160, lr=2.66838e-05, gnorm=1.966, train_wall=5, gb_free=12.5, wall=9677
2024-08-17 20:09:53 | INFO | train_inner | epoch 034:     60 / 94 loss=6.914, nll_loss=3.664, ppl=12.67, wps=1571, ups=0.47, wpb=3337, bsz=108, num_updates=3162, lr=2.66754e-05, gnorm=1.869, train_wall=4, gb_free=18.6, wall=9681
2024-08-17 20:09:58 | INFO | train_inner | epoch 034:     62 / 94 loss=7.157, nll_loss=3.99, ppl=15.89, wps=1303.4, ups=0.37, wpb=3504, bsz=204, num_updates=3164, lr=2.66669e-05, gnorm=2.082, train_wall=5, gb_free=10.7, wall=9686
2024-08-17 20:10:03 | INFO | train_inner | epoch 034:     64 / 94 loss=6.995, nll_loss=3.767, ppl=13.61, wps=1668, ups=0.46, wpb=3656.5, bsz=120, num_updates=3166, lr=2.66585e-05, gnorm=1.838, train_wall=4, gb_free=14.2, wall=9691
2024-08-17 20:10:08 | INFO | train_inner | epoch 034:     66 / 94 loss=6.841, nll_loss=3.592, ppl=12.06, wps=823.7, ups=0.38, wpb=2153, bsz=116, num_updates=3168, lr=2.66501e-05, gnorm=2.452, train_wall=5, gb_free=12.4, wall=9696
2024-08-17 20:10:12 | INFO | train_inner | epoch 034:     68 / 94 loss=6.841, nll_loss=3.585, ppl=12, wps=931.7, ups=0.45, wpb=2073, bsz=108, num_updates=3170, lr=2.66417e-05, gnorm=2.455, train_wall=4, gb_free=14.5, wall=9700
2024-08-17 20:10:18 | INFO | train_inner | epoch 034:     70 / 94 loss=6.798, nll_loss=3.515, ppl=11.43, wps=909.4, ups=0.35, wpb=2624, bsz=112, num_updates=3172, lr=2.66333e-05, gnorm=2.206, train_wall=6, gb_free=11, wall=9706
2024-08-17 20:10:22 | INFO | train_inner | epoch 034:     72 / 94 loss=6.652, nll_loss=3.335, ppl=10.09, wps=1093.5, ups=0.5, wpb=2176.5, bsz=47, num_updates=3174, lr=2.66249e-05, gnorm=2.525, train_wall=4, gb_free=13.7, wall=9710
2024-08-17 20:10:27 | INFO | train_inner | epoch 034:     74 / 94 loss=7.278, nll_loss=4.15, ppl=17.75, wps=1804, ups=0.4, wpb=4482, bsz=200, num_updates=3176, lr=2.66165e-05, gnorm=1.858, train_wall=5, gb_free=9.4, wall=9715
2024-08-17 20:10:31 | INFO | train_inner | epoch 034:     76 / 94 loss=7.031, nll_loss=3.822, ppl=14.15, wps=1480.2, ups=0.48, wpb=3052.5, bsz=60, num_updates=3178, lr=2.66081e-05, gnorm=2.212, train_wall=4, gb_free=16, wall=9719
2024-08-17 20:10:36 | INFO | train_inner | epoch 034:     78 / 94 loss=7.158, nll_loss=3.977, ppl=15.74, wps=1375.7, ups=0.44, wpb=3161.5, bsz=160, num_updates=3180, lr=2.65998e-05, gnorm=2.196, train_wall=5, gb_free=11.6, wall=9724
2024-08-17 20:10:40 | INFO | train_inner | epoch 034:     80 / 94 loss=6.957, nll_loss=3.708, ppl=13.07, wps=1091.8, ups=0.42, wpb=2605, bsz=140, num_updates=3182, lr=2.65914e-05, gnorm=2.21, train_wall=5, gb_free=14.4, wall=9729
2024-08-17 20:10:44 | INFO | train_inner | epoch 034:     82 / 94 loss=6.492, nll_loss=3.114, ppl=8.66, wps=723.5, ups=0.52, wpb=1386.5, bsz=48, num_updates=3184, lr=2.6583e-05, gnorm=3.034, train_wall=4, gb_free=12.8, wall=9733
2024-08-17 20:10:49 | INFO | train_inner | epoch 034:     84 / 94 loss=7.029, nll_loss=3.844, ppl=14.36, wps=1130.4, ups=0.45, wpb=2507.5, bsz=160, num_updates=3186, lr=2.65747e-05, gnorm=2.466, train_wall=4, gb_free=12.9, wall=9737
2024-08-17 20:10:53 | INFO | train_inner | epoch 034:     86 / 94 loss=7.066, nll_loss=3.885, ppl=14.77, wps=1376.7, ups=0.43, wpb=3209.5, bsz=160, num_updates=3188, lr=2.65664e-05, gnorm=2.091, train_wall=5, gb_free=13.8, wall=9742
2024-08-17 20:10:58 | INFO | train_inner | epoch 034:     88 / 94 loss=6.985, nll_loss=3.771, ppl=13.65, wps=1415.7, ups=0.43, wpb=3312.5, bsz=116, num_updates=3190, lr=2.6558e-05, gnorm=2.198, train_wall=5, gb_free=14.2, wall=9746
2024-08-17 20:11:02 | INFO | train_inner | epoch 034:     90 / 94 loss=6.825, nll_loss=3.549, ppl=11.71, wps=1390, ups=0.49, wpb=2824, bsz=72, num_updates=3192, lr=2.65497e-05, gnorm=2.254, train_wall=4, gb_free=15.9, wall=9750
2024-08-17 20:11:07 | INFO | train_inner | epoch 034:     92 / 94 loss=6.852, nll_loss=3.568, ppl=11.86, wps=1252.6, ups=0.42, wpb=2990, bsz=88, num_updates=3194, lr=2.65414e-05, gnorm=2.067, train_wall=5, gb_free=8.5, wall=9755
2024-08-17 20:11:11 | INFO | train_inner | epoch 034:     94 / 94 loss=6.872, nll_loss=3.598, ppl=12.1, wps=987.4, ups=0.51, wpb=1925.5, bsz=68, num_updates=3196, lr=2.65331e-05, gnorm=3.19, train_wall=4, gb_free=18.2, wall=9759
2024-08-17 20:11:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34612.453125Mb; avail=220453.23828125Mb
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000568
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34615.8984375Mb; avail=220449.79296875Mb
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005387
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34620.8203125Mb; avail=220445.36328125Mb
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004808
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011100
2024-08-17 20:11:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34617.375Mb; avail=220448.31640625Mb
2024-08-17 20:11:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.064 | nll_loss 4.937 | ppl 30.63 | wps 2423.3 | wpb 944.1 | bsz 40.1 | num_updates 3196 | best_loss 7.925
2024-08-17 20:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3196 updates
2024-08-17 20:11:24 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:12:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 34 @ 3196 updates, score 8.064) (writing took 38.18177476990968 seconds)
2024-08-17 20:12:02 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-08-17 20:12:02 | INFO | train | epoch 034 | loss 6.952 | nll_loss 3.72 | ppl 13.17 | wps 979.5 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 3196 | lr 2.65331e-05 | gnorm 2.259 | train_wall 220 | gb_free 18.2 | wall 9811
2024-08-17 20:12:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:mr-hi': 11221}; raw total size: 11221
2024-08-17 20:12:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:mr-hi': 11221}; resampled total size: 11221
2024-08-17 20:12:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:mr-hi': 1.0}
2024-08-17 20:12:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000798
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22675.2890625Mb; avail=232390.890625Mb
2024-08-17 20:12:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000157
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001483
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22675.2890625Mb; avail=232390.890625Mb
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22675.2890625Mb; avail=232390.890625Mb
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000534
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002349
2024-08-17 20:12:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22675.2890625Mb; avail=232390.890625Mb
2024-08-17 20:12:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 94
2024-08-17 20:12:03 | INFO | fairseq.trainer | begin training epoch 35
2024-08-17 20:12:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-08-17 20:12:07 | INFO | train_inner | epoch 035:      2 / 94 loss=6.467, nll_loss=3.096, ppl=8.55, wps=63.9, ups=0.04, wpb=1788.5, bsz=64, num_updates=3198, lr=2.65248e-05, gnorm=2.752, train_wall=4, gb_free=12.7, wall=9815
2024-08-17 20:12:11 | INFO | train_inner | epoch 035:      4 / 94 loss=6.652, nll_loss=3.333, ppl=10.08, wps=1111, ups=0.46, wpb=2404, bsz=76, num_updates=3200, lr=2.65165e-05, gnorm=2.248, train_wall=4, gb_free=12.5, wall=9819
2024-08-17 20:12:17 | INFO | train_inner | epoch 035:      6 / 94 loss=6.912, nll_loss=3.674, ppl=12.76, wps=1222.7, ups=0.36, wpb=3360, bsz=164, num_updates=3202, lr=2.65082e-05, gnorm=2.119, train_wall=5, gb_free=12.6, wall=9825
2024-08-17 20:12:21 | INFO | train_inner | epoch 035:      8 / 94 loss=6.803, nll_loss=3.541, ppl=11.64, wps=1274.2, ups=0.47, wpb=2732, bsz=140, num_updates=3204, lr=2.64999e-05, gnorm=2.144, train_wall=4, gb_free=14.3, wall=9829
2024-08-17 20:12:25 | INFO | train_inner | epoch 035:     10 / 94 loss=6.634, nll_loss=3.31, ppl=9.92, wps=1012, ups=0.47, wpb=2140, bsz=72, num_updates=3206, lr=2.64917e-05, gnorm=2.484, train_wall=4, gb_free=15.8, wall=9833
2024-08-17 20:12:30 | INFO | train_inner | epoch 035:     12 / 94 loss=6.854, nll_loss=3.601, ppl=12.13, wps=1175.9, ups=0.41, wpb=2841.5, bsz=136, num_updates=3208, lr=2.64834e-05, gnorm=2.208, train_wall=5, gb_free=14.2, wall=9838
2024-08-17 20:12:34 | INFO | train_inner | epoch 035:     14 / 94 loss=6.449, nll_loss=3.068, ppl=8.39, wps=953.4, ups=0.48, wpb=1982.5, bsz=64, num_updates=3210, lr=2.64752e-05, gnorm=2.565, train_wall=4, gb_free=16.9, wall=9842
2024-08-17 20:12:39 | INFO | train_inner | epoch 035:     16 / 94 loss=6.754, nll_loss=3.463, ppl=11.03, wps=1020.9, ups=0.45, wpb=2280.5, bsz=120, num_updates=3212, lr=2.64669e-05, gnorm=2.364, train_wall=4, gb_free=14.4, wall=9847
2024-08-17 20:12:43 | INFO | train_inner | epoch 035:     18 / 94 loss=7.148, nll_loss=3.972, ppl=15.69, wps=1661.9, ups=0.42, wpb=3939.5, bsz=204, num_updates=3214, lr=2.64587e-05, gnorm=1.86, train_wall=5, gb_free=13.2, wall=9852
2024-08-17 20:12:47 | INFO | train_inner | epoch 035:     20 / 94 loss=6.857, nll_loss=3.599, ppl=12.12, wps=1579.3, ups=0.57, wpb=2792, bsz=100, num_updates=3216, lr=2.64505e-05, gnorm=2.266, train_wall=4, gb_free=22.5, wall=9855
2024-08-17 20:12:51 | INFO | train_inner | epoch 035:     22 / 94 loss=7.024, nll_loss=3.812, ppl=14.04, wps=1385.3, ups=0.48, wpb=2888, bsz=168, num_updates=3218, lr=2.64422e-05, gnorm=2.082, train_wall=4, gb_free=13.9, wall=9859
2024-08-17 20:12:56 | INFO | train_inner | epoch 035:     24 / 94 loss=6.888, nll_loss=3.629, ppl=12.37, wps=1224.5, ups=0.43, wpb=2830.5, bsz=128, num_updates=3220, lr=2.6434e-05, gnorm=2.203, train_wall=5, gb_free=14.7, wall=9864
2024-08-17 20:13:00 | INFO | train_inner | epoch 035:     26 / 94 loss=6.681, nll_loss=3.377, ppl=10.39, wps=1025.9, ups=0.43, wpb=2364.5, bsz=96, num_updates=3222, lr=2.64258e-05, gnorm=2.24, train_wall=5, gb_free=14.7, wall=9869
2024-08-17 20:13:05 | INFO | train_inner | epoch 035:     28 / 94 loss=6.672, nll_loss=3.366, ppl=10.31, wps=771.7, ups=0.4, wpb=1924.5, bsz=88, num_updates=3224, lr=2.64176e-05, gnorm=2.58, train_wall=5, gb_free=13.9, wall=9874
2024-08-17 20:13:16 | INFO | train_inner | epoch 035:     30 / 94 loss=6.541, nll_loss=3.202, ppl=9.2, wps=327.6, ups=0.19, wpb=1690.5, bsz=88, num_updates=3226, lr=2.64094e-05, gnorm=2.569, train_wall=10, gb_free=13.5, wall=9884
2024-08-17 20:13:20 | INFO | train_inner | epoch 035:     32 / 94 loss=6.89, nll_loss=3.638, ppl=12.45, wps=1067.5, ups=0.5, wpb=2144, bsz=84, num_updates=3228, lr=2.64013e-05, gnorm=2.487, train_wall=4, gb_free=15.7, wall=9888
2024-08-17 20:13:24 | INFO | train_inner | epoch 035:     34 / 94 loss=7.073, nll_loss=3.861, ppl=14.53, wps=1642.4, ups=0.42, wpb=3895, bsz=144, num_updates=3230, lr=2.63931e-05, gnorm=2.035, train_wall=5, gb_free=10.4, wall=9893
2024-08-17 20:13:30 | INFO | train_inner | epoch 035:     36 / 94 loss=7.128, nll_loss=3.952, ppl=15.48, wps=1443.5, ups=0.36, wpb=4064.5, bsz=220, num_updates=3232, lr=2.63849e-05, gnorm=1.803, train_wall=6, gb_free=9.2, wall=9898
2024-08-17 20:13:35 | INFO | train_inner | epoch 035:     38 / 94 loss=6.994, nll_loss=3.79, ppl=13.83, wps=1232.2, ups=0.42, wpb=2911.5, bsz=156, num_updates=3234, lr=2.63767e-05, gnorm=2.301, train_wall=5, gb_free=13.7, wall=9903
2024-08-17 20:13:40 | INFO | train_inner | epoch 035:     40 / 94 loss=6.927, nll_loss=3.674, ppl=12.76, wps=1316.2, ups=0.41, wpb=3200.5, bsz=104, num_updates=3236, lr=2.63686e-05, gnorm=2.19, train_wall=5, gb_free=13.2, wall=9908
2024-08-17 20:13:44 | INFO | train_inner | epoch 035:     42 / 94 loss=6.59, nll_loss=3.249, ppl=9.5, wps=776.8, ups=0.5, wpb=1560.5, bsz=56, num_updates=3238, lr=2.63605e-05, gnorm=2.995, train_wall=4, gb_free=15.3, wall=9912
2024-08-17 20:13:48 | INFO | train_inner | epoch 035:     44 / 94 loss=6.928, nll_loss=3.684, ppl=12.85, wps=1435.1, ups=0.47, wpb=3028, bsz=164, num_updates=3240, lr=2.63523e-05, gnorm=1.942, train_wall=4, gb_free=14.7, wall=9916
2024-08-17 20:13:52 | INFO | train_inner | epoch 035:     46 / 94 loss=6.85, nll_loss=3.564, ppl=11.83, wps=1342.1, ups=0.46, wpb=2917.5, bsz=84, num_updates=3242, lr=2.63442e-05, gnorm=2.164, train_wall=4, gb_free=15.1, wall=9920
2024-08-17 20:13:57 | INFO | train_inner | epoch 035:     48 / 94 loss=6.704, nll_loss=3.394, ppl=10.51, wps=1070.8, ups=0.43, wpb=2477, bsz=96, num_updates=3244, lr=2.63361e-05, gnorm=2.283, train_wall=5, gb_free=12.6, wall=9925
2024-08-17 20:14:01 | INFO | train_inner | epoch 035:     50 / 94 loss=6.237, nll_loss=2.794, ppl=6.94, wps=845, ups=0.51, wpb=1667.5, bsz=63.5, num_updates=3246, lr=2.63279e-05, gnorm=2.716, train_wall=4, gb_free=15.9, wall=9929
2024-08-17 20:14:06 | INFO | train_inner | epoch 035:     52 / 94 loss=6.888, nll_loss=3.668, ppl=12.71, wps=972.2, ups=0.41, wpb=2366.5, bsz=136, num_updates=3248, lr=2.63198e-05, gnorm=2.336, train_wall=5, gb_free=11, wall=9934
2024-08-17 20:14:10 | INFO | train_inner | epoch 035:     54 / 94 loss=7.086, nll_loss=3.893, ppl=14.86, wps=1717.6, ups=0.42, wpb=4117.5, bsz=156, num_updates=3250, lr=2.63117e-05, gnorm=1.942, train_wall=5, gb_free=14.8, wall=9939
2024-08-17 20:14:15 | INFO | train_inner | epoch 035:     56 / 94 loss=6.804, nll_loss=3.533, ppl=11.57, wps=1068.4, ups=0.48, wpb=2240, bsz=72, num_updates=3252, lr=2.63036e-05, gnorm=2.51, train_wall=4, gb_free=11.4, wall=9943
2024-08-17 20:14:19 | INFO | train_inner | epoch 035:     58 / 94 loss=6.741, nll_loss=3.44, ppl=10.85, wps=1023.7, ups=0.41, wpb=2488.5, bsz=100, num_updates=3254, lr=2.62956e-05, gnorm=2.325, train_wall=5, gb_free=13.5, wall=9948
2024-08-17 20:14:25 | INFO | train_inner | epoch 035:     60 / 94 loss=7.119, nll_loss=3.924, ppl=15.18, wps=1561.7, ups=0.37, wpb=4244, bsz=164, num_updates=3256, lr=2.62875e-05, gnorm=1.78, train_wall=5, gb_free=10.6, wall=9953
2024-08-17 20:14:30 | INFO | train_inner | epoch 035:     62 / 94 loss=6.704, nll_loss=3.387, ppl=10.46, wps=964.2, ups=0.42, wpb=2312.5, bsz=88, num_updates=3258, lr=2.62794e-05, gnorm=2.361, train_wall=5, gb_free=13.7, wall=9958
2024-08-17 20:14:34 | INFO | train_inner | epoch 035:     64 / 94 loss=6.527, nll_loss=3.172, ppl=9.01, wps=969.4, ups=0.47, wpb=2079, bsz=72, num_updates=3260, lr=2.62714e-05, gnorm=2.412, train_wall=4, gb_free=16.2, wall=9962
2024-08-17 20:14:39 | INFO | train_inner | epoch 035:     66 / 94 loss=6.807, nll_loss=3.548, ppl=11.7, wps=1157.9, ups=0.38, wpb=3019, bsz=120, num_updates=3262, lr=2.62633e-05, gnorm=1.986, train_wall=5, gb_free=13.6, wall=9967
2024-08-17 20:14:43 | INFO | train_inner | epoch 035:     68 / 94 loss=6.875, nll_loss=3.63, ppl=12.38, wps=1657.8, ups=0.47, wpb=3512.5, bsz=100, num_updates=3264, lr=2.62553e-05, gnorm=1.96, train_wall=4, gb_free=14.1, wall=9972
2024-08-17 20:14:48 | INFO | train_inner | epoch 035:     70 / 94 loss=6.959, nll_loss=3.753, ppl=13.48, wps=1136.9, ups=0.41, wpb=2761, bsz=184, num_updates=3266, lr=2.62472e-05, gnorm=2.21, train_wall=5, gb_free=14.3, wall=9977
2024-08-17 20:14:53 | INFO | train_inner | epoch 035:     72 / 94 loss=6.958, nll_loss=3.729, ppl=13.26, wps=1612, ups=0.46, wpb=3476.5, bsz=100, num_updates=3268, lr=2.62392e-05, gnorm=2.01, train_wall=4, gb_free=13.3, wall=9981
2024-08-17 20:14:57 | INFO | train_inner | epoch 035:     74 / 94 loss=6.963, nll_loss=3.731, ppl=13.28, wps=984.4, ups=0.47, wpb=2108, bsz=100, num_updates=3270, lr=2.62312e-05, gnorm=2.699, train_wall=4, gb_free=14.2, wall=9985
2024-08-17 20:15:00 | INFO | train_inner | epoch 035:     76 / 94 loss=6.445, nll_loss=3.07, ppl=8.4, wps=714.9, ups=0.57, wpb=1255.5, bsz=31, num_updates=3272, lr=2.62231e-05, gnorm=3.526, train_wall=4, gb_free=12.1, wall=9989
2024-08-17 20:15:06 | INFO | train_inner | epoch 035:     78 / 94 loss=7.002, nll_loss=3.764, ppl=13.58, wps=1460, ups=0.36, wpb=4024.5, bsz=192, num_updates=3274, lr=2.62151e-05, gnorm=1.786, train_wall=6, gb_free=9.8, wall=9994
2024-08-17 20:15:11 | INFO | train_inner | epoch 035:     80 / 94 loss=7.152, nll_loss=3.957, ppl=15.53, wps=1480, ups=0.37, wpb=3987.5, bsz=196, num_updates=3276, lr=2.62071e-05, gnorm=1.877, train_wall=5, gb_free=9.9, wall=10000
2024-08-17 20:15:15 | INFO | train_inner | epoch 035:     82 / 94 loss=6.894, nll_loss=3.637, ppl=12.44, wps=1803.8, ups=0.5, wpb=3644, bsz=88, num_updates=3278, lr=2.61991e-05, gnorm=2.031, train_wall=4, gb_free=18.4, wall=10004
2024-08-17 20:15:20 | INFO | train_inner | epoch 035:     84 / 94 loss=7.091, nll_loss=3.906, ppl=14.99, wps=1573, ups=0.4, wpb=3884.5, bsz=140, num_updates=3280, lr=2.61911e-05, gnorm=1.857, train_wall=5, gb_free=12.9, wall=10009
2024-08-17 20:15:25 | INFO | train_inner | epoch 035:     86 / 94 loss=6.866, nll_loss=3.637, ppl=12.44, wps=1257.6, ups=0.41, wpb=3058, bsz=132, num_updates=3282, lr=2.61832e-05, gnorm=2.08, train_wall=5, gb_free=14.8, wall=10013
2024-08-17 20:15:31 | INFO | train_inner | epoch 035:     88 / 94 loss=7.116, nll_loss=3.957, ppl=15.53, wps=1396, ups=0.36, wpb=3912.5, bsz=236, num_updates=3284, lr=2.61752e-05, gnorm=1.922, train_wall=6, gb_free=11.7, wall=10019
2024-08-17 20:15:35 | INFO | train_inner | epoch 035:     90 / 94 loss=7.031, nll_loss=3.826, ppl=14.18, wps=1548.5, ups=0.44, wpb=3514, bsz=124, num_updates=3286, lr=2.61672e-05, gnorm=2.023, train_wall=5, gb_free=9.6, wall=10024
2024-08-17 20:15:39 | INFO | train_inner | epoch 035:     92 / 94 loss=6.843, nll_loss=3.561, ppl=11.8, wps=1513.4, ups=0.49, wpb=3097, bsz=92, num_updates=3288, lr=2.61593e-05, gnorm=2.035, train_wall=4, gb_free=14.7, wall=10028
2024-08-17 20:15:44 | INFO | train_inner | epoch 035:     94 / 94 loss=6.946, nll_loss=3.695, ppl=12.95, wps=1223.5, ups=0.48, wpb=2562, bsz=108, num_updates=3290, lr=2.61513e-05, gnorm=2.368, train_wall=4, gb_free=15.4, wall=10032
2024-08-17 20:15:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20894.80078125Mb; avail=234171.37890625Mb
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20894.80078125Mb; avail=234171.37890625Mb
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.005371
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20894.80078125Mb; avail=234171.37890625Mb
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.004805
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.011065
2024-08-17 20:15:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20894.80078125Mb; avail=234171.37890625Mb
2024-08-17 20:15:57 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.091 | nll_loss 4.954 | ppl 31 | wps 2426.2 | wpb 944.1 | bsz 40.1 | num_updates 3290 | best_loss 7.925
2024-08-17 20:15:57 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2024-08-17 20:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3290 updates
2024-08-17 20:15:57 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:16:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt
2024-08-17 20:16:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Marathi/checkpoint1.2B_Mr-Hi/checkpoint_last.pt (epoch 35 @ 3290 updates, score 8.091) (writing took 38.718375185038894 seconds)
2024-08-17 20:16:36 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-08-17 20:16:36 | INFO | train | epoch 035 | loss 6.889 | nll_loss 3.639 | ppl 12.46 | wps 977 | ups 0.34 | wpb 2840.2 | bsz 119.4 | num_updates 3290 | lr 2.61513e-05 | gnorm 2.248 | train_wall 220 | gb_free 15.4 | wall 10084
2024-08-17 20:16:36 | INFO | fairseq_cli.train | done training in 10076.6 seconds
