2024-07-20 09:48:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Nepali/checkpoint1.2B', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='hi-ne', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='hi-ne', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-07-20 09:48:26 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'ne']
2024-07-20 09:48:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-07-20 09:48:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ne] dictionary: 128112 types
2024-07-20 09:48:35 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-07-20 09:48:35 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-07-20 09:48:35 | INFO | fairseq_cli.train | model: TransformerModel
2024-07-20 09:48:35 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-07-20 09:48:35 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-07-20 09:48:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-07-20 09:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-07-20 09:48:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32938.125Mb; avail=222104.0859375Mb
2024-07-20 09:48:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-07-20 09:48:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:hi-ne': 1}
2024-07-20 09:48:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hi-ne src_langtok: 128036; tgt_langtok: 128066
2024-07-20 09:48:35 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.hi-ne.hi
2024-07-20 09:48:35 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.hi-ne.ne
2024-07-20 09:48:35 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin valid hi-ne 521 examples
2024-07-20 09:48:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-07-20 09:48:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-07-20 09:48:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-07-20 09:48:36 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-07-20 09:48:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-07-20 09:48:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-07-20 09:48:36 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-07-20 09:48:36 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/1.2B_last_checkpoint.pt: optimizer, lr scheduler, meters, dataloader will be reset
2024-07-20 09:48:36 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/1.2B_last_checkpoint.pt
2024-07-20 09:48:43 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-07-20 09:48:43 | INFO | fairseq.trainer | Loaded checkpoint /home/krish/content/1.2B_last_checkpoint.pt (epoch 81 @ 0 updates)
2024-07-20 09:48:43 | INFO | fairseq.trainer | loading train data for epoch 1
2024-07-20 09:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-07-20 09:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29288.37109375Mb; avail=225745.8984375Mb
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:hi-ne': 1}
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hi-ne src_langtok: 128036; tgt_langtok: 128066
2024-07-20 09:48:43 | INFO | fairseq.data.data_utils | loaded 4,163 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/train.hi-ne.hi
2024-07-20 09:48:43 | INFO | fairseq.data.data_utils | loaded 4,163 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/train.hi-ne.ne
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin train hi-ne 4163 examples
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:hi-ne', 4163)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:hi-ne', 1.0)]
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000481
2024-07-20 09:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29288.3671875Mb; avail=225745.90234375Mb
2024-07-20 09:48:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000053
2024-07-20 09:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000635
2024-07-20 09:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29288.3671875Mb; avail=225745.90234375Mb
2024-07-20 09:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000029
2024-07-20 09:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29288.859375Mb; avail=225745.41015625Mb
2024-07-20 09:48:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000632
2024-07-20 09:48:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001600
2024-07-20 09:48:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29288.3671875Mb; avail=225745.90234375Mb
2024-07-20 09:48:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 09:48:44 | INFO | fairseq.trainer | begin training epoch 1
2024-07-20 09:48:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 09:48:49 | INFO | train_inner | epoch 001:      2 / 19 loss=11.819, nll_loss=9.735, ppl=852.28, wps=1508.5, ups=0.35, wpb=4737.5, bsz=192, num_updates=2, lr=2.4e-08, gnorm=16.517, train_wall=6, gb_free=11.2, wall=14
2024-07-20 09:48:55 | INFO | train_inner | epoch 001:      4 / 19 loss=11.983, nll_loss=9.943, ppl=984.07, wps=1730.7, ups=0.36, wpb=4794, bsz=176, num_updates=4, lr=4.8e-08, gnorm=18.562, train_wall=6, gb_free=12.5, wall=19
2024-07-20 09:49:01 | INFO | train_inner | epoch 001:      6 / 19 loss=11.82, nll_loss=9.73, ppl=849.17, wps=1806.3, ups=0.34, wpb=5337.5, bsz=276, num_updates=6, lr=7.2e-08, gnorm=15.483, train_wall=6, gb_free=11.2, wall=25
2024-07-20 09:49:06 | INFO | train_inner | epoch 001:      8 / 19 loss=12.045, nll_loss=10.004, ppl=1027.03, wps=1848.3, ups=0.39, wpb=4792, bsz=240, num_updates=8, lr=9.6e-08, gnorm=15.245, train_wall=5, gb_free=11.1, wall=30
2024-07-20 09:49:12 | INFO | train_inner | epoch 001:     10 / 19 loss=11.788, nll_loss=9.684, ppl=822.81, wps=1740.6, ups=0.36, wpb=4840, bsz=260, num_updates=10, lr=1.2e-07, gnorm=14.529, train_wall=6, gb_free=13.4, wall=36
2024-07-20 09:49:17 | INFO | train_inner | epoch 001:     12 / 19 loss=11.973, nll_loss=9.93, ppl=975.74, wps=1842.3, ups=0.36, wpb=5136.5, bsz=204, num_updates=12, lr=1.44e-07, gnorm=16.376, train_wall=6, gb_free=14, wall=41
2024-07-20 09:49:23 | INFO | train_inner | epoch 001:     14 / 19 loss=11.742, nll_loss=9.635, ppl=794.92, wps=1973.9, ups=0.34, wpb=5771.5, bsz=272, num_updates=14, lr=1.68e-07, gnorm=14.982, train_wall=6, gb_free=11, wall=47
2024-07-20 09:49:28 | INFO | train_inner | epoch 001:     16 / 19 loss=11.694, nll_loss=9.576, ppl=763.03, wps=1973.3, ups=0.38, wpb=5220.5, bsz=192, num_updates=16, lr=1.92e-07, gnorm=15.526, train_wall=5, gb_free=12.8, wall=53
2024-07-20 09:49:33 | INFO | train_inner | epoch 001:     18 / 19 loss=11.531, nll_loss=9.365, ppl=659.48, wps=1969.3, ups=0.46, wpb=4316, bsz=221.5, num_updates=18, lr=2.16e-07, gnorm=14.202, train_wall=4, gb_free=10.9, wall=57
2024-07-20 09:49:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16973.41015625Mb; avail=238052.83203125Mb
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16973.41015625Mb; avail=238052.83203125Mb
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002234
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16973.41015625Mb; avail=238052.83203125Mb
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001882
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004994
2024-07-20 09:49:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16973.90234375Mb; avail=238052.33984375Mb
2024-07-20 09:49:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.566 | nll_loss 9.251 | ppl 609.44 | wps 3129.6 | wpb 1665.6 | bsz 74.4 | num_updates 19
2024-07-20 09:49:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 19 updates
2024-07-20 09:49:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:50:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:50:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 1 @ 19 updates, score 11.566) (writing took 49.28988795611076 seconds)
2024-07-20 09:50:27 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-07-20 09:50:27 | INFO | train | epoch 001 | loss 11.806 | nll_loss 9.713 | ppl 839.55 | wps 872.8 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 19 | lr 2.28e-07 | gnorm 15.662 | train_wall 50 | gb_free 17.4 | wall 111
2024-07-20 09:50:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 09:50:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 09:50:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 09:50:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000672
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36668.38671875Mb; avail=218361.76171875Mb
2024-07-20 09:50:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000706
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36666.91015625Mb; avail=218362.18359375Mb
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36667.89453125Mb; avail=218361.68359375Mb
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000217
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001247
2024-07-20 09:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36667.40234375Mb; avail=218362.015625Mb
2024-07-20 09:50:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 09:50:27 | INFO | fairseq.trainer | begin training epoch 2
2024-07-20 09:50:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 09:50:30 | INFO | train_inner | epoch 002:      1 / 19 loss=11.659, nll_loss=9.527, ppl=738, wps=137.9, ups=0.04, wpb=3913, bsz=180, num_updates=20, lr=2.4e-07, gnorm=14.217, train_wall=4, gb_free=13.6, wall=114
2024-07-20 09:50:34 | INFO | train_inner | epoch 002:      3 / 19 loss=11.295, nll_loss=9.073, ppl=538.63, wps=1936.6, ups=0.47, wpb=4123.5, bsz=189.5, num_updates=22, lr=2.64e-07, gnorm=14.039, train_wall=4, gb_free=17.1, wall=118
2024-07-20 09:50:40 | INFO | train_inner | epoch 002:      5 / 19 loss=11.491, nll_loss=9.321, ppl=639.64, wps=1996.5, ups=0.32, wpb=6304, bsz=276, num_updates=24, lr=2.88e-07, gnorm=13.52, train_wall=6, gb_free=10.9, wall=124
2024-07-20 09:50:46 | INFO | train_inner | epoch 002:      7 / 19 loss=11.708, nll_loss=9.606, ppl=779.03, wps=1690.5, ups=0.37, wpb=4592.5, bsz=156, num_updates=26, lr=3.12e-07, gnorm=14.393, train_wall=5, gb_free=13, wall=130
2024-07-20 09:50:50 | INFO | train_inner | epoch 002:      9 / 19 loss=11.537, nll_loss=9.377, ppl=664.7, wps=1870.3, ups=0.41, wpb=4532, bsz=200, num_updates=28, lr=3.36e-07, gnorm=12.409, train_wall=5, gb_free=11.5, wall=135
2024-07-20 09:50:55 | INFO | train_inner | epoch 002:     11 / 19 loss=11.184, nll_loss=8.937, ppl=489.98, wps=1902, ups=0.4, wpb=4799.5, bsz=220, num_updates=30, lr=3.6e-07, gnorm=12.088, train_wall=5, gb_free=12, wall=140
2024-07-20 09:51:01 | INFO | train_inner | epoch 002:     13 / 19 loss=11.367, nll_loss=9.171, ppl=576.25, wps=1837.3, ups=0.36, wpb=5141, bsz=236, num_updates=32, lr=3.84e-07, gnorm=11.782, train_wall=6, gb_free=15.7, wall=145
2024-07-20 09:51:07 | INFO | train_inner | epoch 002:     15 / 19 loss=11.513, nll_loss=9.351, ppl=653.04, wps=1854, ups=0.34, wpb=5474, bsz=272, num_updates=34, lr=4.08e-07, gnorm=10.971, train_wall=6, gb_free=13, wall=151
2024-07-20 09:51:13 | INFO | train_inner | epoch 002:     17 / 19 loss=11.372, nll_loss=9.176, ppl=578.48, wps=1633, ups=0.34, wpb=4791.5, bsz=228, num_updates=36, lr=4.32e-07, gnorm=10.58, train_wall=6, gb_free=12.6, wall=157
2024-07-20 09:51:17 | INFO | train_inner | epoch 002:     19 / 19 loss=11.012, nll_loss=8.726, ppl=423.45, wps=1632.6, ups=0.42, wpb=3845.5, bsz=172, num_updates=38, lr=4.56e-07, gnorm=10.677, train_wall=5, gb_free=16.5, wall=162
2024-07-20 09:51:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21999.703125Mb; avail=233030.4453125Mb
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22000.1953125Mb; avail=233029.953125Mb
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002144
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22000.1953125Mb; avail=233029.953125Mb
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001884
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004905
2024-07-20 09:51:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22000.1953125Mb; avail=233029.953125Mb
2024-07-20 09:51:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.031 | nll_loss 8.615 | ppl 392.21 | wps 4161.8 | wpb 1665.6 | bsz 74.4 | num_updates 38 | best_loss 11.031
2024-07-20 09:51:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 38 updates
2024-07-20 09:51:20 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:52:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 2 @ 38 updates, score 11.031) (writing took 62.00274620507844 seconds)
2024-07-20 09:52:22 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-07-20 09:52:22 | INFO | train | epoch 002 | loss 11.427 | nll_loss 9.243 | ppl 605.82 | wps 801.4 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 38 | lr 4.56e-07 | gnorm 12.348 | train_wall 51 | gb_free 16.5 | wall 226
2024-07-20 09:52:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 09:52:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 09:52:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 09:52:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000797
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37129.05859375Mb; avail=217901.09375Mb
2024-07-20 09:52:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000080
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000734
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37128.56640625Mb; avail=217901.09375Mb
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37129.05859375Mb; avail=217901.09375Mb
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000227
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001289
2024-07-20 09:52:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37129.05859375Mb; avail=217901.09375Mb
2024-07-20 09:52:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 09:52:22 | INFO | fairseq.trainer | begin training epoch 3
2024-07-20 09:52:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 09:52:28 | INFO | train_inner | epoch 003:      2 / 19 loss=11.324, nll_loss=9.122, ppl=557.23, wps=167.7, ups=0.03, wpb=5910.5, bsz=300, num_updates=40, lr=4.8e-07, gnorm=9.629, train_wall=6, gb_free=13.2, wall=232
2024-07-20 09:52:33 | INFO | train_inner | epoch 003:      4 / 19 loss=11.114, nll_loss=8.86, ppl=464.8, wps=2260, ups=0.39, wpb=5803, bsz=248, num_updates=42, lr=5.04e-07, gnorm=9.524, train_wall=5, gb_free=13.8, wall=237
2024-07-20 09:52:38 | INFO | train_inner | epoch 003:      6 / 19 loss=10.849, nll_loss=8.533, ppl=370.47, wps=1861.8, ups=0.4, wpb=4639.5, bsz=212, num_updates=44, lr=5.28e-07, gnorm=9.506, train_wall=5, gb_free=10.7, wall=242
2024-07-20 09:52:44 | INFO | train_inner | epoch 003:      8 / 19 loss=11.085, nll_loss=8.832, ppl=455.85, wps=1837.2, ups=0.34, wpb=5442, bsz=236, num_updates=46, lr=5.52e-07, gnorm=9.5, train_wall=6, gb_free=12.4, wall=248
2024-07-20 09:52:49 | INFO | train_inner | epoch 003:     10 / 19 loss=10.821, nll_loss=8.506, ppl=363.63, wps=1904.8, ups=0.4, wpb=4794, bsz=208, num_updates=48, lr=5.76e-07, gnorm=8.215, train_wall=5, gb_free=12.8, wall=253
2024-07-20 09:52:54 | INFO | train_inner | epoch 003:     12 / 19 loss=10.917, nll_loss=8.619, ppl=393.3, wps=1603.5, ups=0.39, wpb=4069.5, bsz=193.5, num_updates=50, lr=6e-07, gnorm=8.118, train_wall=5, gb_free=16.7, wall=258
2024-07-20 09:52:59 | INFO | train_inner | epoch 003:     14 / 19 loss=10.778, nll_loss=8.448, ppl=349.23, wps=2000.7, ups=0.4, wpb=5041, bsz=240, num_updates=52, lr=6.24e-07, gnorm=7.497, train_wall=5, gb_free=11.7, wall=263
2024-07-20 09:53:05 | INFO | train_inner | epoch 003:     16 / 19 loss=10.809, nll_loss=8.498, ppl=361.51, wps=1895.8, ups=0.33, wpb=5800, bsz=272, num_updates=54, lr=6.48e-07, gnorm=7.318, train_wall=6, gb_free=11.6, wall=269
2024-07-20 09:53:10 | INFO | train_inner | epoch 003:     18 / 19 loss=10.347, nll_loss=7.917, ppl=241.68, wps=1764.4, ups=0.42, wpb=4240, bsz=164, num_updates=56, lr=6.72e-07, gnorm=6.892, train_wall=5, gb_free=14.1, wall=274
2024-07-20 09:53:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22439.90625Mb; avail=232590.16015625Mb
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000590
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22439.90625Mb; avail=232590.16015625Mb
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002096
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22439.90625Mb; avail=232590.16015625Mb
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001854
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004949
2024-07-20 09:53:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22439.90625Mb; avail=232590.16015625Mb
2024-07-20 09:53:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.44 | nll_loss 7.911 | ppl 240.77 | wps 3384.1 | wpb 1665.6 | bsz 74.4 | num_updates 57 | best_loss 10.44
2024-07-20 09:53:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 57 updates
2024-07-20 09:53:14 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:54:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:54:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 3 @ 57 updates, score 10.44) (writing took 73.6099257459864 seconds)
2024-07-20 09:54:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-07-20 09:54:27 | INFO | train | epoch 003 | loss 10.915 | nll_loss 8.619 | ppl 393.21 | wps 736.8 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 57 | lr 6.84e-07 | gnorm 8.421 | train_wall 49 | gb_free 24.9 | wall 352
2024-07-20 09:54:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 09:54:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 09:54:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 09:54:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000924
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42464.875Mb; avail=212565.15625Mb
2024-07-20 09:54:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000110
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001136
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42464.875Mb; avail=212565.15625Mb
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42464.875Mb; avail=212565.15625Mb
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000339
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001995
2024-07-20 09:54:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42464.875Mb; avail=212565.15625Mb
2024-07-20 09:54:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 09:54:28 | INFO | fairseq.trainer | begin training epoch 4
2024-07-20 09:54:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 09:54:29 | INFO | train_inner | epoch 004:      1 / 19 loss=10.678, nll_loss=8.343, ppl=324.77, wps=57.1, ups=0.03, wpb=2259.5, bsz=97.5, num_updates=58, lr=6.96e-07, gnorm=7.063, train_wall=2, gb_free=11.3, wall=353
2024-07-20 09:54:35 | INFO | train_inner | epoch 004:      3 / 19 loss=10.429, nll_loss=8.018, ppl=259.21, wps=2245.2, ups=0.38, wpb=5968.5, bsz=288, num_updates=60, lr=7.2e-07, gnorm=6.368, train_wall=5, gb_free=11.5, wall=359
2024-07-20 09:54:40 | INFO | train_inner | epoch 004:      5 / 19 loss=10.77, nll_loss=8.453, ppl=350.34, wps=1878.5, ups=0.34, wpb=5556.5, bsz=232, num_updates=62, lr=7.44e-07, gnorm=6.309, train_wall=6, gb_free=12.3, wall=365
2024-07-20 09:54:45 | INFO | train_inner | epoch 004:      7 / 19 loss=10.275, nll_loss=7.82, ppl=226.03, wps=1544.3, ups=0.44, wpb=3515, bsz=156, num_updates=64, lr=7.68e-07, gnorm=5.89, train_wall=5, gb_free=14.5, wall=369
2024-07-20 09:54:50 | INFO | train_inner | epoch 004:      9 / 19 loss=10.497, nll_loss=8.119, ppl=278.11, wps=2242.8, ups=0.44, wpb=5050, bsz=200, num_updates=66, lr=7.92e-07, gnorm=5.414, train_wall=4, gb_free=15.5, wall=374
2024-07-20 09:54:55 | INFO | train_inner | epoch 004:     11 / 19 loss=10.581, nll_loss=8.219, ppl=298.02, wps=1775.8, ups=0.35, wpb=5094, bsz=264, num_updates=68, lr=8.16e-07, gnorm=5.825, train_wall=6, gb_free=13.9, wall=379
2024-07-20 09:55:00 | INFO | train_inner | epoch 004:     13 / 19 loss=10.322, nll_loss=7.896, ppl=238.14, wps=2128.2, ups=0.39, wpb=5526.5, bsz=232, num_updates=70, lr=8.4e-07, gnorm=5.063, train_wall=5, gb_free=11.6, wall=385
2024-07-20 09:55:05 | INFO | train_inner | epoch 004:     15 / 19 loss=9.998, nll_loss=7.487, ppl=179.33, wps=1788.6, ups=0.41, wpb=4397.5, bsz=152, num_updates=72, lr=8.64e-07, gnorm=4.55, train_wall=5, gb_free=13.3, wall=390
2024-07-20 09:55:11 | INFO | train_inner | epoch 004:     17 / 19 loss=10.276, nll_loss=7.848, ppl=230.42, wps=1789.9, ups=0.34, wpb=5220.5, bsz=236, num_updates=74, lr=8.88e-07, gnorm=4.95, train_wall=6, gb_free=11.4, wall=395
2024-07-20 09:55:15 | INFO | train_inner | epoch 004:     19 / 19 loss=10.086, nll_loss=7.602, ppl=194.3, wps=2037.3, ups=0.49, wpb=4134.5, bsz=232, num_updates=76, lr=9.12e-07, gnorm=5.47, train_wall=4, gb_free=17.7, wall=399
2024-07-20 09:55:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30564.91796875Mb; avail=224465.0703125Mb
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000584
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30565.41015625Mb; avail=224464.578125Mb
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002092
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30565.41015625Mb; avail=224464.578125Mb
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001851
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004934
2024-07-20 09:55:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30565.41015625Mb; avail=224464.578125Mb
2024-07-20 09:55:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.946 | nll_loss 7.319 | ppl 159.66 | wps 4186.7 | wpb 1665.6 | bsz 74.4 | num_updates 76 | best_loss 9.946
2024-07-20 09:55:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 76 updates
2024-07-20 09:55:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:55:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:56:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 4 @ 76 updates, score 9.946) (writing took 62.960533825913444 seconds)
2024-07-20 09:56:21 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-07-20 09:56:21 | INFO | train | epoch 004 | loss 10.389 | nll_loss 7.978 | ppl 252.05 | wps 816.2 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 76 | lr 9.12e-07 | gnorm 5.59 | train_wall 48 | gb_free 17.7 | wall 465
2024-07-20 09:56:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 09:56:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 09:56:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 09:56:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000801
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=48030.7890625Mb; avail=206999.22265625Mb
2024-07-20 09:56:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000732
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48030.7890625Mb; avail=206999.22265625Mb
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48030.7890625Mb; avail=206999.22265625Mb
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000233
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001294
2024-07-20 09:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48030.7890625Mb; avail=206999.22265625Mb
2024-07-20 09:56:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 09:56:21 | INFO | fairseq.trainer | begin training epoch 5
2024-07-20 09:56:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 09:56:25 | INFO | train_inner | epoch 005:      2 / 19 loss=10.134, nll_loss=7.675, ppl=204.3, wps=119.3, ups=0.03, wpb=4166.5, bsz=156, num_updates=78, lr=9.36e-07, gnorm=4.221, train_wall=4, gb_free=16.5, wall=469
2024-07-20 09:56:32 | INFO | train_inner | epoch 005:      4 / 19 loss=10.079, nll_loss=7.6, ppl=193.96, wps=1778.4, ups=0.3, wpb=5866, bsz=280, num_updates=80, lr=9.6e-07, gnorm=4.261, train_wall=7, gb_free=10.3, wall=476
2024-07-20 09:56:37 | INFO | train_inner | epoch 005:      6 / 19 loss=9.897, nll_loss=7.369, ppl=165.28, wps=1609.2, ups=0.37, wpb=4368.5, bsz=160, num_updates=82, lr=9.84e-07, gnorm=3.859, train_wall=5, gb_free=12.1, wall=481
2024-07-20 09:56:43 | INFO | train_inner | epoch 005:      8 / 19 loss=10.146, nll_loss=7.683, ppl=205.5, wps=2160.2, ups=0.35, wpb=6186, bsz=252, num_updates=84, lr=1.008e-06, gnorm=4.029, train_wall=6, gb_free=11.8, wall=487
2024-07-20 09:56:48 | INFO | train_inner | epoch 005:     10 / 19 loss=9.787, nll_loss=7.22, ppl=149.09, wps=1863.2, ups=0.37, wpb=5032, bsz=228, num_updates=86, lr=1.032e-06, gnorm=3.692, train_wall=5, gb_free=12.9, wall=492
2024-07-20 09:56:54 | INFO | train_inner | epoch 005:     12 / 19 loss=10.216, nll_loss=7.785, ppl=220.55, wps=1966.9, ups=0.38, wpb=5200, bsz=272, num_updates=88, lr=1.056e-06, gnorm=4.348, train_wall=5, gb_free=12.8, wall=498
2024-07-20 09:56:59 | INFO | train_inner | epoch 005:     14 / 19 loss=9.871, nll_loss=7.349, ppl=163.03, wps=1640.8, ups=0.35, wpb=4630.5, bsz=184, num_updates=90, lr=1.08e-06, gnorm=3.545, train_wall=6, gb_free=11, wall=503
2024-07-20 09:57:06 | INFO | train_inner | epoch 005:     16 / 19 loss=9.742, nll_loss=7.177, ppl=144.68, wps=1811.8, ups=0.32, wpb=5675.5, bsz=300, num_updates=92, lr=1.104e-06, gnorm=3.555, train_wall=6, gb_free=10.8, wall=510
2024-07-20 09:57:10 | INFO | train_inner | epoch 005:     18 / 19 loss=9.907, nll_loss=7.388, ppl=167.53, wps=1960.1, ups=0.47, wpb=4184, bsz=217.5, num_updates=94, lr=1.128e-06, gnorm=3.673, train_wall=4, gb_free=15.8, wall=514
2024-07-20 09:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36197.37109375Mb; avail=218832.6171875Mb
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36197.37109375Mb; avail=218832.6171875Mb
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002101
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36197.37109375Mb; avail=218832.6171875Mb
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001828
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004818
2024-07-20 09:57:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36197.37109375Mb; avail=218832.6171875Mb
2024-07-20 09:57:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.547 | nll_loss 6.836 | ppl 114.23 | wps 4192.3 | wpb 1665.6 | bsz 74.4 | num_updates 95 | best_loss 9.547
2024-07-20 09:57:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 95 updates
2024-07-20 09:57:13 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:58:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 09:58:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 5 @ 95 updates, score 9.547) (writing took 75.4184483771678 seconds)
2024-07-20 09:58:29 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-07-20 09:58:29 | INFO | train | epoch 005 | loss 9.97 | nll_loss 7.464 | ppl 176.57 | wps 721.8 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 95 | lr 1.14e-06 | gnorm 3.921 | train_wall 50 | gb_free 18.8 | wall 593
2024-07-20 09:58:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 09:58:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 09:58:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 09:58:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000677
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47903.82421875Mb; avail=207126.2109375Mb
2024-07-20 09:58:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000091
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000749
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47903.82421875Mb; avail=207126.2109375Mb
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000052
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47903.82421875Mb; avail=207126.2109375Mb
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000221
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001328
2024-07-20 09:58:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47903.82421875Mb; avail=207126.2109375Mb
2024-07-20 09:58:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 09:58:29 | INFO | fairseq.trainer | begin training epoch 6
2024-07-20 09:58:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 09:58:31 | INFO | train_inner | epoch 006:      1 / 19 loss=9.746, nll_loss=7.187, ppl=145.67, wps=67.5, ups=0.02, wpb=2728.5, bsz=113.5, num_updates=96, lr=1.152e-06, gnorm=3.88, train_wall=3, gb_free=16.1, wall=595
2024-07-20 09:58:36 | INFO | train_inner | epoch 006:      3 / 19 loss=9.675, nll_loss=7.101, ppl=137.3, wps=1689.5, ups=0.36, wpb=4723, bsz=204, num_updates=98, lr=1.176e-06, gnorm=3.304, train_wall=6, gb_free=12.3, wall=600
2024-07-20 09:58:42 | INFO | train_inner | epoch 006:      5 / 19 loss=9.898, nll_loss=7.381, ppl=166.71, wps=1926.7, ups=0.34, wpb=5640, bsz=316, num_updates=100, lr=1.2e-06, gnorm=3.531, train_wall=6, gb_free=13.7, wall=606
2024-07-20 09:58:48 | INFO | train_inner | epoch 006:      7 / 19 loss=9.7, nll_loss=7.131, ppl=140.16, wps=1834.7, ups=0.33, wpb=5543, bsz=244, num_updates=102, lr=1.224e-06, gnorm=2.974, train_wall=6, gb_free=11.6, wall=612
2024-07-20 09:58:53 | INFO | train_inner | epoch 006:      9 / 19 loss=9.649, nll_loss=7.071, ppl=134.47, wps=2056.1, ups=0.44, wpb=4673.5, bsz=204, num_updates=104, lr=1.248e-06, gnorm=2.919, train_wall=5, gb_free=15.6, wall=617
2024-07-20 09:58:58 | INFO | train_inner | epoch 006:     11 / 19 loss=9.592, nll_loss=6.998, ppl=127.85, wps=1623.3, ups=0.35, wpb=4670, bsz=192, num_updates=106, lr=1.272e-06, gnorm=2.891, train_wall=6, gb_free=10.8, wall=623
2024-07-20 09:59:03 | INFO | train_inner | epoch 006:     13 / 19 loss=9.631, nll_loss=7.046, ppl=132.19, wps=2043.5, ups=0.39, wpb=5175, bsz=256, num_updates=108, lr=1.296e-06, gnorm=2.998, train_wall=5, gb_free=11.3, wall=628
2024-07-20 09:59:09 | INFO | train_inner | epoch 006:     15 / 19 loss=9.436, nll_loss=6.799, ppl=111.37, wps=2181, ups=0.38, wpb=5732, bsz=220, num_updates=110, lr=1.32e-06, gnorm=2.68, train_wall=5, gb_free=10.8, wall=633
2024-07-20 09:59:14 | INFO | train_inner | epoch 006:     17 / 19 loss=9.488, nll_loss=6.868, ppl=116.77, wps=1468.1, ups=0.36, wpb=4080, bsz=184, num_updates=112, lr=1.344e-06, gnorm=2.822, train_wall=6, gb_free=13.2, wall=638
2024-07-20 09:59:19 | INFO | train_inner | epoch 006:     19 / 19 loss=9.614, nll_loss=7.031, ppl=130.79, wps=1867.8, ups=0.45, wpb=4188, bsz=180, num_updates=114, lr=1.368e-06, gnorm=3.079, train_wall=4, gb_free=17.4, wall=643
2024-07-20 09:59:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33231.46484375Mb; avail=221798.515625Mb
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000564
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33231.46484375Mb; avail=221798.515625Mb
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002096
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33231.46484375Mb; avail=221798.515625Mb
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001872
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004877
2024-07-20 09:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33231.46484375Mb; avail=221798.515625Mb
2024-07-20 09:59:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.24 | nll_loss 6.463 | ppl 88.23 | wps 4116.1 | wpb 1665.6 | bsz 74.4 | num_updates 114 | best_loss 9.24
2024-07-20 09:59:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 114 updates
2024-07-20 09:59:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:00:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 6 @ 114 updates, score 9.24) (writing took 65.53665200714022 seconds)
2024-07-20 10:00:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-07-20 10:00:27 | INFO | train | epoch 006 | loss 9.645 | nll_loss 7.065 | ppl 133.91 | wps 783.2 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 114 | lr 1.368e-06 | gnorm 3.054 | train_wall 50 | gb_free 17.4 | wall 711
2024-07-20 10:00:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:00:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:00:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:00:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000818
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58300.9375Mb; avail=196724.55859375Mb
2024-07-20 10:00:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000079
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000735
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58304.875Mb; avail=196724.06640625Mb
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58296.015625Mb; avail=196729.48046875Mb
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000232
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001298
2024-07-20 10:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58298.96875Mb; avail=196727.01953125Mb
2024-07-20 10:00:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:00:27 | INFO | fairseq.trainer | begin training epoch 7
2024-07-20 10:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:00:33 | INFO | train_inner | epoch 007:      2 / 19 loss=9.472, nll_loss=6.848, ppl=115.19, wps=163, ups=0.03, wpb=6050.5, bsz=284, num_updates=116, lr=1.392e-06, gnorm=2.606, train_wall=6, gb_free=11.8, wall=717
2024-07-20 10:00:38 | INFO | train_inner | epoch 007:      4 / 19 loss=9.321, nll_loss=6.661, ppl=101.19, wps=2070.9, ups=0.39, wpb=5284, bsz=228, num_updates=118, lr=1.416e-06, gnorm=2.497, train_wall=5, gb_free=11.3, wall=722
2024-07-20 10:00:44 | INFO | train_inner | epoch 007:      6 / 19 loss=9.586, nll_loss=7.006, ppl=128.5, wps=1740.7, ups=0.35, wpb=4941.5, bsz=220, num_updates=120, lr=1.44e-06, gnorm=2.71, train_wall=6, gb_free=13.4, wall=728
2024-07-20 10:00:49 | INFO | train_inner | epoch 007:      8 / 19 loss=9.414, nll_loss=6.783, ppl=110.13, wps=1561.9, ups=0.36, wpb=4307, bsz=164, num_updates=122, lr=1.464e-06, gnorm=2.597, train_wall=6, gb_free=13.6, wall=733
2024-07-20 10:00:55 | INFO | train_inner | epoch 007:     10 / 19 loss=9.533, nll_loss=6.936, ppl=122.46, wps=1788.4, ups=0.38, wpb=4726, bsz=212, num_updates=124, lr=1.488e-06, gnorm=2.561, train_wall=5, gb_free=11.1, wall=739
2024-07-20 10:01:00 | INFO | train_inner | epoch 007:     12 / 19 loss=9.329, nll_loss=6.679, ppl=102.5, wps=2042.2, ups=0.38, wpb=5356, bsz=264, num_updates=126, lr=1.512e-06, gnorm=2.531, train_wall=5, gb_free=12.7, wall=744
2024-07-20 10:01:05 | INFO | train_inner | epoch 007:     14 / 19 loss=9.358, nll_loss=6.709, ppl=104.61, wps=1740.3, ups=0.4, wpb=4322, bsz=236, num_updates=128, lr=1.536e-06, gnorm=2.564, train_wall=5, gb_free=12.8, wall=749
2024-07-20 10:01:10 | INFO | train_inner | epoch 007:     16 / 19 loss=9.353, nll_loss=6.715, ppl=105.08, wps=1637.1, ups=0.4, wpb=4095.5, bsz=173.5, num_updates=130, lr=1.56e-06, gnorm=2.585, train_wall=5, gb_free=12.6, wall=754
2024-07-20 10:01:16 | INFO | train_inner | epoch 007:     18 / 19 loss=9.057, nll_loss=6.32, ppl=79.92, wps=1815, ups=0.32, wpb=5590, bsz=244, num_updates=132, lr=1.584e-06, gnorm=2.338, train_wall=6, gb_free=12.7, wall=760
2024-07-20 10:01:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30753.9765625Mb; avail=224272.078125Mb
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000574
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30753.9765625Mb; avail=224272.078125Mb
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002073
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30753.9765625Mb; avail=224272.078125Mb
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001882
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004854
2024-07-20 10:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30754.46875Mb; avail=224271.5859375Mb
2024-07-20 10:01:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.975 | nll_loss 6.144 | ppl 70.72 | wps 4187.1 | wpb 1665.6 | bsz 74.4 | num_updates 133 | best_loss 8.975
2024-07-20 10:01:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 133 updates
2024-07-20 10:01:20 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:01:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:02:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 7 @ 133 updates, score 8.975) (writing took 62.657271382864565 seconds)
2024-07-20 10:02:22 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-07-20 10:02:22 | INFO | train | epoch 007 | loss 9.381 | nll_loss 6.741 | ppl 106.96 | wps 800.3 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 133 | lr 1.596e-06 | gnorm 2.575 | train_wall 50 | gb_free 16.7 | wall 827
2024-07-20 10:02:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:02:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:02:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:02:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000767
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=55836.63671875Mb; avail=199189.296875Mb
2024-07-20 10:02:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000090
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000761
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55836.63671875Mb; avail=199189.296875Mb
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55836.63671875Mb; avail=199189.296875Mb
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000235
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001338
2024-07-20 10:02:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55836.14453125Mb; avail=199189.296875Mb
2024-07-20 10:02:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:02:23 | INFO | fairseq.trainer | begin training epoch 8
2024-07-20 10:02:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:02:40 | INFO | train_inner | epoch 008:      1 / 19 loss=9.483, nll_loss=6.89, ppl=118.58, wps=99.1, ups=0.02, wpb=4182.5, bsz=184, num_updates=134, lr=1.608e-06, gnorm=2.739, train_wall=19, gb_free=12.2, wall=845
2024-07-20 10:02:46 | INFO | train_inner | epoch 008:      3 / 19 loss=9.092, nll_loss=6.387, ppl=83.68, wps=1714.8, ups=0.33, wpb=5152.5, bsz=220, num_updates=136, lr=1.632e-06, gnorm=2.351, train_wall=6, gb_free=12.7, wall=851
2024-07-20 10:02:51 | INFO | train_inner | epoch 008:      5 / 19 loss=9.168, nll_loss=6.486, ppl=89.63, wps=2121.2, ups=0.44, wpb=4876, bsz=212, num_updates=138, lr=1.656e-06, gnorm=2.211, train_wall=5, gb_free=12.3, wall=855
2024-07-20 10:02:56 | INFO | train_inner | epoch 008:      7 / 19 loss=9.101, nll_loss=6.386, ppl=83.64, wps=2052.6, ups=0.4, wpb=5116.5, bsz=224, num_updates=140, lr=1.68e-06, gnorm=2.262, train_wall=5, gb_free=12.8, wall=860
2024-07-20 10:03:02 | INFO | train_inner | epoch 008:      9 / 19 loss=9.194, nll_loss=6.514, ppl=91.37, wps=1792.9, ups=0.34, wpb=5221, bsz=248, num_updates=142, lr=1.704e-06, gnorm=2.337, train_wall=6, gb_free=13.7, wall=866
2024-07-20 10:03:07 | INFO | train_inner | epoch 008:     11 / 19 loss=9.02, nll_loss=6.305, ppl=79.05, wps=1916.8, ups=0.4, wpb=4805.5, bsz=216, num_updates=144, lr=1.728e-06, gnorm=2.304, train_wall=5, gb_free=12.5, wall=871
2024-07-20 10:03:13 | INFO | train_inner | epoch 008:     13 / 19 loss=8.987, nll_loss=6.264, ppl=76.85, wps=1957.3, ups=0.32, wpb=6197, bsz=312, num_updates=146, lr=1.752e-06, gnorm=2.15, train_wall=6, gb_free=11, wall=877
2024-07-20 10:03:19 | INFO | train_inner | epoch 008:     15 / 19 loss=9.033, nll_loss=6.322, ppl=80.02, wps=1703.4, ups=0.35, wpb=4895, bsz=216, num_updates=148, lr=1.776e-06, gnorm=2.142, train_wall=6, gb_free=11.5, wall=883
2024-07-20 10:03:24 | INFO | train_inner | epoch 008:     17 / 19 loss=9.282, nll_loss=6.64, ppl=99.74, wps=1738.8, ups=0.37, wpb=4733, bsz=180, num_updates=150, lr=1.8e-06, gnorm=2.636, train_wall=5, gb_free=11.8, wall=889
2024-07-20 10:03:27 | INFO | train_inner | epoch 008:     19 / 19 loss=9.388, nll_loss=6.76, ppl=108.41, wps=1674.4, ups=0.64, wpb=2610.5, bsz=125.5, num_updates=152, lr=1.824e-06, gnorm=3.068, train_wall=3, gb_free=17.5, wall=892
2024-07-20 10:03:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23426.47265625Mb; avail=231599.6640625Mb
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000477
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23426.47265625Mb; avail=231599.6640625Mb
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002049
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23426.47265625Mb; avail=231599.6640625Mb
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001817
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004678
2024-07-20 10:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23426.47265625Mb; avail=231599.6640625Mb
2024-07-20 10:03:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.746 | nll_loss 5.856 | ppl 57.91 | wps 3142.9 | wpb 1665.6 | bsz 74.4 | num_updates 152 | best_loss 8.746
2024-07-20 10:03:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 152 updates
2024-07-20 10:03:31 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:04:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:04:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 8 @ 152 updates, score 8.746) (writing took 63.19305077800527 seconds)
2024-07-20 10:04:34 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-07-20 10:04:34 | INFO | train | epoch 008 | loss 9.142 | nll_loss 6.454 | ppl 87.67 | wps 703.5 | ups 0.14 | wpb 4866.4 | bsz 219.1 | num_updates 152 | lr 1.824e-06 | gnorm 2.392 | train_wall 65 | gb_free 17.5 | wall 958
2024-07-20 10:04:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:04:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:04:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:04:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000691
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43166.25Mb; avail=211859.9375Mb
2024-07-20 10:04:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000717
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43166.24609375Mb; avail=211859.9375Mb
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43166.24609375Mb; avail=211859.9375Mb
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001243
2024-07-20 10:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43166.24609375Mb; avail=211859.9375Mb
2024-07-20 10:04:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:04:34 | INFO | fairseq.trainer | begin training epoch 9
2024-07-20 10:04:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:04:40 | INFO | train_inner | epoch 009:      2 / 19 loss=9.177, nll_loss=6.504, ppl=90.78, wps=152.2, ups=0.03, wpb=5511, bsz=240, num_updates=154, lr=1.848e-06, gnorm=2.185, train_wall=6, gb_free=13.3, wall=964
2024-07-20 10:04:45 | INFO | train_inner | epoch 009:      4 / 19 loss=8.986, nll_loss=6.274, ppl=77.4, wps=2037, ups=0.41, wpb=4968, bsz=220, num_updates=156, lr=1.872e-06, gnorm=2.129, train_wall=5, gb_free=11.5, wall=969
2024-07-20 10:04:50 | INFO | train_inner | epoch 009:      6 / 19 loss=8.993, nll_loss=6.28, ppl=77.7, wps=1672.6, ups=0.35, wpb=4760.5, bsz=232, num_updates=158, lr=1.896e-06, gnorm=2.216, train_wall=6, gb_free=14.5, wall=975
2024-07-20 10:04:56 | INFO | train_inner | epoch 009:      8 / 19 loss=8.97, nll_loss=6.258, ppl=76.53, wps=2246.3, ups=0.39, wpb=5730.5, bsz=244, num_updates=160, lr=1.92e-06, gnorm=2.043, train_wall=5, gb_free=12, wall=980
2024-07-20 10:05:02 | INFO | train_inner | epoch 009:     10 / 19 loss=8.931, nll_loss=6.208, ppl=73.94, wps=1943.4, ups=0.33, wpb=5814.5, bsz=220, num_updates=162, lr=1.944e-06, gnorm=2.097, train_wall=6, gb_free=12.6, wall=986
2024-07-20 10:05:07 | INFO | train_inner | epoch 009:     12 / 19 loss=8.903, nll_loss=6.171, ppl=72.08, wps=1709.4, ups=0.35, wpb=4913.5, bsz=240, num_updates=164, lr=1.968e-06, gnorm=2.112, train_wall=6, gb_free=11.3, wall=991
2024-07-20 10:05:13 | INFO | train_inner | epoch 009:     14 / 19 loss=8.73, nll_loss=5.939, ppl=61.34, wps=1932.9, ups=0.32, wpb=5985.5, bsz=288, num_updates=166, lr=1.992e-06, gnorm=2.178, train_wall=6, gb_free=10.7, wall=998
2024-07-20 10:05:17 | INFO | train_inner | epoch 009:     16 / 19 loss=9.036, nll_loss=6.331, ppl=80.51, wps=1900.9, ups=0.55, wpb=3435.5, bsz=149.5, num_updates=168, lr=2.016e-06, gnorm=3.392, train_wall=4, gb_free=12.2, wall=1001
2024-07-20 10:05:23 | INFO | train_inner | epoch 009:     18 / 19 loss=8.731, nll_loss=5.945, ppl=61.6, wps=1456.6, ups=0.36, wpb=4071, bsz=204, num_updates=170, lr=2.04e-06, gnorm=2.181, train_wall=6, gb_free=12.4, wall=1007
2024-07-20 10:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32897.703125Mb; avail=222128.40625Mb
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000561
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32897.703125Mb; avail=222128.40625Mb
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002082
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32897.703125Mb; avail=222128.40625Mb
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001844
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004805
2024-07-20 10:05:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32897.703125Mb; avail=222128.40625Mb
2024-07-20 10:05:27 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.549 | nll_loss 5.597 | ppl 48.41 | wps 3921.4 | wpb 1665.6 | bsz 74.4 | num_updates 171 | best_loss 8.549
2024-07-20 10:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 171 updates
2024-07-20 10:05:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:06:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:06:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 9 @ 171 updates, score 8.549) (writing took 66.04091698210686 seconds)
2024-07-20 10:06:33 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-07-20 10:06:33 | INFO | train | epoch 009 | loss 8.934 | nll_loss 6.205 | ppl 73.78 | wps 778.3 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 171 | lr 2.052e-06 | gnorm 2.297 | train_wall 50 | gb_free 17.3 | wall 1077
2024-07-20 10:06:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:06:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:06:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:06:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000664
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47644.2734375Mb; avail=207393.77734375Mb
2024-07-20 10:06:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000753
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47639.84375Mb; avail=207389.83984375Mb
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000038
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47640.828125Mb; avail=207388.85546875Mb
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000219
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001297
2024-07-20 10:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47643.78125Mb; avail=207385.90234375Mb
2024-07-20 10:06:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:06:33 | INFO | fairseq.trainer | begin training epoch 10
2024-07-20 10:06:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:06:36 | INFO | train_inner | epoch 010:      1 / 19 loss=8.789, nll_loss=6.03, ppl=65.34, wps=101.6, ups=0.03, wpb=3711.5, bsz=196, num_updates=172, lr=2.064e-06, gnorm=2.264, train_wall=4, gb_free=11.3, wall=1080
2024-07-20 10:06:42 | INFO | train_inner | epoch 010:      3 / 19 loss=8.997, nll_loss=6.291, ppl=78.33, wps=1735.3, ups=0.33, wpb=5276.5, bsz=216, num_updates=174, lr=2.088e-06, gnorm=2.259, train_wall=6, gb_free=11.4, wall=1086
2024-07-20 10:06:47 | INFO | train_inner | epoch 010:      5 / 19 loss=8.867, nll_loss=6.111, ppl=69.12, wps=1720.8, ups=0.37, wpb=4620.5, bsz=196, num_updates=176, lr=2.112e-06, gnorm=2.245, train_wall=5, gb_free=12.3, wall=1091
2024-07-20 10:06:52 | INFO | train_inner | epoch 010:      7 / 19 loss=8.683, nll_loss=5.894, ppl=59.48, wps=1981.5, ups=0.38, wpb=5197, bsz=224, num_updates=178, lr=2.136e-06, gnorm=2.076, train_wall=5, gb_free=12.5, wall=1097
2024-07-20 10:06:56 | INFO | train_inner | epoch 010:      9 / 19 loss=8.907, nll_loss=6.181, ppl=72.57, wps=2028.6, ups=0.54, wpb=3776.5, bsz=145.5, num_updates=180, lr=2.16e-06, gnorm=2.294, train_wall=4, gb_free=15.8, wall=1100
2024-07-20 10:07:02 | INFO | train_inner | epoch 010:     11 / 19 loss=8.732, nll_loss=5.96, ppl=62.23, wps=1804, ups=0.35, wpb=5184, bsz=232, num_updates=182, lr=2.184e-06, gnorm=2.039, train_wall=6, gb_free=11.4, wall=1106
2024-07-20 10:07:07 | INFO | train_inner | epoch 010:     13 / 19 loss=8.766, nll_loss=5.981, ppl=63.18, wps=2060.5, ups=0.41, wpb=4965.5, bsz=200, num_updates=184, lr=2.208e-06, gnorm=2.081, train_wall=5, gb_free=10.9, wall=1111
2024-07-20 10:07:12 | INFO | train_inner | epoch 010:     15 / 19 loss=8.722, nll_loss=5.953, ppl=61.94, wps=1799.1, ups=0.36, wpb=4971.5, bsz=256, num_updates=186, lr=2.232e-06, gnorm=1.972, train_wall=6, gb_free=12.8, wall=1116
2024-07-20 10:07:18 | INFO | train_inner | epoch 010:     17 / 19 loss=8.615, nll_loss=5.81, ppl=56.09, wps=2074.7, ups=0.36, wpb=5778, bsz=308, num_updates=188, lr=2.256e-06, gnorm=2.028, train_wall=6, gb_free=11.6, wall=1122
2024-07-20 10:07:22 | INFO | train_inner | epoch 010:     19 / 19 loss=8.528, nll_loss=5.702, ppl=52.07, wps=2013.9, ups=0.53, wpb=3791, bsz=152, num_updates=190, lr=2.28e-06, gnorm=2.269, train_wall=4, gb_free=17.2, wall=1126
2024-07-20 10:07:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38141.7109375Mb; avail=216888.37890625Mb
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38142.203125Mb; avail=216887.88671875Mb
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002086
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38142.203125Mb; avail=216887.88671875Mb
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001844
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004819
2024-07-20 10:07:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38142.203125Mb; avail=216887.88671875Mb
2024-07-20 10:07:24 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.392 | nll_loss 5.383 | ppl 41.73 | wps 3811.7 | wpb 1665.6 | bsz 74.4 | num_updates 190 | best_loss 8.392
2024-07-20 10:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 190 updates
2024-07-20 10:07:24 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:08:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:08:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 10 @ 190 updates, score 8.392) (writing took 61.17212720098905 seconds)
2024-07-20 10:08:25 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-07-20 10:08:25 | INFO | train | epoch 010 | loss 8.761 | nll_loss 5.991 | ppl 63.62 | wps 819.7 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 190 | lr 2.28e-06 | gnorm 2.13 | train_wall 49 | gb_free 17.2 | wall 1190
2024-07-20 10:08:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:08:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:08:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:08:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000904
2024-07-20 10:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45933.265625Mb; avail=208972.83203125Mb
2024-07-20 10:08:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000131
2024-07-20 10:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000978
2024-07-20 10:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45933.265625Mb; avail=208972.83203125Mb
2024-07-20 10:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-07-20 10:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45933.265625Mb; avail=208972.83203125Mb
2024-07-20 10:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000327
2024-07-20 10:08:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001745
2024-07-20 10:08:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45933.265625Mb; avail=208972.83203125Mb
2024-07-20 10:08:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:08:26 | INFO | fairseq.trainer | begin training epoch 11
2024-07-20 10:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:08:31 | INFO | train_inner | epoch 011:      2 / 19 loss=8.495, nll_loss=5.651, ppl=50.26, wps=142.2, ups=0.03, wpb=4925.5, bsz=192, num_updates=192, lr=2.304e-06, gnorm=2.023, train_wall=5, gb_free=13, wall=1195
2024-07-20 10:08:36 | INFO | train_inner | epoch 011:      4 / 19 loss=8.711, nll_loss=5.932, ppl=61.07, wps=1397.3, ups=0.41, wpb=3394.5, bsz=140, num_updates=194, lr=2.328e-06, gnorm=2.484, train_wall=5, gb_free=13.6, wall=1200
2024-07-20 10:08:41 | INFO | train_inner | epoch 011:      6 / 19 loss=8.477, nll_loss=5.642, ppl=49.92, wps=2136.3, ups=0.38, wpb=5651.5, bsz=256, num_updates=196, lr=2.352e-06, gnorm=1.825, train_wall=5, gb_free=12.8, wall=1205
2024-07-20 10:08:47 | INFO | train_inner | epoch 011:      8 / 19 loss=8.546, nll_loss=5.727, ppl=52.98, wps=2054.7, ups=0.32, wpb=6390.5, bsz=292, num_updates=198, lr=2.376e-06, gnorm=1.877, train_wall=6, gb_free=11.6, wall=1211
2024-07-20 10:08:52 | INFO | train_inner | epoch 011:     10 / 19 loss=8.647, nll_loss=5.853, ppl=57.79, wps=2091.6, ups=0.4, wpb=5171.5, bsz=236, num_updates=200, lr=2.4e-06, gnorm=2.189, train_wall=5, gb_free=11.2, wall=1216
2024-07-20 10:08:58 | INFO | train_inner | epoch 011:     12 / 19 loss=8.529, nll_loss=5.708, ppl=52.29, wps=2182.4, ups=0.36, wpb=5997, bsz=308, num_updates=202, lr=2.424e-06, gnorm=1.878, train_wall=5, gb_free=11.4, wall=1222
2024-07-20 10:09:03 | INFO | train_inner | epoch 011:     14 / 19 loss=8.492, nll_loss=5.651, ppl=50.25, wps=1820, ups=0.35, wpb=5141.5, bsz=216, num_updates=204, lr=2.448e-06, gnorm=2.055, train_wall=6, gb_free=10.7, wall=1228
2024-07-20 10:09:08 | INFO | train_inner | epoch 011:     16 / 19 loss=8.749, nll_loss=5.983, ppl=63.26, wps=1783.3, ups=0.41, wpb=4393.5, bsz=196, num_updates=206, lr=2.472e-06, gnorm=2.443, train_wall=5, gb_free=12.9, wall=1232
2024-07-20 10:09:13 | INFO | train_inner | epoch 011:     18 / 19 loss=8.652, nll_loss=5.863, ppl=58.22, wps=1419.6, ups=0.4, wpb=3528, bsz=173.5, num_updates=208, lr=2.496e-06, gnorm=2.282, train_wall=5, gb_free=12.8, wall=1237
2024-07-20 10:09:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36426.29296875Mb; avail=218479.75Mb
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36426.29296875Mb; avail=218479.75Mb
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002179
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36426.29296875Mb; avail=218479.75Mb
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.002830
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005953
2024-07-20 10:09:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36426.78515625Mb; avail=218479.2578125Mb
2024-07-20 10:09:18 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.24 | nll_loss 5.179 | ppl 36.22 | wps 3258.7 | wpb 1665.6 | bsz 74.4 | num_updates 209 | best_loss 8.24
2024-07-20 10:09:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 209 updates
2024-07-20 10:09:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:10:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 11 @ 209 updates, score 8.24) (writing took 75.95836897403933 seconds)
2024-07-20 10:10:34 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-07-20 10:10:34 | INFO | train | epoch 011 | loss 8.568 | nll_loss 5.753 | ppl 53.93 | wps 718.7 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 209 | lr 2.508e-06 | gnorm 2.126 | train_wall 49 | gb_free 15.3 | wall 1318
2024-07-20 10:10:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:10:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:10:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:10:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000654
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39573.09375Mb; avail=215462.37109375Mb
2024-07-20 10:10:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000094
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000731
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39570.6328125Mb; avail=215458.92578125Mb
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000051
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39571.6171875Mb; avail=215457.94140625Mb
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000211
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001295
2024-07-20 10:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39574.078125Mb; avail=215455.97265625Mb
2024-07-20 10:10:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:10:34 | INFO | fairseq.trainer | begin training epoch 12
2024-07-20 10:10:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:10:37 | INFO | train_inner | epoch 012:      1 / 19 loss=8.401, nll_loss=5.556, ppl=47.06, wps=87.8, ups=0.02, wpb=3660, bsz=192, num_updates=210, lr=2.52e-06, gnorm=2.142, train_wall=4, gb_free=13.1, wall=1321
2024-07-20 10:10:42 | INFO | train_inner | epoch 012:      3 / 19 loss=8.42, nll_loss=5.571, ppl=47.55, wps=2218.6, ups=0.39, wpb=5725.5, bsz=248, num_updates=212, lr=2.544e-06, gnorm=1.791, train_wall=5, gb_free=11.1, wall=1326
2024-07-20 10:10:48 | INFO | train_inner | epoch 012:      5 / 19 loss=8.33, nll_loss=5.458, ppl=43.95, wps=2058.6, ups=0.33, wpb=6311.5, bsz=320, num_updates=214, lr=2.568e-06, gnorm=2.064, train_wall=6, gb_free=12.6, wall=1332
2024-07-20 10:10:53 | INFO | train_inner | epoch 012:      7 / 19 loss=8.538, nll_loss=5.722, ppl=52.77, wps=1683.6, ups=0.36, wpb=4655.5, bsz=192, num_updates=216, lr=2.592e-06, gnorm=2.24, train_wall=6, gb_free=12.6, wall=1338
2024-07-20 10:10:59 | INFO | train_inner | epoch 012:      9 / 19 loss=8.436, nll_loss=5.579, ppl=47.8, wps=1760.4, ups=0.34, wpb=5125, bsz=228, num_updates=218, lr=2.616e-06, gnorm=1.917, train_wall=6, gb_free=11.2, wall=1343
2024-07-20 10:11:04 | INFO | train_inner | epoch 012:     11 / 19 loss=8.667, nll_loss=5.885, ppl=59.08, wps=1824.5, ups=0.41, wpb=4468.5, bsz=188, num_updates=220, lr=2.64e-06, gnorm=2.332, train_wall=5, gb_free=12.3, wall=1348
2024-07-20 10:11:09 | INFO | train_inner | epoch 012:     13 / 19 loss=8.485, nll_loss=5.658, ppl=50.5, wps=1743.1, ups=0.43, wpb=4061, bsz=189.5, num_updates=222, lr=2.664e-06, gnorm=2.271, train_wall=5, gb_free=12.7, wall=1353
2024-07-20 10:11:13 | INFO | train_inner | epoch 012:     15 / 19 loss=8.455, nll_loss=5.594, ppl=48.31, wps=2003.7, ups=0.43, wpb=4703.5, bsz=172, num_updates=224, lr=2.688e-06, gnorm=2.44, train_wall=5, gb_free=13.7, wall=1358
2024-07-20 10:11:19 | INFO | train_inner | epoch 012:     17 / 19 loss=8.446, nll_loss=5.6, ppl=48.51, wps=1790.5, ups=0.37, wpb=4859.5, bsz=216, num_updates=226, lr=2.712e-06, gnorm=2.021, train_wall=5, gb_free=17, wall=1363
2024-07-20 10:11:24 | INFO | train_inner | epoch 012:     19 / 19 loss=8.294, nll_loss=5.409, ppl=42.48, wps=1835.5, ups=0.43, wpb=4298.5, bsz=208, num_updates=228, lr=2.736e-06, gnorm=2.05, train_wall=5, gb_free=16.6, wall=1368
2024-07-20 10:11:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39680.8203125Mb; avail=215349.1796875Mb
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000542
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39681.3125Mb; avail=215348.6875Mb
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002057
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39681.3125Mb; avail=215348.6875Mb
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001842
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004757
2024-07-20 10:11:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39681.3125Mb; avail=215348.6875Mb
2024-07-20 10:11:26 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.109 | nll_loss 4.989 | ppl 31.76 | wps 4191 | wpb 1665.6 | bsz 74.4 | num_updates 228 | best_loss 8.109
2024-07-20 10:11:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 228 updates
2024-07-20 10:11:26 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:12:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 12 @ 228 updates, score 8.109) (writing took 61.0529274630826 seconds)
2024-07-20 10:12:27 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-07-20 10:12:27 | INFO | train | epoch 012 | loss 8.446 | nll_loss 5.602 | ppl 48.56 | wps 817.9 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 228 | lr 2.736e-06 | gnorm 2.119 | train_wall 49 | gb_free 16.6 | wall 1431
2024-07-20 10:12:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:12:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:12:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:12:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000766
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47923.71875Mb; avail=207106.8359375Mb
2024-07-20 10:12:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000099
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000762
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47923.71875Mb; avail=207106.34375Mb
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47923.2265625Mb; avail=207106.8359375Mb
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000224
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001316
2024-07-20 10:12:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47923.2265625Mb; avail=207106.8359375Mb
2024-07-20 10:12:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:12:27 | INFO | fairseq.trainer | begin training epoch 13
2024-07-20 10:12:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:12:32 | INFO | train_inner | epoch 013:      2 / 19 loss=8.354, nll_loss=5.477, ppl=44.53, wps=139.6, ups=0.03, wpb=4777, bsz=232, num_updates=230, lr=2.76e-06, gnorm=2.121, train_wall=5, gb_free=14.7, wall=1436
2024-07-20 10:12:38 | INFO | train_inner | epoch 013:      4 / 19 loss=8.327, nll_loss=5.444, ppl=43.55, wps=1677, ups=0.35, wpb=4835.5, bsz=236, num_updates=232, lr=2.784e-06, gnorm=2.1, train_wall=6, gb_free=13, wall=1442
2024-07-20 10:12:44 | INFO | train_inner | epoch 013:      6 / 19 loss=8.232, nll_loss=5.329, ppl=40.2, wps=1931, ups=0.33, wpb=5820.5, bsz=316, num_updates=234, lr=2.808e-06, gnorm=1.862, train_wall=6, gb_free=12.7, wall=1448
2024-07-20 10:12:50 | INFO | train_inner | epoch 013:      8 / 19 loss=8.292, nll_loss=5.399, ppl=42.19, wps=1884.9, ups=0.33, wpb=5756, bsz=252, num_updates=236, lr=2.832e-06, gnorm=1.847, train_wall=6, gb_free=10.9, wall=1454
2024-07-20 10:12:55 | INFO | train_inner | epoch 013:     10 / 19 loss=8.644, nll_loss=5.852, ppl=57.77, wps=1396.9, ups=0.37, wpb=3825.5, bsz=164, num_updates=238, lr=2.856e-06, gnorm=2.591, train_wall=5, gb_free=12.8, wall=1460
2024-07-20 10:13:01 | INFO | train_inner | epoch 013:     12 / 19 loss=8.184, nll_loss=5.268, ppl=38.52, wps=1907.2, ups=0.34, wpb=5681.5, bsz=240, num_updates=240, lr=2.88e-06, gnorm=2.008, train_wall=6, gb_free=12.1, wall=1466
2024-07-20 10:13:06 | INFO | train_inner | epoch 013:     14 / 19 loss=8.242, nll_loss=5.348, ppl=40.73, wps=2103.6, ups=0.4, wpb=5276.5, bsz=224, num_updates=242, lr=2.904e-06, gnorm=2.035, train_wall=5, gb_free=13, wall=1471
2024-07-20 10:13:10 | INFO | train_inner | epoch 013:     16 / 19 loss=8.645, nll_loss=5.859, ppl=58.03, wps=1661, ups=0.61, wpb=2704, bsz=85.5, num_updates=244, lr=2.928e-06, gnorm=2.873, train_wall=3, gb_free=14.2, wall=1474
2024-07-20 10:13:16 | INFO | train_inner | epoch 013:     18 / 19 loss=8.129, nll_loss=5.194, ppl=36.61, wps=1951.3, ups=0.32, wpb=6005, bsz=264, num_updates=246, lr=2.952e-06, gnorm=1.821, train_wall=6, gb_free=13.5, wall=1480
2024-07-20 10:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40143.6484375Mb; avail=214886.35546875Mb
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000564
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40143.6484375Mb; avail=214886.35546875Mb
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002103
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40143.6484375Mb; avail=214886.35546875Mb
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001844
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004838
2024-07-20 10:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40143.6484375Mb; avail=214886.35546875Mb
2024-07-20 10:13:20 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.973 | nll_loss 4.806 | ppl 27.98 | wps 4188.3 | wpb 1665.6 | bsz 74.4 | num_updates 247 | best_loss 7.973
2024-07-20 10:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 247 updates
2024-07-20 10:13:20 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 13 @ 247 updates, score 7.973) (writing took 62.839388044783846 seconds)
2024-07-20 10:14:23 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-07-20 10:14:23 | INFO | train | epoch 013 | loss 8.298 | nll_loss 5.411 | ppl 42.56 | wps 800.9 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 247 | lr 2.964e-06 | gnorm 2.163 | train_wall 50 | gb_free 16.3 | wall 1547
2024-07-20 10:14:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:14:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:14:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:14:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000754
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=66062.48828125Mb; avail=188967.45703125Mb
2024-07-20 10:14:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000076
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000726
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66062.48828125Mb; avail=188967.45703125Mb
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66062.48828125Mb; avail=188967.45703125Mb
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000224
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001269
2024-07-20 10:14:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=66062.48828125Mb; avail=188967.45703125Mb
2024-07-20 10:14:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:14:23 | INFO | fairseq.trainer | begin training epoch 14
2024-07-20 10:14:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:14:41 | INFO | train_inner | epoch 014:      1 / 19 loss=8.111, nll_loss=5.172, ppl=36.06, wps=82.8, ups=0.02, wpb=3511.5, bsz=132, num_updates=248, lr=2.976e-06, gnorm=2.322, train_wall=19, gb_free=12.1, wall=1565
2024-07-20 10:14:45 | INFO | train_inner | epoch 014:      3 / 19 loss=8.222, nll_loss=5.305, ppl=39.53, wps=2233.4, ups=0.48, wpb=4653.5, bsz=201.5, num_updates=250, lr=3e-06, gnorm=2.367, train_wall=4, gb_free=15.8, wall=1569
2024-07-20 10:14:50 | INFO | train_inner | epoch 014:      5 / 19 loss=8.184, nll_loss=5.277, ppl=38.76, wps=2180, ups=0.36, wpb=6005, bsz=248, num_updates=252, lr=3.024e-06, gnorm=1.701, train_wall=5, gb_free=13.4, wall=1574
2024-07-20 10:14:55 | INFO | train_inner | epoch 014:      7 / 19 loss=8.207, nll_loss=5.302, ppl=39.46, wps=1870.3, ups=0.39, wpb=4804.5, bsz=224, num_updates=254, lr=3.048e-06, gnorm=2.012, train_wall=5, gb_free=11.8, wall=1580
2024-07-20 10:15:01 | INFO | train_inner | epoch 014:      9 / 19 loss=8.052, nll_loss=5.088, ppl=34.02, wps=1851.3, ups=0.34, wpb=5510, bsz=264, num_updates=256, lr=3.072e-06, gnorm=1.836, train_wall=6, gb_free=12.1, wall=1586
2024-07-20 10:15:07 | INFO | train_inner | epoch 014:     11 / 19 loss=8.106, nll_loss=5.171, ppl=36.04, wps=1932.5, ups=0.33, wpb=5878, bsz=308, num_updates=258, lr=3.096e-06, gnorm=2.091, train_wall=6, gb_free=11.6, wall=1592
2024-07-20 10:15:13 | INFO | train_inner | epoch 014:     13 / 19 loss=8.156, nll_loss=5.227, ppl=37.45, wps=1523.4, ups=0.37, wpb=4151, bsz=180, num_updates=260, lr=3.12e-06, gnorm=2.09, train_wall=5, gb_free=14.7, wall=1597
2024-07-20 10:15:18 | INFO | train_inner | epoch 014:     15 / 19 loss=8.045, nll_loss=5.089, ppl=34.05, wps=2156.5, ups=0.38, wpb=5601.5, bsz=248, num_updates=262, lr=3.144e-06, gnorm=1.916, train_wall=5, gb_free=11.8, wall=1602
2024-07-20 10:15:24 | INFO | train_inner | epoch 014:     17 / 19 loss=8.209, nll_loss=5.294, ppl=39.24, wps=1537.3, ups=0.34, wpb=4538, bsz=208, num_updates=264, lr=3.168e-06, gnorm=2.268, train_wall=6, gb_free=11.7, wall=1608
2024-07-20 10:15:28 | INFO | train_inner | epoch 014:     19 / 19 loss=8.542, nll_loss=5.737, ppl=53.35, wps=1613, ups=0.52, wpb=3127.5, bsz=136, num_updates=266, lr=3.192e-06, gnorm=2.764, train_wall=4, gb_free=17.5, wall=1612
2024-07-20 10:15:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35232.98828125Mb; avail=219796.921875Mb
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000488
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35232.98828125Mb; avail=219796.921875Mb
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002052
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35232.98828125Mb; avail=219796.921875Mb
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001824
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004677
2024-07-20 10:15:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35232.98828125Mb; avail=219796.921875Mb
2024-07-20 10:15:30 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.847 | nll_loss 4.641 | ppl 24.95 | wps 4196.3 | wpb 1665.6 | bsz 74.4 | num_updates 266 | best_loss 7.847
2024-07-20 10:15:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 266 updates
2024-07-20 10:15:30 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:16:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:16:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 14 @ 266 updates, score 7.847) (writing took 67.35728259687312 seconds)
2024-07-20 10:16:38 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-07-20 10:16:38 | INFO | train | epoch 014 | loss 8.169 | nll_loss 5.247 | ppl 37.99 | wps 684.1 | ups 0.14 | wpb 4866.4 | bsz 219.1 | num_updates 266 | lr 3.192e-06 | gnorm 2.113 | train_wall 65 | gb_free 17.5 | wall 1682
2024-07-20 10:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000752
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40939.14453125Mb; avail=214090.890625Mb
2024-07-20 10:16:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000099
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000805
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40939.14453125Mb; avail=214090.890625Mb
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40939.14453125Mb; avail=214091.3828125Mb
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000229
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001362
2024-07-20 10:16:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40939.14453125Mb; avail=214090.890625Mb
2024-07-20 10:16:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:16:38 | INFO | fairseq.trainer | begin training epoch 15
2024-07-20 10:16:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:16:42 | INFO | train_inner | epoch 015:      2 / 19 loss=8.161, nll_loss=5.24, ppl=37.79, wps=102.1, ups=0.03, wpb=3780, bsz=149.5, num_updates=268, lr=3.216e-06, gnorm=2.169, train_wall=4, gb_free=11.3, wall=1686
2024-07-20 10:16:47 | INFO | train_inner | epoch 015:      4 / 19 loss=8.098, nll_loss=5.169, ppl=35.98, wps=1654.5, ups=0.37, wpb=4515, bsz=184, num_updates=270, lr=3.24e-06, gnorm=2.062, train_wall=5, gb_free=11.7, wall=1692
2024-07-20 10:16:53 | INFO | train_inner | epoch 015:      6 / 19 loss=8.019, nll_loss=5.062, ppl=33.41, wps=1713.6, ups=0.34, wpb=5087.5, bsz=228, num_updates=272, lr=3.264e-06, gnorm=1.802, train_wall=6, gb_free=12.1, wall=1698
2024-07-20 10:16:59 | INFO | train_inner | epoch 015:      8 / 19 loss=8.138, nll_loss=5.211, ppl=37.04, wps=1657, ups=0.35, wpb=4717, bsz=204, num_updates=274, lr=3.288e-06, gnorm=2.322, train_wall=6, gb_free=13.9, wall=1703
2024-07-20 10:17:04 | INFO | train_inner | epoch 015:     10 / 19 loss=8.22, nll_loss=5.319, ppl=39.93, wps=1704.4, ups=0.39, wpb=4328.5, bsz=196, num_updates=276, lr=3.312e-06, gnorm=2.144, train_wall=5, gb_free=18.2, wall=1708
2024-07-20 10:17:10 | INFO | train_inner | epoch 015:     12 / 19 loss=7.889, nll_loss=4.891, ppl=29.68, wps=1879, ups=0.35, wpb=5293, bsz=264, num_updates=278, lr=3.336e-06, gnorm=1.992, train_wall=6, gb_free=11.6, wall=1714
2024-07-20 10:17:16 | INFO | train_inner | epoch 015:     14 / 19 loss=7.997, nll_loss=5.04, ppl=32.9, wps=2075.3, ups=0.35, wpb=5936.5, bsz=276, num_updates=280, lr=3.36e-06, gnorm=1.765, train_wall=6, gb_free=12.9, wall=1720
2024-07-20 10:17:21 | INFO | train_inner | epoch 015:     16 / 19 loss=7.955, nll_loss=4.979, ppl=31.53, wps=2161.5, ups=0.34, wpb=6289, bsz=272, num_updates=282, lr=3.384e-06, gnorm=1.784, train_wall=6, gb_free=11.9, wall=1726
2024-07-20 10:17:27 | INFO | train_inner | epoch 015:     18 / 19 loss=8.019, nll_loss=5.058, ppl=33.31, wps=1710.3, ups=0.36, wpb=4790, bsz=216, num_updates=284, lr=3.408e-06, gnorm=2.452, train_wall=6, gb_free=12.7, wall=1731
2024-07-20 10:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21697.3671875Mb; avail=233332.70703125Mb
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21697.859375Mb; avail=233332.21484375Mb
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002084
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21697.859375Mb; avail=233332.21484375Mb
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001838
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004829
2024-07-20 10:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21697.859375Mb; avail=233332.21484375Mb
2024-07-20 10:17:32 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.757 | nll_loss 4.497 | ppl 22.59 | wps 3067.9 | wpb 1665.6 | bsz 74.4 | num_updates 285 | best_loss 7.757
2024-07-20 10:17:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 285 updates
2024-07-20 10:17:32 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:18:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:18:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 15 @ 285 updates, score 7.757) (writing took 69.46751863299869 seconds)
2024-07-20 10:18:41 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-07-20 10:18:41 | INFO | train | epoch 015 | loss 8.037 | nll_loss 5.085 | ppl 33.94 | wps 749.1 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 285 | lr 3.42e-06 | gnorm 2.074 | train_wall 50 | gb_free 16.1 | wall 1805
2024-07-20 10:18:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:18:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:18:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:18:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000755
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20921.08984375Mb; avail=234108.4921875Mb
2024-07-20 10:18:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000097
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000815
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20921.0859375Mb; avail=234108.984375Mb
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000042
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20921.578125Mb; avail=234108.4921875Mb
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000222
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001389
2024-07-20 10:18:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20921.578125Mb; avail=234108.4921875Mb
2024-07-20 10:18:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:18:41 | INFO | fairseq.trainer | begin training epoch 16
2024-07-20 10:18:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:18:44 | INFO | train_inner | epoch 016:      1 / 19 loss=8.155, nll_loss=5.221, ppl=37.3, wps=105.6, ups=0.03, wpb=4088, bsz=184, num_updates=286, lr=3.432e-06, gnorm=2.328, train_wall=4, gb_free=11.4, wall=1809
2024-07-20 10:18:49 | INFO | train_inner | epoch 016:      3 / 19 loss=7.872, nll_loss=4.863, ppl=29.1, wps=1962.5, ups=0.41, wpb=4776, bsz=232, num_updates=288, lr=3.456e-06, gnorm=2.169, train_wall=5, gb_free=12.3, wall=1813
2024-07-20 10:18:54 | INFO | train_inner | epoch 016:      5 / 19 loss=7.988, nll_loss=5.02, ppl=32.45, wps=1663.3, ups=0.4, wpb=4163, bsz=185.5, num_updates=290, lr=3.48e-06, gnorm=2.718, train_wall=5, gb_free=12.6, wall=1818
2024-07-20 10:18:59 | INFO | train_inner | epoch 016:      7 / 19 loss=7.955, nll_loss=4.986, ppl=31.7, wps=1981.9, ups=0.42, wpb=4705, bsz=232, num_updates=292, lr=3.504e-06, gnorm=2.232, train_wall=5, gb_free=15.3, wall=1823
2024-07-20 10:19:05 | INFO | train_inner | epoch 016:      9 / 19 loss=7.831, nll_loss=4.827, ppl=28.39, wps=1710.5, ups=0.35, wpb=4912, bsz=232, num_updates=294, lr=3.528e-06, gnorm=1.94, train_wall=6, gb_free=13.2, wall=1829
2024-07-20 10:19:11 | INFO | train_inner | epoch 016:     11 / 19 loss=7.919, nll_loss=4.926, ppl=30.41, wps=1733.2, ups=0.34, wpb=5131, bsz=228, num_updates=296, lr=3.552e-06, gnorm=1.872, train_wall=6, gb_free=11.8, wall=1835
2024-07-20 10:19:16 | INFO | train_inner | epoch 016:     13 / 19 loss=7.884, nll_loss=4.893, ppl=29.72, wps=2009, ups=0.41, wpb=4921, bsz=224, num_updates=298, lr=3.576e-06, gnorm=1.962, train_wall=5, gb_free=12.7, wall=1840
2024-07-20 10:19:22 | INFO | train_inner | epoch 016:     15 / 19 loss=7.79, nll_loss=4.771, ppl=27.3, wps=2099.6, ups=0.33, wpb=6301.5, bsz=296, num_updates=300, lr=3.6e-06, gnorm=1.956, train_wall=6, gb_free=11.1, wall=1846
2024-07-20 10:19:27 | INFO | train_inner | epoch 016:     17 / 19 loss=7.778, nll_loss=4.743, ppl=26.78, wps=1956.5, ups=0.37, wpb=5289, bsz=196, num_updates=302, lr=3.624e-06, gnorm=1.739, train_wall=5, gb_free=12.1, wall=1851
2024-07-20 10:19:30 | INFO | train_inner | epoch 016:     19 / 19 loss=8.034, nll_loss=5.074, ppl=33.69, wps=2096.3, ups=0.61, wpb=3439, bsz=164, num_updates=304, lr=3.648e-06, gnorm=3.657, train_wall=3, gb_free=24.8, wall=1854
2024-07-20 10:19:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20999.7265625Mb; avail=234030.3203125Mb
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000565
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20999.7265625Mb; avail=234030.3203125Mb
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002097
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20999.7265625Mb; avail=234030.3203125Mb
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001857
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004876
2024-07-20 10:19:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20999.7265625Mb; avail=234030.3203125Mb
2024-07-20 10:19:34 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.632 | nll_loss 4.332 | ppl 20.14 | wps 3122.7 | wpb 1665.6 | bsz 74.4 | num_updates 304 | best_loss 7.632
2024-07-20 10:19:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 304 updates
2024-07-20 10:19:34 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:20:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:20:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 16 @ 304 updates, score 7.632) (writing took 62.9020822851453 seconds)
2024-07-20 10:20:36 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-07-20 10:20:36 | INFO | train | epoch 016 | loss 7.909 | nll_loss 4.918 | ppl 30.23 | wps 802.7 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 304 | lr 3.648e-06 | gnorm 2.249 | train_wall 49 | gb_free 24.8 | wall 1921
2024-07-20 10:20:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:20:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:20:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:20:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000817
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29285.9921875Mb; avail=225744.171875Mb
2024-07-20 10:20:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000737
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29285.9921875Mb; avail=225744.171875Mb
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29285.9921875Mb; avail=225744.6640625Mb
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000228
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001310
2024-07-20 10:20:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29285.9921875Mb; avail=225744.171875Mb
2024-07-20 10:20:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:20:36 | INFO | fairseq.trainer | begin training epoch 17
2024-07-20 10:20:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:20:42 | INFO | train_inner | epoch 017:      2 / 19 loss=7.735, nll_loss=4.691, ppl=25.84, wps=155.5, ups=0.03, wpb=5579.5, bsz=288, num_updates=306, lr=3.672e-06, gnorm=1.959, train_wall=5, gb_free=12.4, wall=1926
2024-07-20 10:20:48 | INFO | train_inner | epoch 017:      4 / 19 loss=7.691, nll_loss=4.64, ppl=24.93, wps=1862.7, ups=0.36, wpb=5157, bsz=272, num_updates=308, lr=3.696e-06, gnorm=2, train_wall=6, gb_free=14.7, wall=1932
2024-07-20 10:20:53 | INFO | train_inner | epoch 017:      6 / 19 loss=7.888, nll_loss=4.896, ppl=29.78, wps=1879.6, ups=0.34, wpb=5551, bsz=228, num_updates=310, lr=3.72e-06, gnorm=2.352, train_wall=6, gb_free=11.6, wall=1938
2024-07-20 10:20:58 | INFO | train_inner | epoch 017:      8 / 19 loss=7.727, nll_loss=4.685, ppl=25.72, wps=2015, ups=0.47, wpb=4286, bsz=181.5, num_updates=312, lr=3.744e-06, gnorm=1.909, train_wall=4, gb_free=12.4, wall=1942
2024-07-20 10:21:03 | INFO | train_inner | epoch 017:     10 / 19 loss=7.772, nll_loss=4.752, ppl=26.94, wps=2238.2, ups=0.39, wpb=5739.5, bsz=248, num_updates=314, lr=3.768e-06, gnorm=1.815, train_wall=5, gb_free=11.9, wall=1947
2024-07-20 10:21:08 | INFO | train_inner | epoch 017:     12 / 19 loss=7.982, nll_loss=5.023, ppl=32.51, wps=1516.6, ups=0.37, wpb=4150, bsz=152, num_updates=316, lr=3.792e-06, gnorm=2.321, train_wall=5, gb_free=12.3, wall=1953
2024-07-20 10:21:13 | INFO | train_inner | epoch 017:     14 / 19 loss=7.838, nll_loss=4.828, ppl=28.4, wps=2106.1, ups=0.4, wpb=5330, bsz=248, num_updates=318, lr=3.816e-06, gnorm=2.091, train_wall=5, gb_free=12.8, wall=1958
2024-07-20 10:21:18 | INFO | train_inner | epoch 017:     16 / 19 loss=7.879, nll_loss=4.882, ppl=29.49, wps=1950.7, ups=0.44, wpb=4397, bsz=188, num_updates=320, lr=3.84e-06, gnorm=2.25, train_wall=4, gb_free=11.6, wall=1962
2024-07-20 10:21:24 | INFO | train_inner | epoch 017:     18 / 19 loss=7.829, nll_loss=4.792, ppl=27.71, wps=1588.9, ups=0.35, wpb=4499.5, bsz=212, num_updates=322, lr=3.864e-06, gnorm=2.13, train_wall=6, gb_free=12.1, wall=1968
2024-07-20 10:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29380.76953125Mb; avail=225649.30859375Mb
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000560
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29380.76953125Mb; avail=225649.30859375Mb
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002088
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29380.76953125Mb; avail=225649.30859375Mb
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001866
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004831
2024-07-20 10:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29380.76953125Mb; avail=225649.30859375Mb
2024-07-20 10:21:28 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.533 | nll_loss 4.206 | ppl 18.46 | wps 3124.6 | wpb 1665.6 | bsz 74.4 | num_updates 323 | best_loss 7.533
2024-07-20 10:21:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 323 updates
2024-07-20 10:21:28 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:22:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:22:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 17 @ 323 updates, score 7.533) (writing took 78.02378700091504 seconds)
2024-07-20 10:22:46 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-07-20 10:22:46 | INFO | train | epoch 017 | loss 7.81 | nll_loss 4.791 | ppl 27.69 | wps 712.2 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 323 | lr 3.876e-06 | gnorm 2.108 | train_wall 48 | gb_free 16.4 | wall 2050
2024-07-20 10:22:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:22:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:22:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:22:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000646
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34187.87890625Mb; avail=220842.2421875Mb
2024-07-20 10:22:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000097
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000743
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34187.87890625Mb; avail=220842.2421875Mb
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34187.87890625Mb; avail=220842.2421875Mb
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000208
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001278
2024-07-20 10:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34187.87890625Mb; avail=220842.2421875Mb
2024-07-20 10:22:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:22:46 | INFO | fairseq.trainer | begin training epoch 18
2024-07-20 10:22:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:22:49 | INFO | train_inner | epoch 018:      1 / 19 loss=7.684, nll_loss=4.63, ppl=24.76, wps=108.8, ups=0.02, wpb=4673, bsz=200, num_updates=324, lr=3.888e-06, gnorm=2.017, train_wall=4, gb_free=12.6, wall=2054
2024-07-20 10:22:55 | INFO | train_inner | epoch 018:      3 / 19 loss=7.715, nll_loss=4.666, ppl=25.38, wps=1921.1, ups=0.38, wpb=5064.5, bsz=228, num_updates=326, lr=3.912e-06, gnorm=1.828, train_wall=5, gb_free=12.6, wall=2059
2024-07-20 10:23:00 | INFO | train_inner | epoch 018:      5 / 19 loss=7.799, nll_loss=4.785, ppl=27.58, wps=1319.5, ups=0.38, wpb=3449.5, bsz=144, num_updates=328, lr=3.936e-06, gnorm=2.11, train_wall=5, gb_free=12.8, wall=2064
2024-07-20 10:23:05 | INFO | train_inner | epoch 018:      7 / 19 loss=7.627, nll_loss=4.567, ppl=23.71, wps=1819, ups=0.39, wpb=4629.5, bsz=232, num_updates=330, lr=3.96e-06, gnorm=1.938, train_wall=5, gb_free=13.9, wall=2069
2024-07-20 10:23:11 | INFO | train_inner | epoch 018:      9 / 19 loss=7.549, nll_loss=4.474, ppl=22.22, wps=1927.8, ups=0.32, wpb=5946.5, bsz=300, num_updates=332, lr=3.984e-06, gnorm=1.775, train_wall=6, gb_free=11, wall=2075
2024-07-20 10:23:17 | INFO | train_inner | epoch 018:     11 / 19 loss=7.672, nll_loss=4.63, ppl=24.76, wps=1963.7, ups=0.34, wpb=5693.5, bsz=260, num_updates=334, lr=4.008e-06, gnorm=2.22, train_wall=6, gb_free=12.6, wall=2081
2024-07-20 10:23:21 | INFO | train_inner | epoch 018:     13 / 19 loss=7.688, nll_loss=4.632, ppl=24.8, wps=1896.2, ups=0.45, wpb=4213.5, bsz=192, num_updates=336, lr=4.032e-06, gnorm=2.121, train_wall=4, gb_free=12.1, wall=2086
2024-07-20 10:23:26 | INFO | train_inner | epoch 018:     15 / 19 loss=8.12, nll_loss=5.184, ppl=36.36, wps=1637.2, ups=0.42, wpb=3925, bsz=161.5, num_updates=338, lr=4.056e-06, gnorm=2.471, train_wall=5, gb_free=14.2, wall=2090
2024-07-20 10:23:31 | INFO | train_inner | epoch 018:     17 / 19 loss=7.625, nll_loss=4.549, ppl=23.41, wps=2355.2, ups=0.39, wpb=6040, bsz=252, num_updates=340, lr=4.08e-06, gnorm=1.797, train_wall=5, gb_free=11.5, wall=2096
2024-07-20 10:23:36 | INFO | train_inner | epoch 018:     19 / 19 loss=7.64, nll_loss=4.568, ppl=23.72, wps=1858.9, ups=0.45, wpb=4137.5, bsz=176, num_updates=342, lr=4.104e-06, gnorm=2.029, train_wall=4, gb_free=16.8, wall=2100
2024-07-20 10:23:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34324.5Mb; avail=220705.484375Mb
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34324.9921875Mb; avail=220704.9921875Mb
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002063
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34324.9921875Mb; avail=220704.9921875Mb
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001855
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004825
2024-07-20 10:23:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34324.9921875Mb; avail=220704.9921875Mb
2024-07-20 10:23:39 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.452 | nll_loss 4.078 | ppl 16.89 | wps 3778.9 | wpb 1665.6 | bsz 74.4 | num_updates 342 | best_loss 7.452
2024-07-20 10:23:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 342 updates
2024-07-20 10:23:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:24:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 18 @ 342 updates, score 7.452) (writing took 64.40365419303998 seconds)
2024-07-20 10:24:43 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-07-20 10:24:43 | INFO | train | epoch 018 | loss 7.695 | nll_loss 4.647 | ppl 25.06 | wps 792.1 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 342 | lr 4.104e-06 | gnorm 2.011 | train_wall 49 | gb_free 16.8 | wall 2167
2024-07-20 10:24:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:24:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:24:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:24:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000790
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39327.83984375Mb; avail=215702.6328125Mb
2024-07-20 10:24:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000751
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39327.34765625Mb; avail=215702.6328125Mb
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39327.83984375Mb; avail=215702.140625Mb
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000235
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001324
2024-07-20 10:24:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39327.34765625Mb; avail=215702.6328125Mb
2024-07-20 10:24:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:24:43 | INFO | fairseq.trainer | begin training epoch 19
2024-07-20 10:24:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:24:49 | INFO | train_inner | epoch 019:      2 / 19 loss=7.642, nll_loss=4.565, ppl=23.67, wps=116.5, ups=0.03, wpb=4251, bsz=184, num_updates=344, lr=4.128e-06, gnorm=2.205, train_wall=6, gb_free=11.6, wall=2173
2024-07-20 10:24:55 | INFO | train_inner | epoch 019:      4 / 19 loss=7.56, nll_loss=4.47, ppl=22.16, wps=2041.6, ups=0.35, wpb=5839, bsz=288, num_updates=346, lr=4.152e-06, gnorm=2.172, train_wall=6, gb_free=11.1, wall=2179
2024-07-20 10:24:59 | INFO | train_inner | epoch 019:      6 / 19 loss=7.515, nll_loss=4.421, ppl=21.42, wps=2241.7, ups=0.45, wpb=4946.5, bsz=221.5, num_updates=348, lr=4.176e-06, gnorm=2.12, train_wall=4, gb_free=15.7, wall=2183
2024-07-20 10:25:05 | INFO | train_inner | epoch 019:      8 / 19 loss=7.406, nll_loss=4.293, ppl=19.61, wps=1930.5, ups=0.34, wpb=5747, bsz=264, num_updates=350, lr=4.2e-06, gnorm=1.886, train_wall=6, gb_free=11.7, wall=2189
2024-07-20 10:25:11 | INFO | train_inner | epoch 019:     10 / 19 loss=7.802, nll_loss=4.813, ppl=28.12, wps=1802.2, ups=0.35, wpb=5089, bsz=256, num_updates=352, lr=4.224e-06, gnorm=2.357, train_wall=6, gb_free=13.4, wall=2195
2024-07-20 10:25:15 | INFO | train_inner | epoch 019:     12 / 19 loss=7.621, nll_loss=4.559, ppl=23.56, wps=1908.6, ups=0.41, wpb=4612, bsz=216, num_updates=354, lr=4.248e-06, gnorm=2.399, train_wall=5, gb_free=14.9, wall=2200
2024-07-20 10:25:20 | INFO | train_inner | epoch 019:     14 / 19 loss=7.794, nll_loss=4.758, ppl=27.07, wps=2044.6, ups=0.45, wpb=4509.5, bsz=160, num_updates=356, lr=4.272e-06, gnorm=2.619, train_wall=4, gb_free=11.3, wall=2204
2024-07-20 10:25:26 | INFO | train_inner | epoch 019:     16 / 19 loss=7.554, nll_loss=4.469, ppl=22.14, wps=1773.4, ups=0.35, wpb=5084.5, bsz=232, num_updates=358, lr=4.296e-06, gnorm=2.01, train_wall=6, gb_free=14.5, wall=2210
2024-07-20 10:25:31 | INFO | train_inner | epoch 019:     18 / 19 loss=7.536, nll_loss=4.427, ppl=21.51, wps=1629.5, ups=0.33, wpb=4867, bsz=212, num_updates=360, lr=4.32e-06, gnorm=2.527, train_wall=6, gb_free=11.6, wall=2216
2024-07-20 10:25:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39430.71484375Mb; avail=215599.2734375Mb
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39430.71484375Mb; avail=215599.2734375Mb
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002097
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39430.71484375Mb; avail=215599.2734375Mb
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001831
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004821
2024-07-20 10:25:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39430.71484375Mb; avail=215599.2734375Mb
2024-07-20 10:25:35 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.361 | nll_loss 3.966 | ppl 15.63 | wps 4187.9 | wpb 1665.6 | bsz 74.4 | num_updates 361 | best_loss 7.361
2024-07-20 10:25:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 361 updates
2024-07-20 10:25:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:26:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:26:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 19 @ 361 updates, score 7.361) (writing took 78.34070692118257 seconds)
2024-07-20 10:26:54 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-07-20 10:26:54 | INFO | train | epoch 019 | loss 7.597 | nll_loss 4.522 | ppl 22.97 | wps 707.9 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 361 | lr 4.332e-06 | gnorm 2.278 | train_wall 50 | gb_free 17.5 | wall 2298
2024-07-20 10:26:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:26:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:26:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:26:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000934
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41180.421875Mb; avail=213849.453125Mb
2024-07-20 10:26:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000114
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001207
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41180.421875Mb; avail=213849.453125Mb
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000052
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41180.421875Mb; avail=213849.453125Mb
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000209
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001876
2024-07-20 10:26:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41180.421875Mb; avail=213849.453125Mb
2024-07-20 10:26:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:26:54 | INFO | fairseq.trainer | begin training epoch 20
2024-07-20 10:26:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:26:57 | INFO | train_inner | epoch 020:      1 / 19 loss=7.561, nll_loss=4.458, ppl=21.98, wps=92.8, ups=0.02, wpb=3961, bsz=152, num_updates=362, lr=4.344e-06, gnorm=2.271, train_wall=4, gb_free=11.6, wall=2301
2024-07-20 10:27:03 | INFO | train_inner | epoch 020:      3 / 19 loss=7.344, nll_loss=4.203, ppl=18.42, wps=2039.1, ups=0.35, wpb=5759, bsz=304, num_updates=364, lr=4.368e-06, gnorm=2.325, train_wall=6, gb_free=11.2, wall=2307
2024-07-20 10:27:08 | INFO | train_inner | epoch 020:      5 / 19 loss=7.567, nll_loss=4.49, ppl=22.47, wps=1754.3, ups=0.38, wpb=4601.5, bsz=204, num_updates=366, lr=4.392e-06, gnorm=2.043, train_wall=5, gb_free=13.1, wall=2312
2024-07-20 10:27:13 | INFO | train_inner | epoch 020:      7 / 19 loss=7.456, nll_loss=4.354, ppl=20.44, wps=2333.6, ups=0.38, wpb=6093, bsz=276, num_updates=368, lr=4.416e-06, gnorm=2.14, train_wall=5, gb_free=11.3, wall=2317
2024-07-20 10:27:18 | INFO | train_inner | epoch 020:      9 / 19 loss=7.784, nll_loss=4.771, ppl=27.3, wps=1639.9, ups=0.4, wpb=4090, bsz=176, num_updates=370, lr=4.44e-06, gnorm=2.517, train_wall=5, gb_free=13, wall=2322
2024-07-20 10:27:23 | INFO | train_inner | epoch 020:     11 / 19 loss=7.494, nll_loss=4.402, ppl=21.14, wps=1892.9, ups=0.43, wpb=4389.5, bsz=181.5, num_updates=372, lr=4.464e-06, gnorm=2.143, train_wall=5, gb_free=12.9, wall=2327
2024-07-20 10:27:27 | INFO | train_inner | epoch 020:     13 / 19 loss=7.635, nll_loss=4.584, ppl=23.99, wps=2059.7, ups=0.43, wpb=4754.5, bsz=172, num_updates=374, lr=4.488e-06, gnorm=1.932, train_wall=5, gb_free=12.2, wall=2331
2024-07-20 10:27:33 | INFO | train_inner | epoch 020:     15 / 19 loss=7.381, nll_loss=4.245, ppl=18.96, wps=1814.4, ups=0.36, wpb=4973.5, bsz=216, num_updates=376, lr=4.512e-06, gnorm=1.985, train_wall=5, gb_free=11.5, wall=2337
2024-07-20 10:27:38 | INFO | train_inner | epoch 020:     17 / 19 loss=7.418, nll_loss=4.297, ppl=19.65, wps=1736.8, ups=0.41, wpb=4238, bsz=216, num_updates=378, lr=4.536e-06, gnorm=2.641, train_wall=5, gb_free=13.5, wall=2342
2024-07-20 10:27:43 | INFO | train_inner | epoch 020:     19 / 19 loss=7.308, nll_loss=4.143, ppl=17.67, wps=1833.1, ups=0.39, wpb=4656.5, bsz=232, num_updates=380, lr=4.56e-06, gnorm=2.12, train_wall=5, gb_free=15.2, wall=2347
2024-07-20 10:27:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58350.4609375Mb; avail=196679.04296875Mb
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58351.4453125Mb; avail=196678.55078125Mb
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002064
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58355.3828125Mb; avail=196674.61328125Mb
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001852
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004793
2024-07-20 10:27:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58358.828125Mb; avail=196671.16796875Mb
2024-07-20 10:27:46 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.267 | nll_loss 3.845 | ppl 14.37 | wps 3353.2 | wpb 1665.6 | bsz 74.4 | num_updates 380 | best_loss 7.267
2024-07-20 10:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 380 updates
2024-07-20 10:27:46 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:28:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 20 @ 380 updates, score 7.267) (writing took 64.53330937889405 seconds)
2024-07-20 10:28:50 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-07-20 10:28:50 | INFO | train | epoch 020 | loss 7.483 | nll_loss 4.381 | ppl 20.84 | wps 792.6 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 380 | lr 4.56e-06 | gnorm 2.187 | train_wall 49 | gb_free 15.2 | wall 2414
2024-07-20 10:28:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:28:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:28:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:28:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000753
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39076.9375Mb; avail=215953.1015625Mb
2024-07-20 10:28:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000079
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000757
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39076.9375Mb; avail=215953.1015625Mb
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000056
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39076.9375Mb; avail=215953.1015625Mb
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000220
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001330
2024-07-20 10:28:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39076.93359375Mb; avail=215953.1015625Mb
2024-07-20 10:28:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:28:50 | INFO | fairseq.trainer | begin training epoch 21
2024-07-20 10:28:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:28:55 | INFO | train_inner | epoch 021:      2 / 19 loss=7.446, nll_loss=4.341, ppl=20.27, wps=140.5, ups=0.03, wpb=5097.5, bsz=236, num_updates=382, lr=4.584e-06, gnorm=1.859, train_wall=5, gb_free=11.8, wall=2419
2024-07-20 10:29:00 | INFO | train_inner | epoch 021:      4 / 19 loss=7.763, nll_loss=4.737, ppl=26.66, wps=1592.4, ups=0.38, wpb=4169, bsz=180, num_updates=384, lr=4.608e-06, gnorm=2.51, train_wall=5, gb_free=10.1, wall=2425
2024-07-20 10:29:06 | INFO | train_inner | epoch 021:      6 / 19 loss=7.398, nll_loss=4.268, ppl=19.27, wps=1722.3, ups=0.34, wpb=5043.5, bsz=232, num_updates=386, lr=4.632e-06, gnorm=1.802, train_wall=6, gb_free=11.9, wall=2430
2024-07-20 10:29:11 | INFO | train_inner | epoch 021:      8 / 19 loss=7.411, nll_loss=4.285, ppl=19.49, wps=2116.1, ups=0.4, wpb=5327, bsz=208, num_updates=388, lr=4.656e-06, gnorm=2.021, train_wall=5, gb_free=13.4, wall=2436
2024-07-20 10:29:16 | INFO | train_inner | epoch 021:     10 / 19 loss=7.22, nll_loss=4.054, ppl=16.62, wps=2062.4, ups=0.39, wpb=5254.5, bsz=276, num_updates=390, lr=4.68e-06, gnorm=1.998, train_wall=5, gb_free=10.8, wall=2441
2024-07-20 10:29:22 | INFO | train_inner | epoch 021:     12 / 19 loss=7.278, nll_loss=4.126, ppl=17.46, wps=1758.4, ups=0.33, wpb=5266, bsz=256, num_updates=392, lr=4.704e-06, gnorm=2.137, train_wall=6, gb_free=11.2, wall=2447
2024-07-20 10:29:28 | INFO | train_inner | epoch 021:     14 / 19 loss=7.353, nll_loss=4.222, ppl=18.67, wps=2187.5, ups=0.38, wpb=5831.5, bsz=248, num_updates=394, lr=4.728e-06, gnorm=1.664, train_wall=5, gb_free=11, wall=2452
2024-07-20 10:29:32 | INFO | train_inner | epoch 021:     16 / 19 loss=7.433, nll_loss=4.305, ppl=19.76, wps=1814.4, ups=0.42, wpb=4283.5, bsz=173.5, num_updates=396, lr=4.752e-06, gnorm=1.904, train_wall=5, gb_free=16.1, wall=2457
2024-07-20 10:29:37 | INFO | train_inner | epoch 021:     18 / 19 loss=7.321, nll_loss=4.175, ppl=18.06, wps=1785.6, ups=0.41, wpb=4400, bsz=216, num_updates=398, lr=4.776e-06, gnorm=2.399, train_wall=5, gb_free=12, wall=2462
2024-07-20 10:29:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42652.68359375Mb; avail=212377.3125Mb
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000591
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42653.66796875Mb; avail=212376.328125Mb
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002104
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42657.11328125Mb; avail=212372.8828125Mb
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001895
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004953
2024-07-20 10:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42661.05078125Mb; avail=212368.9453125Mb
2024-07-20 10:29:42 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.19 | nll_loss 3.739 | ppl 13.35 | wps 3303.6 | wpb 1665.6 | bsz 74.4 | num_updates 399 | best_loss 7.19
2024-07-20 10:29:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 399 updates
2024-07-20 10:29:42 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:30:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:31:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 21 @ 399 updates, score 7.19) (writing took 78.00498477788642 seconds)
2024-07-20 10:31:00 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-07-20 10:31:00 | INFO | train | epoch 021 | loss 7.393 | nll_loss 4.267 | ppl 19.25 | wps 712.2 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 399 | lr 4.788e-06 | gnorm 2.046 | train_wall 48 | gb_free 15.7 | wall 2544
2024-07-20 10:31:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:31:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:31:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:31:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000772
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39640.078125Mb; avail=215389.9609375Mb
2024-07-20 10:31:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000725
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39640.078125Mb; avail=215389.9609375Mb
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39640.078125Mb; avail=215389.9609375Mb
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000216
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001277
2024-07-20 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39640.078125Mb; avail=215389.9609375Mb
2024-07-20 10:31:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:31:00 | INFO | fairseq.trainer | begin training epoch 22
2024-07-20 10:31:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:31:03 | INFO | train_inner | epoch 022:      1 / 19 loss=7.383, nll_loss=4.239, ppl=18.88, wps=86.6, ups=0.02, wpb=3688.5, bsz=152, num_updates=400, lr=4.8e-06, gnorm=2.14, train_wall=4, gb_free=12.8, wall=2547
2024-07-20 10:31:08 | INFO | train_inner | epoch 022:      3 / 19 loss=7.398, nll_loss=4.272, ppl=19.32, wps=2139.3, ups=0.41, wpb=5274, bsz=208, num_updates=402, lr=4.824e-06, gnorm=2.099, train_wall=5, gb_free=12.8, wall=2552
2024-07-20 10:31:13 | INFO | train_inner | epoch 022:      5 / 19 loss=7.239, nll_loss=4.077, ppl=16.87, wps=1749.7, ups=0.34, wpb=5082.5, bsz=244, num_updates=404, lr=4.848e-06, gnorm=1.67, train_wall=6, gb_free=12.4, wall=2557
2024-07-20 10:31:19 | INFO | train_inner | epoch 022:      7 / 19 loss=7.358, nll_loss=4.233, ppl=18.8, wps=1835.1, ups=0.35, wpb=5190.5, bsz=212, num_updates=406, lr=4.872e-06, gnorm=1.771, train_wall=6, gb_free=12.7, wall=2563
2024-07-20 10:31:24 | INFO | train_inner | epoch 022:      9 / 19 loss=7.277, nll_loss=4.128, ppl=17.49, wps=1956.1, ups=0.43, wpb=4568.5, bsz=228, num_updates=408, lr=4.896e-06, gnorm=2.111, train_wall=5, gb_free=12.6, wall=2568
2024-07-20 10:31:30 | INFO | train_inner | epoch 022:     11 / 19 loss=7.163, nll_loss=3.97, ppl=15.67, wps=1858.3, ups=0.32, wpb=5796.5, bsz=308, num_updates=410, lr=4.92e-06, gnorm=1.983, train_wall=6, gb_free=10.7, wall=2574
2024-07-20 10:31:35 | INFO | train_inner | epoch 022:     13 / 19 loss=7.291, nll_loss=4.14, ppl=17.63, wps=2097, ups=0.41, wpb=5145, bsz=196, num_updates=412, lr=4.944e-06, gnorm=2.005, train_wall=5, gb_free=14.7, wall=2579
2024-07-20 10:31:40 | INFO | train_inner | epoch 022:     15 / 19 loss=7.431, nll_loss=4.313, ppl=19.87, wps=1894.5, ups=0.37, wpb=5068, bsz=236, num_updates=414, lr=4.968e-06, gnorm=1.904, train_wall=5, gb_free=10.4, wall=2584
2024-07-20 10:31:45 | INFO | train_inner | epoch 022:     17 / 19 loss=7.189, nll_loss=4.002, ppl=16.02, wps=1951.4, ups=0.4, wpb=4819.5, bsz=225.5, num_updates=416, lr=4.992e-06, gnorm=1.71, train_wall=5, gb_free=10.9, wall=2589
2024-07-20 10:31:48 | INFO | train_inner | epoch 022:     19 / 19 loss=7.49, nll_loss=4.398, ppl=21.08, wps=1914, ups=0.61, wpb=3156.5, bsz=128, num_updates=418, lr=5.016e-06, gnorm=2.461, train_wall=3, gb_free=16.5, wall=2593
2024-07-20 10:31:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43916.203125Mb; avail=211113.8046875Mb
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000590
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43916.6953125Mb; avail=211113.3125Mb
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002103
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43916.6953125Mb; avail=211113.3125Mb
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001860
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004961
2024-07-20 10:31:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43916.6953125Mb; avail=211113.8046875Mb
2024-07-20 10:31:51 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.119 | nll_loss 3.645 | ppl 12.51 | wps 4194.3 | wpb 1665.6 | bsz 74.4 | num_updates 418 | best_loss 7.119
2024-07-20 10:31:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 418 updates
2024-07-20 10:31:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:32:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:32:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 22 @ 418 updates, score 7.119) (writing took 68.491869404912 seconds)
2024-07-20 10:32:59 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-07-20 10:32:59 | INFO | train | epoch 022 | loss 7.31 | nll_loss 4.164 | ppl 17.92 | wps 775.1 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 418 | lr 5.016e-06 | gnorm 1.97 | train_wall 48 | gb_free 16.5 | wall 2664
2024-07-20 10:32:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:32:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:32:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:32:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000812
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42947.9453125Mb; avail=212082.09375Mb
2024-07-20 10:32:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000079
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000738
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42947.9453125Mb; avail=212082.09375Mb
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42947.9453125Mb; avail=212082.09375Mb
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000234
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001319
2024-07-20 10:32:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42947.9453125Mb; avail=212082.09375Mb
2024-07-20 10:32:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:32:59 | INFO | fairseq.trainer | begin training epoch 23
2024-07-20 10:32:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:33:05 | INFO | train_inner | epoch 023:      2 / 19 loss=7.293, nll_loss=4.14, ppl=17.64, wps=118.6, ups=0.03, wpb=4515.5, bsz=209.5, num_updates=420, lr=5.04e-06, gnorm=1.899, train_wall=5, gb_free=13, wall=2669
2024-07-20 10:33:10 | INFO | train_inner | epoch 023:      4 / 19 loss=7.519, nll_loss=4.431, ppl=21.56, wps=1843.4, ups=0.38, wpb=4847, bsz=204, num_updates=422, lr=5.064e-06, gnorm=2.589, train_wall=5, gb_free=11.1, wall=2674
2024-07-20 10:33:14 | INFO | train_inner | epoch 023:      6 / 19 loss=7.412, nll_loss=4.295, ppl=19.62, wps=1554.4, ups=0.44, wpb=3555, bsz=172, num_updates=424, lr=5.088e-06, gnorm=2.306, train_wall=5, gb_free=13.1, wall=2679
2024-07-20 10:33:19 | INFO | train_inner | epoch 023:      8 / 19 loss=7.317, nll_loss=4.167, ppl=17.96, wps=2062.5, ups=0.43, wpb=4798.5, bsz=176, num_updates=426, lr=5.112e-06, gnorm=1.921, train_wall=5, gb_free=15.6, wall=2683
2024-07-20 10:33:25 | INFO | train_inner | epoch 023:     10 / 19 loss=7.244, nll_loss=4.084, ppl=16.96, wps=1757.7, ups=0.35, wpb=5078, bsz=224, num_updates=428, lr=5.136e-06, gnorm=1.888, train_wall=6, gb_free=14.4, wall=2689
2024-07-20 10:33:30 | INFO | train_inner | epoch 023:     12 / 19 loss=7.103, nll_loss=3.89, ppl=14.83, wps=2408.3, ups=0.39, wpb=6174, bsz=268, num_updates=430, lr=5.16e-06, gnorm=1.776, train_wall=5, gb_free=13.1, wall=2694
2024-07-20 10:33:35 | INFO | train_inner | epoch 023:     14 / 19 loss=7.201, nll_loss=4.031, ppl=16.35, wps=1960.7, ups=0.4, wpb=4866.5, bsz=220, num_updates=432, lr=5.184e-06, gnorm=1.793, train_wall=5, gb_free=12.9, wall=2699
2024-07-20 10:33:41 | INFO | train_inner | epoch 023:     16 / 19 loss=7.111, nll_loss=3.901, ppl=14.94, wps=1711.1, ups=0.34, wpb=5006, bsz=224, num_updates=434, lr=5.208e-06, gnorm=1.828, train_wall=6, gb_free=12, wall=2705
2024-07-20 10:33:47 | INFO | train_inner | epoch 023:     18 / 19 loss=7.036, nll_loss=3.809, ppl=14.02, wps=1881.2, ups=0.32, wpb=5828, bsz=304, num_updates=436, lr=5.232e-06, gnorm=1.885, train_wall=6, gb_free=11.8, wall=2711
2024-07-20 10:33:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60157.55078125Mb; avail=194872.8125Mb
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60157.55078125Mb; avail=194872.3203125Mb
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002101
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60157.55078125Mb; avail=194872.3203125Mb
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001849
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004858
2024-07-20 10:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60157.546875Mb; avail=194872.3203125Mb
2024-07-20 10:33:51 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.069 | nll_loss 3.569 | ppl 11.86 | wps 4188.2 | wpb 1665.6 | bsz 74.4 | num_updates 437 | best_loss 7.069
2024-07-20 10:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 437 updates
2024-07-20 10:33:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:34:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:35:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 23 @ 437 updates, score 7.069) (writing took 75.47801587195136 seconds)
2024-07-20 10:35:06 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-07-20 10:35:06 | INFO | train | epoch 023 | loss 7.225 | nll_loss 4.053 | ppl 16.59 | wps 727.5 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 437 | lr 5.244e-06 | gnorm 1.998 | train_wall 49 | gb_free 15.5 | wall 2791
2024-07-20 10:35:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:35:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:35:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:35:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000644
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40702.1640625Mb; avail=214327.7109375Mb
2024-07-20 10:35:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000085
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000754
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40702.1640625Mb; avail=214327.7109375Mb
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40702.1640625Mb; avail=214327.7109375Mb
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000221
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001307
2024-07-20 10:35:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40702.1640625Mb; avail=214327.7109375Mb
2024-07-20 10:35:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:35:07 | INFO | fairseq.trainer | begin training epoch 24
2024-07-20 10:35:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:35:10 | INFO | train_inner | epoch 024:      1 / 19 loss=7.389, nll_loss=4.254, ppl=19.08, wps=90.6, ups=0.02, wpb=3751, bsz=144, num_updates=438, lr=5.256e-06, gnorm=2.452, train_wall=5, gb_free=11.6, wall=2794
2024-07-20 10:35:15 | INFO | train_inner | epoch 024:      3 / 19 loss=7.048, nll_loss=3.831, ppl=14.23, wps=2093.1, ups=0.4, wpb=5213, bsz=252, num_updates=440, lr=5.28e-06, gnorm=2.054, train_wall=5, gb_free=12.8, wall=2799
2024-07-20 10:35:20 | INFO | train_inner | epoch 024:      5 / 19 loss=7.131, nll_loss=3.932, ppl=15.27, wps=2213.6, ups=0.41, wpb=5433.5, bsz=232, num_updates=442, lr=5.304e-06, gnorm=2.047, train_wall=5, gb_free=11.6, wall=2804
2024-07-20 10:35:25 | INFO | train_inner | epoch 024:      7 / 19 loss=7.217, nll_loss=4.048, ppl=16.54, wps=1748.9, ups=0.41, wpb=4290, bsz=184, num_updates=444, lr=5.328e-06, gnorm=2.016, train_wall=5, gb_free=12.2, wall=2809
2024-07-20 10:35:31 | INFO | train_inner | epoch 024:      9 / 19 loss=7.166, nll_loss=3.99, ppl=15.89, wps=2025.7, ups=0.33, wpb=6207.5, bsz=264, num_updates=446, lr=5.352e-06, gnorm=1.727, train_wall=6, gb_free=11.9, wall=2815
2024-07-20 10:35:36 | INFO | train_inner | epoch 024:     11 / 19 loss=7.13, nll_loss=3.929, ppl=15.23, wps=2098.1, ups=0.38, wpb=5467, bsz=244, num_updates=448, lr=5.376e-06, gnorm=1.905, train_wall=5, gb_free=11.2, wall=2820
2024-07-20 10:35:41 | INFO | train_inner | epoch 024:     13 / 19 loss=7.172, nll_loss=3.973, ppl=15.7, wps=1808.7, ups=0.4, wpb=4563.5, bsz=192, num_updates=450, lr=5.4e-06, gnorm=2.126, train_wall=5, gb_free=11, wall=2825
2024-07-20 10:35:47 | INFO | train_inner | epoch 024:     15 / 19 loss=7.054, nll_loss=3.812, ppl=14.05, wps=1782.7, ups=0.33, wpb=5353, bsz=268, num_updates=452, lr=5.424e-06, gnorm=1.724, train_wall=6, gb_free=12.1, wall=2831
2024-07-20 10:35:53 | INFO | train_inner | epoch 024:     17 / 19 loss=7.076, nll_loss=3.877, ppl=14.69, wps=1607.8, ups=0.35, wpb=4547.5, bsz=240, num_updates=454, lr=5.448e-06, gnorm=1.988, train_wall=6, gb_free=12.7, wall=2837
2024-07-20 10:35:56 | INFO | train_inner | epoch 024:     19 / 19 loss=7.19, nll_loss=4.018, ppl=16.21, wps=1975.7, ups=0.67, wpb=2967.5, bsz=141.5, num_updates=456, lr=5.472e-06, gnorm=4.246, train_wall=3, gb_free=27, wall=2840
2024-07-20 10:35:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43130.75Mb; avail=211899.30078125Mb
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000551
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43130.2578125Mb; avail=211899.79296875Mb
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002078
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43130.2578125Mb; avail=211899.79296875Mb
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001854
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004820
2024-07-20 10:35:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43130.75Mb; avail=211899.30078125Mb
2024-07-20 10:35:58 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.97 | nll_loss 3.463 | ppl 11.03 | wps 4193.3 | wpb 1665.6 | bsz 74.4 | num_updates 456 | best_loss 6.97
2024-07-20 10:35:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 456 updates
2024-07-20 10:35:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:36:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:37:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 24 @ 456 updates, score 6.97) (writing took 63.56184005504474 seconds)
2024-07-20 10:37:02 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-07-20 10:37:02 | INFO | train | epoch 024 | loss 7.154 | nll_loss 3.962 | ppl 15.59 | wps 802.9 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 456 | lr 5.472e-06 | gnorm 2.23 | train_wall 49 | gb_free 27 | wall 2906
2024-07-20 10:37:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:37:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:37:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:37:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000758
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36802.89453125Mb; avail=218227.140625Mb
2024-07-20 10:37:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000102
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000794
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36802.89453125Mb; avail=218227.140625Mb
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000038
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36802.89453125Mb; avail=218227.140625Mb
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000283
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001425
2024-07-20 10:37:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36802.89453125Mb; avail=218227.140625Mb
2024-07-20 10:37:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:37:02 | INFO | fairseq.trainer | begin training epoch 25
2024-07-20 10:37:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:37:06 | INFO | train_inner | epoch 025:      2 / 19 loss=7.101, nll_loss=3.914, ppl=15.08, wps=117.9, ups=0.03, wpb=4156.5, bsz=208, num_updates=458, lr=5.496e-06, gnorm=2.047, train_wall=4, gb_free=16.1, wall=2910
2024-07-20 10:37:11 | INFO | train_inner | epoch 025:      4 / 19 loss=7.21, nll_loss=4.05, ppl=16.57, wps=2203, ups=0.4, wpb=5466, bsz=256, num_updates=460, lr=5.52e-06, gnorm=2.015, train_wall=5, gb_free=11.4, wall=2915
2024-07-20 10:37:17 | INFO | train_inner | epoch 025:      6 / 19 loss=7.095, nll_loss=3.889, ppl=14.82, wps=2060.3, ups=0.34, wpb=5978, bsz=248, num_updates=462, lr=5.544e-06, gnorm=2.009, train_wall=6, gb_free=12.9, wall=2921
2024-07-20 10:37:22 | INFO | train_inner | epoch 025:      8 / 19 loss=7.113, nll_loss=3.903, ppl=14.96, wps=2130.7, ups=0.38, wpb=5544, bsz=232, num_updates=464, lr=5.568e-06, gnorm=1.787, train_wall=5, gb_free=12.5, wall=2926
2024-07-20 10:37:28 | INFO | train_inner | epoch 025:     10 / 19 loss=6.962, nll_loss=3.697, ppl=12.97, wps=1988.5, ups=0.33, wpb=5993, bsz=296, num_updates=466, lr=5.592e-06, gnorm=1.832, train_wall=6, gb_free=11.3, wall=2932
2024-07-20 10:37:33 | INFO | train_inner | epoch 025:     12 / 19 loss=7.031, nll_loss=3.793, ppl=13.86, wps=1865, ups=0.41, wpb=4601.5, bsz=216, num_updates=468, lr=5.616e-06, gnorm=2.048, train_wall=5, gb_free=11.6, wall=2937
2024-07-20 10:37:39 | INFO | train_inner | epoch 025:     14 / 19 loss=6.978, nll_loss=3.73, ppl=13.27, wps=1609, ups=0.34, wpb=4673, bsz=212, num_updates=470, lr=5.64e-06, gnorm=1.886, train_wall=6, gb_free=11.2, wall=2943
2024-07-20 10:37:43 | INFO | train_inner | epoch 025:     16 / 19 loss=7.207, nll_loss=4.036, ppl=16.4, wps=1885.9, ups=0.51, wpb=3666.5, bsz=145.5, num_updates=472, lr=5.664e-06, gnorm=2.878, train_wall=4, gb_free=19.1, wall=2947
2024-07-20 10:37:48 | INFO | train_inner | epoch 025:     18 / 19 loss=7.057, nll_loss=3.847, ppl=14.39, wps=1814.3, ups=0.39, wpb=4603, bsz=200, num_updates=474, lr=5.688e-06, gnorm=2.079, train_wall=5, gb_free=11.3, wall=2952
2024-07-20 10:37:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=54044.1328125Mb; avail=200985.77734375Mb
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54044.625Mb; avail=200985.28515625Mb
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002086
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54045.1171875Mb; avail=200984.79296875Mb
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001878
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004875
2024-07-20 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54045.1171875Mb; avail=200984.79296875Mb
2024-07-20 10:37:53 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.915 | nll_loss 3.401 | ppl 10.56 | wps 3170.8 | wpb 1665.6 | bsz 74.4 | num_updates 475 | best_loss 6.915
2024-07-20 10:37:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 475 updates
2024-07-20 10:37:53 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:38:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 25 @ 475 updates, score 6.915) (writing took 77.59332181001082 seconds)
2024-07-20 10:39:10 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-07-20 10:39:10 | INFO | train | epoch 025 | loss 7.073 | nll_loss 3.859 | ppl 14.51 | wps 719.8 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 475 | lr 5.7e-06 | gnorm 2.076 | train_wall 47 | gb_free 15.6 | wall 3034
2024-07-20 10:39:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:39:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:39:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:39:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000637
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33264.3515625Mb; avail=221765.64453125Mb
2024-07-20 10:39:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000721
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33264.3515625Mb; avail=221765.64453125Mb
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33264.3515625Mb; avail=221765.64453125Mb
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000215
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001259
2024-07-20 10:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33264.3515625Mb; avail=221765.64453125Mb
2024-07-20 10:39:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:39:10 | INFO | fairseq.trainer | begin training epoch 26
2024-07-20 10:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:39:12 | INFO | train_inner | epoch 026:      1 / 19 loss=7.001, nll_loss=3.788, ppl=13.81, wps=68.5, ups=0.02, wpb=2878, bsz=129.5, num_updates=476, lr=5.712e-06, gnorm=2.254, train_wall=3, gb_free=17.3, wall=3036
2024-07-20 10:39:16 | INFO | train_inner | epoch 026:      3 / 19 loss=7.018, nll_loss=3.827, ppl=14.2, wps=2070.2, ups=0.42, wpb=4902.5, bsz=236, num_updates=478, lr=5.736e-06, gnorm=1.791, train_wall=5, gb_free=12.5, wall=3041
2024-07-20 10:39:22 | INFO | train_inner | epoch 026:      5 / 19 loss=6.846, nll_loss=3.574, ppl=11.91, wps=1583.4, ups=0.35, wpb=4556.5, bsz=248, num_updates=480, lr=5.76e-06, gnorm=2.007, train_wall=6, gb_free=11, wall=3046
2024-07-20 10:39:27 | INFO | train_inner | epoch 026:      7 / 19 loss=6.998, nll_loss=3.773, ppl=13.67, wps=2167.4, ups=0.42, wpb=5117.5, bsz=236, num_updates=482, lr=5.784e-06, gnorm=1.761, train_wall=5, gb_free=13.3, wall=3051
2024-07-20 10:39:32 | INFO | train_inner | epoch 026:      9 / 19 loss=6.977, nll_loss=3.718, ppl=13.16, wps=2029.6, ups=0.38, wpb=5385.5, bsz=208, num_updates=484, lr=5.808e-06, gnorm=2.08, train_wall=5, gb_free=11.9, wall=3056
2024-07-20 10:39:38 | INFO | train_inner | epoch 026:     11 / 19 loss=6.977, nll_loss=3.733, ppl=13.3, wps=1776.1, ups=0.35, wpb=5086, bsz=240, num_updates=486, lr=5.832e-06, gnorm=1.998, train_wall=6, gb_free=11.8, wall=3062
2024-07-20 10:39:44 | INFO | train_inner | epoch 026:     13 / 19 loss=7.217, nll_loss=4.042, ppl=16.48, wps=1582.1, ups=0.36, wpb=4409.5, bsz=184, num_updates=488, lr=5.856e-06, gnorm=2.044, train_wall=6, gb_free=12.8, wall=3068
2024-07-20 10:39:49 | INFO | train_inner | epoch 026:     15 / 19 loss=6.988, nll_loss=3.764, ppl=13.59, wps=1911.8, ups=0.37, wpb=5216, bsz=232, num_updates=490, lr=5.88e-06, gnorm=2.006, train_wall=5, gb_free=11.8, wall=3073
2024-07-20 10:39:55 | INFO | train_inner | epoch 026:     17 / 19 loss=6.842, nll_loss=3.569, ppl=11.87, wps=1979.5, ups=0.34, wpb=5908, bsz=276, num_updates=492, lr=5.904e-06, gnorm=1.851, train_wall=6, gb_free=11.5, wall=3079
2024-07-20 10:39:59 | INFO | train_inner | epoch 026:     19 / 19 loss=7.008, nll_loss=3.782, ppl=13.75, wps=2026.1, ups=0.47, wpb=4321, bsz=160, num_updates=494, lr=5.928e-06, gnorm=2.204, train_wall=4, gb_free=15.7, wall=3083
2024-07-20 10:39:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30822.9453125Mb; avail=224206.64453125Mb
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000574
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30827.375Mb; avail=224202.70703125Mb
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002082
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30822.9453125Mb; avail=224206.64453125Mb
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001856
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004828
2024-07-20 10:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30826.390625Mb; avail=224203.19921875Mb
2024-07-20 10:40:02 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.854 | nll_loss 3.299 | ppl 9.84 | wps 3894.7 | wpb 1665.6 | bsz 74.4 | num_updates 494 | best_loss 6.854
2024-07-20 10:40:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 494 updates
2024-07-20 10:40:02 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:40:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:41:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 26 @ 494 updates, score 6.854) (writing took 66.71434521209449 seconds)
2024-07-20 10:41:09 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-07-20 10:41:09 | INFO | train | epoch 026 | loss 6.986 | nll_loss 3.755 | ppl 13.5 | wps 779.5 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 494 | lr 5.928e-06 | gnorm 1.984 | train_wall 49 | gb_free 15.7 | wall 3153
2024-07-20 10:41:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:41:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:41:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:41:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000757
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42585.796875Mb; avail=212444.2734375Mb
2024-07-20 10:41:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000100
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000793
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42585.796875Mb; avail=212444.2734375Mb
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42585.796875Mb; avail=212444.2734375Mb
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000245
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001362
2024-07-20 10:41:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42585.796875Mb; avail=212444.2734375Mb
2024-07-20 10:41:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:41:09 | INFO | fairseq.trainer | begin training epoch 27
2024-07-20 10:41:09 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:41:14 | INFO | train_inner | epoch 027:      2 / 19 loss=6.811, nll_loss=3.521, ppl=11.48, wps=157.3, ups=0.03, wpb=5890, bsz=268, num_updates=496, lr=5.952e-06, gnorm=1.74, train_wall=5, gb_free=11, wall=3158
2024-07-20 10:41:19 | INFO | train_inner | epoch 027:      4 / 19 loss=7.19, nll_loss=4.015, ppl=16.16, wps=1749.3, ups=0.42, wpb=4158.5, bsz=156, num_updates=498, lr=5.976e-06, gnorm=1.966, train_wall=5, gb_free=12.9, wall=3163
2024-07-20 10:41:25 | INFO | train_inner | epoch 027:      6 / 19 loss=6.833, nll_loss=3.567, ppl=11.85, wps=1917.7, ups=0.33, wpb=5726.5, bsz=276, num_updates=500, lr=6e-06, gnorm=1.654, train_wall=6, gb_free=12.9, wall=3169
2024-07-20 10:41:29 | INFO | train_inner | epoch 027:      8 / 19 loss=6.881, nll_loss=3.612, ppl=12.22, wps=2148.4, ups=0.47, wpb=4595, bsz=201.5, num_updates=502, lr=6.024e-06, gnorm=1.968, train_wall=4, gb_free=11.8, wall=3173
2024-07-20 10:41:35 | INFO | train_inner | epoch 027:     10 / 19 loss=6.877, nll_loss=3.616, ppl=12.26, wps=1681.4, ups=0.34, wpb=4909.5, bsz=220, num_updates=504, lr=6.048e-06, gnorm=1.954, train_wall=6, gb_free=10.7, wall=3179
2024-07-20 10:41:40 | INFO | train_inner | epoch 027:     12 / 19 loss=6.823, nll_loss=3.552, ppl=11.72, wps=2001.9, ups=0.39, wpb=5105.5, bsz=248, num_updates=506, lr=6.072e-06, gnorm=1.789, train_wall=5, gb_free=11.8, wall=3184
2024-07-20 10:41:45 | INFO | train_inner | epoch 027:     14 / 19 loss=6.841, nll_loss=3.581, ppl=11.97, wps=2207.7, ups=0.39, wpb=5713.5, bsz=272, num_updates=508, lr=6.096e-06, gnorm=1.827, train_wall=5, gb_free=12.1, wall=3189
2024-07-20 10:41:51 | INFO | train_inner | epoch 027:     16 / 19 loss=7.081, nll_loss=3.886, ppl=14.79, wps=1637.1, ups=0.38, wpb=4270.5, bsz=188, num_updates=510, lr=6.12e-06, gnorm=2.049, train_wall=5, gb_free=16.6, wall=3195
2024-07-20 10:41:56 | INFO | train_inner | epoch 027:     18 / 19 loss=6.872, nll_loss=3.6, ppl=12.12, wps=1838.1, ups=0.4, wpb=4614, bsz=204, num_updates=512, lr=6.144e-06, gnorm=1.999, train_wall=5, gb_free=12.1, wall=3200
2024-07-20 10:41:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=48382.359375Mb; avail=206647.578125Mb
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48382.8515625Mb; avail=206647.0859375Mb
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002090
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48382.8515625Mb; avail=206647.0859375Mb
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001829
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004809
2024-07-20 10:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48382.8515625Mb; avail=206647.578125Mb
2024-07-20 10:42:00 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.81 | nll_loss 3.226 | ppl 9.36 | wps 3475.2 | wpb 1665.6 | bsz 74.4 | num_updates 513 | best_loss 6.81
2024-07-20 10:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 513 updates
2024-07-20 10:42:00 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:42:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:43:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 27 @ 513 updates, score 6.81) (writing took 77.76905709900893 seconds)
2024-07-20 10:43:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-07-20 10:43:18 | INFO | train | epoch 027 | loss 6.901 | nll_loss 3.646 | ppl 12.52 | wps 715.5 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 513 | lr 6.156e-06 | gnorm 1.941 | train_wall 48 | gb_free 16.7 | wall 3282
2024-07-20 10:43:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:43:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:43:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:43:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000630
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58239.1796875Mb; avail=196790.859375Mb
2024-07-20 10:43:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000719
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58239.1796875Mb; avail=196790.859375Mb
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58239.1796875Mb; avail=196790.859375Mb
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000220
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001261
2024-07-20 10:43:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58239.1796875Mb; avail=196790.859375Mb
2024-07-20 10:43:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:43:18 | INFO | fairseq.trainer | begin training epoch 28
2024-07-20 10:43:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:43:21 | INFO | train_inner | epoch 028:      1 / 19 loss=6.821, nll_loss=3.523, ppl=11.49, wps=89.3, ups=0.02, wpb=3800, bsz=156, num_updates=514, lr=6.168e-06, gnorm=2.372, train_wall=4, gb_free=10.8, wall=3285
2024-07-20 10:43:26 | INFO | train_inner | epoch 028:      3 / 19 loss=6.752, nll_loss=3.449, ppl=10.92, wps=1988.4, ups=0.4, wpb=4912, bsz=256, num_updates=516, lr=6.192e-06, gnorm=1.786, train_wall=5, gb_free=13.4, wall=3290
2024-07-20 10:43:31 | INFO | train_inner | epoch 028:      5 / 19 loss=6.796, nll_loss=3.512, ppl=11.41, wps=1774.6, ups=0.4, wpb=4490, bsz=225.5, num_updates=518, lr=6.216e-06, gnorm=2.035, train_wall=5, gb_free=13.1, wall=3295
2024-07-20 10:43:36 | INFO | train_inner | epoch 028:      7 / 19 loss=6.74, nll_loss=3.446, ppl=10.9, wps=1724.3, ups=0.36, wpb=4827.5, bsz=212, num_updates=520, lr=6.24e-06, gnorm=1.816, train_wall=6, gb_free=11.8, wall=3300
2024-07-20 10:43:42 | INFO | train_inner | epoch 028:      9 / 19 loss=6.704, nll_loss=3.405, ppl=10.6, wps=2260.7, ups=0.37, wpb=6190.5, bsz=336, num_updates=522, lr=6.264e-06, gnorm=1.794, train_wall=5, gb_free=11, wall=3306
2024-07-20 10:43:48 | INFO | train_inner | epoch 028:     11 / 19 loss=6.796, nll_loss=3.535, ppl=11.59, wps=1884.7, ups=0.33, wpb=5695.5, bsz=276, num_updates=524, lr=6.288e-06, gnorm=1.727, train_wall=6, gb_free=11.1, wall=3312
2024-07-20 10:43:54 | INFO | train_inner | epoch 028:     13 / 19 loss=6.828, nll_loss=3.545, ppl=11.67, wps=1836.7, ups=0.33, wpb=5532, bsz=228, num_updates=526, lr=6.312e-06, gnorm=1.983, train_wall=6, gb_free=11.6, wall=3318
2024-07-20 10:43:59 | INFO | train_inner | epoch 028:     15 / 19 loss=7.213, nll_loss=4.048, ppl=16.54, wps=1817.3, ups=0.42, wpb=4320, bsz=152, num_updates=528, lr=6.336e-06, gnorm=2.454, train_wall=5, gb_free=11.9, wall=3323
2024-07-20 10:44:04 | INFO | train_inner | epoch 028:     17 / 19 loss=7.024, nll_loss=3.788, ppl=13.81, wps=1761.8, ups=0.38, wpb=4583.5, bsz=156, num_updates=530, lr=6.36e-06, gnorm=1.919, train_wall=5, gb_free=12.3, wall=3328
2024-07-20 10:44:07 | INFO | train_inner | epoch 028:     19 / 19 loss=6.863, nll_loss=3.575, ppl=11.92, wps=1695.3, ups=0.54, wpb=3128, bsz=132, num_updates=532, lr=6.384e-06, gnorm=2.258, train_wall=4, gb_free=17.2, wall=3332
2024-07-20 10:44:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42368.2890625Mb; avail=212662.18359375Mb
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000561
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42368.2890625Mb; avail=212661.19921875Mb
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002085
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42368.78125Mb; avail=212660.70703125Mb
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001837
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004800
2024-07-20 10:44:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42368.78125Mb; avail=212661.19921875Mb
2024-07-20 10:44:10 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.751 | nll_loss 3.149 | ppl 8.87 | wps 4189.6 | wpb 1665.6 | bsz 74.4 | num_updates 532 | best_loss 6.751
2024-07-20 10:44:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 532 updates
2024-07-20 10:44:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:45:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 28 @ 532 updates, score 6.751) (writing took 67.66482193511911 seconds)
2024-07-20 10:45:18 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-07-20 10:45:18 | INFO | train | epoch 028 | loss 6.842 | nll_loss 3.57 | ppl 11.88 | wps 772.1 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 532 | lr 6.384e-06 | gnorm 1.963 | train_wall 49 | gb_free 17.2 | wall 3402
2024-07-20 10:45:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:45:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:45:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:45:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000779
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64137.59375Mb; avail=190892.4453125Mb
2024-07-20 10:45:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000096
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000789
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64139.0703125Mb; avail=190890.4765625Mb
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64139.5625Mb; avail=190890.4765625Mb
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000241
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001349
2024-07-20 10:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64140.546875Mb; avail=190889.4921875Mb
2024-07-20 10:45:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:45:18 | INFO | fairseq.trainer | begin training epoch 29
2024-07-20 10:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:45:23 | INFO | train_inner | epoch 029:      2 / 19 loss=6.656, nll_loss=3.324, ppl=10.02, wps=122.6, ups=0.03, wpb=4605, bsz=252, num_updates=534, lr=6.408e-06, gnorm=2.008, train_wall=5, gb_free=12.9, wall=3407
2024-07-20 10:45:27 | INFO | train_inner | epoch 029:      4 / 19 loss=6.88, nll_loss=3.622, ppl=12.31, wps=2131.1, ups=0.45, wpb=4709, bsz=184, num_updates=536, lr=6.432e-06, gnorm=1.84, train_wall=4, gb_free=11.4, wall=3411
2024-07-20 10:45:33 | INFO | train_inner | epoch 029:      6 / 19 loss=6.788, nll_loss=3.505, ppl=11.35, wps=1818.6, ups=0.34, wpb=5384, bsz=236, num_updates=538, lr=6.456e-06, gnorm=1.893, train_wall=6, gb_free=13, wall=3417
2024-07-20 10:45:39 | INFO | train_inner | epoch 029:      8 / 19 loss=6.935, nll_loss=3.695, ppl=12.95, wps=1860.3, ups=0.35, wpb=5305.5, bsz=216, num_updates=540, lr=6.48e-06, gnorm=1.969, train_wall=6, gb_free=13.2, wall=3423
2024-07-20 10:45:44 | INFO | train_inner | epoch 029:     10 / 19 loss=6.771, nll_loss=3.47, ppl=11.08, wps=1922.7, ups=0.37, wpb=5224, bsz=208, num_updates=542, lr=6.504e-06, gnorm=1.694, train_wall=5, gb_free=11.8, wall=3428
2024-07-20 10:45:50 | INFO | train_inner | epoch 029:     12 / 19 loss=6.707, nll_loss=3.417, ppl=10.68, wps=2128.1, ups=0.35, wpb=6025.5, bsz=300, num_updates=544, lr=6.528e-06, gnorm=1.665, train_wall=6, gb_free=12.1, wall=3434
2024-07-20 10:45:54 | INFO | train_inner | epoch 029:     14 / 19 loss=6.829, nll_loss=3.554, ppl=11.75, wps=1853.5, ups=0.47, wpb=3911.5, bsz=173.5, num_updates=546, lr=6.552e-06, gnorm=2.206, train_wall=4, gb_free=11, wall=3438
2024-07-20 10:46:00 | INFO | train_inner | epoch 029:     16 / 19 loss=6.687, nll_loss=3.372, ppl=10.35, wps=1652.1, ups=0.34, wpb=4845, bsz=260, num_updates=548, lr=6.576e-06, gnorm=1.859, train_wall=6, gb_free=12.6, wall=3444
2024-07-20 10:46:05 | INFO | train_inner | epoch 029:     18 / 19 loss=6.722, nll_loss=3.404, ppl=10.58, wps=1880.5, ups=0.4, wpb=4663, bsz=196, num_updates=550, lr=6.6e-06, gnorm=1.818, train_wall=5, gb_free=12.2, wall=3449
2024-07-20 10:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=61221.1171875Mb; avail=193808.8359375Mb
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61220.625Mb; avail=193808.8359375Mb
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002092
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61220.625Mb; avail=193809.328125Mb
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001880
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004886
2024-07-20 10:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=61220.625Mb; avail=193809.328125Mb
2024-07-20 10:46:09 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.71 | nll_loss 3.1 | ppl 8.57 | wps 4185.2 | wpb 1665.6 | bsz 74.4 | num_updates 551 | best_loss 6.71
2024-07-20 10:46:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 551 updates
2024-07-20 10:46:09 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:47:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:47:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 29 @ 551 updates, score 6.71) (writing took 77.83607554715127 seconds)
2024-07-20 10:47:27 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-07-20 10:47:27 | INFO | train | epoch 029 | loss 6.778 | nll_loss 3.489 | ppl 11.23 | wps 717.4 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 551 | lr 6.612e-06 | gnorm 1.935 | train_wall 48 | gb_free 16 | wall 3531
2024-07-20 10:47:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:47:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:47:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:47:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000642
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=56433.61328125Mb; avail=198596.015625Mb
2024-07-20 10:47:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000095
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000727
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56433.61328125Mb; avail=198596.5078125Mb
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56433.61328125Mb; avail=198596.5078125Mb
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000206
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001246
2024-07-20 10:47:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56433.61328125Mb; avail=198596.5078125Mb
2024-07-20 10:47:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:47:27 | INFO | fairseq.trainer | begin training epoch 30
2024-07-20 10:47:27 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:47:29 | INFO | train_inner | epoch 030:      1 / 19 loss=6.685, nll_loss=3.365, ppl=10.3, wps=99.5, ups=0.02, wpb=4203.5, bsz=200, num_updates=552, lr=6.624e-06, gnorm=2.225, train_wall=4, gb_free=12.6, wall=3533
2024-07-20 10:47:34 | INFO | train_inner | epoch 030:      3 / 19 loss=6.75, nll_loss=3.447, ppl=10.9, wps=2076.2, ups=0.39, wpb=5297.5, bsz=212, num_updates=554, lr=6.648e-06, gnorm=1.729, train_wall=5, gb_free=10.6, wall=3539
2024-07-20 10:47:41 | INFO | train_inner | epoch 030:      5 / 19 loss=6.69, nll_loss=3.389, ppl=10.48, wps=2097.8, ups=0.33, wpb=6404.5, bsz=280, num_updates=556, lr=6.672e-06, gnorm=1.568, train_wall=6, gb_free=11.2, wall=3545
2024-07-20 10:47:44 | INFO | train_inner | epoch 030:      7 / 19 loss=6.913, nll_loss=3.69, ppl=12.91, wps=1765.4, ups=0.51, wpb=3458, bsz=165.5, num_updates=558, lr=6.696e-06, gnorm=2.91, train_wall=4, gb_free=17, wall=3549
2024-07-20 10:47:50 | INFO | train_inner | epoch 030:      9 / 19 loss=6.669, nll_loss=3.373, ppl=10.36, wps=1801.6, ups=0.34, wpb=5298.5, bsz=276, num_updates=560, lr=6.72e-06, gnorm=1.857, train_wall=6, gb_free=11.1, wall=3555
2024-07-20 10:47:56 | INFO | train_inner | epoch 030:     11 / 19 loss=6.633, nll_loss=3.297, ppl=9.83, wps=1523.7, ups=0.36, wpb=4190.5, bsz=176, num_updates=562, lr=6.744e-06, gnorm=2.194, train_wall=5, gb_free=11.6, wall=3560
2024-07-20 10:48:01 | INFO | train_inner | epoch 030:     13 / 19 loss=6.683, nll_loss=3.362, ppl=10.28, wps=2166.1, ups=0.39, wpb=5534.5, bsz=280, num_updates=564, lr=6.768e-06, gnorm=1.933, train_wall=5, gb_free=11.4, wall=3565
2024-07-20 10:48:06 | INFO | train_inner | epoch 030:     15 / 19 loss=6.807, nll_loss=3.517, ppl=11.45, wps=1912.3, ups=0.38, wpb=5052.5, bsz=208, num_updates=566, lr=6.792e-06, gnorm=2.029, train_wall=5, gb_free=12.6, wall=3570
2024-07-20 10:48:11 | INFO | train_inner | epoch 030:     17 / 19 loss=6.82, nll_loss=3.54, ppl=11.63, wps=1871, ups=0.41, wpb=4539.5, bsz=188, num_updates=568, lr=6.816e-06, gnorm=1.879, train_wall=5, gb_free=11.6, wall=3575
2024-07-20 10:48:16 | INFO | train_inner | epoch 030:     19 / 19 loss=6.719, nll_loss=3.417, ppl=10.68, wps=1647.5, ups=0.43, wpb=3810.5, bsz=152, num_updates=570, lr=6.84e-06, gnorm=2.002, train_wall=5, gb_free=16.2, wall=3580
2024-07-20 10:48:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45586.28125Mb; avail=209443.80078125Mb
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000620
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45586.28125Mb; avail=209443.80078125Mb
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002255
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45586.28125Mb; avail=209443.80078125Mb
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001838
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005084
2024-07-20 10:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45586.27734375Mb; avail=209443.80078125Mb
2024-07-20 10:48:18 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.646 | nll_loss 3.04 | ppl 8.22 | wps 4185.5 | wpb 1665.6 | bsz 74.4 | num_updates 570 | best_loss 6.646
2024-07-20 10:48:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 570 updates
2024-07-20 10:48:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:49:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:49:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 30 @ 570 updates, score 6.646) (writing took 67.3743579920847 seconds)
2024-07-20 10:49:26 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-07-20 10:49:26 | INFO | train | epoch 030 | loss 6.726 | nll_loss 3.426 | ppl 10.75 | wps 776.6 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 570 | lr 6.84e-06 | gnorm 1.989 | train_wall 49 | gb_free 16.2 | wall 3650
2024-07-20 10:49:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:49:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:49:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:49:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000773
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=76421.54296875Mb; avail=178608.45703125Mb
2024-07-20 10:49:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000141
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000796
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=76421.54296875Mb; avail=178608.45703125Mb
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=76421.54296875Mb; avail=178608.45703125Mb
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000223
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001359
2024-07-20 10:49:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=76421.54296875Mb; avail=178608.45703125Mb
2024-07-20 10:49:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:49:26 | INFO | fairseq.trainer | begin training epoch 31
2024-07-20 10:49:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:49:31 | INFO | train_inner | epoch 031:      2 / 19 loss=6.578, nll_loss=3.248, ppl=9.5, wps=157.6, ups=0.03, wpb=5940.5, bsz=276, num_updates=572, lr=6.864e-06, gnorm=1.748, train_wall=5, gb_free=11, wall=3655
2024-07-20 10:49:36 | INFO | train_inner | epoch 031:      4 / 19 loss=6.624, nll_loss=3.308, ppl=9.9, wps=1703, ups=0.38, wpb=4539.5, bsz=236, num_updates=574, lr=6.888e-06, gnorm=2.006, train_wall=5, gb_free=15.2, wall=3661
2024-07-20 10:49:41 | INFO | train_inner | epoch 031:      6 / 19 loss=6.804, nll_loss=3.526, ppl=11.52, wps=2123.8, ups=0.41, wpb=5186.5, bsz=244, num_updates=576, lr=6.912e-06, gnorm=1.868, train_wall=5, gb_free=13.5, wall=3665
2024-07-20 10:49:47 | INFO | train_inner | epoch 031:      8 / 19 loss=6.613, nll_loss=3.288, ppl=9.77, wps=2145.2, ups=0.35, wpb=6132.5, bsz=276, num_updates=578, lr=6.936e-06, gnorm=1.826, train_wall=6, gb_free=11.8, wall=3671
2024-07-20 10:49:53 | INFO | train_inner | epoch 031:     10 / 19 loss=6.613, nll_loss=3.261, ppl=9.58, wps=1876.5, ups=0.37, wpb=5138.5, bsz=224, num_updates=580, lr=6.96e-06, gnorm=1.777, train_wall=5, gb_free=12.3, wall=3677
2024-07-20 10:49:58 | INFO | train_inner | epoch 031:     12 / 19 loss=6.748, nll_loss=3.434, ppl=10.81, wps=1387, ups=0.35, wpb=3929.5, bsz=168, num_updates=582, lr=6.984e-06, gnorm=2.064, train_wall=6, gb_free=12.2, wall=3682
2024-07-20 10:50:02 | INFO | train_inner | epoch 031:     14 / 19 loss=6.635, nll_loss=3.292, ppl=9.79, wps=2104.7, ups=0.48, wpb=4343, bsz=193.5, num_updates=584, lr=7.008e-06, gnorm=2.014, train_wall=4, gb_free=12.3, wall=3686
2024-07-20 10:50:08 | INFO | train_inner | epoch 031:     16 / 19 loss=6.767, nll_loss=3.482, ppl=11.18, wps=1707.9, ups=0.36, wpb=4724, bsz=180, num_updates=586, lr=7.032e-06, gnorm=1.721, train_wall=6, gb_free=12, wall=3692
2024-07-20 10:50:13 | INFO | train_inner | epoch 031:     18 / 19 loss=6.731, nll_loss=3.443, ppl=10.88, wps=1968.5, ups=0.42, wpb=4654.5, bsz=204, num_updates=588, lr=7.056e-06, gnorm=1.824, train_wall=5, gb_free=11.1, wall=3697
2024-07-20 10:50:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67788.171875Mb; avail=187225.76953125Mb
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000558
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67788.6640625Mb; avail=187225.27734375Mb
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002087
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67788.6640625Mb; avail=187225.27734375Mb
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001847
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004823
2024-07-20 10:50:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67788.6640625Mb; avail=187225.27734375Mb
2024-07-20 10:50:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.594 | nll_loss 2.977 | ppl 7.87 | wps 3108.4 | wpb 1665.6 | bsz 74.4 | num_updates 589 | best_loss 6.594
2024-07-20 10:50:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 589 updates
2024-07-20 10:50:17 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:51:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:51:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 31 @ 589 updates, score 6.594) (writing took 72.59040192887187 seconds)
2024-07-20 10:51:30 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-07-20 10:51:30 | INFO | train | epoch 031 | loss 6.665 | nll_loss 3.348 | ppl 10.18 | wps 743.3 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 589 | lr 7.068e-06 | gnorm 1.903 | train_wall 48 | gb_free 15.6 | wall 3774
2024-07-20 10:51:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:51:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:51:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:51:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000665
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60946.51171875Mb; avail=194067.58203125Mb
2024-07-20 10:51:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000093
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000787
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60946.51171875Mb; avail=194067.58203125Mb
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60946.51171875Mb; avail=194067.58203125Mb
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000230
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001347
2024-07-20 10:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60946.51171875Mb; avail=194067.08984375Mb
2024-07-20 10:51:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:51:30 | INFO | fairseq.trainer | begin training epoch 32
2024-07-20 10:51:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:51:34 | INFO | train_inner | epoch 032:      1 / 19 loss=6.544, nll_loss=3.206, ppl=9.23, wps=100.9, ups=0.02, wpb=4084, bsz=184, num_updates=590, lr=7.08e-06, gnorm=2.079, train_wall=5, gb_free=11.7, wall=3778
2024-07-20 10:51:39 | INFO | train_inner | epoch 032:      3 / 19 loss=6.626, nll_loss=3.314, ppl=9.94, wps=2312.9, ups=0.38, wpb=6089, bsz=264, num_updates=592, lr=7.104e-06, gnorm=1.78, train_wall=5, gb_free=12.9, wall=3783
2024-07-20 10:51:44 | INFO | train_inner | epoch 032:      5 / 19 loss=6.467, nll_loss=3.089, ppl=8.51, wps=1756, ups=0.37, wpb=4785, bsz=256, num_updates=594, lr=7.128e-06, gnorm=1.733, train_wall=5, gb_free=12.5, wall=3788
2024-07-20 10:51:50 | INFO | train_inner | epoch 032:      7 / 19 loss=6.569, nll_loss=3.214, ppl=9.28, wps=2014.9, ups=0.37, wpb=5408.5, bsz=216, num_updates=596, lr=7.152e-06, gnorm=1.648, train_wall=5, gb_free=13.7, wall=3794
2024-07-20 10:51:55 | INFO | train_inner | epoch 032:      9 / 19 loss=6.468, nll_loss=3.07, ppl=8.4, wps=1631, ups=0.36, wpb=4470.5, bsz=236, num_updates=598, lr=7.176e-06, gnorm=2.065, train_wall=5, gb_free=14.1, wall=3799
2024-07-20 10:52:01 | INFO | train_inner | epoch 032:     11 / 19 loss=6.509, nll_loss=3.136, ppl=8.79, wps=1867.2, ups=0.36, wpb=5212.5, bsz=252, num_updates=600, lr=7.2e-06, gnorm=1.865, train_wall=6, gb_free=12.9, wall=3805
2024-07-20 10:52:06 | INFO | train_inner | epoch 032:     13 / 19 loss=6.6, nll_loss=3.271, ppl=9.65, wps=2003.2, ups=0.38, wpb=5288.5, bsz=224, num_updates=602, lr=7.224e-06, gnorm=1.752, train_wall=5, gb_free=11, wall=3810
2024-07-20 10:52:11 | INFO | train_inner | epoch 032:     15 / 19 loss=6.701, nll_loss=3.419, ppl=10.7, wps=1856.4, ups=0.38, wpb=4875.5, bsz=212, num_updates=604, lr=7.248e-06, gnorm=1.903, train_wall=5, gb_free=11.1, wall=3815
2024-07-20 10:52:16 | INFO | train_inner | epoch 032:     17 / 19 loss=6.872, nll_loss=3.637, ppl=12.44, wps=1927.1, ups=0.39, wpb=4952, bsz=172, num_updates=606, lr=7.272e-06, gnorm=1.854, train_wall=5, gb_free=12.6, wall=3821
2024-07-20 10:52:20 | INFO | train_inner | epoch 032:     19 / 19 loss=6.516, nll_loss=3.174, ppl=9.03, wps=1653.3, ups=0.61, wpb=2708, bsz=145.5, num_updates=608, lr=7.296e-06, gnorm=2.438, train_wall=3, gb_free=16.9, wall=3824
2024-07-20 10:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=53309.203125Mb; avail=201704.671875Mb
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53309.203125Mb; avail=201704.671875Mb
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002075
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53309.203125Mb; avail=201704.671875Mb
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001866
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004816
2024-07-20 10:52:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53309.6953125Mb; avail=201704.1796875Mb
2024-07-20 10:52:23 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.542 | nll_loss 2.905 | ppl 7.49 | wps 3462.5 | wpb 1665.6 | bsz 74.4 | num_updates 608 | best_loss 6.542
2024-07-20 10:52:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 608 updates
2024-07-20 10:52:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:53:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:53:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 32 @ 608 updates, score 6.542) (writing took 65.92201292980462 seconds)
2024-07-20 10:53:29 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-07-20 10:53:29 | INFO | train | epoch 032 | loss 6.6 | nll_loss 3.268 | ppl 9.63 | wps 779.3 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 608 | lr 7.296e-06 | gnorm 1.882 | train_wall 49 | gb_free 16.9 | wall 3893
2024-07-20 10:53:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:53:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:53:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:53:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000789
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=84148.6484375Mb; avail=170856.546875Mb
2024-07-20 10:53:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000094
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000768
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=84148.6484375Mb; avail=170856.515625Mb
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000038
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=84149.140625Mb; avail=170857.0078125Mb
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000237
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001339
2024-07-20 10:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=84148.6484375Mb; avail=170856.4921875Mb
2024-07-20 10:53:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:53:29 | INFO | fairseq.trainer | begin training epoch 33
2024-07-20 10:53:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:53:35 | INFO | train_inner | epoch 033:      2 / 19 loss=6.78, nll_loss=3.501, ppl=11.32, wps=152.5, ups=0.03, wpb=5732.5, bsz=228, num_updates=610, lr=7.32e-06, gnorm=1.604, train_wall=6, gb_free=11.7, wall=3899
2024-07-20 10:53:40 | INFO | train_inner | epoch 033:      4 / 19 loss=6.565, nll_loss=3.194, ppl=9.15, wps=2086.5, ups=0.4, wpb=5244.5, bsz=208, num_updates=612, lr=7.344e-06, gnorm=1.773, train_wall=5, gb_free=11.6, wall=3904
2024-07-20 10:53:45 | INFO | train_inner | epoch 033:      6 / 19 loss=6.479, nll_loss=3.095, ppl=8.54, wps=1774.9, ups=0.37, wpb=4793, bsz=240, num_updates=614, lr=7.368e-06, gnorm=1.71, train_wall=5, gb_free=12.9, wall=3909
2024-07-20 10:53:50 | INFO | train_inner | epoch 033:      8 / 19 loss=6.498, nll_loss=3.135, ppl=8.79, wps=2182.8, ups=0.43, wpb=5131.5, bsz=232, num_updates=616, lr=7.392e-06, gnorm=1.805, train_wall=5, gb_free=13.8, wall=3914
2024-07-20 10:53:55 | INFO | train_inner | epoch 033:     10 / 19 loss=6.534, nll_loss=3.185, ppl=9.1, wps=2082.9, ups=0.38, wpb=5507.5, bsz=276, num_updates=618, lr=7.416e-06, gnorm=1.848, train_wall=5, gb_free=11.2, wall=3919
2024-07-20 10:54:01 | INFO | train_inner | epoch 033:     12 / 19 loss=6.477, nll_loss=3.106, ppl=8.61, wps=1736.8, ups=0.34, wpb=5130, bsz=228, num_updates=620, lr=7.44e-06, gnorm=1.813, train_wall=6, gb_free=12.1, wall=3925
2024-07-20 10:54:06 | INFO | train_inner | epoch 033:     14 / 19 loss=6.619, nll_loss=3.293, ppl=9.8, wps=1568.2, ups=0.45, wpb=3479.5, bsz=137.5, num_updates=622, lr=7.464e-06, gnorm=2.468, train_wall=4, gb_free=13.6, wall=3930
2024-07-20 10:54:11 | INFO | train_inner | epoch 033:     16 / 19 loss=6.449, nll_loss=3.072, ppl=8.41, wps=1915.1, ups=0.36, wpb=5388, bsz=260, num_updates=624, lr=7.488e-06, gnorm=1.658, train_wall=6, gb_free=12, wall=3935
2024-07-20 10:54:16 | INFO | train_inner | epoch 033:     18 / 19 loss=6.395, nll_loss=2.992, ppl=7.95, wps=1949.2, ups=0.42, wpb=4592, bsz=228, num_updates=626, lr=7.512e-06, gnorm=1.949, train_wall=5, gb_free=13, wall=3940
2024-07-20 10:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=75505.0234375Mb; avail=179524.9296875Mb
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000563
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75505.0234375Mb; avail=179524.9296875Mb
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002118
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75505.0234375Mb; avail=179524.9296875Mb
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001859
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004866
2024-07-20 10:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75505.0234375Mb; avail=179524.9296875Mb
2024-07-20 10:54:21 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.52 | nll_loss 2.869 | ppl 7.3 | wps 3120 | wpb 1665.6 | bsz 74.4 | num_updates 627 | best_loss 6.52
2024-07-20 10:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 627 updates
2024-07-20 10:54:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:55:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:55:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 33 @ 627 updates, score 6.52) (writing took 79.92605394590646 seconds)
2024-07-20 10:55:41 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-07-20 10:55:41 | INFO | train | epoch 033 | loss 6.538 | nll_loss 3.181 | ppl 9.07 | wps 701.2 | ups 0.14 | wpb 4866.4 | bsz 219.1 | num_updates 627 | lr 7.524e-06 | gnorm 1.874 | train_wall 48 | gb_free 16.9 | wall 4025
2024-07-20 10:55:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:55:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:55:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:55:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000674
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58979.56640625Mb; avail=195514.5625Mb
2024-07-20 10:55:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000086
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000738
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58979.56640625Mb; avail=195514.5625Mb
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000040
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58979.56640625Mb; avail=195514.5625Mb
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000220
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001289
2024-07-20 10:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58979.56640625Mb; avail=195514.5625Mb
2024-07-20 10:55:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:55:41 | INFO | fairseq.trainer | begin training epoch 34
2024-07-20 10:55:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:55:44 | INFO | train_inner | epoch 034:      1 / 19 loss=6.462, nll_loss=3.08, ppl=8.46, wps=100.1, ups=0.02, wpb=4418, bsz=188, num_updates=628, lr=7.536e-06, gnorm=1.892, train_wall=5, gb_free=11.1, wall=4028
2024-07-20 10:55:50 | INFO | train_inner | epoch 034:      3 / 19 loss=6.436, nll_loss=3.064, ppl=8.36, wps=1995.1, ups=0.34, wpb=5903.5, bsz=272, num_updates=630, lr=7.56e-06, gnorm=1.63, train_wall=6, gb_free=12.3, wall=4034
2024-07-20 10:55:56 | INFO | train_inner | epoch 034:      5 / 19 loss=6.411, nll_loss=3.032, ppl=8.18, wps=1934, ups=0.34, wpb=5675.5, bsz=268, num_updates=632, lr=7.584e-06, gnorm=1.719, train_wall=6, gb_free=11, wall=4040
2024-07-20 10:56:02 | INFO | train_inner | epoch 034:      7 / 19 loss=6.503, nll_loss=3.147, ppl=8.86, wps=1890.8, ups=0.36, wpb=5215.5, bsz=212, num_updates=634, lr=7.608e-06, gnorm=1.924, train_wall=6, gb_free=12.7, wall=4046
2024-07-20 10:56:06 | INFO | train_inner | epoch 034:      9 / 19 loss=6.731, nll_loss=3.441, ppl=10.86, wps=1798.5, ups=0.44, wpb=4088.5, bsz=160, num_updates=636, lr=7.632e-06, gnorm=2.099, train_wall=5, gb_free=12.5, wall=4050
2024-07-20 10:56:12 | INFO | train_inner | epoch 034:     11 / 19 loss=6.709, nll_loss=3.415, ppl=10.66, wps=1804.2, ups=0.37, wpb=4918.5, bsz=168, num_updates=638, lr=7.656e-06, gnorm=1.763, train_wall=5, gb_free=12.5, wall=4056
2024-07-20 10:56:16 | INFO | train_inner | epoch 034:     13 / 19 loss=6.454, nll_loss=3.078, ppl=8.44, wps=1586.4, ups=0.48, wpb=3290.5, bsz=173.5, num_updates=640, lr=7.68e-06, gnorm=2.24, train_wall=4, gb_free=15.7, wall=4060
2024-07-20 10:56:21 | INFO | train_inner | epoch 034:     15 / 19 loss=6.432, nll_loss=3.079, ppl=8.45, wps=1796.7, ups=0.35, wpb=5188.5, bsz=280, num_updates=642, lr=7.704e-06, gnorm=2.068, train_wall=6, gb_free=12.5, wall=4066
2024-07-20 10:56:27 | INFO | train_inner | epoch 034:     17 / 19 loss=6.469, nll_loss=3.106, ppl=8.61, wps=2039, ups=0.39, wpb=5202, bsz=252, num_updates=644, lr=7.728e-06, gnorm=1.859, train_wall=5, gb_free=12.7, wall=4071
2024-07-20 10:56:30 | INFO | train_inner | epoch 034:     19 / 19 loss=6.545, nll_loss=3.185, ppl=9.1, wps=1978, ups=0.56, wpb=3563, bsz=152, num_updates=646, lr=7.752e-06, gnorm=2.764, train_wall=4, gb_free=19.8, wall=4074
2024-07-20 10:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=52372.7109375Mb; avail=202121.3359375Mb
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000561
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52372.7109375Mb; avail=202121.3359375Mb
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002111
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52372.7109375Mb; avail=202121.3359375Mb
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001879
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004884
2024-07-20 10:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52373.203125Mb; avail=202120.84375Mb
2024-07-20 10:56:33 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.51 | nll_loss 2.828 | ppl 7.1 | wps 4192.7 | wpb 1665.6 | bsz 74.4 | num_updates 646 | best_loss 6.51
2024-07-20 10:56:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 646 updates
2024-07-20 10:56:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:57:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 34 @ 646 updates, score 6.51) (writing took 62.53532881103456 seconds)
2024-07-20 10:57:35 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-07-20 10:57:35 | INFO | train | epoch 034 | loss 6.506 | nll_loss 3.152 | ppl 8.89 | wps 806.3 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 646 | lr 7.752e-06 | gnorm 1.977 | train_wall 49 | gb_free 19.8 | wall 4139
2024-07-20 10:57:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:57:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:57:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:57:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001853
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=82205.03515625Mb; avail=172816.7265625Mb
2024-07-20 10:57:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000115
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001266
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=82204.05078125Mb; avail=172818.15625Mb
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000062
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=82204.05078125Mb; avail=172818.640625Mb
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000376
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002189
2024-07-20 10:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=82203.55859375Mb; avail=172818.609375Mb
2024-07-20 10:57:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:57:35 | INFO | fairseq.trainer | begin training epoch 35
2024-07-20 10:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:57:41 | INFO | train_inner | epoch 035:      2 / 19 loss=6.342, nll_loss=2.917, ppl=7.55, wps=159.6, ups=0.03, wpb=5667, bsz=268, num_updates=648, lr=7.776e-06, gnorm=1.701, train_wall=6, gb_free=11.7, wall=4145
2024-07-20 10:57:47 | INFO | train_inner | epoch 035:      4 / 19 loss=6.386, nll_loss=2.968, ppl=7.82, wps=2297.4, ups=0.37, wpb=6248, bsz=288, num_updates=650, lr=7.8e-06, gnorm=1.504, train_wall=5, gb_free=10.9, wall=4151
2024-07-20 10:57:52 | INFO | train_inner | epoch 035:      6 / 19 loss=6.543, nll_loss=3.184, ppl=9.09, wps=1546.1, ups=0.36, wpb=4336.5, bsz=164, num_updates=652, lr=7.824e-06, gnorm=1.813, train_wall=6, gb_free=12.9, wall=4156
2024-07-20 10:57:57 | INFO | train_inner | epoch 035:      8 / 19 loss=6.482, nll_loss=3.149, ppl=8.87, wps=2329.5, ups=0.4, wpb=5774.5, bsz=236, num_updates=654, lr=7.848e-06, gnorm=1.71, train_wall=5, gb_free=12.5, wall=4161
2024-07-20 10:58:03 | INFO | train_inner | epoch 035:     10 / 19 loss=6.373, nll_loss=2.986, ppl=7.92, wps=1749.4, ups=0.35, wpb=4938.5, bsz=228, num_updates=656, lr=7.872e-06, gnorm=1.793, train_wall=6, gb_free=12.3, wall=4167
2024-07-20 10:58:07 | INFO | train_inner | epoch 035:     12 / 19 loss=6.451, nll_loss=3.077, ppl=8.44, wps=1828.5, ups=0.47, wpb=3904, bsz=193.5, num_updates=658, lr=7.896e-06, gnorm=2.129, train_wall=4, gb_free=13.2, wall=4171
2024-07-20 10:58:12 | INFO | train_inner | epoch 035:     14 / 19 loss=6.224, nll_loss=2.781, ppl=6.87, wps=1735.4, ups=0.42, wpb=4159, bsz=220, num_updates=660, lr=7.92e-06, gnorm=2.057, train_wall=5, gb_free=12, wall=4176
2024-07-20 10:58:17 | INFO | train_inner | epoch 035:     16 / 19 loss=6.593, nll_loss=3.254, ppl=9.54, wps=2101.3, ups=0.38, wpb=5494, bsz=244, num_updates=662, lr=7.944e-06, gnorm=1.709, train_wall=5, gb_free=11.2, wall=4181
2024-07-20 10:58:23 | INFO | train_inner | epoch 035:     18 / 19 loss=6.551, nll_loss=3.2, ppl=9.19, wps=1728.8, ups=0.36, wpb=4769.5, bsz=184, num_updates=664, lr=7.968e-06, gnorm=1.98, train_wall=6, gb_free=12.6, wall=4187
2024-07-20 10:58:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=75537.19140625Mb; avail=179476.7734375Mb
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75537.68359375Mb; avail=179476.28125Mb
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002112
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75537.68359375Mb; avail=179476.28125Mb
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001847
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004831
2024-07-20 10:58:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=75537.68359375Mb; avail=179476.28125Mb
2024-07-20 10:58:27 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.441 | nll_loss 2.78 | ppl 6.87 | wps 3191.1 | wpb 1665.6 | bsz 74.4 | num_updates 665 | best_loss 6.441
2024-07-20 10:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 665 updates
2024-07-20 10:58:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:59:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 10:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 35 @ 665 updates, score 6.441) (writing took 74.3848823660519 seconds)
2024-07-20 10:59:42 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-07-20 10:59:42 | INFO | train | epoch 035 | loss 6.433 | nll_loss 3.05 | ppl 8.28 | wps 731.4 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 665 | lr 7.98e-06 | gnorm 1.875 | train_wall 48 | gb_free 16.6 | wall 4266
2024-07-20 10:59:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 10:59:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 10:59:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 10:59:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.002635
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=53739.0703125Mb; avail=201279.0625Mb
2024-07-20 10:59:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000113
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001120
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53739.0703125Mb; avail=201279.0625Mb
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000055
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53739.0703125Mb; avail=201279.0625Mb
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000334
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001996
2024-07-20 10:59:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53739.0703125Mb; avail=201279.0625Mb
2024-07-20 10:59:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 10:59:42 | INFO | fairseq.trainer | begin training epoch 36
2024-07-20 10:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 10:59:44 | INFO | train_inner | epoch 036:      1 / 19 loss=6.269, nll_loss=2.841, ppl=7.16, wps=94.7, ups=0.02, wpb=3868, bsz=184, num_updates=666, lr=7.992e-06, gnorm=2.153, train_wall=4, gb_free=11.3, wall=4269
2024-07-20 10:59:49 | INFO | train_inner | epoch 036:      3 / 19 loss=6.54, nll_loss=3.194, ppl=9.15, wps=1538.9, ups=0.42, wpb=3676.5, bsz=145.5, num_updates=668, lr=8.016e-06, gnorm=2.069, train_wall=5, gb_free=17.3, wall=4273
2024-07-20 10:59:55 | INFO | train_inner | epoch 036:      5 / 19 loss=6.335, nll_loss=2.933, ppl=7.64, wps=1950.8, ups=0.33, wpb=5970.5, bsz=264, num_updates=670, lr=8.04e-06, gnorm=1.566, train_wall=6, gb_free=12.3, wall=4279
2024-07-20 11:00:01 | INFO | train_inner | epoch 036:      7 / 19 loss=6.284, nll_loss=2.86, ppl=7.26, wps=1871.1, ups=0.34, wpb=5553.5, bsz=232, num_updates=672, lr=8.064e-06, gnorm=1.59, train_wall=6, gb_free=12.3, wall=4285
2024-07-20 11:00:06 | INFO | train_inner | epoch 036:      9 / 19 loss=6.53, nll_loss=3.176, ppl=9.04, wps=1876.4, ups=0.44, wpb=4233.5, bsz=192, num_updates=674, lr=8.088e-06, gnorm=2.089, train_wall=5, gb_free=12.4, wall=4290
2024-07-20 11:00:11 | INFO | train_inner | epoch 036:     11 / 19 loss=6.557, nll_loss=3.221, ppl=9.32, wps=1903.6, ups=0.39, wpb=4863.5, bsz=212, num_updates=676, lr=8.112e-06, gnorm=1.778, train_wall=5, gb_free=13.5, wall=4295
2024-07-20 11:00:16 | INFO | train_inner | epoch 036:     13 / 19 loss=6.382, nll_loss=2.997, ppl=7.98, wps=1910.8, ups=0.37, wpb=5219.5, bsz=216, num_updates=678, lr=8.136e-06, gnorm=1.735, train_wall=5, gb_free=12.8, wall=4300
2024-07-20 11:00:22 | INFO | train_inner | epoch 036:     15 / 19 loss=6.188, nll_loss=2.748, ppl=6.72, wps=1639.2, ups=0.35, wpb=4661, bsz=248, num_updates=680, lr=8.16e-06, gnorm=1.621, train_wall=6, gb_free=12.3, wall=4306
2024-07-20 11:00:28 | INFO | train_inner | epoch 036:     17 / 19 loss=6.301, nll_loss=2.885, ppl=7.39, wps=1941.4, ups=0.35, wpb=5557, bsz=284, num_updates=682, lr=8.184e-06, gnorm=1.614, train_wall=6, gb_free=11.5, wall=4312
2024-07-20 11:00:32 | INFO | train_inner | epoch 036:     19 / 19 loss=6.441, nll_loss=3.069, ppl=8.39, wps=1667.8, ups=0.47, wpb=3568, bsz=160, num_updates=684, lr=8.208e-06, gnorm=1.966, train_wall=4, gb_free=15.6, wall=4316
2024-07-20 11:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=48297.59375Mb; avail=206720.45703125Mb
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000555
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48297.59375Mb; avail=206720.45703125Mb
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002073
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48297.59375Mb; avail=206720.45703125Mb
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001856
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004806
2024-07-20 11:00:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48297.59375Mb; avail=206720.45703125Mb
2024-07-20 11:00:34 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.406 | nll_loss 2.742 | ppl 6.69 | wps 4184.5 | wpb 1665.6 | bsz 74.4 | num_updates 684 | best_loss 6.406
2024-07-20 11:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 684 updates
2024-07-20 11:00:34 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:01:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 36 @ 684 updates, score 6.406) (writing took 59.29211060889065 seconds)
2024-07-20 11:01:34 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-07-20 11:01:34 | INFO | train | epoch 036 | loss 6.38 | nll_loss 2.989 | ppl 7.94 | wps 824.5 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 684 | lr 8.208e-06 | gnorm 1.765 | train_wall 50 | gb_free 15.6 | wall 4378
2024-07-20 11:01:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:01:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:01:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:01:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000793
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=72628.6171875Mb; avail=182389.85546875Mb
2024-07-20 11:01:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000091
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000758
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72628.6171875Mb; avail=182389.36328125Mb
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72628.125Mb; avail=182389.85546875Mb
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000232
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001323
2024-07-20 11:01:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=72627.6328125Mb; avail=182390.34765625Mb
2024-07-20 11:01:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:01:34 | INFO | fairseq.trainer | begin training epoch 37
2024-07-20 11:01:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:01:39 | INFO | train_inner | epoch 037:      2 / 19 loss=6.196, nll_loss=2.762, ppl=6.78, wps=124.9, ups=0.03, wpb=4158.5, bsz=228, num_updates=686, lr=8.232e-06, gnorm=1.758, train_wall=5, gb_free=12.8, wall=4383
2024-07-20 11:01:44 | INFO | train_inner | epoch 037:      4 / 19 loss=6.358, nll_loss=2.968, ppl=7.83, wps=1803.1, ups=0.34, wpb=5254.5, bsz=224, num_updates=688, lr=8.256e-06, gnorm=1.8, train_wall=6, gb_free=12.3, wall=4389
2024-07-20 11:01:49 | INFO | train_inner | epoch 037:      6 / 19 loss=6.293, nll_loss=2.874, ppl=7.33, wps=2070.3, ups=0.46, wpb=4503, bsz=197.5, num_updates=690, lr=8.28e-06, gnorm=1.755, train_wall=4, gb_free=15.2, wall=4393
2024-07-20 11:01:55 | INFO | train_inner | epoch 037:      8 / 19 loss=6.319, nll_loss=2.899, ppl=7.46, wps=1845, ups=0.33, wpb=5561.5, bsz=240, num_updates=692, lr=8.304e-06, gnorm=1.51, train_wall=6, gb_free=10.8, wall=4399
2024-07-20 11:02:00 | INFO | train_inner | epoch 037:     10 / 19 loss=6.562, nll_loss=3.214, ppl=9.28, wps=1912.6, ups=0.4, wpb=4775, bsz=192, num_updates=694, lr=8.328e-06, gnorm=1.833, train_wall=5, gb_free=11.8, wall=4404
2024-07-20 11:02:05 | INFO | train_inner | epoch 037:     12 / 19 loss=6.639, nll_loss=3.324, ppl=10.01, wps=1481.7, ups=0.39, wpb=3808.5, bsz=136, num_updates=696, lr=8.352e-06, gnorm=1.981, train_wall=5, gb_free=11.9, wall=4409
2024-07-20 11:02:11 | INFO | train_inner | epoch 037:     14 / 19 loss=6.187, nll_loss=2.76, ppl=6.78, wps=1890.8, ups=0.33, wpb=5766.5, bsz=288, num_updates=698, lr=8.376e-06, gnorm=1.563, train_wall=6, gb_free=11.7, wall=4415
2024-07-20 11:02:16 | INFO | train_inner | epoch 037:     16 / 19 loss=6.281, nll_loss=2.887, ppl=7.4, wps=2123.3, ups=0.39, wpb=5428, bsz=284, num_updates=700, lr=8.4e-06, gnorm=1.823, train_wall=5, gb_free=13.5, wall=4420
2024-07-20 11:02:22 | INFO | train_inner | epoch 037:     18 / 19 loss=6.205, nll_loss=2.767, ppl=6.81, wps=1824.2, ups=0.33, wpb=5557, bsz=240, num_updates=702, lr=8.424e-06, gnorm=1.672, train_wall=6, gb_free=11.2, wall=4426
2024-07-20 11:02:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=67190.68359375Mb; avail=187827.2421875Mb
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67190.68359375Mb; avail=187827.2421875Mb
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002125
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67190.68359375Mb; avail=187827.2421875Mb
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001865
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004888
2024-07-20 11:02:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=67191.17578125Mb; avail=187826.75Mb
2024-07-20 11:02:26 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.389 | nll_loss 2.688 | ppl 6.44 | wps 4185.1 | wpb 1665.6 | bsz 74.4 | num_updates 703 | best_loss 6.389
2024-07-20 11:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 703 updates
2024-07-20 11:02:26 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 37 @ 703 updates, score 6.389) (writing took 73.52446302794851 seconds)
2024-07-20 11:03:40 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-07-20 11:03:40 | INFO | train | epoch 037 | loss 6.328 | nll_loss 2.927 | ppl 7.61 | wps 735.1 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 703 | lr 8.436e-06 | gnorm 1.784 | train_wall 50 | gb_free 17.3 | wall 4504
2024-07-20 11:03:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:03:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:03:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:03:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000684
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45289.89453125Mb; avail=209736.12109375Mb
2024-07-20 11:03:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000721
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45289.89453125Mb; avail=209736.12109375Mb
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000736
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45289.89453125Mb; avail=209736.12109375Mb
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000232
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002001
2024-07-20 11:03:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45289.89453125Mb; avail=209736.12109375Mb
2024-07-20 11:03:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:03:40 | INFO | fairseq.trainer | begin training epoch 38
2024-07-20 11:03:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:03:42 | INFO | train_inner | epoch 038:      1 / 19 loss=6.28, nll_loss=2.846, ppl=7.19, wps=98.9, ups=0.03, wpb=3954, bsz=188, num_updates=704, lr=8.448e-06, gnorm=2.041, train_wall=4, gb_free=12.7, wall=4506
2024-07-20 11:03:48 | INFO | train_inner | epoch 038:      3 / 19 loss=6.187, nll_loss=2.731, ppl=6.64, wps=1779.4, ups=0.35, wpb=5044.5, bsz=240, num_updates=706, lr=8.472e-06, gnorm=1.636, train_wall=6, gb_free=12.8, wall=4512
2024-07-20 11:03:53 | INFO | train_inner | epoch 038:      5 / 19 loss=6.625, nll_loss=3.292, ppl=9.79, wps=1346.1, ups=0.4, wpb=3375.5, bsz=104, num_updates=708, lr=8.496e-06, gnorm=2.102, train_wall=5, gb_free=14.4, wall=4517
2024-07-20 11:03:59 | INFO | train_inner | epoch 038:      7 / 19 loss=6.287, nll_loss=2.87, ppl=7.31, wps=1779, ups=0.35, wpb=5134, bsz=228, num_updates=710, lr=8.52e-06, gnorm=1.795, train_wall=6, gb_free=13, wall=4523
2024-07-20 11:04:04 | INFO | train_inner | epoch 038:      9 / 19 loss=6.22, nll_loss=2.788, ppl=6.91, wps=1951.3, ups=0.4, wpb=4922, bsz=240, num_updates=712, lr=8.544e-06, gnorm=1.72, train_wall=5, gb_free=12.9, wall=4528
2024-07-20 11:04:09 | INFO | train_inner | epoch 038:     11 / 19 loss=6.257, nll_loss=2.836, ppl=7.14, wps=1620.6, ups=0.41, wpb=3941.5, bsz=181.5, num_updates=714, lr=8.568e-06, gnorm=1.914, train_wall=5, gb_free=11, wall=4533
2024-07-20 11:04:15 | INFO | train_inner | epoch 038:     13 / 19 loss=6.225, nll_loss=2.792, ppl=6.92, wps=1936.4, ups=0.33, wpb=5806.5, bsz=276, num_updates=716, lr=8.592e-06, gnorm=1.705, train_wall=6, gb_free=12.2, wall=4539
2024-07-20 11:04:20 | INFO | train_inner | epoch 038:     15 / 19 loss=6.328, nll_loss=2.924, ppl=7.59, wps=2391.8, ups=0.39, wpb=6174, bsz=244, num_updates=718, lr=8.616e-06, gnorm=1.738, train_wall=5, gb_free=13.3, wall=4544
2024-07-20 11:04:26 | INFO | train_inner | epoch 038:     17 / 19 loss=6.308, nll_loss=2.891, ppl=7.42, wps=1667.1, ups=0.34, wpb=4922.5, bsz=228, num_updates=720, lr=8.64e-06, gnorm=1.839, train_wall=6, gb_free=12.7, wall=4550
2024-07-20 11:04:30 | INFO | train_inner | epoch 038:     19 / 19 loss=6.253, nll_loss=2.83, ppl=7.11, wps=2015.9, ups=0.46, wpb=4375, bsz=204, num_updates=722, lr=8.664e-06, gnorm=2.579, train_wall=4, gb_free=15.5, wall=4554
2024-07-20 11:04:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30559.546875Mb; avail=224466.50390625Mb
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000550
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30559.546875Mb; avail=224466.50390625Mb
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002098
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30559.546875Mb; avail=224466.50390625Mb
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001863
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004844
2024-07-20 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30559.546875Mb; avail=224466.50390625Mb
2024-07-20 11:04:33 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.333 | nll_loss 2.664 | ppl 6.34 | wps 3469 | wpb 1665.6 | bsz 74.4 | num_updates 722 | best_loss 6.333
2024-07-20 11:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 722 updates
2024-07-20 11:04:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:05:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:05:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 38 @ 722 updates, score 6.333) (writing took 63.39199478807859 seconds)
2024-07-20 11:05:36 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-07-20 11:05:36 | INFO | train | epoch 038 | loss 6.284 | nll_loss 2.865 | ppl 7.28 | wps 791.5 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 722 | lr 8.664e-06 | gnorm 1.876 | train_wall 50 | gb_free 15.5 | wall 4621
2024-07-20 11:05:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:05:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:05:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:05:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001128
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=53114.21875Mb; avail=201911.83203125Mb
2024-07-20 11:05:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000112
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001138
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53114.21875Mb; avail=201911.83203125Mb
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000050
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53114.21875Mb; avail=201911.83203125Mb
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000353
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002182
2024-07-20 11:05:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53114.21875Mb; avail=201911.83203125Mb
2024-07-20 11:05:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:05:36 | INFO | fairseq.trainer | begin training epoch 39
2024-07-20 11:05:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:05:42 | INFO | train_inner | epoch 039:      2 / 19 loss=6.401, nll_loss=3.027, ppl=8.15, wps=149.7, ups=0.03, wpb=5392, bsz=212, num_updates=724, lr=8.688e-06, gnorm=1.654, train_wall=5, gb_free=11.5, wall=4626
2024-07-20 11:05:48 | INFO | train_inner | epoch 039:      4 / 19 loss=6.223, nll_loss=2.803, ppl=6.98, wps=1643.1, ups=0.34, wpb=4779.5, bsz=200, num_updates=726, lr=8.712e-06, gnorm=1.723, train_wall=6, gb_free=12.3, wall=4632
2024-07-20 11:05:53 | INFO | train_inner | epoch 039:      6 / 19 loss=6.263, nll_loss=2.857, ppl=7.25, wps=1441.1, ups=0.37, wpb=3904, bsz=204, num_updates=728, lr=8.736e-06, gnorm=1.953, train_wall=5, gb_free=12.5, wall=4637
2024-07-20 11:05:59 | INFO | train_inner | epoch 039:      8 / 19 loss=6.209, nll_loss=2.756, ppl=6.75, wps=1777.9, ups=0.36, wpb=4888, bsz=212, num_updates=730, lr=8.76e-06, gnorm=2.051, train_wall=5, gb_free=10.9, wall=4643
2024-07-20 11:06:04 | INFO | train_inner | epoch 039:     10 / 19 loss=6.159, nll_loss=2.684, ppl=6.42, wps=2241.3, ups=0.39, wpb=5815.5, bsz=288, num_updates=732, lr=8.784e-06, gnorm=1.676, train_wall=5, gb_free=12.4, wall=4648
2024-07-20 11:06:08 | INFO | train_inner | epoch 039:     12 / 19 loss=6.329, nll_loss=2.886, ppl=7.39, wps=1997.7, ups=0.49, wpb=4103, bsz=165.5, num_updates=734, lr=8.808e-06, gnorm=1.933, train_wall=4, gb_free=17.5, wall=4652
2024-07-20 11:06:14 | INFO | train_inner | epoch 039:     14 / 19 loss=6.306, nll_loss=2.897, ppl=7.45, wps=1774.3, ups=0.35, wpb=5094.5, bsz=236, num_updates=736, lr=8.832e-06, gnorm=1.746, train_wall=6, gb_free=15.8, wall=4658
2024-07-20 11:06:19 | INFO | train_inner | epoch 039:     16 / 19 loss=6.189, nll_loss=2.762, ppl=6.78, wps=2239.6, ups=0.39, wpb=5738.5, bsz=260, num_updates=738, lr=8.856e-06, gnorm=1.677, train_wall=5, gb_free=11.6, wall=4663
2024-07-20 11:06:24 | INFO | train_inner | epoch 039:     18 / 19 loss=6.235, nll_loss=2.824, ppl=7.08, wps=2054.6, ups=0.37, wpb=5594, bsz=272, num_updates=740, lr=8.88e-06, gnorm=1.759, train_wall=5, gb_free=12.5, wall=4668
2024-07-20 11:06:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38399.453125Mb; avail=216626.35546875Mb
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38399.44921875Mb; avail=216626.35546875Mb
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002086
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38399.44921875Mb; avail=216626.35546875Mb
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001848
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004805
2024-07-20 11:06:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38399.44921875Mb; avail=216626.35546875Mb
2024-07-20 11:06:29 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.32 | nll_loss 2.637 | ppl 6.22 | wps 4077.8 | wpb 1665.6 | bsz 74.4 | num_updates 741 | best_loss 6.32
2024-07-20 11:06:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 741 updates
2024-07-20 11:06:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:07:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 39 @ 741 updates, score 6.32) (writing took 73.98657857906073 seconds)
2024-07-20 11:07:43 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-07-20 11:07:43 | INFO | train | epoch 039 | loss 6.258 | nll_loss 2.835 | ppl 7.14 | wps 733 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 741 | lr 8.892e-06 | gnorm 1.845 | train_wall 49 | gb_free 19.3 | wall 4747
2024-07-20 11:07:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:07:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:07:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:07:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001220
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37349.87109375Mb; avail=217676.2578125Mb
2024-07-20 11:07:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000152
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001547
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37349.87109375Mb; avail=217676.2578125Mb
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000062
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37349.87109375Mb; avail=217676.2578125Mb
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000483
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002707
2024-07-20 11:07:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37349.87109375Mb; avail=217676.2578125Mb
2024-07-20 11:07:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:07:43 | INFO | fairseq.trainer | begin training epoch 40
2024-07-20 11:07:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:07:45 | INFO | train_inner | epoch 040:      1 / 19 loss=6.297, nll_loss=2.9, ppl=7.46, wps=76.6, ups=0.02, wpb=3094.5, bsz=132, num_updates=742, lr=8.904e-06, gnorm=2.23, train_wall=4, gb_free=12.3, wall=4749
2024-07-20 11:07:49 | INFO | train_inner | epoch 040:      3 / 19 loss=6.635, nll_loss=3.298, ppl=9.83, wps=1248.7, ups=0.5, wpb=2477, bsz=77.5, num_updates=744, lr=8.928e-06, gnorm=2.461, train_wall=4, gb_free=17.2, wall=4753
2024-07-20 11:07:54 | INFO | train_inner | epoch 040:      5 / 19 loss=6.121, nll_loss=2.65, ppl=6.28, wps=2178.5, ups=0.39, wpb=5532, bsz=280, num_updates=746, lr=8.952e-06, gnorm=1.602, train_wall=5, gb_free=10.7, wall=4758
2024-07-20 11:08:00 | INFO | train_inner | epoch 040:      7 / 19 loss=6.143, nll_loss=2.67, ppl=6.36, wps=1903.7, ups=0.34, wpb=5669, bsz=256, num_updates=748, lr=8.976e-06, gnorm=1.452, train_wall=6, gb_free=12.1, wall=4764
2024-07-20 11:08:05 | INFO | train_inner | epoch 040:      9 / 19 loss=6.204, nll_loss=2.768, ppl=6.81, wps=2206.8, ups=0.38, wpb=5789, bsz=264, num_updates=750, lr=9e-06, gnorm=1.711, train_wall=5, gb_free=10.8, wall=4770
2024-07-20 11:08:11 | INFO | train_inner | epoch 040:     11 / 19 loss=6.182, nll_loss=2.743, ppl=6.7, wps=1568.9, ups=0.36, wpb=4410.5, bsz=232, num_updates=752, lr=9.024e-06, gnorm=1.619, train_wall=6, gb_free=12.7, wall=4775
2024-07-20 11:08:16 | INFO | train_inner | epoch 040:     13 / 19 loss=6.186, nll_loss=2.773, ppl=6.84, wps=2018.5, ups=0.39, wpb=5192.5, bsz=236, num_updates=754, lr=9.048e-06, gnorm=1.752, train_wall=5, gb_free=11.1, wall=4780
2024-07-20 11:08:22 | INFO | train_inner | epoch 040:     15 / 19 loss=6.385, nll_loss=3.008, ppl=8.04, wps=1742.1, ups=0.35, wpb=4998, bsz=204, num_updates=756, lr=9.072e-06, gnorm=1.768, train_wall=6, gb_free=11.5, wall=4786
2024-07-20 11:08:27 | INFO | train_inner | epoch 040:     17 / 19 loss=6.058, nll_loss=2.572, ppl=5.95, wps=2167.7, ups=0.38, wpb=5716.5, bsz=252, num_updates=758, lr=9.096e-06, gnorm=2.068, train_wall=5, gb_free=13.5, wall=4791
2024-07-20 11:08:32 | INFO | train_inner | epoch 040:     19 / 19 loss=6.163, nll_loss=2.7, ppl=6.5, wps=1883.5, ups=0.44, wpb=4274, bsz=180, num_updates=760, lr=9.12e-06, gnorm=1.973, train_wall=5, gb_free=16.9, wall=4796
2024-07-20 11:08:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22656.51953125Mb; avail=232369.53125Mb
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22657.01171875Mb; avail=232369.0390625Mb
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002096
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22657.01171875Mb; avail=232369.0390625Mb
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001856
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004877
2024-07-20 11:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22657.01171875Mb; avail=232369.0390625Mb
2024-07-20 11:08:34 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.315 | nll_loss 2.595 | ppl 6.04 | wps 3997.1 | wpb 1665.6 | bsz 74.4 | num_updates 760 | best_loss 6.315
2024-07-20 11:08:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 760 updates
2024-07-20 11:08:34 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 40 @ 760 updates, score 6.315) (writing took 63.726573365041986 seconds)
2024-07-20 11:09:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-07-20 11:09:38 | INFO | train | epoch 040 | loss 6.204 | nll_loss 2.766 | ppl 6.8 | wps 800.4 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 760 | lr 9.12e-06 | gnorm 1.819 | train_wall 49 | gb_free 16.9 | wall 4862
2024-07-20 11:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000812
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45343.16015625Mb; avail=209682.39453125Mb
2024-07-20 11:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000083
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000779
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45338.23828125Mb; avail=209687.80859375Mb
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45339.22265625Mb; avail=209686.33203125Mb
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000237
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001397
2024-07-20 11:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45342.17578125Mb; avail=209683.37890625Mb
2024-07-20 11:09:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:09:38 | INFO | fairseq.trainer | begin training epoch 41
2024-07-20 11:09:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:09:44 | INFO | train_inner | epoch 041:      2 / 19 loss=6.063, nll_loss=2.565, ppl=5.92, wps=157, ups=0.03, wpb=5681, bsz=316, num_updates=762, lr=9.144e-06, gnorm=1.928, train_wall=6, gb_free=11.9, wall=4868
2024-07-20 11:09:49 | INFO | train_inner | epoch 041:      4 / 19 loss=6.313, nll_loss=2.899, ppl=7.46, wps=1766.9, ups=0.41, wpb=4344, bsz=176, num_updates=764, lr=9.168e-06, gnorm=1.807, train_wall=5, gb_free=13.6, wall=4873
2024-07-20 11:09:54 | INFO | train_inner | epoch 041:      6 / 19 loss=6.179, nll_loss=2.768, ppl=6.81, wps=1904.7, ups=0.42, wpb=4488.5, bsz=212, num_updates=766, lr=9.192e-06, gnorm=1.888, train_wall=5, gb_free=12.5, wall=4878
2024-07-20 11:09:59 | INFO | train_inner | epoch 041:      8 / 19 loss=6.119, nll_loss=2.674, ppl=6.38, wps=1822.6, ups=0.4, wpb=4521.5, bsz=224, num_updates=768, lr=9.216e-06, gnorm=1.631, train_wall=5, gb_free=12.2, wall=4883
2024-07-20 11:10:04 | INFO | train_inner | epoch 041:     10 / 19 loss=6.049, nll_loss=2.574, ppl=5.95, wps=1953, ups=0.4, wpb=4874, bsz=241.5, num_updates=770, lr=9.24e-06, gnorm=1.723, train_wall=5, gb_free=11.4, wall=4888
2024-07-20 11:10:08 | INFO | train_inner | epoch 041:     12 / 19 loss=6.244, nll_loss=2.802, ppl=6.97, wps=1936.8, ups=0.41, wpb=4730, bsz=188, num_updates=772, lr=9.264e-06, gnorm=1.763, train_wall=5, gb_free=13, wall=4893
2024-07-20 11:10:14 | INFO | train_inner | epoch 041:     14 / 19 loss=6.142, nll_loss=2.665, ppl=6.34, wps=2121.1, ups=0.38, wpb=5655.5, bsz=220, num_updates=774, lr=9.288e-06, gnorm=1.548, train_wall=5, gb_free=12.5, wall=4898
2024-07-20 11:10:20 | INFO | train_inner | epoch 041:     16 / 19 loss=6.135, nll_loss=2.665, ppl=6.34, wps=1727.3, ups=0.33, wpb=5190.5, bsz=220, num_updates=776, lr=9.312e-06, gnorm=1.63, train_wall=6, gb_free=12.7, wall=4904
2024-07-20 11:10:25 | INFO | train_inner | epoch 041:     18 / 19 loss=6.1, nll_loss=2.628, ppl=6.18, wps=2338.5, ups=0.37, wpb=6254.5, bsz=276, num_updates=778, lr=9.336e-06, gnorm=1.481, train_wall=5, gb_free=11.1, wall=4909
2024-07-20 11:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30634.80078125Mb; avail=224391.16015625Mb
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30634.80078125Mb; avail=224391.16015625Mb
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002085
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30634.80078125Mb; avail=224391.16015625Mb
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001848
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004820
2024-07-20 11:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30634.80078125Mb; avail=224391.16015625Mb
2024-07-20 11:10:29 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.256 | nll_loss 2.574 | ppl 5.96 | wps 3026.3 | wpb 1665.6 | bsz 74.4 | num_updates 779 | best_loss 6.256
2024-07-20 11:10:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 779 updates
2024-07-20 11:10:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:11:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 41 @ 779 updates, score 6.256) (writing took 61.88460275391117 seconds)
2024-07-20 11:11:31 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-07-20 11:11:31 | INFO | train | epoch 041 | loss 6.158 | nll_loss 2.704 | ppl 6.52 | wps 817.4 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 779 | lr 9.348e-06 | gnorm 1.836 | train_wall 48 | gb_free 25.3 | wall 4975
2024-07-20 11:11:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:11:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:11:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:11:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000746
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=51247.203125Mb; avail=203778.71484375Mb
2024-07-20 11:11:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000081
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000743
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51246.7109375Mb; avail=203779.20703125Mb
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51246.7109375Mb; avail=203779.20703125Mb
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000236
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001314
2024-07-20 11:11:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51246.7109375Mb; avail=203779.20703125Mb
2024-07-20 11:11:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:11:31 | INFO | fairseq.trainer | begin training epoch 42
2024-07-20 11:11:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:11:48 | INFO | train_inner | epoch 042:      1 / 19 loss=6.671, nll_loss=3.376, ppl=10.38, wps=46.3, ups=0.02, wpb=1923, bsz=72, num_updates=780, lr=9.36e-06, gnorm=3.162, train_wall=18, gb_free=17.2, wall=4992
2024-07-20 11:11:53 | INFO | train_inner | epoch 042:      3 / 19 loss=6.226, nll_loss=2.8, ppl=6.97, wps=1636.5, ups=0.41, wpb=4040, bsz=161.5, num_updates=782, lr=9.384e-06, gnorm=1.921, train_wall=5, gb_free=11.2, wall=4997
2024-07-20 11:11:59 | INFO | train_inner | epoch 042:      5 / 19 loss=6.084, nll_loss=2.61, ppl=6.1, wps=1730, ups=0.34, wpb=5075.5, bsz=220, num_updates=784, lr=9.408e-06, gnorm=1.622, train_wall=6, gb_free=12.2, wall=5003
2024-07-20 11:12:05 | INFO | train_inner | epoch 042:      7 / 19 loss=6.123, nll_loss=2.66, ppl=6.32, wps=1935.4, ups=0.32, wpb=5957.5, bsz=248, num_updates=786, lr=9.432e-06, gnorm=1.773, train_wall=6, gb_free=11.3, wall=5009
2024-07-20 11:12:10 | INFO | train_inner | epoch 042:      9 / 19 loss=6.088, nll_loss=2.623, ppl=6.16, wps=2232.1, ups=0.4, wpb=5604.5, bsz=240, num_updates=788, lr=9.456e-06, gnorm=1.669, train_wall=5, gb_free=13.4, wall=5014
2024-07-20 11:12:16 | INFO | train_inner | epoch 042:     11 / 19 loss=5.99, nll_loss=2.478, ppl=5.57, wps=1860.5, ups=0.33, wpb=5676.5, bsz=288, num_updates=790, lr=9.48e-06, gnorm=1.493, train_wall=6, gb_free=12.7, wall=5020
2024-07-20 11:12:22 | INFO | train_inner | epoch 042:     13 / 19 loss=6.484, nll_loss=3.116, ppl=8.67, wps=1546.5, ups=0.35, wpb=4362.5, bsz=148, num_updates=792, lr=9.504e-06, gnorm=1.91, train_wall=6, gb_free=14.4, wall=5026
2024-07-20 11:12:28 | INFO | train_inner | epoch 042:     15 / 19 loss=5.912, nll_loss=2.41, ppl=5.31, wps=1741.6, ups=0.34, wpb=5191, bsz=308, num_updates=794, lr=9.528e-06, gnorm=1.675, train_wall=6, gb_free=12.3, wall=5032
2024-07-20 11:12:34 | INFO | train_inner | epoch 042:     17 / 19 loss=6.064, nll_loss=2.62, ppl=6.15, wps=1546.4, ups=0.35, wpb=4384, bsz=216, num_updates=796, lr=9.552e-06, gnorm=1.691, train_wall=6, gb_free=12.2, wall=5038
2024-07-20 11:12:38 | INFO | train_inner | epoch 042:     19 / 19 loss=6.045, nll_loss=2.572, ppl=5.94, wps=2274.3, ups=0.5, wpb=4508, bsz=188, num_updates=798, lr=9.576e-06, gnorm=2.02, train_wall=4, gb_free=17.9, wall=5042
2024-07-20 11:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23431.46484375Mb; avail=231594.42578125Mb
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000470
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23431.46484375Mb; avail=231594.42578125Mb
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002077
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23431.46484375Mb; avail=231594.42578125Mb
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001815
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004674
2024-07-20 11:12:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23431.46484375Mb; avail=231594.42578125Mb
2024-07-20 11:12:41 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.247 | nll_loss 2.542 | ppl 5.82 | wps 3271 | wpb 1665.6 | bsz 74.4 | num_updates 798 | best_loss 6.247
2024-07-20 11:12:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 798 updates
2024-07-20 11:12:41 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:13:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:13:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 42 @ 798 updates, score 6.247) (writing took 59.8293612760026 seconds)
2024-07-20 11:13:41 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-07-20 11:13:41 | INFO | train | epoch 042 | loss 6.112 | nll_loss 2.653 | ppl 6.29 | wps 714.5 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 798 | lr 9.576e-06 | gnorm 1.778 | train_wall 66 | gb_free 17.9 | wall 5105
2024-07-20 11:13:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:13:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:13:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:13:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000737
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31621.3515625Mb; avail=223404.20703125Mb
2024-07-20 11:13:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000095
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000719
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31615.4453125Mb; avail=223410.11328125Mb
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31616.4296875Mb; avail=223409.12890625Mb
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000217
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001245
2024-07-20 11:13:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31619.3828125Mb; avail=223406.66796875Mb
2024-07-20 11:13:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:13:41 | INFO | fairseq.trainer | begin training epoch 43
2024-07-20 11:13:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:13:46 | INFO | train_inner | epoch 043:      2 / 19 loss=6.003, nll_loss=2.513, ppl=5.71, wps=153, ups=0.03, wpb=5209.5, bsz=264, num_updates=800, lr=9.6e-06, gnorm=1.551, train_wall=5, gb_free=14.7, wall=5110
2024-07-20 11:13:52 | INFO | train_inner | epoch 043:      4 / 19 loss=6.039, nll_loss=2.539, ppl=5.81, wps=1756.4, ups=0.32, wpb=5435.5, bsz=260, num_updates=802, lr=9.624e-06, gnorm=1.702, train_wall=6, gb_free=11.7, wall=5116
2024-07-20 11:13:58 | INFO | train_inner | epoch 043:      6 / 19 loss=6.234, nll_loss=2.789, ppl=6.91, wps=1582.4, ups=0.31, wpb=5084, bsz=196, num_updates=804, lr=9.648e-06, gnorm=2.328, train_wall=6, gb_free=11.6, wall=5122
2024-07-20 11:14:02 | INFO | train_inner | epoch 043:      8 / 19 loss=6.363, nll_loss=2.963, ppl=7.8, wps=1608.6, ups=0.59, wpb=2722, bsz=109.5, num_updates=806, lr=9.672e-06, gnorm=2.234, train_wall=3, gb_free=12.6, wall=5126
2024-07-20 11:14:07 | INFO | train_inner | epoch 043:     10 / 19 loss=6.3, nll_loss=2.89, ppl=7.41, wps=1528, ups=0.36, wpb=4289.5, bsz=156, num_updates=808, lr=9.696e-06, gnorm=1.912, train_wall=6, gb_free=13.3, wall=5131
2024-07-20 11:14:13 | INFO | train_inner | epoch 043:     12 / 19 loss=6.044, nll_loss=2.59, ppl=6.02, wps=2003, ups=0.33, wpb=6093, bsz=296, num_updates=810, lr=9.72e-06, gnorm=1.563, train_wall=6, gb_free=11.8, wall=5138
2024-07-20 11:14:19 | INFO | train_inner | epoch 043:     14 / 19 loss=5.974, nll_loss=2.501, ppl=5.66, wps=1672.1, ups=0.33, wpb=5070, bsz=252, num_updates=812, lr=9.744e-06, gnorm=1.595, train_wall=6, gb_free=13.4, wall=5144
2024-07-20 11:14:25 | INFO | train_inner | epoch 043:     16 / 19 loss=5.86, nll_loss=2.318, ppl=4.99, wps=1905.6, ups=0.36, wpb=5355.5, bsz=260, num_updates=814, lr=9.768e-06, gnorm=1.447, train_wall=6, gb_free=12, wall=5149
2024-07-20 11:14:31 | INFO | train_inner | epoch 043:     18 / 19 loss=6.087, nll_loss=2.613, ppl=6.12, wps=1884.9, ups=0.33, wpb=5739.5, bsz=244, num_updates=816, lr=9.792e-06, gnorm=1.668, train_wall=6, gb_free=12.3, wall=5155
2024-07-20 11:14:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31700.94140625Mb; avail=223325.0234375Mb
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000591
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31700.94140625Mb; avail=223325.0234375Mb
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002101
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31700.94140625Mb; avail=223325.0234375Mb
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001871
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004976
2024-07-20 11:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31700.94140625Mb; avail=223325.0234375Mb
2024-07-20 11:14:35 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.241 | nll_loss 2.51 | ppl 5.7 | wps 4181.1 | wpb 1665.6 | bsz 74.4 | num_updates 817 | best_loss 6.241
2024-07-20 11:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 817 updates
2024-07-20 11:14:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:15:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:15:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 43 @ 817 updates, score 6.241) (writing took 63.05958976689726 seconds)
2024-07-20 11:15:38 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-07-20 11:15:38 | INFO | train | epoch 043 | loss 6.083 | nll_loss 2.613 | ppl 6.12 | wps 787.2 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 817 | lr 9.804e-06 | gnorm 1.799 | train_wall 52 | gb_free 16.8 | wall 5222
2024-07-20 11:15:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:15:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:15:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:15:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001178
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46279.9609375Mb; avail=208746.08984375Mb
2024-07-20 11:15:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000141
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001297
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46279.9609375Mb; avail=208746.08984375Mb
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000059
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46279.9609375Mb; avail=208746.08984375Mb
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000367
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002262
2024-07-20 11:15:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46279.9609375Mb; avail=208746.08984375Mb
2024-07-20 11:15:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:15:38 | INFO | fairseq.trainer | begin training epoch 44
2024-07-20 11:15:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:15:51 | INFO | train_inner | epoch 044:      1 / 19 loss=6.104, nll_loss=2.619, ppl=6.14, wps=86.7, ups=0.02, wpb=3474, bsz=128, num_updates=818, lr=9.816e-06, gnorm=1.898, train_wall=14, gb_free=11.8, wall=5235
2024-07-20 11:15:56 | INFO | train_inner | epoch 044:      3 / 19 loss=6.146, nll_loss=2.676, ppl=6.39, wps=2029.6, ups=0.4, wpb=5036, bsz=192, num_updates=820, lr=9.84e-06, gnorm=1.57, train_wall=5, gb_free=12.7, wall=5240
2024-07-20 11:16:02 | INFO | train_inner | epoch 044:      5 / 19 loss=5.926, nll_loss=2.413, ppl=5.33, wps=1734.8, ups=0.34, wpb=5051, bsz=236, num_updates=822, lr=9.864e-06, gnorm=1.584, train_wall=6, gb_free=12.9, wall=5246
2024-07-20 11:16:08 | INFO | train_inner | epoch 044:      7 / 19 loss=6.034, nll_loss=2.57, ppl=5.94, wps=1968.6, ups=0.34, wpb=5876.5, bsz=284, num_updates=824, lr=9.888e-06, gnorm=1.464, train_wall=6, gb_free=13, wall=5252
2024-07-20 11:16:13 | INFO | train_inner | epoch 044:      9 / 19 loss=6.212, nll_loss=2.791, ppl=6.92, wps=1611.5, ups=0.37, wpb=4359.5, bsz=172, num_updates=826, lr=9.912e-06, gnorm=1.992, train_wall=5, gb_free=12.3, wall=5258
2024-07-20 11:16:19 | INFO | train_inner | epoch 044:     11 / 19 loss=5.932, nll_loss=2.432, ppl=5.4, wps=2016.8, ups=0.36, wpb=5629, bsz=324, num_updates=828, lr=9.936e-06, gnorm=1.44, train_wall=6, gb_free=10.6, wall=5263
2024-07-20 11:16:26 | INFO | train_inner | epoch 044:     13 / 19 loss=5.92, nll_loss=2.394, ppl=5.26, wps=1903.3, ups=0.3, wpb=6401, bsz=288, num_updates=830, lr=9.96e-06, gnorm=1.393, train_wall=7, gb_free=10.9, wall=5270
2024-07-20 11:16:31 | INFO | train_inner | epoch 044:     15 / 19 loss=5.946, nll_loss=2.436, ppl=5.41, wps=1747.5, ups=0.37, wpb=4723, bsz=220, num_updates=832, lr=9.984e-06, gnorm=1.646, train_wall=5, gb_free=12.7, wall=5275
2024-07-20 11:16:37 | INFO | train_inner | epoch 044:     17 / 19 loss=5.987, nll_loss=2.492, ppl=5.63, wps=1651, ups=0.32, wpb=5217, bsz=228, num_updates=834, lr=1.0008e-05, gnorm=1.683, train_wall=6, gb_free=12.7, wall=5282
2024-07-20 11:16:40 | INFO | train_inner | epoch 044:     19 / 19 loss=6.614, nll_loss=3.3, ppl=9.85, wps=1524, ups=0.9, wpb=1696.5, bsz=53.5, num_updates=836, lr=1.0032e-05, gnorm=2.847, train_wall=2, gb_free=17.8, wall=5284
2024-07-20 11:16:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24256.5703125Mb; avail=230769.5703125Mb
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000476
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24256.5703125Mb; avail=230769.5703125Mb
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002074
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24256.5703125Mb; avail=230769.5703125Mb
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001815
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004688
2024-07-20 11:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24256.5703125Mb; avail=230769.5703125Mb
2024-07-20 11:16:42 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.203 | nll_loss 2.499 | ppl 5.65 | wps 4181.4 | wpb 1665.6 | bsz 74.4 | num_updates 836 | best_loss 6.203
2024-07-20 11:16:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 836 updates
2024-07-20 11:16:42 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:17:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:17:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 44 @ 836 updates, score 6.203) (writing took 58.624820071971044 seconds)
2024-07-20 11:17:41 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-07-20 11:17:41 | INFO | train | epoch 044 | loss 6.03 | nll_loss 2.547 | ppl 5.84 | wps 753.2 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 836 | lr 1.0032e-05 | gnorm 1.729 | train_wall 61 | gb_free 17.8 | wall 5345
2024-07-20 11:17:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:17:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:17:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:17:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000724
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37633.734375Mb; avail=217391.9140625Mb
2024-07-20 11:17:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000095
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000783
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37631.2734375Mb; avail=217398.3125Mb
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37628.8125Mb; avail=217397.328125Mb
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000237
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001358
2024-07-20 11:17:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37631.2734375Mb; avail=217394.8671875Mb
2024-07-20 11:17:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:17:41 | INFO | fairseq.trainer | begin training epoch 45
2024-07-20 11:17:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:17:45 | INFO | train_inner | epoch 045:      2 / 19 loss=6.155, nll_loss=2.706, ppl=6.53, wps=104.3, ups=0.03, wpb=3433.5, bsz=117.5, num_updates=838, lr=1.0056e-05, gnorm=2.101, train_wall=5, gb_free=19.4, wall=5350
2024-07-20 11:17:51 | INFO | train_inner | epoch 045:      4 / 19 loss=5.96, nll_loss=2.465, ppl=5.52, wps=2000.6, ups=0.37, wpb=5446, bsz=240, num_updates=840, lr=1.008e-05, gnorm=1.589, train_wall=5, gb_free=13.6, wall=5355
2024-07-20 11:17:56 | INFO | train_inner | epoch 045:      6 / 19 loss=5.954, nll_loss=2.427, ppl=5.38, wps=2113.9, ups=0.37, wpb=5689, bsz=268, num_updates=842, lr=1.0104e-05, gnorm=1.794, train_wall=5, gb_free=11, wall=5360
2024-07-20 11:18:02 | INFO | train_inner | epoch 045:      8 / 19 loss=5.968, nll_loss=2.438, ppl=5.42, wps=1806.8, ups=0.34, wpb=5297.5, bsz=216, num_updates=844, lr=1.0128e-05, gnorm=1.557, train_wall=6, gb_free=12.7, wall=5366
2024-07-20 11:18:07 | INFO | train_inner | epoch 045:     10 / 19 loss=6.009, nll_loss=2.527, ppl=5.76, wps=2086.9, ups=0.4, wpb=5275.5, bsz=252, num_updates=846, lr=1.0152e-05, gnorm=1.666, train_wall=5, gb_free=11.6, wall=5371
2024-07-20 11:18:13 | INFO | train_inner | epoch 045:     12 / 19 loss=5.988, nll_loss=2.503, ppl=5.67, wps=1731.8, ups=0.33, wpb=5243, bsz=260, num_updates=848, lr=1.0176e-05, gnorm=1.511, train_wall=6, gb_free=12.4, wall=5377
2024-07-20 11:18:18 | INFO | train_inner | epoch 045:     14 / 19 loss=5.947, nll_loss=2.46, ppl=5.5, wps=2079.4, ups=0.44, wpb=4763.5, bsz=228, num_updates=850, lr=1.02e-05, gnorm=1.771, train_wall=5, gb_free=12.7, wall=5382
2024-07-20 11:18:24 | INFO | train_inner | epoch 045:     16 / 19 loss=6.256, nll_loss=2.844, ppl=7.18, wps=1649.6, ups=0.33, wpb=4934.5, bsz=180, num_updates=852, lr=1.0224e-05, gnorm=1.916, train_wall=6, gb_free=13, wall=5388
2024-07-20 11:18:30 | INFO | train_inner | epoch 045:     18 / 19 loss=5.898, nll_loss=2.374, ppl=5.18, wps=1880.4, ups=0.34, wpb=5612.5, bsz=300, num_updates=854, lr=1.0248e-05, gnorm=1.613, train_wall=6, gb_free=12.2, wall=5394
2024-07-20 11:18:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36874.9375Mb; avail=218151.1171875Mb
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36874.9375Mb; avail=218151.1171875Mb
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002100
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36874.9375Mb; avail=218151.1171875Mb
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001848
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004840
2024-07-20 11:18:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36875.4296875Mb; avail=218150.625Mb
2024-07-20 11:18:33 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.192 | nll_loss 2.469 | ppl 5.54 | wps 4181.4 | wpb 1665.6 | bsz 74.4 | num_updates 855 | best_loss 6.192
2024-07-20 11:18:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 855 updates
2024-07-20 11:18:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:19:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 45 @ 855 updates, score 6.192) (writing took 68.68891422613524 seconds)
2024-07-20 11:19:42 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-07-20 11:19:42 | INFO | train | epoch 045 | loss 6.005 | nll_loss 2.514 | ppl 5.71 | wps 761.6 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 855 | lr 1.026e-05 | gnorm 1.794 | train_wall 50 | gb_free 19 | wall 5466
2024-07-20 11:19:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:19:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:19:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:19:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000621
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45089.171875Mb; avail=209936.9765625Mb
2024-07-20 11:19:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000740
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45089.171875Mb; avail=209936.9765625Mb
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45089.171875Mb; avail=209936.9765625Mb
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000212
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001280
2024-07-20 11:19:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45089.171875Mb; avail=209936.9765625Mb
2024-07-20 11:19:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:19:42 | INFO | fairseq.trainer | begin training epoch 46
2024-07-20 11:19:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:19:44 | INFO | train_inner | epoch 046:      1 / 19 loss=6.121, nll_loss=2.638, ppl=6.22, wps=59.1, ups=0.03, wpb=2189, bsz=77.5, num_updates=856, lr=1.0272e-05, gnorm=2.555, train_wall=3, gb_free=12, wall=5468
2024-07-20 11:19:49 | INFO | train_inner | epoch 046:      3 / 19 loss=6.03, nll_loss=2.555, ppl=5.88, wps=2270.6, ups=0.39, wpb=5826.5, bsz=240, num_updates=858, lr=1.0296e-05, gnorm=1.526, train_wall=5, gb_free=10.9, wall=5473
2024-07-20 11:19:55 | INFO | train_inner | epoch 046:      5 / 19 loss=5.841, nll_loss=2.305, ppl=4.94, wps=1973.5, ups=0.32, wpb=6105, bsz=292, num_updates=860, lr=1.032e-05, gnorm=1.458, train_wall=6, gb_free=11.5, wall=5479
2024-07-20 11:20:00 | INFO | train_inner | epoch 046:      7 / 19 loss=5.924, nll_loss=2.414, ppl=5.33, wps=2139.9, ups=0.4, wpb=5353.5, bsz=232, num_updates=862, lr=1.0344e-05, gnorm=1.677, train_wall=5, gb_free=12.1, wall=5484
2024-07-20 11:20:05 | INFO | train_inner | epoch 046:      9 / 19 loss=6.127, nll_loss=2.672, ppl=6.37, wps=1698.7, ups=0.39, wpb=4328.5, bsz=156, num_updates=864, lr=1.0368e-05, gnorm=1.731, train_wall=5, gb_free=11.5, wall=5489
2024-07-20 11:20:10 | INFO | train_inner | epoch 046:     11 / 19 loss=6.045, nll_loss=2.565, ppl=5.92, wps=1762.8, ups=0.42, wpb=4233.5, bsz=168, num_updates=866, lr=1.0392e-05, gnorm=1.925, train_wall=5, gb_free=14.3, wall=5494
2024-07-20 11:20:16 | INFO | train_inner | epoch 046:     13 / 19 loss=6.109, nll_loss=2.66, ppl=6.32, wps=1662.8, ups=0.35, wpb=4705, bsz=228, num_updates=868, lr=1.0416e-05, gnorm=1.833, train_wall=6, gb_free=12.3, wall=5500
2024-07-20 11:20:22 | INFO | train_inner | epoch 046:     15 / 19 loss=5.84, nll_loss=2.301, ppl=4.93, wps=1738.4, ups=0.33, wpb=5227, bsz=268, num_updates=870, lr=1.044e-05, gnorm=1.495, train_wall=6, gb_free=12.6, wall=5506
2024-07-20 11:20:27 | INFO | train_inner | epoch 046:     17 / 19 loss=5.852, nll_loss=2.315, ppl=4.98, wps=1968.3, ups=0.38, wpb=5152.5, bsz=268, num_updates=872, lr=1.0464e-05, gnorm=1.955, train_wall=5, gb_free=10.9, wall=5511
2024-07-20 11:20:32 | INFO | train_inner | epoch 046:     19 / 19 loss=5.849, nll_loss=2.32, ppl=4.99, wps=1595.9, ups=0.44, wpb=3646.5, bsz=172, num_updates=874, lr=1.0488e-05, gnorm=2.369, train_wall=5, gb_free=19.1, wall=5516
2024-07-20 11:20:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35762.6484375Mb; avail=219263.44140625Mb
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000578
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35763.140625Mb; avail=219262.94921875Mb
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002056
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35763.140625Mb; avail=219262.94921875Mb
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001844
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004800
2024-07-20 11:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35763.140625Mb; avail=219262.94921875Mb
2024-07-20 11:20:35 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.181 | nll_loss 2.462 | ppl 5.51 | wps 3441.6 | wpb 1665.6 | bsz 74.4 | num_updates 874 | best_loss 6.181
2024-07-20 11:20:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 874 updates
2024-07-20 11:20:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:21:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:21:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 46 @ 874 updates, score 6.181) (writing took 62.41159779694863 seconds)
2024-07-20 11:21:37 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-07-20 11:21:37 | INFO | train | epoch 046 | loss 5.962 | nll_loss 2.461 | ppl 5.51 | wps 804.9 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 874 | lr 1.0488e-05 | gnorm 1.789 | train_wall 49 | gb_free 19.1 | wall 5581
2024-07-20 11:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001185
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58188.7890625Mb; avail=196837.3046875Mb
2024-07-20 11:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000112
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001198
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58188.7890625Mb; avail=196837.3046875Mb
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000069
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58188.7890625Mb; avail=196837.3046875Mb
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000358
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002088
2024-07-20 11:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58188.7890625Mb; avail=196837.3046875Mb
2024-07-20 11:21:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:21:37 | INFO | fairseq.trainer | begin training epoch 47
2024-07-20 11:21:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:21:43 | INFO | train_inner | epoch 047:      2 / 19 loss=5.734, nll_loss=2.167, ppl=4.49, wps=143.7, ups=0.03, wpb=5151, bsz=288, num_updates=876, lr=1.0512e-05, gnorm=1.482, train_wall=6, gb_free=11.6, wall=5587
2024-07-20 11:21:48 | INFO | train_inner | epoch 047:      4 / 19 loss=5.931, nll_loss=2.429, ppl=5.38, wps=2173.8, ups=0.38, wpb=5679, bsz=228, num_updates=878, lr=1.0536e-05, gnorm=1.436, train_wall=5, gb_free=12.7, wall=5593
2024-07-20 11:21:54 | INFO | train_inner | epoch 047:      6 / 19 loss=5.852, nll_loss=2.325, ppl=5.01, wps=1734.3, ups=0.34, wpb=5096, bsz=256, num_updates=880, lr=1.056e-05, gnorm=1.5, train_wall=6, gb_free=12, wall=5599
2024-07-20 11:22:00 | INFO | train_inner | epoch 047:      8 / 19 loss=5.802, nll_loss=2.263, ppl=4.8, wps=1780, ups=0.33, wpb=5435, bsz=264, num_updates=882, lr=1.0584e-05, gnorm=1.573, train_wall=6, gb_free=10.7, wall=5605
2024-07-20 11:22:05 | INFO | train_inner | epoch 047:     10 / 19 loss=5.968, nll_loss=2.476, ppl=5.56, wps=1892.2, ups=0.4, wpb=4726.5, bsz=200, num_updates=884, lr=1.0608e-05, gnorm=1.643, train_wall=5, gb_free=11.6, wall=5610
2024-07-20 11:22:10 | INFO | train_inner | epoch 047:     12 / 19 loss=5.891, nll_loss=2.38, ppl=5.21, wps=2101, ups=0.47, wpb=4454.5, bsz=213.5, num_updates=886, lr=1.0632e-05, gnorm=1.932, train_wall=4, gb_free=11.4, wall=5614
2024-07-20 11:22:15 | INFO | train_inner | epoch 047:     14 / 19 loss=6.162, nll_loss=2.709, ppl=6.54, wps=1755.5, ups=0.35, wpb=4952, bsz=176, num_updates=888, lr=1.0656e-05, gnorm=1.804, train_wall=6, gb_free=13.4, wall=5620
2024-07-20 11:22:20 | INFO | train_inner | epoch 047:     16 / 19 loss=5.997, nll_loss=2.486, ppl=5.6, wps=1852.7, ups=0.48, wpb=3854.5, bsz=140, num_updates=890, lr=1.068e-05, gnorm=1.903, train_wall=4, gb_free=19.7, wall=5624
2024-07-20 11:22:25 | INFO | train_inner | epoch 047:     18 / 19 loss=5.85, nll_loss=2.313, ppl=4.97, wps=2023.4, ups=0.35, wpb=5765.5, bsz=268, num_updates=892, lr=1.0704e-05, gnorm=1.464, train_wall=6, gb_free=12, wall=5629
2024-07-20 11:22:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43487.23828125Mb; avail=211538.77734375Mb
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43487.23828125Mb; avail=211538.77734375Mb
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002112
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43487.23828125Mb; avail=211538.77734375Mb
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001866
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004856
2024-07-20 11:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43487.23828125Mb; avail=211538.77734375Mb
2024-07-20 11:22:30 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.155 | nll_loss 2.438 | ppl 5.42 | wps 4182.4 | wpb 1665.6 | bsz 74.4 | num_updates 893 | best_loss 6.155
2024-07-20 11:22:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 893 updates
2024-07-20 11:22:30 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:23:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:23:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 47 @ 893 updates, score 6.155) (writing took 74.22667364403605 seconds)
2024-07-20 11:23:44 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-07-20 11:23:44 | INFO | train | epoch 047 | loss 5.907 | nll_loss 2.391 | ppl 5.24 | wps 729.3 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 893 | lr 1.0716e-05 | gnorm 1.673 | train_wall 50 | gb_free 15.8 | wall 5708
2024-07-20 11:23:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:23:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:23:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:23:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000670
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36216.01953125Mb; avail=218810.125Mb
2024-07-20 11:23:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000100
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000799
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36216.01953125Mb; avail=218810.125Mb
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000042
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36216.01953125Mb; avail=218810.125Mb
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000221
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001356
2024-07-20 11:23:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36216.01953125Mb; avail=218810.125Mb
2024-07-20 11:23:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:23:44 | INFO | fairseq.trainer | begin training epoch 48
2024-07-20 11:23:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:23:46 | INFO | train_inner | epoch 048:      1 / 19 loss=6.021, nll_loss=2.543, ppl=5.83, wps=88.7, ups=0.02, wpb=3597.5, bsz=140, num_updates=894, lr=1.0728e-05, gnorm=2.019, train_wall=4, gb_free=12.6, wall=5710
2024-07-20 11:23:52 | INFO | train_inner | epoch 048:      3 / 19 loss=5.993, nll_loss=2.497, ppl=5.64, wps=1640, ups=0.37, wpb=4406.5, bsz=188, num_updates=896, lr=1.0752e-05, gnorm=2.194, train_wall=5, gb_free=12.5, wall=5716
2024-07-20 11:23:57 | INFO | train_inner | epoch 048:      5 / 19 loss=6.085, nll_loss=2.618, ppl=6.14, wps=1734.4, ups=0.35, wpb=4948.5, bsz=196, num_updates=898, lr=1.0776e-05, gnorm=1.759, train_wall=6, gb_free=12.7, wall=5722
2024-07-20 11:24:03 | INFO | train_inner | epoch 048:      7 / 19 loss=5.742, nll_loss=2.165, ppl=4.48, wps=1717.1, ups=0.37, wpb=4675, bsz=248, num_updates=900, lr=1.08e-05, gnorm=1.464, train_wall=5, gb_free=14.4, wall=5727
2024-07-20 11:24:09 | INFO | train_inner | epoch 048:      9 / 19 loss=5.864, nll_loss=2.334, ppl=5.04, wps=1873.2, ups=0.31, wpb=6124.5, bsz=272, num_updates=902, lr=1.0824e-05, gnorm=1.458, train_wall=7, gb_free=10.6, wall=5734
2024-07-20 11:24:14 | INFO | train_inner | epoch 048:     11 / 19 loss=5.982, nll_loss=2.517, ppl=5.72, wps=1766.6, ups=0.42, wpb=4195.5, bsz=185.5, num_updates=904, lr=1.0848e-05, gnorm=1.942, train_wall=5, gb_free=13.1, wall=5738
2024-07-20 11:24:20 | INFO | train_inner | epoch 048:     13 / 19 loss=5.83, nll_loss=2.311, ppl=4.96, wps=1806.7, ups=0.34, wpb=5384, bsz=252, num_updates=906, lr=1.0872e-05, gnorm=1.393, train_wall=6, gb_free=11.7, wall=5744
2024-07-20 11:24:26 | INFO | train_inner | epoch 048:     15 / 19 loss=5.852, nll_loss=2.311, ppl=4.96, wps=1721.9, ups=0.36, wpb=4796.5, bsz=236, num_updates=908, lr=1.0896e-05, gnorm=1.638, train_wall=6, gb_free=11.6, wall=5750
2024-07-20 11:24:31 | INFO | train_inner | epoch 048:     17 / 19 loss=5.809, nll_loss=2.258, ppl=4.78, wps=2038.2, ups=0.36, wpb=5709, bsz=264, num_updates=910, lr=1.092e-05, gnorm=2, train_wall=6, gb_free=12.6, wall=5755
2024-07-20 11:24:35 | INFO | train_inner | epoch 048:     19 / 19 loss=5.898, nll_loss=2.365, ppl=5.15, wps=1895, ups=0.54, wpb=3511, bsz=148, num_updates=912, lr=1.0944e-05, gnorm=2.115, train_wall=4, gb_free=18.5, wall=5759
2024-07-20 11:24:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21507.3125Mb; avail=233518.74609375Mb
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000551
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21507.3125Mb; avail=233518.74609375Mb
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002078
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21507.3125Mb; avail=233518.74609375Mb
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001854
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004801
2024-07-20 11:24:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21507.8046875Mb; avail=233518.25390625Mb
2024-07-20 11:24:38 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.142 | nll_loss 2.419 | ppl 5.35 | wps 3336.4 | wpb 1665.6 | bsz 74.4 | num_updates 912 | best_loss 6.142
2024-07-20 11:24:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 912 updates
2024-07-20 11:24:38 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:25:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:25:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 48 @ 912 updates, score 6.142) (writing took 63.0560007840395 seconds)
2024-07-20 11:25:41 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-07-20 11:25:41 | INFO | train | epoch 048 | loss 5.898 | nll_loss 2.379 | ppl 5.2 | wps 788.4 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 912 | lr 1.0944e-05 | gnorm 1.771 | train_wall 51 | gb_free 18.5 | wall 5825
2024-07-20 11:25:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:25:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:25:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:25:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000800
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44467.94921875Mb; avail=210557.6171875Mb
2024-07-20 11:25:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000082
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000752
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44473.36328125Mb; avail=210552.6953125Mb
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000038
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44474.34765625Mb; avail=210551.21875Mb
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000235
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001320
2024-07-20 11:25:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44466.47265625Mb; avail=210559.09375Mb
2024-07-20 11:25:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:25:41 | INFO | fairseq.trainer | begin training epoch 49
2024-07-20 11:25:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:25:46 | INFO | train_inner | epoch 049:      2 / 19 loss=6.095, nll_loss=2.619, ppl=6.14, wps=121.1, ups=0.03, wpb=4284, bsz=160, num_updates=914, lr=1.0968e-05, gnorm=1.838, train_wall=4, gb_free=10.6, wall=5830
2024-07-20 11:25:51 | INFO | train_inner | epoch 049:      4 / 19 loss=5.819, nll_loss=2.29, ppl=4.89, wps=2122, ups=0.41, wpb=5208, bsz=212, num_updates=916, lr=1.0992e-05, gnorm=1.594, train_wall=5, gb_free=13.2, wall=5835
2024-07-20 11:25:56 | INFO | train_inner | epoch 049:      6 / 19 loss=5.857, nll_loss=2.338, ppl=5.06, wps=1857.2, ups=0.35, wpb=5311, bsz=204, num_updates=918, lr=1.1016e-05, gnorm=1.578, train_wall=6, gb_free=13.3, wall=5840
2024-07-20 11:26:01 | INFO | train_inner | epoch 049:      8 / 19 loss=5.742, nll_loss=2.178, ppl=4.53, wps=2133.8, ups=0.4, wpb=5324, bsz=248, num_updates=920, lr=1.104e-05, gnorm=1.617, train_wall=5, gb_free=12.5, wall=5845
2024-07-20 11:26:07 | INFO | train_inner | epoch 049:     10 / 19 loss=5.848, nll_loss=2.298, ppl=4.92, wps=1994.5, ups=0.38, wpb=5198.5, bsz=232, num_updates=922, lr=1.1064e-05, gnorm=1.565, train_wall=5, gb_free=12.2, wall=5851
2024-07-20 11:26:12 | INFO | train_inner | epoch 049:     12 / 19 loss=5.977, nll_loss=2.489, ppl=5.61, wps=1720.9, ups=0.38, wpb=4475.5, bsz=213.5, num_updates=924, lr=1.1088e-05, gnorm=1.833, train_wall=5, gb_free=11.8, wall=5856
2024-07-20 11:26:18 | INFO | train_inner | epoch 049:     14 / 19 loss=5.89, nll_loss=2.364, ppl=5.15, wps=1938.3, ups=0.33, wpb=5809.5, bsz=276, num_updates=926, lr=1.1112e-05, gnorm=1.704, train_wall=6, gb_free=12.2, wall=5862
2024-07-20 11:26:24 | INFO | train_inner | epoch 049:     16 / 19 loss=5.742, nll_loss=2.168, ppl=4.49, wps=1647.5, ups=0.32, wpb=5143.5, bsz=256, num_updates=928, lr=1.1136e-05, gnorm=1.645, train_wall=6, gb_free=10.8, wall=5868
2024-07-20 11:26:29 | INFO | train_inner | epoch 049:     18 / 19 loss=5.699, nll_loss=2.143, ppl=4.42, wps=1728.1, ups=0.37, wpb=4630, bsz=256, num_updates=930, lr=1.116e-05, gnorm=1.57, train_wall=5, gb_free=12.7, wall=5874
2024-07-20 11:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29782.94921875Mb; avail=225243.015625Mb
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29782.94921875Mb; avail=225243.015625Mb
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002096
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29783.44140625Mb; avail=225242.5234375Mb
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001846
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004837
2024-07-20 11:26:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29783.44140625Mb; avail=225242.5234375Mb
2024-07-20 11:26:33 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.135 | nll_loss 2.409 | ppl 5.31 | wps 4181 | wpb 1665.6 | bsz 74.4 | num_updates 931 | best_loss 6.135
2024-07-20 11:26:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 931 updates
2024-07-20 11:26:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:27:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:27:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 49 @ 931 updates, score 6.135) (writing took 76.3350315860007 seconds)
2024-07-20 11:27:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-07-20 11:27:50 | INFO | train | epoch 049 | loss 5.86 | nll_loss 2.331 | ppl 5.03 | wps 719.9 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 931 | lr 1.1172e-05 | gnorm 1.733 | train_wall 49 | gb_free 18.2 | wall 5954
2024-07-20 11:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000631
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34034.72265625Mb; avail=220991.421875Mb
2024-07-20 11:27:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000094
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000720
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34034.72265625Mb; avail=220991.421875Mb
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34034.72265625Mb; avail=220992.40625Mb
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000208
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001234
2024-07-20 11:27:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34024.87890625Mb; avail=221004.7109375Mb
2024-07-20 11:27:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:27:50 | INFO | fairseq.trainer | begin training epoch 50
2024-07-20 11:27:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:27:52 | INFO | train_inner | epoch 050:      1 / 19 loss=5.9, nll_loss=2.387, ppl=5.23, wps=93.7, ups=0.02, wpb=3883, bsz=180, num_updates=932, lr=1.1184e-05, gnorm=2.344, train_wall=4, gb_free=11.9, wall=5956
2024-07-20 11:27:57 | INFO | train_inner | epoch 050:      3 / 19 loss=5.935, nll_loss=2.414, ppl=5.33, wps=1477.1, ups=0.4, wpb=3669, bsz=152, num_updates=934, lr=1.1208e-05, gnorm=1.962, train_wall=5, gb_free=13.4, wall=5961
2024-07-20 11:28:02 | INFO | train_inner | epoch 050:      5 / 19 loss=5.929, nll_loss=2.422, ppl=5.36, wps=1953.2, ups=0.4, wpb=4881.5, bsz=208, num_updates=936, lr=1.1232e-05, gnorm=1.706, train_wall=5, gb_free=13.1, wall=5966
2024-07-20 11:28:08 | INFO | train_inner | epoch 050:      7 / 19 loss=5.695, nll_loss=2.123, ppl=4.36, wps=1879.6, ups=0.33, wpb=5760.5, bsz=300, num_updates=938, lr=1.1256e-05, gnorm=1.387, train_wall=6, gb_free=11, wall=5972
2024-07-20 11:28:13 | INFO | train_inner | epoch 050:      9 / 19 loss=5.792, nll_loss=2.238, ppl=4.72, wps=1864.6, ups=0.41, wpb=4528.5, bsz=213.5, num_updates=940, lr=1.128e-05, gnorm=1.855, train_wall=5, gb_free=11.5, wall=5977
2024-07-20 11:28:18 | INFO | train_inner | epoch 050:     11 / 19 loss=5.85, nll_loss=2.313, ppl=4.97, wps=1637.2, ups=0.41, wpb=4030.5, bsz=176, num_updates=942, lr=1.1304e-05, gnorm=1.864, train_wall=5, gb_free=12.3, wall=5982
2024-07-20 11:28:24 | INFO | train_inner | epoch 050:     13 / 19 loss=5.85, nll_loss=2.307, ppl=4.95, wps=1662.3, ups=0.34, wpb=4879.5, bsz=196, num_updates=944, lr=1.1328e-05, gnorm=1.571, train_wall=6, gb_free=12.7, wall=5988
2024-07-20 11:28:29 | INFO | train_inner | epoch 050:     15 / 19 loss=5.768, nll_loss=2.21, ppl=4.63, wps=2174.2, ups=0.4, wpb=5444, bsz=244, num_updates=946, lr=1.1352e-05, gnorm=1.496, train_wall=5, gb_free=14.2, wall=5993
2024-07-20 11:28:35 | INFO | train_inner | epoch 050:     17 / 19 loss=5.771, nll_loss=2.227, ppl=4.68, wps=1921.2, ups=0.33, wpb=5889, bsz=264, num_updates=948, lr=1.1376e-05, gnorm=1.564, train_wall=6, gb_free=11.4, wall=5999
2024-07-20 11:28:40 | INFO | train_inner | epoch 050:     19 / 19 loss=5.985, nll_loss=2.489, ppl=5.61, wps=1729.9, ups=0.42, wpb=4112.5, bsz=172, num_updates=950, lr=1.14e-05, gnorm=2.24, train_wall=5, gb_free=18.2, wall=6004
2024-07-20 11:28:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26355.68359375Mb; avail=228670.375Mb
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000549
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26356.17578125Mb; avail=228669.8828125Mb
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002097
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26356.17578125Mb; avail=228669.8828125Mb
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001855
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004814
2024-07-20 11:28:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26356.17578125Mb; avail=228669.8828125Mb
2024-07-20 11:28:43 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.108 | nll_loss 2.391 | ppl 5.25 | wps 3048.7 | wpb 1665.6 | bsz 74.4 | num_updates 950 | best_loss 6.108
2024-07-20 11:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 950 updates
2024-07-20 11:28:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:29:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:29:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 50 @ 950 updates, score 6.108) (writing took 62.37559598009102 seconds)
2024-07-20 11:29:46 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-07-20 11:29:46 | INFO | train | epoch 050 | loss 5.824 | nll_loss 2.283 | ppl 4.87 | wps 796.7 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 950 | lr 1.14e-05 | gnorm 1.734 | train_wall 50 | gb_free 18.2 | wall 6070
2024-07-20 11:29:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:29:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:29:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:29:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000823
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50165.34765625Mb; avail=204860.171875Mb
2024-07-20 11:29:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000082
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000777
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50171.25390625Mb; avail=204854.7578125Mb
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50165.34765625Mb; avail=204864.109375Mb
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000231
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001345
2024-07-20 11:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50164.36328125Mb; avail=204861.6484375Mb
2024-07-20 11:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:29:46 | INFO | fairseq.trainer | begin training epoch 51
2024-07-20 11:29:46 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:29:51 | INFO | train_inner | epoch 051:      2 / 19 loss=5.929, nll_loss=2.42, ppl=5.35, wps=136.3, ups=0.03, wpb=4824, bsz=192, num_updates=952, lr=1.1424e-05, gnorm=1.697, train_wall=5, gb_free=11.5, wall=6075
2024-07-20 11:29:56 | INFO | train_inner | epoch 051:      4 / 19 loss=5.801, nll_loss=2.242, ppl=4.73, wps=1840.6, ups=0.38, wpb=4828.5, bsz=216, num_updates=954, lr=1.1448e-05, gnorm=1.777, train_wall=5, gb_free=11.5, wall=6080
2024-07-20 11:30:00 | INFO | train_inner | epoch 051:      6 / 19 loss=5.893, nll_loss=2.373, ppl=5.18, wps=1793.1, ups=0.43, wpb=4133.5, bsz=176, num_updates=956, lr=1.1472e-05, gnorm=1.95, train_wall=5, gb_free=12.6, wall=6085
2024-07-20 11:30:05 | INFO | train_inner | epoch 051:      8 / 19 loss=5.708, nll_loss=2.121, ppl=4.35, wps=2067.3, ups=0.48, wpb=4281.5, bsz=205.5, num_updates=958, lr=1.1496e-05, gnorm=1.642, train_wall=4, gb_free=17.7, wall=6089
2024-07-20 11:30:11 | INFO | train_inner | epoch 051:     10 / 19 loss=5.7, nll_loss=2.136, ppl=4.39, wps=1778.3, ups=0.34, wpb=5230.5, bsz=268, num_updates=960, lr=1.152e-05, gnorm=1.432, train_wall=6, gb_free=11.6, wall=6095
2024-07-20 11:30:16 | INFO | train_inner | epoch 051:     12 / 19 loss=5.676, nll_loss=2.103, ppl=4.3, wps=2367.8, ups=0.37, wpb=6326.5, bsz=304, num_updates=962, lr=1.1544e-05, gnorm=1.342, train_wall=5, gb_free=10.8, wall=6100
2024-07-20 11:30:22 | INFO | train_inner | epoch 051:     14 / 19 loss=5.778, nll_loss=2.228, ppl=4.69, wps=1804.6, ups=0.34, wpb=5312, bsz=256, num_updates=964, lr=1.1568e-05, gnorm=1.664, train_wall=6, gb_free=13.1, wall=6106
2024-07-20 11:30:27 | INFO | train_inner | epoch 051:     16 / 19 loss=5.797, nll_loss=2.252, ppl=4.76, wps=1790.2, ups=0.35, wpb=5088.5, bsz=244, num_updates=966, lr=1.1592e-05, gnorm=1.521, train_wall=6, gb_free=12, wall=6112
2024-07-20 11:30:33 | INFO | train_inner | epoch 051:     18 / 19 loss=5.97, nll_loss=2.465, ppl=5.52, wps=1923.9, ups=0.38, wpb=5089, bsz=172, num_updates=968, lr=1.1616e-05, gnorm=1.921, train_wall=5, gb_free=12, wall=6117
2024-07-20 11:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38941.703125Mb; avail=216084.2265625Mb
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000551
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38941.703125Mb; avail=216084.2265625Mb
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002106
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38941.703125Mb; avail=216084.2265625Mb
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001886
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004864
2024-07-20 11:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38942.1953125Mb; avail=216083.734375Mb
2024-07-20 11:30:37 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.104 | nll_loss 2.373 | ppl 5.18 | wps 3991.3 | wpb 1665.6 | bsz 74.4 | num_updates 969 | best_loss 6.104
2024-07-20 11:30:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 969 updates
2024-07-20 11:30:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:31:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:31:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 51 @ 969 updates, score 6.104) (writing took 77.09000604995526 seconds)
2024-07-20 11:31:54 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-07-20 11:31:54 | INFO | train | epoch 051 | loss 5.803 | nll_loss 2.256 | ppl 4.78 | wps 717.5 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 969 | lr 1.1628e-05 | gnorm 1.69 | train_wall 49 | gb_free 16.1 | wall 6199
2024-07-20 11:31:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:31:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:31:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:31:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000671
2024-07-20 11:31:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30777.82421875Mb; avail=224248.27734375Mb
2024-07-20 11:31:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000092
2024-07-20 11:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000855
2024-07-20 11:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30777.82421875Mb; avail=224248.27734375Mb
2024-07-20 11:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000043
2024-07-20 11:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30777.82421875Mb; avail=224248.27734375Mb
2024-07-20 11:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000220
2024-07-20 11:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001413
2024-07-20 11:31:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30777.82421875Mb; avail=224248.27734375Mb
2024-07-20 11:31:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:31:55 | INFO | fairseq.trainer | begin training epoch 52
2024-07-20 11:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:31:57 | INFO | train_inner | epoch 052:      1 / 19 loss=5.737, nll_loss=2.165, ppl=4.48, wps=90, ups=0.02, wpb=3800.5, bsz=172, num_updates=970, lr=1.164e-05, gnorm=1.851, train_wall=5, gb_free=12.8, wall=6201
2024-07-20 11:32:03 | INFO | train_inner | epoch 052:      3 / 19 loss=5.79, nll_loss=2.258, ppl=4.78, wps=1973.8, ups=0.35, wpb=5686, bsz=272, num_updates=972, lr=1.1664e-05, gnorm=1.512, train_wall=6, gb_free=13.9, wall=6207
2024-07-20 11:32:08 | INFO | train_inner | epoch 052:      5 / 19 loss=5.759, nll_loss=2.19, ppl=4.56, wps=1937.3, ups=0.42, wpb=4605.5, bsz=200, num_updates=974, lr=1.1688e-05, gnorm=1.511, train_wall=5, gb_free=13.3, wall=6212
2024-07-20 11:32:14 | INFO | train_inner | epoch 052:      7 / 19 loss=5.756, nll_loss=2.188, ppl=4.56, wps=1866, ups=0.34, wpb=5443.5, bsz=228, num_updates=976, lr=1.1712e-05, gnorm=1.618, train_wall=6, gb_free=11.6, wall=6218
2024-07-20 11:32:20 | INFO | train_inner | epoch 052:      9 / 19 loss=5.727, nll_loss=2.144, ppl=4.42, wps=1886.9, ups=0.33, wpb=5733, bsz=236, num_updates=978, lr=1.1736e-05, gnorm=1.392, train_wall=6, gb_free=11.1, wall=6224
2024-07-20 11:32:25 | INFO | train_inner | epoch 052:     11 / 19 loss=5.71, nll_loss=2.143, ppl=4.42, wps=2283.2, ups=0.36, wpb=6361.5, bsz=292, num_updates=980, lr=1.176e-05, gnorm=1.423, train_wall=6, gb_free=10.9, wall=6229
2024-07-20 11:32:30 | INFO | train_inner | epoch 052:     13 / 19 loss=5.673, nll_loss=2.113, ppl=4.33, wps=1691.3, ups=0.42, wpb=4017.5, bsz=192, num_updates=982, lr=1.1784e-05, gnorm=1.56, train_wall=5, gb_free=12.7, wall=6234
2024-07-20 11:32:36 | INFO | train_inner | epoch 052:     15 / 19 loss=5.69, nll_loss=2.129, ppl=4.37, wps=1679.9, ups=0.34, wpb=4890.5, bsz=244, num_updates=984, lr=1.1808e-05, gnorm=1.608, train_wall=6, gb_free=11.2, wall=6240
2024-07-20 11:32:40 | INFO | train_inner | epoch 052:     17 / 19 loss=6.127, nll_loss=2.673, ppl=6.38, wps=1696.4, ups=0.48, wpb=3514, bsz=128, num_updates=986, lr=1.1832e-05, gnorm=2.107, train_wall=4, gb_free=14.2, wall=6244
2024-07-20 11:32:43 | INFO | train_inner | epoch 052:     19 / 19 loss=5.74, nll_loss=2.152, ppl=4.45, wps=2062.8, ups=0.63, wpb=3296, bsz=165.5, num_updates=988, lr=1.1856e-05, gnorm=1.906, train_wall=3, gb_free=16, wall=6247
2024-07-20 11:32:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30875.0078125Mb; avail=224151.0078125Mb
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000576
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30875.0078125Mb; avail=224151.0078125Mb
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002096
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30875.0078125Mb; avail=224151.0078125Mb
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001871
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004872
2024-07-20 11:32:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30875.5Mb; avail=224150.515625Mb
2024-07-20 11:32:46 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.116 | nll_loss 2.358 | ppl 5.13 | wps 4190 | wpb 1665.6 | bsz 74.4 | num_updates 988 | best_loss 6.104
2024-07-20 11:32:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 988 updates
2024-07-20 11:32:46 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:33:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:33:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 52 @ 988 updates, score 6.116) (writing took 43.3609560909681 seconds)
2024-07-20 11:33:29 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-07-20 11:33:29 | INFO | train | epoch 052 | loss 5.759 | nll_loss 2.201 | ppl 4.6 | wps 978.2 | ups 0.2 | wpb 4866.4 | bsz 219.1 | num_updates 988 | lr 1.1856e-05 | gnorm 1.619 | train_wall 48 | gb_free 16 | wall 6293
2024-07-20 11:33:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:33:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:33:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:33:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000757
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39890.7890625Mb; avail=215135.10546875Mb
2024-07-20 11:33:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000089
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000761
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39891.28125Mb; avail=215135.10546875Mb
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39890.7890625Mb; avail=215134.61328125Mb
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000226
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001326
2024-07-20 11:33:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39890.7890625Mb; avail=215135.10546875Mb
2024-07-20 11:33:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:33:29 | INFO | fairseq.trainer | begin training epoch 53
2024-07-20 11:33:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:33:34 | INFO | train_inner | epoch 053:      2 / 19 loss=5.702, nll_loss=2.114, ppl=4.33, wps=175.6, ups=0.04, wpb=4505.5, bsz=220, num_updates=990, lr=1.188e-05, gnorm=1.703, train_wall=5, gb_free=12.7, wall=6299
2024-07-20 11:33:39 | INFO | train_inner | epoch 053:      4 / 19 loss=5.714, nll_loss=2.147, ppl=4.43, wps=2111.7, ups=0.45, wpb=4662.5, bsz=213.5, num_updates=992, lr=1.1904e-05, gnorm=1.597, train_wall=4, gb_free=18, wall=6303
2024-07-20 11:33:44 | INFO | train_inner | epoch 053:      6 / 19 loss=5.676, nll_loss=2.133, ppl=4.39, wps=2270.4, ups=0.39, wpb=5784, bsz=292, num_updates=994, lr=1.1928e-05, gnorm=1.659, train_wall=5, gb_free=11.7, wall=6308
2024-07-20 11:33:50 | INFO | train_inner | epoch 053:      8 / 19 loss=5.821, nll_loss=2.289, ppl=4.89, wps=1922.9, ups=0.34, wpb=5677, bsz=244, num_updates=996, lr=1.1952e-05, gnorm=1.583, train_wall=6, gb_free=11.4, wall=6314
2024-07-20 11:33:55 | INFO | train_inner | epoch 053:     10 / 19 loss=5.786, nll_loss=2.234, ppl=4.7, wps=2012.3, ups=0.4, wpb=5064.5, bsz=264, num_updates=998, lr=1.1976e-05, gnorm=1.493, train_wall=5, gb_free=11.7, wall=6319
2024-07-20 11:34:01 | INFO | train_inner | epoch 053:     12 / 19 loss=5.632, nll_loss=2.015, ppl=4.04, wps=1597.2, ups=0.33, wpb=4824.5, bsz=232, num_updates=1000, lr=1.2e-05, gnorm=1.722, train_wall=6, gb_free=12.5, wall=6325
2024-07-20 11:34:06 | INFO | train_inner | epoch 053:     14 / 19 loss=5.8, nll_loss=2.221, ppl=4.66, wps=2109.5, ups=0.43, wpb=4961, bsz=184, num_updates=1002, lr=1.2024e-05, gnorm=1.674, train_wall=5, gb_free=15.6, wall=6330
2024-07-20 11:34:12 | INFO | train_inner | epoch 053:     16 / 19 loss=5.655, nll_loss=2.056, ppl=4.16, wps=1666.7, ups=0.33, wpb=5001, bsz=212, num_updates=1004, lr=1.2048e-05, gnorm=1.468, train_wall=6, gb_free=11.8, wall=6336
2024-07-20 11:34:17 | INFO | train_inner | epoch 053:     18 / 19 loss=5.853, nll_loss=2.325, ppl=5.01, wps=1900.2, ups=0.4, wpb=4710, bsz=176, num_updates=1006, lr=1.2072e-05, gnorm=1.671, train_wall=5, gb_free=12.7, wall=6341
2024-07-20 11:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39987.828125Mb; avail=215038.140625Mb
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39988.3203125Mb; avail=215037.6484375Mb
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002068
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39988.3203125Mb; avail=215037.6484375Mb
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001856
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004807
2024-07-20 11:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39988.3203125Mb; avail=215037.6484375Mb
2024-07-20 11:34:20 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.07 | nll_loss 2.354 | ppl 5.11 | wps 4175.7 | wpb 1665.6 | bsz 74.4 | num_updates 1007 | best_loss 6.07
2024-07-20 11:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 1007 updates
2024-07-20 11:34:20 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:35:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:35:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 53 @ 1007 updates, score 6.07) (writing took 69.1628947108984 seconds)
2024-07-20 11:35:29 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-07-20 11:35:29 | INFO | train | epoch 053 | loss 5.742 | nll_loss 2.178 | ppl 4.52 | wps 767.5 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 1007 | lr 1.2084e-05 | gnorm 1.664 | train_wall 49 | gb_free 18 | wall 6414
2024-07-20 11:35:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:35:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:35:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:35:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000632
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27054.734375Mb; avail=227970.3984375Mb
2024-07-20 11:35:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000741
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27048.828125Mb; avail=227976.796875Mb
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27050.3046875Mb; avail=227975.8125Mb
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000216
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001288
2024-07-20 11:35:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27052.765625Mb; avail=227973.3515625Mb
2024-07-20 11:35:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:35:30 | INFO | fairseq.trainer | begin training epoch 54
2024-07-20 11:35:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:35:32 | INFO | train_inner | epoch 054:      1 / 19 loss=5.865, nll_loss=2.364, ppl=5.15, wps=65.7, ups=0.03, wpb=2472.5, bsz=108, num_updates=1008, lr=1.2096e-05, gnorm=2.27, train_wall=3, gb_free=16.9, wall=6416
2024-07-20 11:35:37 | INFO | train_inner | epoch 054:      3 / 19 loss=5.672, nll_loss=2.078, ppl=4.22, wps=1770.5, ups=0.36, wpb=4871.5, bsz=224, num_updates=1010, lr=1.212e-05, gnorm=1.458, train_wall=5, gb_free=11.2, wall=6422
2024-07-20 11:35:43 | INFO | train_inner | epoch 054:      5 / 19 loss=5.63, nll_loss=2.019, ppl=4.05, wps=1944.6, ups=0.36, wpb=5359.5, bsz=256, num_updates=1012, lr=1.2144e-05, gnorm=1.5, train_wall=6, gb_free=12.3, wall=6427
2024-07-20 11:35:48 | INFO | train_inner | epoch 054:      7 / 19 loss=5.656, nll_loss=2.069, ppl=4.2, wps=1925.3, ups=0.42, wpb=4622, bsz=228, num_updates=1014, lr=1.2168e-05, gnorm=1.547, train_wall=5, gb_free=11.7, wall=6432
2024-07-20 11:35:53 | INFO | train_inner | epoch 054:      9 / 19 loss=5.704, nll_loss=2.128, ppl=4.37, wps=1727.6, ups=0.36, wpb=4847, bsz=212, num_updates=1016, lr=1.2192e-05, gnorm=1.568, train_wall=6, gb_free=12.3, wall=6437
2024-07-20 11:35:59 | INFO | train_inner | epoch 054:     11 / 19 loss=5.684, nll_loss=2.102, ppl=4.29, wps=2215.8, ups=0.37, wpb=5983, bsz=252, num_updates=1018, lr=1.2216e-05, gnorm=1.456, train_wall=5, gb_free=12, wall=6443
2024-07-20 11:36:03 | INFO | train_inner | epoch 054:     13 / 19 loss=5.756, nll_loss=2.231, ppl=4.7, wps=1711, ups=0.42, wpb=4055, bsz=185.5, num_updates=1020, lr=1.224e-05, gnorm=1.984, train_wall=5, gb_free=13.2, wall=6448
2024-07-20 11:36:10 | INFO | train_inner | epoch 054:     15 / 19 loss=5.657, nll_loss=2.067, ppl=4.19, wps=1954.6, ups=0.32, wpb=6163.5, bsz=300, num_updates=1022, lr=1.2264e-05, gnorm=1.272, train_wall=6, gb_free=11.3, wall=6454
2024-07-20 11:36:15 | INFO | train_inner | epoch 054:     17 / 19 loss=5.722, nll_loss=2.141, ppl=4.41, wps=1942.7, ups=0.4, wpb=4815, bsz=208, num_updates=1024, lr=1.2288e-05, gnorm=1.614, train_wall=5, gb_free=12.5, wall=6459
2024-07-20 11:36:19 | INFO | train_inner | epoch 054:     19 / 19 loss=5.807, nll_loss=2.239, ppl=4.72, wps=2127, ups=0.52, wpb=4083, bsz=152, num_updates=1026, lr=1.2312e-05, gnorm=2.334, train_wall=4, gb_free=17.9, wall=6463
2024-07-20 11:36:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27133.80859375Mb; avail=227892.25Mb
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000553
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27133.80859375Mb; avail=227892.25Mb
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002089
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27133.80859375Mb; avail=227892.25Mb
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001864
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004830
2024-07-20 11:36:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27133.80859375Mb; avail=227892.25Mb
2024-07-20 11:36:21 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.061 | nll_loss 2.334 | ppl 5.04 | wps 4179.6 | wpb 1665.6 | bsz 74.4 | num_updates 1026 | best_loss 6.061
2024-07-20 11:36:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 1026 updates
2024-07-20 11:36:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:36:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:37:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 54 @ 1026 updates, score 6.061) (writing took 61.41040469799191 seconds)
2024-07-20 11:37:22 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-07-20 11:37:22 | INFO | train | epoch 054 | loss 5.698 | nll_loss 2.119 | ppl 4.34 | wps 818.4 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 1026 | lr 1.2312e-05 | gnorm 1.659 | train_wall 49 | gb_free 17.9 | wall 6527
2024-07-20 11:37:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:37:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:37:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:37:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000807
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=49713.6875Mb; avail=205311.87890625Mb
2024-07-20 11:37:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000107
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000751
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49708.2734375Mb; avail=205317.29296875Mb
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49709.75Mb; avail=205316.30859375Mb
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000231
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001298
2024-07-20 11:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49712.2109375Mb; avail=205313.35546875Mb
2024-07-20 11:37:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:37:23 | INFO | fairseq.trainer | begin training epoch 55
2024-07-20 11:37:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:37:28 | INFO | train_inner | epoch 055:      2 / 19 loss=5.649, nll_loss=2.072, ppl=4.2, wps=176.5, ups=0.03, wpb=6106.5, bsz=264, num_updates=1028, lr=1.2336e-05, gnorm=1.419, train_wall=5, gb_free=11.2, wall=6532
2024-07-20 11:37:33 | INFO | train_inner | epoch 055:      4 / 19 loss=5.782, nll_loss=2.247, ppl=4.75, wps=1724.5, ups=0.4, wpb=4281.5, bsz=193.5, num_updates=1030, lr=1.236e-05, gnorm=2.116, train_wall=5, gb_free=17.8, wall=6537
2024-07-20 11:37:38 | INFO | train_inner | epoch 055:      6 / 19 loss=5.467, nll_loss=1.846, ppl=3.59, wps=1806.6, ups=0.35, wpb=5143.5, bsz=280, num_updates=1032, lr=1.2384e-05, gnorm=1.405, train_wall=6, gb_free=12.5, wall=6543
2024-07-20 11:37:44 | INFO | train_inner | epoch 055:      8 / 19 loss=5.655, nll_loss=2.071, ppl=4.2, wps=1698.2, ups=0.33, wpb=5112.5, bsz=248, num_updates=1034, lr=1.2408e-05, gnorm=1.587, train_wall=6, gb_free=12.3, wall=6549
2024-07-20 11:37:50 | INFO | train_inner | epoch 055:     10 / 19 loss=5.672, nll_loss=2.069, ppl=4.2, wps=1478, ups=0.37, wpb=4016, bsz=156, num_updates=1036, lr=1.2432e-05, gnorm=2.016, train_wall=5, gb_free=11.8, wall=6554
2024-07-20 11:37:56 | INFO | train_inner | epoch 055:     12 / 19 loss=5.717, nll_loss=2.139, ppl=4.41, wps=1645.6, ups=0.35, wpb=4686, bsz=204, num_updates=1038, lr=1.2456e-05, gnorm=1.774, train_wall=6, gb_free=12.9, wall=6560
2024-07-20 11:38:01 | INFO | train_inner | epoch 055:     14 / 19 loss=5.861, nll_loss=2.331, ppl=5.03, wps=1928.1, ups=0.38, wpb=5137.5, bsz=212, num_updates=1040, lr=1.248e-05, gnorm=1.931, train_wall=5, gb_free=11.9, wall=6565
2024-07-20 11:38:06 | INFO | train_inner | epoch 055:     16 / 19 loss=5.727, nll_loss=2.141, ppl=4.41, wps=1859.4, ups=0.41, wpb=4535, bsz=188, num_updates=1042, lr=1.2504e-05, gnorm=1.665, train_wall=5, gb_free=13.1, wall=6570
2024-07-20 11:38:12 | INFO | train_inner | epoch 055:     18 / 19 loss=5.639, nll_loss=2.047, ppl=4.13, wps=2007.6, ups=0.34, wpb=5980, bsz=292, num_updates=1044, lr=1.2528e-05, gnorm=1.558, train_wall=6, gb_free=11.4, wall=6576
2024-07-20 11:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38535.83984375Mb; avail=216490.0859375Mb
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38535.83984375Mb; avail=216490.0859375Mb
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002123
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38536.33203125Mb; avail=216489.59375Mb
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001837
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004855
2024-07-20 11:38:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38536.33203125Mb; avail=216489.59375Mb
2024-07-20 11:38:16 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.043 | nll_loss 2.328 | ppl 5.02 | wps 4168.3 | wpb 1665.6 | bsz 74.4 | num_updates 1045 | best_loss 6.043
2024-07-20 11:38:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 1045 updates
2024-07-20 11:38:16 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:39:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 55 @ 1045 updates, score 6.043) (writing took 75.94635212887079 seconds)
2024-07-20 11:39:32 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-07-20 11:39:32 | INFO | train | epoch 055 | loss 5.683 | nll_loss 2.104 | ppl 4.3 | wps 714.5 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 1045 | lr 1.254e-05 | gnorm 1.732 | train_wall 51 | gb_free 16.5 | wall 6656
2024-07-20 11:39:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:39:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:39:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:39:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000653
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32791.43359375Mb; avail=222234.62109375Mb
2024-07-20 11:39:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000708
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32791.43359375Mb; avail=222234.62109375Mb
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32791.43359375Mb; avail=222234.62109375Mb
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000209
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001223
2024-07-20 11:39:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32791.4296875Mb; avail=222234.62109375Mb
2024-07-20 11:39:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:39:32 | INFO | fairseq.trainer | begin training epoch 56
2024-07-20 11:39:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:39:34 | INFO | train_inner | epoch 056:      1 / 19 loss=5.912, nll_loss=2.377, ppl=5.2, wps=76.1, ups=0.02, wpb=3141, bsz=112, num_updates=1046, lr=1.2552e-05, gnorm=1.964, train_wall=4, gb_free=12.7, wall=6658
2024-07-20 11:39:39 | INFO | train_inner | epoch 056:      3 / 19 loss=5.782, nll_loss=2.243, ppl=4.73, wps=1758.3, ups=0.39, wpb=4497, bsz=161.5, num_updates=1048, lr=1.2576e-05, gnorm=1.765, train_wall=5, gb_free=12.4, wall=6664
2024-07-20 11:39:43 | INFO | train_inner | epoch 056:      5 / 19 loss=5.83, nll_loss=2.325, ppl=5.01, wps=1861.9, ups=0.53, wpb=3535, bsz=132, num_updates=1050, lr=1.26e-05, gnorm=2.215, train_wall=4, gb_free=19.4, wall=6667
2024-07-20 11:39:48 | INFO | train_inner | epoch 056:      7 / 19 loss=5.678, nll_loss=2.083, ppl=4.24, wps=2306.5, ups=0.39, wpb=5947.5, bsz=272, num_updates=1052, lr=1.2624e-05, gnorm=1.543, train_wall=5, gb_free=12.1, wall=6673
2024-07-20 11:39:54 | INFO | train_inner | epoch 056:      9 / 19 loss=5.492, nll_loss=1.838, ppl=3.58, wps=1577.7, ups=0.36, wpb=4435, bsz=228, num_updates=1054, lr=1.2648e-05, gnorm=1.502, train_wall=6, gb_free=12.4, wall=6678
2024-07-20 11:40:00 | INFO | train_inner | epoch 056:     11 / 19 loss=5.563, nll_loss=1.954, ppl=3.88, wps=1932.4, ups=0.33, wpb=5863, bsz=276, num_updates=1056, lr=1.2672e-05, gnorm=1.372, train_wall=6, gb_free=11.4, wall=6684
2024-07-20 11:40:06 | INFO | train_inner | epoch 056:     13 / 19 loss=5.593, nll_loss=2.01, ppl=4.03, wps=2120.2, ups=0.34, wpb=6256.5, bsz=316, num_updates=1058, lr=1.2696e-05, gnorm=1.288, train_wall=6, gb_free=11.1, wall=6690
2024-07-20 11:40:12 | INFO | train_inner | epoch 056:     15 / 19 loss=5.572, nll_loss=1.977, ppl=3.94, wps=1906.9, ups=0.36, wpb=5330.5, bsz=264, num_updates=1060, lr=1.272e-05, gnorm=1.427, train_wall=6, gb_free=11.8, wall=6696
2024-07-20 11:40:17 | INFO | train_inner | epoch 056:     17 / 19 loss=5.582, nll_loss=1.983, ppl=3.95, wps=1881, ups=0.34, wpb=5574, bsz=268, num_updates=1062, lr=1.2744e-05, gnorm=1.435, train_wall=6, gb_free=11.1, wall=6702
2024-07-20 11:40:21 | INFO | train_inner | epoch 056:     19 / 19 loss=5.775, nll_loss=2.186, ppl=4.55, wps=1631.1, ups=0.57, wpb=2884, bsz=96, num_updates=1064, lr=1.2768e-05, gnorm=2.264, train_wall=4, gb_free=18.8, wall=6705
2024-07-20 11:40:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32898.359375Mb; avail=222127.6484375Mb
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32898.359375Mb; avail=222127.6484375Mb
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002087
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32898.359375Mb; avail=222127.6484375Mb
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001864
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004843
2024-07-20 11:40:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32898.359375Mb; avail=222127.6484375Mb
2024-07-20 11:40:24 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.066 | nll_loss 2.317 | ppl 4.98 | wps 4129.1 | wpb 1665.6 | bsz 74.4 | num_updates 1064 | best_loss 6.043
2024-07-20 11:40:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 1064 updates
2024-07-20 11:40:24 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:41:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 56 @ 1064 updates, score 6.066) (writing took 43.10822388390079 seconds)
2024-07-20 11:41:07 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-07-20 11:41:07 | INFO | train | epoch 056 | loss 5.652 | nll_loss 2.068 | ppl 4.19 | wps 975.2 | ups 0.2 | wpb 4866.4 | bsz 219.1 | num_updates 1064 | lr 1.2768e-05 | gnorm 1.662 | train_wall 49 | gb_free 18.8 | wall 6751
2024-07-20 11:41:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:41:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:41:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:41:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000756
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44635.90234375Mb; avail=210394.1171875Mb
2024-07-20 11:41:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000076
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000717
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44635.90234375Mb; avail=210394.1171875Mb
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44635.90234375Mb; avail=210394.1171875Mb
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000227
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001244
2024-07-20 11:41:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44635.90234375Mb; avail=210394.1171875Mb
2024-07-20 11:41:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:41:07 | INFO | fairseq.trainer | begin training epoch 57
2024-07-20 11:41:07 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:41:12 | INFO | train_inner | epoch 057:      2 / 19 loss=5.579, nll_loss=1.957, ppl=3.88, wps=212.2, ups=0.04, wpb=5451.5, bsz=228, num_updates=1066, lr=1.2792e-05, gnorm=1.688, train_wall=6, gb_free=11.8, wall=6757
2024-07-20 11:41:17 | INFO | train_inner | epoch 057:      4 / 19 loss=5.682, nll_loss=2.103, ppl=4.3, wps=2085.3, ups=0.42, wpb=5005, bsz=196, num_updates=1068, lr=1.2816e-05, gnorm=1.763, train_wall=5, gb_free=11.8, wall=6761
2024-07-20 11:41:22 | INFO | train_inner | epoch 057:      6 / 19 loss=5.684, nll_loss=2.105, ppl=4.3, wps=1865.2, ups=0.41, wpb=4550.5, bsz=212, num_updates=1070, lr=1.284e-05, gnorm=1.475, train_wall=5, gb_free=11, wall=6766
2024-07-20 11:41:28 | INFO | train_inner | epoch 057:      8 / 19 loss=5.645, nll_loss=2.065, ppl=4.18, wps=1867.9, ups=0.33, wpb=5586, bsz=224, num_updates=1072, lr=1.2864e-05, gnorm=1.479, train_wall=6, gb_free=12.6, wall=6772
2024-07-20 11:41:34 | INFO | train_inner | epoch 057:     10 / 19 loss=5.501, nll_loss=1.889, ppl=3.7, wps=1944.5, ups=0.34, wpb=5654, bsz=280, num_updates=1074, lr=1.2888e-05, gnorm=1.442, train_wall=6, gb_free=12.4, wall=6778
2024-07-20 11:41:38 | INFO | train_inner | epoch 057:     12 / 19 loss=5.782, nll_loss=2.216, ppl=4.65, wps=1751.4, ups=0.45, wpb=3934, bsz=160, num_updates=1076, lr=1.2912e-05, gnorm=2.213, train_wall=4, gb_free=12.3, wall=6783
2024-07-20 11:41:42 | INFO | train_inner | epoch 057:     14 / 19 loss=5.569, nll_loss=1.921, ppl=3.79, wps=1825.8, ups=0.49, wpb=3730, bsz=189.5, num_updates=1078, lr=1.2936e-05, gnorm=1.814, train_wall=4, gb_free=12.2, wall=6787
2024-07-20 11:41:48 | INFO | train_inner | epoch 057:     16 / 19 loss=5.607, nll_loss=2.003, ppl=4.01, wps=2199.3, ups=0.38, wpb=5821.5, bsz=272, num_updates=1080, lr=1.296e-05, gnorm=1.465, train_wall=5, gb_free=11.9, wall=6792
2024-07-20 11:41:54 | INFO | train_inner | epoch 057:     18 / 19 loss=5.562, nll_loss=1.961, ppl=3.89, wps=1765.4, ups=0.33, wpb=5334, bsz=260, num_updates=1082, lr=1.2984e-05, gnorm=1.364, train_wall=6, gb_free=12.3, wall=6798
2024-07-20 11:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=51234.90234375Mb; avail=203794.58203125Mb
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51236.37890625Mb; avail=203793.59765625Mb
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002131
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51241.79296875Mb; avail=203788.18359375Mb
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001876
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004920
2024-07-20 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51246.22265625Mb; avail=203783.26171875Mb
2024-07-20 11:41:58 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.036 | nll_loss 2.316 | ppl 4.98 | wps 4169.5 | wpb 1665.6 | bsz 74.4 | num_updates 1083 | best_loss 6.036
2024-07-20 11:41:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 1083 updates
2024-07-20 11:41:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:42:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 57 @ 1083 updates, score 6.036) (writing took 77.89587744814344 seconds)
2024-07-20 11:43:15 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-07-20 11:43:15 | INFO | train | epoch 057 | loss 5.617 | nll_loss 2.018 | ppl 4.05 | wps 718.1 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 1083 | lr 1.2996e-05 | gnorm 1.655 | train_wall 48 | gb_free 17.8 | wall 6880
2024-07-20 11:43:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:43:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:43:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:43:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000634
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32424.78125Mb; avail=222605.32421875Mb
2024-07-20 11:43:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000093
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000739
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32424.78125Mb; avail=222605.32421875Mb
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32424.78125Mb; avail=222605.32421875Mb
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000223
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001277
2024-07-20 11:43:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32424.78125Mb; avail=222605.32421875Mb
2024-07-20 11:43:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:43:16 | INFO | fairseq.trainer | begin training epoch 58
2024-07-20 11:43:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:43:18 | INFO | train_inner | epoch 058:      1 / 19 loss=5.597, nll_loss=2.022, ppl=4.06, wps=103.5, ups=0.02, wpb=4368, bsz=204, num_updates=1084, lr=1.3008e-05, gnorm=1.658, train_wall=4, gb_free=11.8, wall=6882
2024-07-20 11:43:24 | INFO | train_inner | epoch 058:      3 / 19 loss=5.705, nll_loss=2.126, ppl=4.36, wps=1741.7, ups=0.35, wpb=5012, bsz=192, num_updates=1086, lr=1.3032e-05, gnorm=1.657, train_wall=6, gb_free=13.1, wall=6888
2024-07-20 11:43:30 | INFO | train_inner | epoch 058:      5 / 19 loss=5.61, nll_loss=1.998, ppl=3.99, wps=1730.7, ups=0.36, wpb=4843.5, bsz=232, num_updates=1088, lr=1.3056e-05, gnorm=1.525, train_wall=6, gb_free=13.3, wall=6894
2024-07-20 11:43:35 | INFO | train_inner | epoch 058:      7 / 19 loss=5.604, nll_loss=1.995, ppl=3.99, wps=1994.3, ups=0.34, wpb=5853, bsz=288, num_updates=1090, lr=1.308e-05, gnorm=1.384, train_wall=6, gb_free=12.6, wall=6900
2024-07-20 11:43:41 | INFO | train_inner | epoch 058:      9 / 19 loss=5.548, nll_loss=1.914, ppl=3.77, wps=1908.4, ups=0.39, wpb=4872, bsz=256, num_updates=1092, lr=1.3104e-05, gnorm=1.456, train_wall=5, gb_free=12.7, wall=6905
2024-07-20 11:43:46 | INFO | train_inner | epoch 058:     11 / 19 loss=5.621, nll_loss=2.026, ppl=4.07, wps=1837.8, ups=0.39, wpb=4753, bsz=208, num_updates=1094, lr=1.3128e-05, gnorm=1.821, train_wall=5, gb_free=12.6, wall=6910
2024-07-20 11:43:51 | INFO | train_inner | epoch 058:     13 / 19 loss=5.618, nll_loss=2.05, ppl=4.14, wps=2283.8, ups=0.41, wpb=5594.5, bsz=228, num_updates=1096, lr=1.3152e-05, gnorm=1.547, train_wall=5, gb_free=13.1, wall=6915
2024-07-20 11:43:56 | INFO | train_inner | epoch 058:     15 / 19 loss=5.502, nll_loss=1.874, ppl=3.67, wps=1388, ups=0.37, wpb=3781, bsz=156, num_updates=1098, lr=1.3176e-05, gnorm=1.795, train_wall=5, gb_free=12.7, wall=6920
2024-07-20 11:44:01 | INFO | train_inner | epoch 058:     17 / 19 loss=5.572, nll_loss=1.941, ppl=3.84, wps=1758.7, ups=0.41, wpb=4320, bsz=197.5, num_updates=1100, lr=1.32e-05, gnorm=1.865, train_wall=5, gb_free=11.2, wall=6925
2024-07-20 11:44:05 | INFO | train_inner | epoch 058:     19 / 19 loss=5.615, nll_loss=2.007, ppl=4.02, wps=2039, ups=0.51, wpb=3998.5, bsz=180, num_updates=1102, lr=1.3224e-05, gnorm=1.753, train_wall=4, gb_free=16.1, wall=6929
2024-07-20 11:44:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=49631.3671875Mb; avail=205398.16796875Mb
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49631.859375Mb; avail=205398.16796875Mb
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002083
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49632.3515625Mb; avail=205397.67578125Mb
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001834
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004805
2024-07-20 11:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49631.859375Mb; avail=205398.16796875Mb
2024-07-20 11:44:08 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.034 | nll_loss 2.295 | ppl 4.91 | wps 3621.4 | wpb 1665.6 | bsz 74.4 | num_updates 1102 | best_loss 6.034
2024-07-20 11:44:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 1102 updates
2024-07-20 11:44:08 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:44:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:45:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 58 @ 1102 updates, score 6.034) (writing took 68.11342534911819 seconds)
2024-07-20 11:45:16 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-07-20 11:45:16 | INFO | train | epoch 058 | loss 5.603 | nll_loss 2 | ppl 4 | wps 767.6 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 1102 | lr 1.3224e-05 | gnorm 1.626 | train_wall 49 | gb_free 16.1 | wall 7000
2024-07-20 11:45:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:45:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:45:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:45:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000767
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40999.4140625Mb; avail=214030.59765625Mb
2024-07-20 11:45:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000106
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000781
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40999.4140625Mb; avail=214030.59765625Mb
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40999.4140625Mb; avail=214030.59765625Mb
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000229
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001347
2024-07-20 11:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40999.4140625Mb; avail=214030.59765625Mb
2024-07-20 11:45:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:45:16 | INFO | fairseq.trainer | begin training epoch 59
2024-07-20 11:45:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:45:22 | INFO | train_inner | epoch 059:      2 / 19 loss=5.467, nll_loss=1.827, ppl=3.55, wps=124.7, ups=0.03, wpb=4800.5, bsz=240, num_updates=1104, lr=1.3248e-05, gnorm=1.447, train_wall=6, gb_free=10.9, wall=7006
2024-07-20 11:45:27 | INFO | train_inner | epoch 059:      4 / 19 loss=5.704, nll_loss=2.131, ppl=4.38, wps=2093.4, ups=0.41, wpb=5048, bsz=192, num_updates=1106, lr=1.3272e-05, gnorm=1.627, train_wall=5, gb_free=12.3, wall=7011
2024-07-20 11:45:31 | INFO | train_inner | epoch 059:      6 / 19 loss=5.694, nll_loss=2.119, ppl=4.34, wps=1892.6, ups=0.42, wpb=4473.5, bsz=180, num_updates=1108, lr=1.3296e-05, gnorm=1.557, train_wall=5, gb_free=13.2, wall=7016
2024-07-20 11:45:38 | INFO | train_inner | epoch 059:      8 / 19 loss=5.458, nll_loss=1.81, ppl=3.51, wps=1971.4, ups=0.32, wpb=6086, bsz=292, num_updates=1110, lr=1.332e-05, gnorm=1.442, train_wall=6, gb_free=11.4, wall=7022
2024-07-20 11:45:43 | INFO | train_inner | epoch 059:     10 / 19 loss=5.576, nll_loss=1.975, ppl=3.93, wps=1752.9, ups=0.4, wpb=4433.5, bsz=225.5, num_updates=1112, lr=1.3344e-05, gnorm=2.149, train_wall=5, gb_free=16.5, wall=7027
2024-07-20 11:45:48 | INFO | train_inner | epoch 059:     12 / 19 loss=5.554, nll_loss=1.936, ppl=3.83, wps=2152.2, ups=0.38, wpb=5707.5, bsz=256, num_updates=1114, lr=1.3368e-05, gnorm=1.445, train_wall=5, gb_free=13, wall=7032
2024-07-20 11:45:54 | INFO | train_inner | epoch 059:     14 / 19 loss=5.482, nll_loss=1.845, ppl=3.59, wps=1760.6, ups=0.35, wpb=5102.5, bsz=272, num_updates=1116, lr=1.3392e-05, gnorm=1.433, train_wall=6, gb_free=12.6, wall=7038
2024-07-20 11:45:59 | INFO | train_inner | epoch 059:     16 / 19 loss=5.735, nll_loss=2.151, ppl=4.44, wps=1270.3, ups=0.42, wpb=3001, bsz=96, num_updates=1118, lr=1.3416e-05, gnorm=2.083, train_wall=5, gb_free=16.5, wall=7043
2024-07-20 11:46:04 | INFO | train_inner | epoch 059:     18 / 19 loss=5.512, nll_loss=1.903, ppl=3.74, wps=2210.9, ups=0.37, wpb=5979, bsz=268, num_updates=1120, lr=1.344e-05, gnorm=1.438, train_wall=5, gb_free=12.9, wall=7048
2024-07-20 11:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46463.19921875Mb; avail=208567.30859375Mb
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46463.19921875Mb; avail=208567.30859375Mb
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002130
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46463.19921875Mb; avail=208566.81640625Mb
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001833
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004861
2024-07-20 11:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46463.19921875Mb; avail=208566.81640625Mb
2024-07-20 11:46:08 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.033 | nll_loss 2.295 | ppl 4.91 | wps 4189.4 | wpb 1665.6 | bsz 74.4 | num_updates 1121 | best_loss 6.033
2024-07-20 11:46:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 1121 updates
2024-07-20 11:46:08 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:47:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:47:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 59 @ 1121 updates, score 6.033) (writing took 76.9883812640328 seconds)
2024-07-20 11:47:25 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-07-20 11:47:25 | INFO | train | epoch 059 | loss 5.561 | nll_loss 1.949 | ppl 3.86 | wps 716.8 | ups 0.15 | wpb 4866.4 | bsz 219.1 | num_updates 1121 | lr 1.3452e-05 | gnorm 1.625 | train_wall 49 | gb_free 15.5 | wall 7129
2024-07-20 11:47:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:47:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:47:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:47:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000651
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42409.10546875Mb; avail=212621.0Mb
2024-07-20 11:47:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000093
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000751
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42409.59765625Mb; avail=212620.5078125Mb
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42409.10546875Mb; avail=212621.0Mb
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000232
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001305
2024-07-20 11:47:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42409.10546875Mb; avail=212621.0Mb
2024-07-20 11:47:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:47:25 | INFO | fairseq.trainer | begin training epoch 60
2024-07-20 11:47:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:47:27 | INFO | train_inner | epoch 060:      1 / 19 loss=5.613, nll_loss=2.018, ppl=4.05, wps=82.7, ups=0.02, wpb=3419, bsz=133.5, num_updates=1122, lr=1.3464e-05, gnorm=1.765, train_wall=3, gb_free=15.7, wall=7131
2024-07-20 11:47:32 | INFO | train_inner | epoch 060:      3 / 19 loss=5.669, nll_loss=2.072, ppl=4.2, wps=1945.5, ups=0.41, wpb=4728.5, bsz=188, num_updates=1124, lr=1.3488e-05, gnorm=2.07, train_wall=5, gb_free=12, wall=7136
2024-07-20 11:47:36 | INFO | train_inner | epoch 060:      5 / 19 loss=5.652, nll_loss=2.06, ppl=4.17, wps=2145.8, ups=0.47, wpb=4536.5, bsz=172, num_updates=1126, lr=1.3512e-05, gnorm=1.816, train_wall=4, gb_free=12.9, wall=7140
2024-07-20 11:47:42 | INFO | train_inner | epoch 060:      7 / 19 loss=5.5, nll_loss=1.863, ppl=3.64, wps=1957.3, ups=0.34, wpb=5790, bsz=288, num_updates=1128, lr=1.3536e-05, gnorm=1.527, train_wall=6, gb_free=11.9, wall=7146
2024-07-20 11:47:46 | INFO | train_inner | epoch 060:      9 / 19 loss=5.465, nll_loss=1.844, ppl=3.59, wps=2042.4, ups=0.42, wpb=4904.5, bsz=252, num_updates=1130, lr=1.356e-05, gnorm=1.557, train_wall=5, gb_free=12.1, wall=7151
2024-07-20 11:47:52 | INFO | train_inner | epoch 060:     11 / 19 loss=5.454, nll_loss=1.808, ppl=3.5, wps=1828.2, ups=0.33, wpb=5490.5, bsz=244, num_updates=1132, lr=1.3584e-05, gnorm=1.429, train_wall=6, gb_free=12.8, wall=7157
2024-07-20 11:47:57 | INFO | train_inner | epoch 060:     13 / 19 loss=5.71, nll_loss=2.122, ppl=4.35, wps=2076.7, ups=0.4, wpb=5180, bsz=184, num_updates=1134, lr=1.3608e-05, gnorm=1.659, train_wall=5, gb_free=11.4, wall=7162
2024-07-20 11:48:03 | INFO | train_inner | epoch 060:     15 / 19 loss=5.492, nll_loss=1.854, ppl=3.61, wps=2031.5, ups=0.39, wpb=5147, bsz=260, num_updates=1136, lr=1.3632e-05, gnorm=1.791, train_wall=5, gb_free=12.7, wall=7167
2024-07-20 11:48:09 | INFO | train_inner | epoch 060:     17 / 19 loss=5.5, nll_loss=1.871, ppl=3.66, wps=1885.5, ups=0.33, wpb=5680, bsz=264, num_updates=1138, lr=1.3656e-05, gnorm=1.486, train_wall=6, gb_free=12.4, wall=7173
2024-07-20 11:48:12 | INFO | train_inner | epoch 060:     19 / 19 loss=5.35, nll_loss=1.708, ppl=3.27, wps=1737.7, ups=0.59, wpb=2954.5, bsz=156, num_updates=1140, lr=1.368e-05, gnorm=1.775, train_wall=3, gb_free=17.4, wall=7176
2024-07-20 11:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31090.1796875Mb; avail=223939.87890625Mb
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31090.1796875Mb; avail=223939.87890625Mb
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002117
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31090.1796875Mb; avail=223939.87890625Mb
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001850
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004854
2024-07-20 11:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31090.1796875Mb; avail=223939.87890625Mb
2024-07-20 11:48:15 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.018 | nll_loss 2.283 | ppl 4.87 | wps 3037.7 | wpb 1665.6 | bsz 74.4 | num_updates 1140 | best_loss 6.018
2024-07-20 11:48:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 1140 updates
2024-07-20 11:48:15 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:48:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 60 @ 1140 updates, score 6.018) (writing took 66.78794309590012 seconds)
2024-07-20 11:49:22 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-07-20 11:49:22 | INFO | train | epoch 060 | loss 5.544 | nll_loss 1.925 | ppl 3.8 | wps 788.7 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 1140 | lr 1.368e-05 | gnorm 1.69 | train_wall 47 | gb_free 17.4 | wall 7246
2024-07-20 11:49:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:49:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:49:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:49:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000794
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=56371.42578125Mb; avail=198658.59375Mb
2024-07-20 11:49:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000108
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000811
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56371.42578125Mb; avail=198658.59375Mb
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000045
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56371.42578125Mb; avail=198658.59375Mb
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000257
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001452
2024-07-20 11:49:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56371.42578125Mb; avail=198658.59375Mb
2024-07-20 11:49:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:49:22 | INFO | fairseq.trainer | begin training epoch 61
2024-07-20 11:49:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:49:28 | INFO | train_inner | epoch 061:      2 / 19 loss=5.471, nll_loss=1.836, ppl=3.57, wps=151.6, ups=0.03, wpb=5779.5, bsz=260, num_updates=1142, lr=1.3704e-05, gnorm=1.484, train_wall=6, gb_free=11.6, wall=7252
2024-07-20 11:49:32 | INFO | train_inner | epoch 061:      4 / 19 loss=5.629, nll_loss=2.029, ppl=4.08, wps=2292.1, ups=0.49, wpb=4703.5, bsz=189.5, num_updates=1144, lr=1.3728e-05, gnorm=1.691, train_wall=4, gb_free=15.7, wall=7256
2024-07-20 11:49:37 | INFO | train_inner | epoch 061:      6 / 19 loss=5.461, nll_loss=1.804, ppl=3.49, wps=2129, ups=0.4, wpb=5324, bsz=272, num_updates=1146, lr=1.3752e-05, gnorm=1.494, train_wall=5, gb_free=11.8, wall=7261
2024-07-20 11:49:43 | INFO | train_inner | epoch 061:      8 / 19 loss=5.518, nll_loss=1.883, ppl=3.69, wps=1623, ups=0.35, wpb=4677.5, bsz=180, num_updates=1148, lr=1.3776e-05, gnorm=1.619, train_wall=6, gb_free=12.6, wall=7267
2024-07-20 11:49:49 | INFO | train_inner | epoch 061:     10 / 19 loss=5.414, nll_loss=1.794, ppl=3.47, wps=1643.4, ups=0.34, wpb=4793, bsz=232, num_updates=1150, lr=1.38e-05, gnorm=1.455, train_wall=6, gb_free=11.9, wall=7273
2024-07-20 11:49:54 | INFO | train_inner | epoch 061:     12 / 19 loss=5.513, nll_loss=1.904, ppl=3.74, wps=1959.8, ups=0.4, wpb=4929, bsz=220, num_updates=1152, lr=1.3824e-05, gnorm=1.476, train_wall=5, gb_free=12.2, wall=7278
2024-07-20 11:49:59 | INFO | train_inner | epoch 061:     14 / 19 loss=5.666, nll_loss=2.09, ppl=4.26, wps=1426.9, ups=0.41, wpb=3455, bsz=140, num_updates=1154, lr=1.3848e-05, gnorm=2.103, train_wall=5, gb_free=18, wall=7283
2024-07-20 11:50:04 | INFO | train_inner | epoch 061:     16 / 19 loss=5.452, nll_loss=1.771, ppl=3.41, wps=2091.9, ups=0.4, wpb=5268.5, bsz=248, num_updates=1156, lr=1.3872e-05, gnorm=1.513, train_wall=5, gb_free=11.6, wall=7288
2024-07-20 11:50:10 | INFO | train_inner | epoch 061:     18 / 19 loss=5.566, nll_loss=1.926, ppl=3.8, wps=1895.7, ups=0.33, wpb=5715.5, bsz=272, num_updates=1158, lr=1.3896e-05, gnorm=1.487, train_wall=6, gb_free=12.3, wall=7294
2024-07-20 11:50:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40123.65625Mb; avail=214906.37890625Mb
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000563
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40123.65625Mb; avail=214906.37890625Mb
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002082
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40123.65625Mb; avail=214905.88671875Mb
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001861
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004831
2024-07-20 11:50:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40124.1484375Mb; avail=214905.88671875Mb
2024-07-20 11:50:15 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.048 | nll_loss 2.286 | ppl 4.88 | wps 3111.7 | wpb 1665.6 | bsz 74.4 | num_updates 1159 | best_loss 6.018
2024-07-20 11:50:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 1159 updates
2024-07-20 11:50:15 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:51:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:51:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 61 @ 1159 updates, score 6.048) (writing took 52.97041639708914 seconds)
2024-07-20 11:51:08 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-07-20 11:51:08 | INFO | train | epoch 061 | loss 5.515 | nll_loss 1.884 | ppl 3.69 | wps 877.3 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1159 | lr 1.3908e-05 | gnorm 1.605 | train_wall 49 | gb_free 16.7 | wall 7352
2024-07-20 11:51:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:51:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:51:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:51:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000607
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36480.72265625Mb; avail=218549.3828125Mb
2024-07-20 11:51:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000723
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36480.72265625Mb; avail=218549.3828125Mb
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36480.72265625Mb; avail=218549.3828125Mb
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000207
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001240
2024-07-20 11:51:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36480.72265625Mb; avail=218549.3828125Mb
2024-07-20 11:51:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:51:08 | INFO | fairseq.trainer | begin training epoch 62
2024-07-20 11:51:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:51:11 | INFO | train_inner | epoch 062:      1 / 19 loss=5.503, nll_loss=1.852, ppl=3.61, wps=140.9, ups=0.03, wpb=4302, bsz=176, num_updates=1160, lr=1.392e-05, gnorm=1.711, train_wall=5, gb_free=12, wall=7355
2024-07-20 11:51:16 | INFO | train_inner | epoch 062:      3 / 19 loss=5.516, nll_loss=1.899, ppl=3.73, wps=1942.9, ups=0.38, wpb=5094, bsz=224, num_updates=1162, lr=1.3944e-05, gnorm=1.621, train_wall=5, gb_free=11.5, wall=7360
2024-07-20 11:51:21 | INFO | train_inner | epoch 062:      5 / 19 loss=5.385, nll_loss=1.758, ppl=3.38, wps=1787.4, ups=0.38, wpb=4686.5, bsz=244, num_updates=1164, lr=1.3968e-05, gnorm=1.54, train_wall=5, gb_free=10.6, wall=7366
2024-07-20 11:51:26 | INFO | train_inner | epoch 062:      7 / 19 loss=5.539, nll_loss=1.924, ppl=3.79, wps=2252.3, ups=0.4, wpb=5660, bsz=224, num_updates=1166, lr=1.3992e-05, gnorm=1.458, train_wall=5, gb_free=13.3, wall=7371
2024-07-20 11:51:32 | INFO | train_inner | epoch 062:      9 / 19 loss=5.413, nll_loss=1.762, ppl=3.39, wps=1640.5, ups=0.35, wpb=4663.5, bsz=252, num_updates=1168, lr=1.4016e-05, gnorm=1.487, train_wall=6, gb_free=13.2, wall=7376
2024-07-20 11:51:38 | INFO | train_inner | epoch 062:     11 / 19 loss=5.641, nll_loss=2.018, ppl=4.05, wps=1829, ups=0.34, wpb=5323.5, bsz=204, num_updates=1170, lr=1.404e-05, gnorm=1.511, train_wall=6, gb_free=11.5, wall=7382
2024-07-20 11:51:43 | INFO | train_inner | epoch 062:     13 / 19 loss=5.489, nll_loss=1.825, ppl=3.54, wps=1879.8, ups=0.42, wpb=4526.5, bsz=176, num_updates=1172, lr=1.4064e-05, gnorm=1.613, train_wall=5, gb_free=14.1, wall=7387
2024-07-20 11:51:49 | INFO | train_inner | epoch 062:     15 / 19 loss=5.452, nll_loss=1.806, ppl=3.5, wps=1866.2, ups=0.32, wpb=5878, bsz=288, num_updates=1174, lr=1.4088e-05, gnorm=1.693, train_wall=6, gb_free=10.8, wall=7393
2024-07-20 11:51:53 | INFO | train_inner | epoch 062:     17 / 19 loss=5.463, nll_loss=1.855, ppl=3.62, wps=2283.8, ups=0.5, wpb=4577, bsz=205.5, num_updates=1176, lr=1.4112e-05, gnorm=1.586, train_wall=4, gb_free=11.8, wall=7397
2024-07-20 11:51:57 | INFO | train_inner | epoch 062:     19 / 19 loss=5.366, nll_loss=1.751, ppl=3.37, wps=1777.1, ups=0.57, wpb=3105.5, bsz=156, num_updates=1178, lr=1.4136e-05, gnorm=2.024, train_wall=3, gb_free=17.5, wall=7401
2024-07-20 11:51:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44258.859375Mb; avail=210771.16015625Mb
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000568
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44259.3515625Mb; avail=210770.66796875Mb
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002090
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44259.3515625Mb; avail=210770.66796875Mb
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001869
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004866
2024-07-20 11:51:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44259.3515625Mb; avail=210770.66796875Mb
2024-07-20 11:51:59 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.004 | nll_loss 2.258 | ppl 4.78 | wps 4180.9 | wpb 1665.6 | bsz 74.4 | num_updates 1178 | best_loss 6.004
2024-07-20 11:51:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 1178 updates
2024-07-20 11:51:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 62 @ 1178 updates, score 6.004) (writing took 66.39329019701108 seconds)
2024-07-20 11:53:05 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-07-20 11:53:05 | INFO | train | epoch 062 | loss 5.482 | nll_loss 1.85 | ppl 3.61 | wps 783.9 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 1178 | lr 1.4136e-05 | gnorm 1.612 | train_wall 49 | gb_free 17.5 | wall 7470
2024-07-20 11:53:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:53:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:53:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:53:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000774
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=59551.5Mb; avail=195478.578125Mb
2024-07-20 11:53:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000103
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000788
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59551.0078125Mb; avail=195479.0703125Mb
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000041
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59551.5Mb; avail=195478.578125Mb
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000234
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001383
2024-07-20 11:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=59551.5Mb; avail=195478.578125Mb
2024-07-20 11:53:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:53:06 | INFO | fairseq.trainer | begin training epoch 63
2024-07-20 11:53:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:53:12 | INFO | train_inner | epoch 063:      2 / 19 loss=5.395, nll_loss=1.724, ppl=3.3, wps=150.2, ups=0.03, wpb=5641, bsz=280, num_updates=1180, lr=1.416e-05, gnorm=1.332, train_wall=6, gb_free=12.3, wall=7476
2024-07-20 11:53:18 | INFO | train_inner | epoch 063:      4 / 19 loss=5.394, nll_loss=1.699, ppl=3.25, wps=1878.4, ups=0.34, wpb=5574, bsz=264, num_updates=1182, lr=1.4184e-05, gnorm=1.51, train_wall=6, gb_free=12.4, wall=7482
2024-07-20 11:53:23 | INFO | train_inner | epoch 063:      6 / 19 loss=5.555, nll_loss=1.928, ppl=3.8, wps=1855.4, ups=0.41, wpb=4575.5, bsz=204, num_updates=1184, lr=1.4208e-05, gnorm=1.554, train_wall=5, gb_free=11.2, wall=7487
2024-07-20 11:53:28 | INFO | train_inner | epoch 063:      8 / 19 loss=5.442, nll_loss=1.802, ppl=3.49, wps=1798, ups=0.34, wpb=5260, bsz=212, num_updates=1186, lr=1.4232e-05, gnorm=1.629, train_wall=6, gb_free=14.6, wall=7493
2024-07-20 11:53:34 | INFO | train_inner | epoch 063:     10 / 19 loss=5.32, nll_loss=1.661, ppl=3.16, wps=1623.6, ups=0.36, wpb=4513, bsz=212, num_updates=1188, lr=1.4256e-05, gnorm=1.448, train_wall=6, gb_free=14.6, wall=7498
2024-07-20 11:53:40 | INFO | train_inner | epoch 063:     12 / 19 loss=5.426, nll_loss=1.796, ppl=3.47, wps=1800.8, ups=0.34, wpb=5286.5, bsz=240, num_updates=1190, lr=1.428e-05, gnorm=1.382, train_wall=6, gb_free=12.5, wall=7504
2024-07-20 11:53:43 | INFO | train_inner | epoch 063:     14 / 19 loss=5.667, nll_loss=2.081, ppl=4.23, wps=1933.1, ups=0.61, wpb=3163, bsz=113.5, num_updates=1192, lr=1.4304e-05, gnorm=1.944, train_wall=3, gb_free=13.7, wall=7507
2024-07-20 11:53:49 | INFO | train_inner | epoch 063:     16 / 19 loss=5.444, nll_loss=1.769, ppl=3.41, wps=1799.5, ups=0.32, wpb=5590.5, bsz=232, num_updates=1194, lr=1.4328e-05, gnorm=1.469, train_wall=6, gb_free=11.2, wall=7513
2024-07-20 11:53:55 | INFO | train_inner | epoch 063:     18 / 19 loss=5.387, nll_loss=1.718, ppl=3.29, wps=1890.2, ups=0.33, wpb=5780.5, bsz=300, num_updates=1196, lr=1.4352e-05, gnorm=1.336, train_wall=6, gb_free=11.3, wall=7520
2024-07-20 11:53:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50713.953125Mb; avail=204316.06640625Mb
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000598
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50714.4453125Mb; avail=204315.57421875Mb
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002113
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50714.4453125Mb; avail=204315.57421875Mb
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001852
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004910
2024-07-20 11:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50714.4453125Mb; avail=204315.57421875Mb
2024-07-20 11:53:59 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.006 | nll_loss 2.276 | ppl 4.84 | wps 4177.9 | wpb 1665.6 | bsz 74.4 | num_updates 1197 | best_loss 6.004
2024-07-20 11:53:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 1197 updates
2024-07-20 11:53:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:54:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:54:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 63 @ 1197 updates, score 6.006) (writing took 50.29929542611353 seconds)
2024-07-20 11:54:50 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-07-20 11:54:50 | INFO | train | epoch 063 | loss 5.442 | nll_loss 1.788 | ppl 3.45 | wps 888.2 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1197 | lr 1.4364e-05 | gnorm 1.591 | train_wall 51 | gb_free 16.6 | wall 7574
2024-07-20 11:54:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:54:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:54:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:54:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000611
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=52235.15234375Mb; avail=202794.87890625Mb
2024-07-20 11:54:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000709
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52234.66015625Mb; avail=202795.37109375Mb
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52234.66015625Mb; avail=202794.87890625Mb
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000206
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001221
2024-07-20 11:54:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52234.66015625Mb; avail=202795.37109375Mb
2024-07-20 11:54:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:54:50 | INFO | fairseq.trainer | begin training epoch 64
2024-07-20 11:54:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:54:53 | INFO | train_inner | epoch 064:      1 / 19 loss=5.341, nll_loss=1.671, ppl=3.18, wps=123.9, ups=0.03, wpb=3572, bsz=168, num_updates=1198, lr=1.4376e-05, gnorm=2.127, train_wall=5, gb_free=11.1, wall=7577
2024-07-20 11:54:58 | INFO | train_inner | epoch 064:      3 / 19 loss=5.314, nll_loss=1.664, ppl=3.17, wps=2191, ups=0.41, wpb=5372, bsz=252, num_updates=1200, lr=1.44e-05, gnorm=1.385, train_wall=5, gb_free=12, wall=7582
2024-07-20 11:55:03 | INFO | train_inner | epoch 064:      5 / 19 loss=5.432, nll_loss=1.79, ppl=3.46, wps=1653.1, ups=0.37, wpb=4436.5, bsz=184, num_updates=1202, lr=1.4424e-05, gnorm=1.619, train_wall=5, gb_free=13.1, wall=7588
2024-07-20 11:55:09 | INFO | train_inner | epoch 064:      7 / 19 loss=5.371, nll_loss=1.71, ppl=3.27, wps=1447, ups=0.36, wpb=3973.5, bsz=196, num_updates=1204, lr=1.4448e-05, gnorm=1.497, train_wall=5, gb_free=15.1, wall=7593
2024-07-20 11:55:14 | INFO | train_inner | epoch 064:      9 / 19 loss=5.51, nll_loss=1.877, ppl=3.67, wps=2347.6, ups=0.38, wpb=6205.5, bsz=296, num_updates=1206, lr=1.4472e-05, gnorm=1.447, train_wall=5, gb_free=12.1, wall=7598
2024-07-20 11:55:19 | INFO | train_inner | epoch 064:     11 / 19 loss=5.407, nll_loss=1.725, ppl=3.31, wps=2228, ups=0.38, wpb=5807.5, bsz=244, num_updates=1208, lr=1.4496e-05, gnorm=1.507, train_wall=5, gb_free=12.1, wall=7604
2024-07-20 11:55:24 | INFO | train_inner | epoch 064:     13 / 19 loss=5.414, nll_loss=1.745, ppl=3.35, wps=1510.5, ups=0.4, wpb=3800.5, bsz=173.5, num_updates=1210, lr=1.452e-05, gnorm=1.782, train_wall=5, gb_free=12.8, wall=7609
2024-07-20 11:55:30 | INFO | train_inner | epoch 064:     15 / 19 loss=5.556, nll_loss=1.94, ppl=3.84, wps=1549.3, ups=0.35, wpb=4442, bsz=164, num_updates=1212, lr=1.4544e-05, gnorm=1.644, train_wall=6, gb_free=12.4, wall=7614
2024-07-20 11:55:35 | INFO | train_inner | epoch 064:     17 / 19 loss=5.417, nll_loss=1.799, ppl=3.48, wps=2163.2, ups=0.38, wpb=5667.5, bsz=264, num_updates=1214, lr=1.4568e-05, gnorm=1.387, train_wall=5, gb_free=11.2, wall=7620
2024-07-20 11:55:40 | INFO | train_inner | epoch 064:     19 / 19 loss=5.621, nll_loss=2.03, ppl=4.08, wps=1655.5, ups=0.44, wpb=3801, bsz=164, num_updates=1216, lr=1.4592e-05, gnorm=2.057, train_wall=5, gb_free=17.3, wall=7624
2024-07-20 11:55:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40884.7265625Mb; avail=214145.29296875Mb
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40884.7265625Mb; avail=214145.29296875Mb
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002109
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40885.21875Mb; avail=214144.80078125Mb
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001845
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004849
2024-07-20 11:55:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40885.21875Mb; avail=214144.80078125Mb
2024-07-20 11:55:42 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.015 | nll_loss 2.241 | ppl 4.73 | wps 4174.9 | wpb 1665.6 | bsz 74.4 | num_updates 1216 | best_loss 6.004
2024-07-20 11:55:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 1216 updates
2024-07-20 11:55:42 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:56:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 64 @ 1216 updates, score 6.015) (writing took 41.7028521199245 seconds)
2024-07-20 11:56:24 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-07-20 11:56:24 | INFO | train | epoch 064 | loss 5.432 | nll_loss 1.788 | ppl 3.45 | wps 977.7 | ups 0.2 | wpb 4866.4 | bsz 219.1 | num_updates 1216 | lr 1.4592e-05 | gnorm 1.572 | train_wall 50 | gb_free 17.3 | wall 7668
2024-07-20 11:56:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:56:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:56:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:56:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000742
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=53024.52734375Mb; avail=202005.4921875Mb
2024-07-20 11:56:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000096
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000734
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53026.0Mb; avail=202004.015625Mb
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53026.4921875Mb; avail=202003.5234375Mb
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001256
2024-07-20 11:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=53027.4765625Mb; avail=202002.5390625Mb
2024-07-20 11:56:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:56:24 | INFO | fairseq.trainer | begin training epoch 65
2024-07-20 11:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:56:28 | INFO | train_inner | epoch 065:      2 / 19 loss=5.37, nll_loss=1.67, ppl=3.18, wps=170.7, ups=0.04, wpb=4124.5, bsz=181.5, num_updates=1218, lr=1.4616e-05, gnorm=1.629, train_wall=4, gb_free=12.7, wall=7672
2024-07-20 11:56:34 | INFO | train_inner | epoch 065:      4 / 19 loss=5.275, nll_loss=1.565, ppl=2.96, wps=1594.8, ups=0.35, wpb=4591, bsz=272, num_updates=1220, lr=1.464e-05, gnorm=1.435, train_wall=6, gb_free=12.5, wall=7678
2024-07-20 11:56:39 | INFO | train_inner | epoch 065:      6 / 19 loss=5.342, nll_loss=1.682, ppl=3.21, wps=2073.7, ups=0.39, wpb=5267.5, bsz=244, num_updates=1222, lr=1.4664e-05, gnorm=1.484, train_wall=5, gb_free=11.7, wall=7683
2024-07-20 11:56:45 | INFO | train_inner | epoch 065:      8 / 19 loss=5.449, nll_loss=1.829, ppl=3.55, wps=1612, ups=0.36, wpb=4532, bsz=172, num_updates=1224, lr=1.4688e-05, gnorm=1.933, train_wall=6, gb_free=12, wall=7689
2024-07-20 11:56:49 | INFO | train_inner | epoch 065:     10 / 19 loss=5.593, nll_loss=1.952, ppl=3.87, wps=2011.8, ups=0.45, wpb=4423, bsz=172, num_updates=1226, lr=1.4712e-05, gnorm=1.633, train_wall=4, gb_free=15.3, wall=7693
2024-07-20 11:56:54 | INFO | train_inner | epoch 065:     12 / 19 loss=5.396, nll_loss=1.686, ppl=3.22, wps=1874.2, ups=0.39, wpb=4840, bsz=208, num_updates=1228, lr=1.4736e-05, gnorm=1.618, train_wall=5, gb_free=11.8, wall=7698
2024-07-20 11:57:00 | INFO | train_inner | epoch 065:     14 / 19 loss=5.453, nll_loss=1.799, ppl=3.48, wps=1745.1, ups=0.34, wpb=5159, bsz=204, num_updates=1230, lr=1.476e-05, gnorm=1.721, train_wall=6, gb_free=14.3, wall=7704
2024-07-20 11:57:05 | INFO | train_inner | epoch 065:     16 / 19 loss=5.376, nll_loss=1.753, ppl=3.37, wps=2217.8, ups=0.39, wpb=5723.5, bsz=260, num_updates=1232, lr=1.4784e-05, gnorm=1.394, train_wall=5, gb_free=12.5, wall=7710
2024-07-20 11:57:12 | INFO | train_inner | epoch 065:     18 / 19 loss=5.322, nll_loss=1.668, ppl=3.18, wps=1901.8, ups=0.32, wpb=5985, bsz=300, num_updates=1234, lr=1.4808e-05, gnorm=1.199, train_wall=6, gb_free=10.9, wall=7716
2024-07-20 11:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33141.640625Mb; avail=221888.2578125Mb
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000576
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33141.640625Mb; avail=221888.2578125Mb
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002094
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33141.640625Mb; avail=221888.2578125Mb
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001853
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004841
2024-07-20 11:57:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33141.640625Mb; avail=221888.2578125Mb
2024-07-20 11:57:15 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.009 | nll_loss 2.237 | ppl 4.71 | wps 4172.2 | wpb 1665.6 | bsz 74.4 | num_updates 1235 | best_loss 6.004
2024-07-20 11:57:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 1235 updates
2024-07-20 11:57:15 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:58:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 11:58:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 65 @ 1235 updates, score 6.009) (writing took 49.70261836005375 seconds)
2024-07-20 11:58:05 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-07-20 11:58:05 | INFO | train | epoch 065 | loss 5.396 | nll_loss 1.735 | ppl 3.33 | wps 915.1 | ups 0.19 | wpb 4866.4 | bsz 219.1 | num_updates 1235 | lr 1.482e-05 | gnorm 1.573 | train_wall 49 | gb_free 17.3 | wall 7769
2024-07-20 11:58:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:58:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:58:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:58:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000625
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36176.0625Mb; avail=218854.0390625Mb
2024-07-20 11:58:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000710
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36177.5390625Mb; avail=218852.5625Mb
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36178.03125Mb; avail=218852.0703125Mb
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000211
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001219
2024-07-20 11:58:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36178.5234375Mb; avail=218851.578125Mb
2024-07-20 11:58:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:58:05 | INFO | fairseq.trainer | begin training epoch 66
2024-07-20 11:58:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 11:58:09 | INFO | train_inner | epoch 066:      1 / 19 loss=5.336, nll_loss=1.654, ppl=3.15, wps=127.9, ups=0.04, wpb=3640.5, bsz=164, num_updates=1236, lr=1.4832e-05, gnorm=1.589, train_wall=5, gb_free=12.2, wall=7773
2024-07-20 11:58:14 | INFO | train_inner | epoch 066:      3 / 19 loss=5.45, nll_loss=1.76, ppl=3.39, wps=2091.6, ups=0.39, wpb=5306, bsz=216, num_updates=1238, lr=1.4856e-05, gnorm=1.605, train_wall=5, gb_free=12.9, wall=7778
2024-07-20 11:58:19 | INFO | train_inner | epoch 066:      5 / 19 loss=5.299, nll_loss=1.614, ppl=3.06, wps=2055.3, ups=0.4, wpb=5150, bsz=256, num_updates=1240, lr=1.488e-05, gnorm=1.285, train_wall=5, gb_free=10.9, wall=7783
2024-07-20 11:58:23 | INFO | train_inner | epoch 066:      7 / 19 loss=5.449, nll_loss=1.777, ppl=3.43, wps=1790.3, ups=0.45, wpb=3983, bsz=168, num_updates=1242, lr=1.4904e-05, gnorm=1.77, train_wall=4, gb_free=12.4, wall=7787
2024-07-20 11:58:28 | INFO | train_inner | epoch 066:      9 / 19 loss=5.471, nll_loss=1.852, ppl=3.61, wps=1795.7, ups=0.39, wpb=4559, bsz=189.5, num_updates=1244, lr=1.4928e-05, gnorm=1.719, train_wall=5, gb_free=12.4, wall=7792
2024-07-20 11:58:33 | INFO | train_inner | epoch 066:     11 / 19 loss=5.365, nll_loss=1.722, ppl=3.3, wps=1950.1, ups=0.41, wpb=4757.5, bsz=212, num_updates=1246, lr=1.4952e-05, gnorm=1.467, train_wall=5, gb_free=12.9, wall=7797
2024-07-20 11:58:39 | INFO | train_inner | epoch 066:     13 / 19 loss=5.27, nll_loss=1.578, ppl=2.99, wps=1947.4, ups=0.35, wpb=5534, bsz=260, num_updates=1248, lr=1.4976e-05, gnorm=1.305, train_wall=6, gb_free=11.9, wall=7803
2024-07-20 11:58:45 | INFO | train_inner | epoch 066:     15 / 19 loss=5.413, nll_loss=1.752, ppl=3.37, wps=1896.2, ups=0.34, wpb=5613, bsz=280, num_updates=1250, lr=1.5e-05, gnorm=1.434, train_wall=6, gb_free=12, wall=7809
2024-07-20 11:58:50 | INFO | train_inner | epoch 066:     17 / 19 loss=5.347, nll_loss=1.66, ppl=3.16, wps=2360.3, ups=0.38, wpb=6232, bsz=296, num_updates=1252, lr=1.5024e-05, gnorm=1.281, train_wall=5, gb_free=11.9, wall=7814
2024-07-20 11:58:54 | INFO | train_inner | epoch 066:     19 / 19 loss=5.468, nll_loss=1.777, ppl=3.43, wps=1651.2, ups=0.54, wpb=3041.5, bsz=108, num_updates=1254, lr=1.5048e-05, gnorm=1.999, train_wall=4, gb_free=17.2, wall=7818
2024-07-20 11:58:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30463.12890625Mb; avail=224566.890625Mb
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000567
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30463.12890625Mb; avail=224566.890625Mb
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002092
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30463.12890625Mb; avail=224566.890625Mb
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001835
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004809
2024-07-20 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30463.12890625Mb; avail=224566.890625Mb
2024-07-20 11:58:56 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 5.984 | nll_loss 2.258 | ppl 4.78 | wps 4177.1 | wpb 1665.6 | bsz 74.4 | num_updates 1254 | best_loss 5.984
2024-07-20 11:58:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 1254 updates
2024-07-20 11:58:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 11:59:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 66 @ 1254 updates, score 5.984) (writing took 62.954550561029464 seconds)
2024-07-20 11:59:59 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-07-20 11:59:59 | INFO | train | epoch 066 | loss 5.378 | nll_loss 1.705 | ppl 3.26 | wps 811.3 | ups 0.17 | wpb 4866.4 | bsz 219.1 | num_updates 1254 | lr 1.5048e-05 | gnorm 1.532 | train_wall 48 | gb_free 17.2 | wall 7883
2024-07-20 11:59:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 11:59:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 11:59:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 11:59:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000785
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45075.140625Mb; avail=209955.421875Mb
2024-07-20 11:59:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000083
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000766
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45075.140625Mb; avail=209954.9296875Mb
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45075.140625Mb; avail=209955.421875Mb
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000230
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001311
2024-07-20 11:59:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45075.140625Mb; avail=209954.9296875Mb
2024-07-20 11:59:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 11:59:59 | INFO | fairseq.trainer | begin training epoch 67
2024-07-20 11:59:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:00:05 | INFO | train_inner | epoch 067:      2 / 19 loss=5.379, nll_loss=1.73, ppl=3.32, wps=149.4, ups=0.03, wpb=5331.5, bsz=232, num_updates=1256, lr=1.5072e-05, gnorm=1.488, train_wall=6, gb_free=12.1, wall=7889
2024-07-20 12:00:10 | INFO | train_inner | epoch 067:      4 / 19 loss=5.314, nll_loss=1.679, ppl=3.2, wps=2122.1, ups=0.41, wpb=5152.5, bsz=244, num_updates=1258, lr=1.5096e-05, gnorm=1.453, train_wall=5, gb_free=13.4, wall=7894
2024-07-20 12:00:16 | INFO | train_inner | epoch 067:      6 / 19 loss=5.28, nll_loss=1.592, ppl=3.01, wps=1818.2, ups=0.35, wpb=5226.5, bsz=252, num_updates=1260, lr=1.512e-05, gnorm=1.288, train_wall=6, gb_free=14.7, wall=7900
2024-07-20 12:00:20 | INFO | train_inner | epoch 067:      8 / 19 loss=5.476, nll_loss=1.797, ppl=3.47, wps=2161.6, ups=0.42, wpb=5201.5, bsz=184, num_updates=1262, lr=1.5144e-05, gnorm=1.602, train_wall=5, gb_free=12.9, wall=7905
2024-07-20 12:00:26 | INFO | train_inner | epoch 067:     10 / 19 loss=5.294, nll_loss=1.585, ppl=3, wps=1602, ups=0.35, wpb=4545.5, bsz=232, num_updates=1264, lr=1.5168e-05, gnorm=1.633, train_wall=6, gb_free=12.6, wall=7910
2024-07-20 12:00:31 | INFO | train_inner | epoch 067:     12 / 19 loss=5.304, nll_loss=1.641, ppl=3.12, wps=2141, ups=0.39, wpb=5442.5, bsz=236, num_updates=1266, lr=1.5192e-05, gnorm=1.291, train_wall=5, gb_free=12, wall=7915
2024-07-20 12:00:35 | INFO | train_inner | epoch 067:     14 / 19 loss=5.485, nll_loss=1.864, ppl=3.64, wps=1914.6, ups=0.57, wpb=3378.5, bsz=125.5, num_updates=1268, lr=1.5216e-05, gnorm=1.814, train_wall=4, gb_free=17.2, wall=7919
2024-07-20 12:00:41 | INFO | train_inner | epoch 067:     16 / 19 loss=5.358, nll_loss=1.681, ppl=3.21, wps=1687.2, ups=0.33, wpb=5062.5, bsz=204, num_updates=1270, lr=1.524e-05, gnorm=1.6, train_wall=6, gb_free=12.2, wall=7925
2024-07-20 12:00:47 | INFO | train_inner | epoch 067:     18 / 19 loss=5.285, nll_loss=1.585, ppl=3, wps=1795.2, ups=0.33, wpb=5372, bsz=280, num_updates=1272, lr=1.5264e-05, gnorm=1.305, train_wall=6, gb_free=10.6, wall=7931
2024-07-20 12:00:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30362.3828125Mb; avail=224667.625Mb
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000583
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30362.875Mb; avail=224667.1328125Mb
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002071
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30362.875Mb; avail=224667.1328125Mb
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001842
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004835
2024-07-20 12:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30362.875Mb; avail=224667.1328125Mb
2024-07-20 12:00:51 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 5.998 | nll_loss 2.24 | ppl 4.72 | wps 4178.1 | wpb 1665.6 | bsz 74.4 | num_updates 1273 | best_loss 5.984
2024-07-20 12:00:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 1273 updates
2024-07-20 12:00:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:01:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:01:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 67 @ 1273 updates, score 5.998) (writing took 48.765160266077146 seconds)
2024-07-20 12:01:39 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-07-20 12:01:40 | INFO | train | epoch 067 | loss 5.345 | nll_loss 1.673 | ppl 3.19 | wps 921.5 | ups 0.19 | wpb 4866.4 | bsz 219.1 | num_updates 1273 | lr 1.5276e-05 | gnorm 1.505 | train_wall 49 | gb_free 15.6 | wall 7984
2024-07-20 12:01:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:01:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:01:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:01:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000638
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30476.89453125Mb; avail=224553.2109375Mb
2024-07-20 12:01:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000707
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30478.37109375Mb; avail=224551.734375Mb
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000044
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30478.37109375Mb; avail=224551.734375Mb
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000208
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001235
2024-07-20 12:01:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30479.35546875Mb; avail=224550.75Mb
2024-07-20 12:01:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:01:40 | INFO | fairseq.trainer | begin training epoch 68
2024-07-20 12:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:01:42 | INFO | train_inner | epoch 068:      1 / 19 loss=5.268, nll_loss=1.554, ppl=2.94, wps=115.1, ups=0.04, wpb=3171, bsz=160, num_updates=1274, lr=1.5288e-05, gnorm=1.604, train_wall=4, gb_free=11.9, wall=7986
2024-07-20 12:01:47 | INFO | train_inner | epoch 068:      3 / 19 loss=5.351, nll_loss=1.699, ppl=3.25, wps=1865.4, ups=0.39, wpb=4826, bsz=217.5, num_updates=1276, lr=1.5312e-05, gnorm=1.505, train_wall=5, gb_free=11.5, wall=7991
2024-07-20 12:01:52 | INFO | train_inner | epoch 068:      5 / 19 loss=5.343, nll_loss=1.69, ppl=3.23, wps=2211.3, ups=0.42, wpb=5207.5, bsz=244, num_updates=1278, lr=1.5336e-05, gnorm=1.512, train_wall=5, gb_free=11.4, wall=7996
2024-07-20 12:01:57 | INFO | train_inner | epoch 068:      7 / 19 loss=5.303, nll_loss=1.608, ppl=3.05, wps=2088.9, ups=0.4, wpb=5164, bsz=212, num_updates=1280, lr=1.536e-05, gnorm=1.483, train_wall=5, gb_free=12.3, wall=8001
2024-07-20 12:02:02 | INFO | train_inner | epoch 068:      9 / 19 loss=5.356, nll_loss=1.68, ppl=3.2, wps=1619.1, ups=0.36, wpb=4519.5, bsz=228, num_updates=1282, lr=1.5384e-05, gnorm=1.579, train_wall=6, gb_free=13.3, wall=8006
2024-07-20 12:02:08 | INFO | train_inner | epoch 068:     11 / 19 loss=5.342, nll_loss=1.645, ppl=3.13, wps=1851.1, ups=0.34, wpb=5405.5, bsz=216, num_updates=1284, lr=1.5408e-05, gnorm=1.575, train_wall=6, gb_free=13, wall=8012
2024-07-20 12:02:14 | INFO | train_inner | epoch 068:     13 / 19 loss=5.308, nll_loss=1.635, ppl=3.11, wps=2151.8, ups=0.36, wpb=5993, bsz=280, num_updates=1286, lr=1.5432e-05, gnorm=1.343, train_wall=6, gb_free=11.2, wall=8018
2024-07-20 12:02:20 | INFO | train_inner | epoch 068:     15 / 19 loss=5.423, nll_loss=1.806, ppl=3.5, wps=1934.9, ups=0.34, wpb=5721.5, bsz=256, num_updates=1288, lr=1.5456e-05, gnorm=1.575, train_wall=6, gb_free=11, wall=8024
2024-07-20 12:02:25 | INFO | train_inner | epoch 068:     17 / 19 loss=5.207, nll_loss=1.481, ppl=2.79, wps=1664.2, ups=0.36, wpb=4559.5, bsz=220, num_updates=1290, lr=1.548e-05, gnorm=1.457, train_wall=5, gb_free=14.9, wall=8029
2024-07-20 12:02:30 | INFO | train_inner | epoch 068:     19 / 19 loss=5.244, nll_loss=1.486, ppl=2.8, wps=1392, ups=0.44, wpb=3181.5, bsz=140, num_updates=1292, lr=1.5504e-05, gnorm=2.924, train_wall=5, gb_free=16.5, wall=8034
2024-07-20 12:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28959.75Mb; avail=226070.265625Mb
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28960.2421875Mb; avail=226069.7734375Mb
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002129
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28960.2421875Mb; avail=226069.7734375Mb
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001898
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004931
2024-07-20 12:02:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28960.2421875Mb; avail=226069.7734375Mb
2024-07-20 12:02:32 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.014 | nll_loss 2.25 | ppl 4.76 | wps 4160.9 | wpb 1665.6 | bsz 74.4 | num_updates 1292 | best_loss 5.984
2024-07-20 12:02:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 1292 updates
2024-07-20 12:02:32 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:03:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:03:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 68 @ 1292 updates, score 6.014) (writing took 38.50257701589726 seconds)
2024-07-20 12:03:11 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-07-20 12:03:11 | INFO | train | epoch 068 | loss 5.323 | nll_loss 1.644 | ppl 3.12 | wps 1014.6 | ups 0.21 | wpb 4866.4 | bsz 219.1 | num_updates 1292 | lr 1.5504e-05 | gnorm 1.656 | train_wall 50 | gb_free 16.5 | wall 8075
2024-07-20 12:03:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:03:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:03:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:03:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000743
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40194.93359375Mb; avail=214835.08203125Mb
2024-07-20 12:03:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000717
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40196.41015625Mb; avail=214833.60546875Mb
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40196.41015625Mb; avail=214833.11328125Mb
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000220
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001235
2024-07-20 12:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40197.39453125Mb; avail=214832.62109375Mb
2024-07-20 12:03:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:03:11 | INFO | fairseq.trainer | begin training epoch 69
2024-07-20 12:03:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:03:15 | INFO | train_inner | epoch 069:      2 / 19 loss=5.291, nll_loss=1.583, ppl=3, wps=168.3, ups=0.04, wpb=3805, bsz=161.5, num_updates=1294, lr=1.5528e-05, gnorm=1.691, train_wall=4, gb_free=12.4, wall=8079
2024-07-20 12:03:21 | INFO | train_inner | epoch 069:      4 / 19 loss=5.318, nll_loss=1.679, ppl=3.2, wps=1794.4, ups=0.34, wpb=5219.5, bsz=244, num_updates=1296, lr=1.5552e-05, gnorm=1.725, train_wall=6, gb_free=12.1, wall=8085
2024-07-20 12:03:25 | INFO | train_inner | epoch 069:      6 / 19 loss=5.312, nll_loss=1.642, ppl=3.12, wps=2100.6, ups=0.44, wpb=4797, bsz=188, num_updates=1298, lr=1.5576e-05, gnorm=1.647, train_wall=5, gb_free=12.3, wall=8089
2024-07-20 12:03:31 | INFO | train_inner | epoch 069:      8 / 19 loss=5.229, nll_loss=1.522, ppl=2.87, wps=1879.1, ups=0.33, wpb=5763, bsz=288, num_updates=1300, lr=1.56e-05, gnorm=1.237, train_wall=6, gb_free=10.9, wall=8095
2024-07-20 12:03:36 | INFO | train_inner | epoch 069:     10 / 19 loss=5.292, nll_loss=1.583, ppl=3, wps=1946.4, ups=0.42, wpb=4677, bsz=212, num_updates=1302, lr=1.5624e-05, gnorm=1.744, train_wall=5, gb_free=12.3, wall=8100
2024-07-20 12:03:42 | INFO | train_inner | epoch 069:     12 / 19 loss=5.282, nll_loss=1.566, ppl=2.96, wps=1863.5, ups=0.34, wpb=5540, bsz=268, num_updates=1304, lr=1.5648e-05, gnorm=1.36, train_wall=6, gb_free=13, wall=8106
2024-07-20 12:03:47 | INFO | train_inner | epoch 069:     14 / 19 loss=5.346, nll_loss=1.655, ppl=3.15, wps=1912.4, ups=0.42, wpb=4565, bsz=176, num_updates=1306, lr=1.5672e-05, gnorm=1.519, train_wall=5, gb_free=12.1, wall=8111
2024-07-20 12:03:53 | INFO | train_inner | epoch 069:     16 / 19 loss=5.407, nll_loss=1.764, ppl=3.4, wps=1782.8, ups=0.34, wpb=5218, bsz=252, num_updates=1308, lr=1.5696e-05, gnorm=1.511, train_wall=6, gb_free=13.6, wall=8117
2024-07-20 12:03:58 | INFO | train_inner | epoch 069:     18 / 19 loss=5.342, nll_loss=1.685, ppl=3.22, wps=2193.2, ups=0.39, wpb=5564, bsz=228, num_updates=1310, lr=1.572e-05, gnorm=1.462, train_wall=5, gb_free=14.1, wall=8122
2024-07-20 12:04:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34013.375Mb; avail=221016.64453125Mb
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34013.375Mb; avail=221016.64453125Mb
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002076
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34013.375Mb; avail=221016.64453125Mb
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001859
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004814
2024-07-20 12:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34013.375Mb; avail=221016.64453125Mb
2024-07-20 12:04:02 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 5.988 | nll_loss 2.246 | ppl 4.74 | wps 3879.7 | wpb 1665.6 | bsz 74.4 | num_updates 1311 | best_loss 5.984
2024-07-20 12:04:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 1311 updates
2024-07-20 12:04:02 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 69 @ 1311 updates, score 5.988) (writing took 52.87957197194919 seconds)
2024-07-20 12:04:55 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-07-20 12:04:55 | INFO | train | epoch 069 | loss 5.308 | nll_loss 1.625 | ppl 3.08 | wps 883.2 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1311 | lr 1.5732e-05 | gnorm 1.554 | train_wall 49 | gb_free 16.6 | wall 8179
2024-07-20 12:04:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:04:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:04:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:04:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000637
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46139.91015625Mb; avail=208890.1953125Mb
2024-07-20 12:04:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000092
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000717
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46139.91015625Mb; avail=208890.1953125Mb
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46139.91015625Mb; avail=208890.1953125Mb
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000208
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001231
2024-07-20 12:04:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46139.90625Mb; avail=208890.1953125Mb
2024-07-20 12:04:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:04:55 | INFO | fairseq.trainer | begin training epoch 70
2024-07-20 12:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:04:58 | INFO | train_inner | epoch 070:      1 / 19 loss=5.256, nll_loss=1.553, ppl=2.94, wps=136.1, ups=0.03, wpb=4112, bsz=200, num_updates=1312, lr=1.5744e-05, gnorm=1.531, train_wall=5, gb_free=11.5, wall=8182
2024-07-20 12:05:03 | INFO | train_inner | epoch 070:      3 / 19 loss=5.224, nll_loss=1.507, ppl=2.84, wps=1959.7, ups=0.41, wpb=4810.5, bsz=201.5, num_updates=1314, lr=1.5768e-05, gnorm=1.384, train_wall=5, gb_free=15.8, wall=8187
2024-07-20 12:05:08 | INFO | train_inner | epoch 070:      5 / 19 loss=5.201, nll_loss=1.48, ppl=2.79, wps=1729.1, ups=0.43, wpb=4065, bsz=192, num_updates=1316, lr=1.5792e-05, gnorm=1.577, train_wall=5, gb_free=12.7, wall=8192
2024-07-20 12:05:12 | INFO | train_inner | epoch 070:      7 / 19 loss=5.379, nll_loss=1.705, ppl=3.26, wps=1908.9, ups=0.47, wpb=4089, bsz=144, num_updates=1318, lr=1.5816e-05, gnorm=1.77, train_wall=4, gb_free=14.6, wall=8196
2024-07-20 12:05:18 | INFO | train_inner | epoch 070:      9 / 19 loss=5.297, nll_loss=1.631, ppl=3.1, wps=1841.6, ups=0.34, wpb=5471.5, bsz=248, num_updates=1320, lr=1.584e-05, gnorm=1.372, train_wall=6, gb_free=13, wall=8202
2024-07-20 12:05:23 | INFO | train_inner | epoch 070:     11 / 19 loss=5.298, nll_loss=1.616, ppl=3.07, wps=2370.9, ups=0.37, wpb=6405, bsz=292, num_updates=1322, lr=1.5864e-05, gnorm=1.32, train_wall=5, gb_free=12.2, wall=8208
2024-07-20 12:05:29 | INFO | train_inner | epoch 070:     13 / 19 loss=5.126, nll_loss=1.387, ppl=2.62, wps=1761.5, ups=0.37, wpb=4803.5, bsz=276, num_updates=1324, lr=1.5888e-05, gnorm=1.257, train_wall=5, gb_free=12.8, wall=8213
2024-07-20 12:05:34 | INFO | train_inner | epoch 070:     15 / 19 loss=5.332, nll_loss=1.646, ppl=3.13, wps=1690.9, ups=0.38, wpb=4422.5, bsz=180, num_updates=1326, lr=1.5912e-05, gnorm=1.51, train_wall=5, gb_free=13.2, wall=8218
2024-07-20 12:05:40 | INFO | train_inner | epoch 070:     17 / 19 loss=5.309, nll_loss=1.62, ppl=3.07, wps=1651.3, ups=0.32, wpb=5214.5, bsz=232, num_updates=1328, lr=1.5936e-05, gnorm=1.363, train_wall=6, gb_free=13.3, wall=8225
2024-07-20 12:05:45 | INFO | train_inner | epoch 070:     19 / 19 loss=5.208, nll_loss=1.486, ppl=2.8, wps=1841.6, ups=0.47, wpb=3920, bsz=180, num_updates=1330, lr=1.596e-05, gnorm=1.559, train_wall=4, gb_free=15, wall=8229
2024-07-20 12:05:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34796.25Mb; avail=220233.76953125Mb
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000586
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34796.25Mb; avail=220233.76953125Mb
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002088
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34796.25Mb; avail=220233.76953125Mb
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001869
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004852
2024-07-20 12:05:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34796.25Mb; avail=220233.76953125Mb
2024-07-20 12:05:47 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 5.992 | nll_loss 2.246 | ppl 4.74 | wps 4171.9 | wpb 1665.6 | bsz 74.4 | num_updates 1330 | best_loss 5.984
2024-07-20 12:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 1330 updates
2024-07-20 12:05:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:06:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 70 @ 1330 updates, score 5.992) (writing took 37.927044607000425 seconds)
2024-07-20 12:06:25 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-07-20 12:06:25 | INFO | train | epoch 070 | loss 5.269 | nll_loss 1.572 | ppl 2.97 | wps 1029.1 | ups 0.21 | wpb 4866.4 | bsz 219.1 | num_updates 1330 | lr 1.596e-05 | gnorm 1.45 | train_wall 49 | gb_free 15 | wall 8269
2024-07-20 12:06:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:06:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:06:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:06:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000758
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=55279.04296875Mb; avail=199750.94921875Mb
2024-07-20 12:06:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000092
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000765
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55281.01171875Mb; avail=199748.98046875Mb
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55281.01171875Mb; avail=199748.48828125Mb
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000225
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001316
2024-07-20 12:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=55281.99609375Mb; avail=199747.99609375Mb
2024-07-20 12:06:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:06:25 | INFO | fairseq.trainer | begin training epoch 71
2024-07-20 12:06:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:06:31 | INFO | train_inner | epoch 071:      2 / 19 loss=5.268, nll_loss=1.574, ppl=2.98, wps=232.5, ups=0.04, wpb=5411, bsz=244, num_updates=1332, lr=1.5984e-05, gnorm=1.328, train_wall=6, gb_free=11.7, wall=8275
2024-07-20 12:06:37 | INFO | train_inner | epoch 071:      4 / 19 loss=5.305, nll_loss=1.621, ppl=3.07, wps=1938.5, ups=0.35, wpb=5486.5, bsz=220, num_updates=1334, lr=1.6008e-05, gnorm=1.326, train_wall=6, gb_free=13.4, wall=8281
2024-07-20 12:06:42 | INFO | train_inner | epoch 071:      6 / 19 loss=5.265, nll_loss=1.58, ppl=2.99, wps=1423.5, ups=0.39, wpb=3691, bsz=180, num_updates=1336, lr=1.6032e-05, gnorm=1.677, train_wall=5, gb_free=13.4, wall=8286
2024-07-20 12:06:47 | INFO | train_inner | epoch 071:      8 / 19 loss=5.206, nll_loss=1.49, ppl=2.81, wps=2224.7, ups=0.38, wpb=5871.5, bsz=256, num_updates=1338, lr=1.6056e-05, gnorm=1.241, train_wall=5, gb_free=12.5, wall=8292
2024-07-20 12:06:53 | INFO | train_inner | epoch 071:     10 / 19 loss=5.218, nll_loss=1.49, ppl=2.81, wps=2386.4, ups=0.38, wpb=6289, bsz=304, num_updates=1340, lr=1.608e-05, gnorm=1.184, train_wall=5, gb_free=11.9, wall=8297
2024-07-20 12:06:57 | INFO | train_inner | epoch 071:     12 / 19 loss=5.253, nll_loss=1.535, ppl=2.9, wps=1538.7, ups=0.44, wpb=3526, bsz=149.5, num_updates=1342, lr=1.6104e-05, gnorm=1.706, train_wall=5, gb_free=11.2, wall=8301
2024-07-20 12:07:03 | INFO | train_inner | epoch 071:     14 / 19 loss=5.194, nll_loss=1.523, ppl=2.87, wps=2005.2, ups=0.35, wpb=5736.5, bsz=292, num_updates=1344, lr=1.6128e-05, gnorm=1.251, train_wall=6, gb_free=11.6, wall=8307
2024-07-20 12:07:08 | INFO | train_inner | epoch 071:     16 / 19 loss=5.227, nll_loss=1.545, ppl=2.92, wps=1749.2, ups=0.43, wpb=4099.5, bsz=176, num_updates=1346, lr=1.6152e-05, gnorm=1.491, train_wall=5, gb_free=15.2, wall=8312
2024-07-20 12:07:14 | INFO | train_inner | epoch 071:     18 / 19 loss=5.179, nll_loss=1.439, ppl=2.71, wps=1723.4, ups=0.34, wpb=5079, bsz=216, num_updates=1348, lr=1.6176e-05, gnorm=1.325, train_wall=6, gb_free=13, wall=8318
2024-07-20 12:07:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30321.31640625Mb; avail=224708.6953125Mb
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30321.31640625Mb; avail=224708.6953125Mb
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002097
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30321.31640625Mb; avail=224708.6953125Mb
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001847
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004833
2024-07-20 12:07:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30321.31640625Mb; avail=224708.6953125Mb
2024-07-20 12:07:18 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.022 | nll_loss 2.234 | ppl 4.7 | wps 3482.3 | wpb 1665.6 | bsz 74.4 | num_updates 1349 | best_loss 5.984
2024-07-20 12:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 1349 updates
2024-07-20 12:07:18 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:08:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 71 @ 1349 updates, score 6.022) (writing took 49.715508551103994 seconds)
2024-07-20 12:08:08 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-07-20 12:08:08 | INFO | train | epoch 071 | loss 5.232 | nll_loss 1.527 | ppl 2.88 | wps 903 | ups 0.19 | wpb 4866.4 | bsz 219.1 | num_updates 1349 | lr 1.6188e-05 | gnorm 1.492 | train_wall 49 | gb_free 17.1 | wall 8372
2024-07-20 12:08:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:08:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:08:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:08:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000610
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39171.984375Mb; avail=215858.07421875Mb
2024-07-20 12:08:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000086
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000718
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39172.4765625Mb; avail=215857.58203125Mb
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39171.984375Mb; avail=215858.07421875Mb
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000201
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001223
2024-07-20 12:08:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39172.4765625Mb; avail=215858.07421875Mb
2024-07-20 12:08:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:08:08 | INFO | fairseq.trainer | begin training epoch 72
2024-07-20 12:08:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:08:10 | INFO | train_inner | epoch 072:      1 / 19 loss=5.202, nll_loss=1.426, ppl=2.69, wps=101.6, ups=0.04, wpb=2862.5, bsz=112, num_updates=1350, lr=1.62e-05, gnorm=2.418, train_wall=4, gb_free=12.7, wall=8374
2024-07-20 12:08:13 | INFO | train_inner | epoch 072:      3 / 19 loss=5.254, nll_loss=1.542, ppl=2.91, wps=1778.6, ups=0.57, wpb=3102.5, bsz=149.5, num_updates=1352, lr=1.6224e-05, gnorm=1.794, train_wall=3, gb_free=12.3, wall=8378
2024-07-20 12:08:19 | INFO | train_inner | epoch 072:      5 / 19 loss=5.182, nll_loss=1.489, ppl=2.81, wps=1820.9, ups=0.34, wpb=5362, bsz=228, num_updates=1354, lr=1.6248e-05, gnorm=1.316, train_wall=6, gb_free=11.3, wall=8383
2024-07-20 12:08:25 | INFO | train_inner | epoch 072:      7 / 19 loss=5.326, nll_loss=1.679, ppl=3.2, wps=2298.4, ups=0.37, wpb=6174, bsz=244, num_updates=1356, lr=1.6272e-05, gnorm=1.422, train_wall=5, gb_free=11.8, wall=8389
2024-07-20 12:08:30 | INFO | train_inner | epoch 072:      9 / 19 loss=5.106, nll_loss=1.355, ppl=2.56, wps=1873.4, ups=0.37, wpb=5051.5, bsz=268, num_updates=1358, lr=1.6296e-05, gnorm=1.348, train_wall=5, gb_free=11.8, wall=8394
2024-07-20 12:08:35 | INFO | train_inner | epoch 072:     11 / 19 loss=5.209, nll_loss=1.475, ppl=2.78, wps=2061.5, ups=0.4, wpb=5214.5, bsz=248, num_updates=1360, lr=1.632e-05, gnorm=1.446, train_wall=5, gb_free=12.4, wall=8399
2024-07-20 12:08:41 | INFO | train_inner | epoch 072:     13 / 19 loss=5.227, nll_loss=1.5, ppl=2.83, wps=1626.2, ups=0.34, wpb=4801, bsz=220, num_updates=1362, lr=1.6344e-05, gnorm=1.371, train_wall=6, gb_free=12.7, wall=8405
2024-07-20 12:08:46 | INFO | train_inner | epoch 072:     15 / 19 loss=5.223, nll_loss=1.528, ppl=2.88, wps=2323.8, ups=0.37, wpb=6240.5, bsz=276, num_updates=1364, lr=1.6368e-05, gnorm=1.373, train_wall=5, gb_free=11.6, wall=8411
2024-07-20 12:08:52 | INFO | train_inner | epoch 072:     17 / 19 loss=5.278, nll_loss=1.607, ppl=3.05, wps=1613.6, ups=0.35, wpb=4620, bsz=208, num_updates=1366, lr=1.6392e-05, gnorm=1.5, train_wall=6, gb_free=11.7, wall=8416
2024-07-20 12:08:56 | INFO | train_inner | epoch 072:     19 / 19 loss=5.2, nll_loss=1.472, ppl=2.77, wps=1867.4, ups=0.49, wpb=3843.5, bsz=172, num_updates=1368, lr=1.6416e-05, gnorm=1.648, train_wall=4, gb_free=15.9, wall=8420
2024-07-20 12:08:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24527.60546875Mb; avail=230502.4140625Mb
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000582
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24527.60546875Mb; avail=230502.4140625Mb
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002106
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24527.60546875Mb; avail=230502.4140625Mb
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001908
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004921
2024-07-20 12:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24528.09765625Mb; avail=230501.921875Mb
2024-07-20 12:08:59 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 5.994 | nll_loss 2.243 | ppl 4.74 | wps 3324.9 | wpb 1665.6 | bsz 74.4 | num_updates 1368 | best_loss 5.984
2024-07-20 12:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 1368 updates
2024-07-20 12:08:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 72 @ 1368 updates, score 5.994) (writing took 38.703799397917464 seconds)
2024-07-20 12:09:38 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-07-20 12:09:38 | INFO | train | epoch 072 | loss 5.224 | nll_loss 1.517 | ppl 2.86 | wps 1021.8 | ups 0.21 | wpb 4866.4 | bsz 219.1 | num_updates 1368 | lr 1.6416e-05 | gnorm 1.473 | train_wall 48 | gb_free 15.9 | wall 8462
2024-07-20 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000732
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45144.8046875Mb; avail=209885.2109375Mb
2024-07-20 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000081
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000762
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45144.8046875Mb; avail=209885.2109375Mb
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45144.8046875Mb; avail=209885.2109375Mb
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000224
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001328
2024-07-20 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45144.8046875Mb; avail=209885.2109375Mb
2024-07-20 12:09:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:09:38 | INFO | fairseq.trainer | begin training epoch 73
2024-07-20 12:09:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:09:44 | INFO | train_inner | epoch 073:      2 / 19 loss=5.15, nll_loss=1.417, ppl=2.67, wps=229.1, ups=0.04, wpb=5438, bsz=272, num_updates=1370, lr=1.644e-05, gnorm=1.293, train_wall=6, gb_free=11.6, wall=8468
2024-07-20 12:09:50 | INFO | train_inner | epoch 073:      4 / 19 loss=5.238, nll_loss=1.535, ppl=2.9, wps=1845, ups=0.34, wpb=5392.5, bsz=220, num_updates=1372, lr=1.6464e-05, gnorm=1.69, train_wall=6, gb_free=12.7, wall=8474
2024-07-20 12:09:54 | INFO | train_inner | epoch 073:      6 / 19 loss=5.217, nll_loss=1.508, ppl=2.84, wps=2209.6, ups=0.43, wpb=5198.5, bsz=256, num_updates=1374, lr=1.6488e-05, gnorm=1.467, train_wall=5, gb_free=11.5, wall=8478
2024-07-20 12:09:59 | INFO | train_inner | epoch 073:      8 / 19 loss=5.159, nll_loss=1.443, ppl=2.72, wps=2295.3, ups=0.4, wpb=5685, bsz=260, num_updates=1376, lr=1.6512e-05, gnorm=1.377, train_wall=5, gb_free=12.9, wall=8483
2024-07-20 12:10:06 | INFO | train_inner | epoch 073:     10 / 19 loss=5.154, nll_loss=1.412, ppl=2.66, wps=1686.7, ups=0.3, wpb=5634, bsz=280, num_updates=1378, lr=1.6536e-05, gnorm=1.196, train_wall=7, gb_free=11.7, wall=8490
2024-07-20 12:10:12 | INFO | train_inner | epoch 073:     12 / 19 loss=5.215, nll_loss=1.516, ppl=2.86, wps=1651.4, ups=0.32, wpb=5118, bsz=212, num_updates=1380, lr=1.656e-05, gnorm=1.371, train_wall=6, gb_free=12, wall=8496
2024-07-20 12:10:17 | INFO | train_inner | epoch 073:     14 / 19 loss=5.16, nll_loss=1.463, ppl=2.76, wps=1850.4, ups=0.41, wpb=4546, bsz=236, num_updates=1382, lr=1.6584e-05, gnorm=1.382, train_wall=5, gb_free=13.3, wall=8501
2024-07-20 12:10:23 | INFO | train_inner | epoch 073:     16 / 19 loss=5.219, nll_loss=1.502, ppl=2.83, wps=1732.4, ups=0.35, wpb=4903.5, bsz=204, num_updates=1384, lr=1.6608e-05, gnorm=1.457, train_wall=6, gb_free=10.9, wall=8507
2024-07-20 12:10:27 | INFO | train_inner | epoch 073:     18 / 19 loss=5.359, nll_loss=1.623, ppl=3.08, wps=1472.7, ups=0.43, wpb=3393.5, bsz=109.5, num_updates=1386, lr=1.6632e-05, gnorm=1.95, train_wall=5, gb_free=13.5, wall=8511
2024-07-20 12:10:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22173.51171875Mb; avail=232856.5078125Mb
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000585
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22174.00390625Mb; avail=232856.015625Mb
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002095
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22174.00390625Mb; avail=232856.015625Mb
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001847
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004852
2024-07-20 12:10:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22174.00390625Mb; avail=232856.015625Mb
2024-07-20 12:10:31 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 5.999 | nll_loss 2.24 | ppl 4.73 | wps 4168.3 | wpb 1665.6 | bsz 74.4 | num_updates 1387 | best_loss 5.984
2024-07-20 12:10:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 1387 updates
2024-07-20 12:10:31 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:11:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 73 @ 1387 updates, score 5.999) (writing took 48.26461968594231 seconds)
2024-07-20 12:11:19 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-07-20 12:11:19 | INFO | train | epoch 073 | loss 5.202 | nll_loss 1.484 | ppl 2.8 | wps 911.7 | ups 0.19 | wpb 4866.4 | bsz 219.1 | num_updates 1387 | lr 1.6644e-05 | gnorm 1.514 | train_wall 50 | gb_free 19 | wall 8564
2024-07-20 12:11:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:11:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:11:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:11:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000620
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36496.984375Mb; avail=218533.55078125Mb
2024-07-20 12:11:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000076
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000700
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36496.4921875Mb; avail=218533.55078125Mb
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36496.4921875Mb; avail=218533.55078125Mb
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000206
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001209
2024-07-20 12:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36496.4921875Mb; avail=218533.55078125Mb
2024-07-20 12:11:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:11:20 | INFO | fairseq.trainer | begin training epoch 74
2024-07-20 12:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:11:22 | INFO | train_inner | epoch 074:      1 / 19 loss=5.175, nll_loss=1.423, ppl=2.68, wps=145.7, ups=0.04, wpb=4002, bsz=192, num_updates=1388, lr=1.6656e-05, gnorm=1.814, train_wall=4, gb_free=12.4, wall=8566
2024-07-20 12:11:28 | INFO | train_inner | epoch 074:      3 / 19 loss=5.207, nll_loss=1.512, ppl=2.85, wps=1822.4, ups=0.33, wpb=5542.5, bsz=236, num_updates=1390, lr=1.668e-05, gnorm=1.291, train_wall=6, gb_free=11.5, wall=8572
2024-07-20 12:11:33 | INFO | train_inner | epoch 074:      5 / 19 loss=5.123, nll_loss=1.426, ppl=2.69, wps=1887.2, ups=0.42, wpb=4526.5, bsz=180, num_updates=1392, lr=1.6704e-05, gnorm=1.586, train_wall=5, gb_free=12.3, wall=8577
2024-07-20 12:11:39 | INFO | train_inner | epoch 074:      7 / 19 loss=5.123, nll_loss=1.397, ppl=2.63, wps=1750.9, ups=0.34, wpb=5208.5, bsz=252, num_updates=1394, lr=1.6728e-05, gnorm=1.344, train_wall=6, gb_free=12.8, wall=8583
2024-07-20 12:11:44 | INFO | train_inner | epoch 074:      9 / 19 loss=5.225, nll_loss=1.474, ppl=2.78, wps=1985.6, ups=0.43, wpb=4581.5, bsz=176, num_updates=1396, lr=1.6752e-05, gnorm=1.557, train_wall=5, gb_free=12, wall=8588
2024-07-20 12:11:50 | INFO | train_inner | epoch 074:     11 / 19 loss=5.151, nll_loss=1.392, ppl=2.62, wps=1722.3, ups=0.33, wpb=5217.5, bsz=264, num_updates=1398, lr=1.6776e-05, gnorm=1.436, train_wall=6, gb_free=12, wall=8594
2024-07-20 12:11:55 | INFO | train_inner | epoch 074:     13 / 19 loss=5.124, nll_loss=1.401, ppl=2.64, wps=1906.1, ups=0.36, wpb=5260, bsz=256, num_updates=1400, lr=1.68e-05, gnorm=1.492, train_wall=6, gb_free=13.2, wall=8599
2024-07-20 12:12:01 | INFO | train_inner | epoch 074:     15 / 19 loss=5.178, nll_loss=1.488, ppl=2.8, wps=2170.4, ups=0.35, wpb=6257, bsz=260, num_updates=1402, lr=1.6824e-05, gnorm=1.289, train_wall=6, gb_free=11.4, wall=8605
2024-07-20 12:12:06 | INFO | train_inner | epoch 074:     17 / 19 loss=5.156, nll_loss=1.434, ppl=2.7, wps=1919.2, ups=0.39, wpb=4949.5, bsz=240, num_updates=1404, lr=1.6848e-05, gnorm=1.31, train_wall=5, gb_free=13.3, wall=8610
2024-07-20 12:12:09 | INFO | train_inner | epoch 074:     19 / 19 loss=5.301, nll_loss=1.55, ppl=2.93, wps=1009.7, ups=0.63, wpb=1608, bsz=57.5, num_updates=1406, lr=1.6872e-05, gnorm=2.694, train_wall=3, gb_free=17, wall=8613
2024-07-20 12:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25086.47265625Mb; avail=229943.58984375Mb
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000564
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25086.47265625Mb; avail=229943.58984375Mb
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002155
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25086.47265625Mb; avail=229943.58984375Mb
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001860
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004923
2024-07-20 12:12:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25086.47265625Mb; avail=229943.58984375Mb
2024-07-20 12:12:12 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 5.998 | nll_loss 2.238 | ppl 4.72 | wps 4169.8 | wpb 1665.6 | bsz 74.4 | num_updates 1406 | best_loss 5.984
2024-07-20 12:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 1406 updates
2024-07-20 12:12:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:12:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:12:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 74 @ 1406 updates, score 5.998) (writing took 41.96370261302218 seconds)
2024-07-20 12:12:54 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2024-07-20 12:12:54 | INFO | train | epoch 074 | loss 5.166 | nll_loss 1.444 | ppl 2.72 | wps 980.4 | ups 0.2 | wpb 4866.4 | bsz 219.1 | num_updates 1406 | lr 1.6872e-05 | gnorm 1.538 | train_wall 50 | gb_free 17 | wall 8658
2024-07-20 12:12:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:12:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:12:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:12:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000735
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30176.8984375Mb; avail=224853.12109375Mb
2024-07-20 12:12:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000718
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30176.8984375Mb; avail=224853.12109375Mb
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30176.8984375Mb; avail=224853.12109375Mb
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000216
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001233
2024-07-20 12:12:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30176.8984375Mb; avail=224853.12109375Mb
2024-07-20 12:12:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:12:54 | INFO | fairseq.trainer | begin training epoch 75
2024-07-20 12:12:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:13:00 | INFO | train_inner | epoch 075:      2 / 19 loss=5.158, nll_loss=1.412, ppl=2.66, wps=202.7, ups=0.04, wpb=5133, bsz=232, num_updates=1408, lr=1.6896e-05, gnorm=1.283, train_wall=6, gb_free=11.9, wall=8664
2024-07-20 12:13:04 | INFO | train_inner | epoch 075:      4 / 19 loss=5.078, nll_loss=1.333, ppl=2.52, wps=1512.9, ups=0.45, wpb=3342, bsz=161.5, num_updates=1410, lr=1.692e-05, gnorm=1.592, train_wall=4, gb_free=12.1, wall=8669
2024-07-20 12:13:10 | INFO | train_inner | epoch 075:      6 / 19 loss=5.091, nll_loss=1.351, ppl=2.55, wps=2033.1, ups=0.39, wpb=5249.5, bsz=216, num_updates=1412, lr=1.6944e-05, gnorm=1.667, train_wall=5, gb_free=11.8, wall=8674
2024-07-20 12:13:15 | INFO | train_inner | epoch 075:      8 / 19 loss=5.202, nll_loss=1.515, ppl=2.86, wps=2140.7, ups=0.35, wpb=6184.5, bsz=276, num_updates=1414, lr=1.6968e-05, gnorm=1.343, train_wall=6, gb_free=11.2, wall=8679
2024-07-20 12:13:20 | INFO | train_inner | epoch 075:     10 / 19 loss=5.11, nll_loss=1.342, ppl=2.54, wps=1898.2, ups=0.4, wpb=4756, bsz=200, num_updates=1416, lr=1.6992e-05, gnorm=1.389, train_wall=5, gb_free=15, wall=8684
2024-07-20 12:13:26 | INFO | train_inner | epoch 075:     12 / 19 loss=5.168, nll_loss=1.415, ppl=2.67, wps=1771.6, ups=0.35, wpb=5123.5, bsz=204, num_updates=1418, lr=1.7016e-05, gnorm=1.346, train_wall=6, gb_free=11, wall=8690
2024-07-20 12:13:32 | INFO | train_inner | epoch 075:     14 / 19 loss=5.149, nll_loss=1.428, ppl=2.69, wps=1464.7, ups=0.33, wpb=4445.5, bsz=216, num_updates=1420, lr=1.704e-05, gnorm=1.43, train_wall=6, gb_free=10.7, wall=8696
2024-07-20 12:13:37 | INFO | train_inner | epoch 075:     16 / 19 loss=5.253, nll_loss=1.57, ppl=2.97, wps=2143.2, ups=0.4, wpb=5376, bsz=220, num_updates=1422, lr=1.7064e-05, gnorm=1.583, train_wall=5, gb_free=13.1, wall=8701
2024-07-20 12:13:43 | INFO | train_inner | epoch 075:     18 / 19 loss=5.119, nll_loss=1.395, ppl=2.63, wps=1851.2, ups=0.33, wpb=5681, bsz=300, num_updates=1424, lr=1.7088e-05, gnorm=1.257, train_wall=6, gb_free=10.9, wall=8708
2024-07-20 12:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42590.6640625Mb; avail=212439.3984375Mb
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000562
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42591.15625Mb; avail=212438.90625Mb
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002091
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42591.15625Mb; avail=212438.90625Mb
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001867
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004831
2024-07-20 12:13:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42591.15625Mb; avail=212438.90625Mb
2024-07-20 12:13:48 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 5.991 | nll_loss 2.232 | ppl 4.7 | wps 3395.8 | wpb 1665.6 | bsz 74.4 | num_updates 1425 | best_loss 5.984
2024-07-20 12:13:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 1425 updates
2024-07-20 12:13:48 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:14:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 75 @ 1425 updates, score 5.991) (writing took 48.76693801302463 seconds)
2024-07-20 12:14:37 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2024-07-20 12:14:37 | INFO | train | epoch 075 | loss 5.148 | nll_loss 1.42 | ppl 2.68 | wps 897.5 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1425 | lr 1.71e-05 | gnorm 1.48 | train_wall 51 | gb_free 16.8 | wall 8761
2024-07-20 12:14:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:14:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:14:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:14:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000611
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30762.00390625Mb; avail=224268.03125Mb
2024-07-20 12:14:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000096
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000756
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30762.00390625Mb; avail=224268.03125Mb
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30762.00390625Mb; avail=224268.03125Mb
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000223
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001342
2024-07-20 12:14:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30762.00390625Mb; avail=224268.03125Mb
2024-07-20 12:14:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:14:37 | INFO | fairseq.trainer | begin training epoch 76
2024-07-20 12:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:14:40 | INFO | train_inner | epoch 076:      1 / 19 loss=5.059, nll_loss=1.306, ppl=2.47, wps=141.5, ups=0.04, wpb=3982.5, bsz=220, num_updates=1426, lr=1.7112e-05, gnorm=1.741, train_wall=4, gb_free=11.6, wall=8764
2024-07-20 12:14:45 | INFO | train_inner | epoch 076:      3 / 19 loss=5.142, nll_loss=1.401, ppl=2.64, wps=1819.1, ups=0.34, wpb=5313, bsz=252, num_updates=1428, lr=1.7136e-05, gnorm=1.399, train_wall=6, gb_free=11.1, wall=8770
2024-07-20 12:14:50 | INFO | train_inner | epoch 076:      5 / 19 loss=5.064, nll_loss=1.288, ppl=2.44, wps=1670.9, ups=0.4, wpb=4203.5, bsz=185.5, num_updates=1430, lr=1.716e-05, gnorm=1.667, train_wall=5, gb_free=13, wall=8775
2024-07-20 12:14:57 | INFO | train_inner | epoch 076:      7 / 19 loss=5.105, nll_loss=1.381, ppl=2.6, wps=1932.6, ups=0.33, wpb=5911.5, bsz=252, num_updates=1432, lr=1.7184e-05, gnorm=1.359, train_wall=6, gb_free=12.6, wall=8781
2024-07-20 12:15:02 | INFO | train_inner | epoch 076:      9 / 19 loss=5.049, nll_loss=1.31, ppl=2.48, wps=1929.3, ups=0.39, wpb=4997, bsz=276, num_updates=1434, lr=1.7208e-05, gnorm=1.207, train_wall=5, gb_free=11.3, wall=8786
2024-07-20 12:15:07 | INFO | train_inner | epoch 076:     11 / 19 loss=5.168, nll_loss=1.44, ppl=2.71, wps=1787.9, ups=0.4, wpb=4518, bsz=204, num_updates=1436, lr=1.7232e-05, gnorm=1.475, train_wall=5, gb_free=15.3, wall=8791
2024-07-20 12:15:12 | INFO | train_inner | epoch 076:     13 / 19 loss=5.16, nll_loss=1.428, ppl=2.69, wps=1771.9, ups=0.36, wpb=4922, bsz=220, num_updates=1438, lr=1.7256e-05, gnorm=1.401, train_wall=6, gb_free=12.9, wall=8797
2024-07-20 12:15:17 | INFO | train_inner | epoch 076:     15 / 19 loss=5.086, nll_loss=1.315, ppl=2.49, wps=2148.6, ups=0.41, wpb=5280.5, bsz=216, num_updates=1440, lr=1.728e-05, gnorm=1.229, train_wall=5, gb_free=14.5, wall=8801
2024-07-20 12:15:22 | INFO | train_inner | epoch 076:     17 / 19 loss=5.223, nll_loss=1.491, ppl=2.81, wps=1830.5, ups=0.44, wpb=4127, bsz=156, num_updates=1442, lr=1.7304e-05, gnorm=1.5, train_wall=4, gb_free=15.5, wall=8806
2024-07-20 12:15:27 | INFO | train_inner | epoch 076:     19 / 19 loss=5.085, nll_loss=1.364, ppl=2.57, wps=1638.6, ups=0.42, wpb=3916, bsz=156, num_updates=1444, lr=1.7328e-05, gnorm=1.502, train_wall=5, gb_free=16.7, wall=8811
2024-07-20 12:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37816.48828125Mb; avail=217213.53125Mb
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37816.98046875Mb; avail=217213.0390625Mb
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002080
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37816.98046875Mb; avail=217213.0390625Mb
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001916
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004905
2024-07-20 12:15:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37816.98046875Mb; avail=217213.0390625Mb
2024-07-20 12:15:29 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 5.976 | nll_loss 2.246 | ppl 4.74 | wps 4166.9 | wpb 1665.6 | bsz 74.4 | num_updates 1444 | best_loss 5.976
2024-07-20 12:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 1444 updates
2024-07-20 12:15:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 12:16:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 12:16:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 76 @ 1444 updates, score 5.976) (writing took 63.48832750599831 seconds)
2024-07-20 12:16:33 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2024-07-20 12:16:33 | INFO | train | epoch 076 | loss 5.117 | nll_loss 1.376 | ppl 2.6 | wps 798.3 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 1444 | lr 1.7328e-05 | gnorm 1.401 | train_wall 50 | gb_free 16.7 | wall 8877
2024-07-20 12:16:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:16:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:16:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:16:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000786
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=51451.02734375Mb; avail=203579.04296875Mb
2024-07-20 12:16:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000158
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000809
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51451.02734375Mb; avail=203579.04296875Mb
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51451.02734375Mb; avail=203579.04296875Mb
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000227
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001374
2024-07-20 12:16:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51451.02734375Mb; avail=203579.04296875Mb
2024-07-20 12:16:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:16:33 | INFO | fairseq.trainer | begin training epoch 77
2024-07-20 12:16:33 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:16:38 | INFO | train_inner | epoch 077:      2 / 19 loss=5.083, nll_loss=1.328, ppl=2.51, wps=132.4, ups=0.03, wpb=4728, bsz=192, num_updates=1446, lr=1.7352e-05, gnorm=1.48, train_wall=5, gb_free=12.6, wall=8882
2024-07-20 12:16:44 | INFO | train_inner | epoch 077:      4 / 19 loss=5.159, nll_loss=1.426, ppl=2.69, wps=1977.4, ups=0.34, wpb=5796.5, bsz=260, num_updates=1448, lr=1.7376e-05, gnorm=1.276, train_wall=6, gb_free=11.5, wall=8888
2024-07-20 12:16:49 | INFO | train_inner | epoch 077:      6 / 19 loss=5.007, nll_loss=1.216, ppl=2.32, wps=1991.3, ups=0.39, wpb=5045, bsz=248, num_updates=1450, lr=1.74e-05, gnorm=1.273, train_wall=5, gb_free=12.4, wall=8893
2024-07-20 12:16:55 | INFO | train_inner | epoch 077:      8 / 19 loss=5.062, nll_loss=1.327, ppl=2.51, wps=1718.1, ups=0.34, wpb=5122, bsz=264, num_updates=1452, lr=1.7424e-05, gnorm=1.267, train_wall=6, gb_free=11.7, wall=8899
2024-07-20 12:16:59 | INFO | train_inner | epoch 077:     10 / 19 loss=5.255, nll_loss=1.548, ppl=2.92, wps=1664.5, ups=0.46, wpb=3638, bsz=113.5, num_updates=1454, lr=1.7448e-05, gnorm=2.051, train_wall=4, gb_free=13.1, wall=8903
2024-07-20 12:17:05 | INFO | train_inner | epoch 077:     12 / 19 loss=5.066, nll_loss=1.324, ppl=2.5, wps=1664.1, ups=0.38, wpb=4340, bsz=184, num_updates=1456, lr=1.7472e-05, gnorm=1.453, train_wall=5, gb_free=14.9, wall=8909
2024-07-20 12:17:10 | INFO | train_inner | epoch 077:     14 / 19 loss=5.137, nll_loss=1.394, ppl=2.63, wps=1879.4, ups=0.36, wpb=5216.5, bsz=224, num_updates=1458, lr=1.7496e-05, gnorm=1.346, train_wall=6, gb_free=11.4, wall=8914
2024-07-20 12:17:16 | INFO | train_inner | epoch 077:     16 / 19 loss=5.032, nll_loss=1.262, ppl=2.4, wps=2010.7, ups=0.32, wpb=6217.5, bsz=292, num_updates=1460, lr=1.752e-05, gnorm=1.256, train_wall=6, gb_free=11.4, wall=8920
2024-07-20 12:17:22 | INFO | train_inner | epoch 077:     18 / 19 loss=5.072, nll_loss=1.326, ppl=2.51, wps=1834.5, ups=0.37, wpb=5010.5, bsz=256, num_updates=1462, lr=1.7544e-05, gnorm=1.275, train_wall=5, gb_free=10.7, wall=8926
2024-07-20 12:17:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41403.515625Mb; avail=213626.50390625Mb
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000582
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41403.515625Mb; avail=213626.50390625Mb
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002127
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41403.515625Mb; avail=213626.50390625Mb
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001827
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004861
2024-07-20 12:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41403.515625Mb; avail=213626.50390625Mb
2024-07-20 12:17:26 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 5.991 | nll_loss 2.261 | ppl 4.79 | wps 3973.9 | wpb 1665.6 | bsz 74.4 | num_updates 1463 | best_loss 5.976
2024-07-20 12:17:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 1463 updates
2024-07-20 12:17:26 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:18:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:18:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 77 @ 1463 updates, score 5.991) (writing took 53.03646372607909 seconds)
2024-07-20 12:18:19 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2024-07-20 12:18:19 | INFO | train | epoch 077 | loss 5.092 | nll_loss 1.343 | ppl 2.54 | wps 865.7 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1463 | lr 1.7556e-05 | gnorm 1.431 | train_wall 51 | gb_free 15.8 | wall 8984
2024-07-20 12:18:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:18:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:18:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:18:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000636
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=54212.98828125Mb; avail=200817.0859375Mb
2024-07-20 12:18:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000094
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000717
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54213.48046875Mb; avail=200816.59375Mb
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54212.98828125Mb; avail=200817.0859375Mb
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000207
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001221
2024-07-20 12:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=54213.48046875Mb; avail=200817.0859375Mb
2024-07-20 12:18:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:18:20 | INFO | fairseq.trainer | begin training epoch 78
2024-07-20 12:18:20 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:18:22 | INFO | train_inner | epoch 078:      1 / 19 loss=5.011, nll_loss=1.238, ppl=2.36, wps=132.7, ups=0.03, wpb=4007.5, bsz=176, num_updates=1464, lr=1.7568e-05, gnorm=1.559, train_wall=5, gb_free=12.3, wall=8986
2024-07-20 12:18:26 | INFO | train_inner | epoch 078:      3 / 19 loss=5.104, nll_loss=1.35, ppl=2.55, wps=1629.3, ups=0.47, wpb=3473.5, bsz=121.5, num_updates=1466, lr=1.7592e-05, gnorm=1.547, train_wall=4, gb_free=12.2, wall=8991
2024-07-20 12:18:32 | INFO | train_inner | epoch 078:      5 / 19 loss=5.009, nll_loss=1.269, ppl=2.41, wps=1802, ups=0.39, wpb=4630.5, bsz=216, num_updates=1468, lr=1.7616e-05, gnorm=1.309, train_wall=5, gb_free=12.7, wall=8996
2024-07-20 12:18:37 | INFO | train_inner | epoch 078:      7 / 19 loss=5.028, nll_loss=1.271, ppl=2.41, wps=1742.1, ups=0.34, wpb=5135.5, bsz=268, num_updates=1470, lr=1.764e-05, gnorm=1.246, train_wall=6, gb_free=11.2, wall=9002
2024-07-20 12:18:43 | INFO | train_inner | epoch 078:      9 / 19 loss=5.047, nll_loss=1.263, ppl=2.4, wps=1898.8, ups=0.39, wpb=4875.5, bsz=224, num_updates=1472, lr=1.7664e-05, gnorm=1.326, train_wall=5, gb_free=13.7, wall=9007
2024-07-20 12:18:48 | INFO | train_inner | epoch 078:     11 / 19 loss=5.026, nll_loss=1.261, ppl=2.4, wps=1918, ups=0.35, wpb=5490.5, bsz=284, num_updates=1474, lr=1.7688e-05, gnorm=1.159, train_wall=6, gb_free=12.8, wall=9012
2024-07-20 12:18:53 | INFO | train_inner | epoch 078:     13 / 19 loss=5.083, nll_loss=1.361, ppl=2.57, wps=2179.3, ups=0.4, wpb=5488, bsz=240, num_updates=1476, lr=1.7712e-05, gnorm=1.328, train_wall=5, gb_free=12.8, wall=9018
2024-07-20 12:18:59 | INFO | train_inner | epoch 078:     15 / 19 loss=5.115, nll_loss=1.385, ppl=2.61, wps=1908.9, ups=0.34, wpb=5684, bsz=236, num_updates=1478, lr=1.7736e-05, gnorm=1.337, train_wall=6, gb_free=11.7, wall=9023
2024-07-20 12:19:04 | INFO | train_inner | epoch 078:     17 / 19 loss=5.165, nll_loss=1.402, ppl=2.64, wps=1854.4, ups=0.41, wpb=4538, bsz=188, num_updates=1480, lr=1.776e-05, gnorm=1.54, train_wall=5, gb_free=10.6, wall=9028
2024-07-20 12:19:09 | INFO | train_inner | epoch 078:     19 / 19 loss=5.165, nll_loss=1.414, ppl=2.66, wps=1667.6, ups=0.41, wpb=4025, bsz=176, num_updates=1482, lr=1.7784e-05, gnorm=1.557, train_wall=5, gb_free=15.8, wall=9033
2024-07-20 12:19:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44179.96875Mb; avail=210850.05078125Mb
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44179.96875Mb; avail=210850.05078125Mb
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002072
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44179.96875Mb; avail=210850.05078125Mb
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001855
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004815
2024-07-20 12:19:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44179.96875Mb; avail=210850.05078125Mb
2024-07-20 12:19:12 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 6.002 | nll_loss 2.268 | ppl 4.82 | wps 4165.2 | wpb 1665.6 | bsz 74.4 | num_updates 1482 | best_loss 5.976
2024-07-20 12:19:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 1482 updates
2024-07-20 12:19:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:19:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:19:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 78 @ 1482 updates, score 6.002) (writing took 39.124218684853986 seconds)
2024-07-20 12:19:51 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2024-07-20 12:19:51 | INFO | train | epoch 078 | loss 5.074 | nll_loss 1.321 | ppl 2.5 | wps 1013.3 | ups 0.21 | wpb 4866.4 | bsz 219.1 | num_updates 1482 | lr 1.7784e-05 | gnorm 1.367 | train_wall 49 | gb_free 15.8 | wall 9075
2024-07-20 12:19:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:19:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:19:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:19:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000736
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=56916.99609375Mb; avail=198112.5859375Mb
2024-07-20 12:19:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000095
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000748
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56917.48828125Mb; avail=198113.078125Mb
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56916.99609375Mb; avail=198112.5859375Mb
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001266
2024-07-20 12:19:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=56916.99609375Mb; avail=198113.078125Mb
2024-07-20 12:19:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:19:51 | INFO | fairseq.trainer | begin training epoch 79
2024-07-20 12:19:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:19:56 | INFO | train_inner | epoch 079:      2 / 19 loss=5.078, nll_loss=1.328, ppl=2.51, wps=163, ups=0.04, wpb=3806.5, bsz=173.5, num_updates=1484, lr=1.7808e-05, gnorm=1.759, train_wall=5, gb_free=13.4, wall=9080
2024-07-20 12:20:01 | INFO | train_inner | epoch 079:      4 / 19 loss=5.041, nll_loss=1.307, ppl=2.47, wps=1925.3, ups=0.39, wpb=4952, bsz=224, num_updates=1486, lr=1.7832e-05, gnorm=1.366, train_wall=5, gb_free=14.2, wall=9085
2024-07-20 12:20:07 | INFO | train_inner | epoch 079:      6 / 19 loss=5.05, nll_loss=1.297, ppl=2.46, wps=1840.5, ups=0.35, wpb=5216, bsz=228, num_updates=1488, lr=1.7856e-05, gnorm=1.361, train_wall=6, gb_free=12.7, wall=9091
2024-07-20 12:20:12 | INFO | train_inner | epoch 079:      8 / 19 loss=5.029, nll_loss=1.247, ppl=2.37, wps=2297.6, ups=0.38, wpb=6087.5, bsz=296, num_updates=1490, lr=1.788e-05, gnorm=1.319, train_wall=5, gb_free=13.5, wall=9096
2024-07-20 12:20:18 | INFO | train_inner | epoch 079:     10 / 19 loss=5.06, nll_loss=1.282, ppl=2.43, wps=1509.6, ups=0.35, wpb=4298.5, bsz=164, num_updates=1492, lr=1.7904e-05, gnorm=1.574, train_wall=6, gb_free=13.3, wall=9102
2024-07-20 12:20:22 | INFO | train_inner | epoch 079:     12 / 19 loss=5.152, nll_loss=1.42, ppl=2.68, wps=1950, ups=0.45, wpb=4288, bsz=160, num_updates=1494, lr=1.7928e-05, gnorm=1.602, train_wall=4, gb_free=13, wall=9106
2024-07-20 12:20:27 | INFO | train_inner | epoch 079:     14 / 19 loss=5.001, nll_loss=1.269, ppl=2.41, wps=1720.8, ups=0.36, wpb=4782, bsz=236, num_updates=1496, lr=1.7952e-05, gnorm=1.459, train_wall=6, gb_free=12.7, wall=9112
2024-07-20 12:20:34 | INFO | train_inner | epoch 079:     16 / 19 loss=5.04, nll_loss=1.272, ppl=2.42, wps=1768.8, ups=0.32, wpb=5472, bsz=232, num_updates=1498, lr=1.7976e-05, gnorm=1.263, train_wall=6, gb_free=11.6, wall=9118
2024-07-20 12:20:40 | INFO | train_inner | epoch 079:     18 / 19 loss=5.071, nll_loss=1.285, ppl=2.44, wps=2055.5, ups=0.33, wpb=6300.5, bsz=296, num_updates=1500, lr=1.8e-05, gnorm=1.16, train_wall=6, gb_free=11.9, wall=9124
2024-07-20 12:20:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47285.921875Mb; avail=207744.09375Mb
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000575
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47285.921875Mb; avail=207744.09375Mb
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002083
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47285.921875Mb; avail=207744.09375Mb
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001830
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004807
2024-07-20 12:20:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47285.921875Mb; avail=207744.09375Mb
2024-07-20 12:20:44 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 5.988 | nll_loss 2.267 | ppl 4.81 | wps 4156.9 | wpb 1665.6 | bsz 74.4 | num_updates 1501 | best_loss 5.976
2024-07-20 12:20:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 1501 updates
2024-07-20 12:20:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:21:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:21:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 79 @ 1501 updates, score 5.988) (writing took 53.56403655395843 seconds)
2024-07-20 12:21:37 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2024-07-20 12:21:37 | INFO | train | epoch 079 | loss 5.051 | nll_loss 1.29 | ppl 2.45 | wps 866.5 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1501 | lr 1.8012e-05 | gnorm 1.433 | train_wall 50 | gb_free 17 | wall 9182
2024-07-20 12:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000640
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60086.8125Mb; avail=194942.2265625Mb
2024-07-20 12:21:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000101
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000747
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60082.41015625Mb; avail=194947.12109375Mb
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60085.890625Mb; avail=194942.65625Mb
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000215
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001278
2024-07-20 12:21:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60088.3515625Mb; avail=194940.6875Mb
2024-07-20 12:21:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:21:38 | INFO | fairseq.trainer | begin training epoch 80
2024-07-20 12:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:21:40 | INFO | train_inner | epoch 080:      1 / 19 loss=5.064, nll_loss=1.311, ppl=2.48, wps=115.6, ups=0.03, wpb=3478.5, bsz=184, num_updates=1502, lr=1.8024e-05, gnorm=1.52, train_wall=4, gb_free=12.4, wall=9184
2024-07-20 12:21:44 | INFO | train_inner | epoch 080:      3 / 19 loss=5.097, nll_loss=1.388, ppl=2.62, wps=1859.5, ups=0.5, wpb=3710, bsz=161.5, num_updates=1504, lr=1.8048e-05, gnorm=1.7, train_wall=4, gb_free=16.9, wall=9188
2024-07-20 12:21:50 | INFO | train_inner | epoch 080:      5 / 19 loss=4.992, nll_loss=1.224, ppl=2.34, wps=1732.9, ups=0.34, wpb=5112, bsz=244, num_updates=1506, lr=1.8072e-05, gnorm=1.711, train_wall=6, gb_free=12.7, wall=9194
2024-07-20 12:21:56 | INFO | train_inner | epoch 080:      7 / 19 loss=5.011, nll_loss=1.214, ppl=2.32, wps=1890.3, ups=0.34, wpb=5538.5, bsz=256, num_updates=1508, lr=1.8096e-05, gnorm=1.275, train_wall=6, gb_free=11.5, wall=9200
2024-07-20 12:22:02 | INFO | train_inner | epoch 080:      9 / 19 loss=5.028, nll_loss=1.24, ppl=2.36, wps=1766.9, ups=0.34, wpb=5123.5, bsz=228, num_updates=1510, lr=1.812e-05, gnorm=1.399, train_wall=6, gb_free=11.4, wall=9206
2024-07-20 12:22:07 | INFO | train_inner | epoch 080:     11 / 19 loss=5.024, nll_loss=1.272, ppl=2.41, wps=2242.4, ups=0.39, wpb=5783.5, bsz=244, num_updates=1512, lr=1.8144e-05, gnorm=1.279, train_wall=5, gb_free=13.2, wall=9211
2024-07-20 12:22:13 | INFO | train_inner | epoch 080:     13 / 19 loss=5.067, nll_loss=1.356, ppl=2.56, wps=1776.5, ups=0.34, wpb=5191, bsz=240, num_updates=1514, lr=1.8168e-05, gnorm=1.385, train_wall=6, gb_free=12.1, wall=9217
2024-07-20 12:22:18 | INFO | train_inner | epoch 080:     15 / 19 loss=4.956, nll_loss=1.176, ppl=2.26, wps=1911.1, ups=0.39, wpb=4878, bsz=244, num_updates=1516, lr=1.8192e-05, gnorm=1.168, train_wall=5, gb_free=14.4, wall=9222
2024-07-20 12:22:24 | INFO | train_inner | epoch 080:     17 / 19 loss=5.06, nll_loss=1.254, ppl=2.39, wps=1704.1, ups=0.33, wpb=5149, bsz=204, num_updates=1518, lr=1.8216e-05, gnorm=1.46, train_wall=6, gb_free=12.2, wall=9228
2024-07-20 12:22:27 | INFO | train_inner | epoch 080:     19 / 19 loss=5.081, nll_loss=1.314, ppl=2.49, wps=1958, ups=0.59, wpb=3295, bsz=148, num_updates=1520, lr=1.824e-05, gnorm=1.677, train_wall=3, gb_free=17.1, wall=9231
2024-07-20 12:22:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38318.69921875Mb; avail=216711.40625Mb
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000588
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38318.69921875Mb; avail=216711.40625Mb
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002104
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38318.69921875Mb; avail=216711.40625Mb
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001874
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004899
2024-07-20 12:22:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38319.19140625Mb; avail=216710.9140625Mb
2024-07-20 12:22:30 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 5.978 | nll_loss 2.29 | ppl 4.89 | wps 4165.2 | wpb 1665.6 | bsz 74.4 | num_updates 1520 | best_loss 5.976
2024-07-20 12:22:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 1520 updates
2024-07-20 12:22:30 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:23:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:23:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 80 @ 1520 updates, score 5.978) (writing took 38.65146841504611 seconds)
2024-07-20 12:23:08 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2024-07-20 12:23:08 | INFO | train | epoch 080 | loss 5.039 | nll_loss 1.275 | ppl 2.42 | wps 1017.5 | ups 0.21 | wpb 4866.4 | bsz 219.1 | num_updates 1520 | lr 1.824e-05 | gnorm 1.456 | train_wall 49 | gb_free 17.1 | wall 9272
2024-07-20 12:23:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:23:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:23:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:23:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000741
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45161.34765625Mb; avail=209868.7265625Mb
2024-07-20 12:23:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000076
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000733
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45162.82421875Mb; avail=209867.25Mb
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45163.31640625Mb; avail=209866.7578125Mb
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000219
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001258
2024-07-20 12:23:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45163.80859375Mb; avail=209866.265625Mb
2024-07-20 12:23:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:23:08 | INFO | fairseq.trainer | begin training epoch 81
2024-07-20 12:23:08 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:23:14 | INFO | train_inner | epoch 081:      2 / 19 loss=4.976, nll_loss=1.234, ppl=2.35, wps=200.3, ups=0.04, wpb=4665, bsz=232, num_updates=1522, lr=1.8264e-05, gnorm=1.44, train_wall=5, gb_free=12.5, wall=9278
2024-07-20 12:23:19 | INFO | train_inner | epoch 081:      4 / 19 loss=5.001, nll_loss=1.266, ppl=2.4, wps=2030.4, ups=0.34, wpb=5899, bsz=292, num_updates=1524, lr=1.8288e-05, gnorm=1.272, train_wall=6, gb_free=11.8, wall=9284
2024-07-20 12:23:25 | INFO | train_inner | epoch 081:      6 / 19 loss=4.971, nll_loss=1.17, ppl=2.25, wps=1910, ups=0.37, wpb=5226.5, bsz=232, num_updates=1526, lr=1.8312e-05, gnorm=1.321, train_wall=5, gb_free=12.6, wall=9289
2024-07-20 12:23:30 | INFO | train_inner | epoch 081:      8 / 19 loss=5.008, nll_loss=1.226, ppl=2.34, wps=1896.4, ups=0.4, wpb=4798.5, bsz=208, num_updates=1528, lr=1.8336e-05, gnorm=1.36, train_wall=5, gb_free=13.3, wall=9294
2024-07-20 12:23:35 | INFO | train_inner | epoch 081:     10 / 19 loss=5.176, nll_loss=1.434, ppl=2.7, wps=1477.1, ups=0.38, wpb=3897.5, bsz=120, num_updates=1530, lr=1.836e-05, gnorm=1.788, train_wall=5, gb_free=12.3, wall=9299
2024-07-20 12:23:40 | INFO | train_inner | epoch 081:     12 / 19 loss=5.064, nll_loss=1.304, ppl=2.47, wps=2023.8, ups=0.47, wpb=4317.5, bsz=165.5, num_updates=1532, lr=1.8384e-05, gnorm=1.662, train_wall=4, gb_free=11.4, wall=9304
2024-07-20 12:23:44 | INFO | train_inner | epoch 081:     14 / 19 loss=4.997, nll_loss=1.237, ppl=2.36, wps=1836.7, ups=0.41, wpb=4497, bsz=228, num_updates=1534, lr=1.8408e-05, gnorm=1.659, train_wall=5, gb_free=12.1, wall=9309
2024-07-20 12:23:51 | INFO | train_inner | epoch 081:     16 / 19 loss=4.964, nll_loss=1.174, ppl=2.26, wps=1825.3, ups=0.32, wpb=5721, bsz=276, num_updates=1536, lr=1.8432e-05, gnorm=1.18, train_wall=6, gb_free=11, wall=9315
2024-07-20 12:23:56 | INFO | train_inner | epoch 081:     18 / 19 loss=5.016, nll_loss=1.245, ppl=2.37, wps=2144.2, ups=0.39, wpb=5553.5, bsz=256, num_updates=1538, lr=1.8456e-05, gnorm=1.492, train_wall=5, gb_free=13.1, wall=9320
2024-07-20 12:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45051.1953125Mb; avail=209978.8203125Mb
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000595
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45051.1953125Mb; avail=209978.8203125Mb
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002129
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45051.6875Mb; avail=209978.328125Mb
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001845
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004922
2024-07-20 12:23:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45051.6875Mb; avail=209978.328125Mb
2024-07-20 12:24:01 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 5.978 | nll_loss 2.25 | ppl 4.76 | wps 2999.4 | wpb 1665.6 | bsz 74.4 | num_updates 1539 | best_loss 5.976
2024-07-20 12:24:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 1539 updates
2024-07-20 12:24:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:24:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:24:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 81 @ 1539 updates, score 5.978) (writing took 50.388946337858215 seconds)
2024-07-20 12:24:52 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2024-07-20 12:24:52 | INFO | train | epoch 081 | loss 5.016 | nll_loss 1.252 | ppl 2.38 | wps 894.4 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1539 | lr 1.8468e-05 | gnorm 1.464 | train_wall 49 | gb_free 15.5 | wall 9376
2024-07-20 12:24:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:24:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:24:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:24:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000626
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47711.48046875Mb; avail=207318.65625Mb
2024-07-20 12:24:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000090
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000711
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47710.98828125Mb; avail=207319.1484375Mb
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47711.48046875Mb; avail=207318.65625Mb
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000206
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001219
2024-07-20 12:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47711.48046875Mb; avail=207318.65625Mb
2024-07-20 12:24:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:24:52 | INFO | fairseq.trainer | begin training epoch 82
2024-07-20 12:24:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:24:55 | INFO | train_inner | epoch 082:      1 / 19 loss=4.994, nll_loss=1.224, ppl=2.34, wps=124.9, ups=0.03, wpb=3666.5, bsz=156, num_updates=1540, lr=1.848e-05, gnorm=1.381, train_wall=5, gb_free=12.3, wall=9379
2024-07-20 12:25:00 | INFO | train_inner | epoch 082:      3 / 19 loss=5.092, nll_loss=1.345, ppl=2.54, wps=1620.6, ups=0.37, wpb=4358.5, bsz=184, num_updates=1542, lr=1.8504e-05, gnorm=1.488, train_wall=5, gb_free=13.5, wall=9384
2024-07-20 12:25:05 | INFO | train_inner | epoch 082:      5 / 19 loss=5.026, nll_loss=1.233, ppl=2.35, wps=2057.8, ups=0.39, wpb=5238, bsz=212, num_updates=1544, lr=1.8528e-05, gnorm=1.367, train_wall=5, gb_free=16.7, wall=9389
2024-07-20 12:25:11 | INFO | train_inner | epoch 082:      7 / 19 loss=4.941, nll_loss=1.149, ppl=2.22, wps=1961.5, ups=0.35, wpb=5528.5, bsz=256, num_updates=1546, lr=1.8552e-05, gnorm=1.213, train_wall=6, gb_free=11.2, wall=9395
2024-07-20 12:25:17 | INFO | train_inner | epoch 082:      9 / 19 loss=4.953, nll_loss=1.198, ppl=2.29, wps=1517.6, ups=0.33, wpb=4634.5, bsz=216, num_updates=1548, lr=1.8576e-05, gnorm=1.469, train_wall=6, gb_free=12.1, wall=9401
2024-07-20 12:25:22 | INFO | train_inner | epoch 082:     11 / 19 loss=5.026, nll_loss=1.281, ppl=2.43, wps=2038.6, ups=0.35, wpb=5812.5, bsz=276, num_updates=1550, lr=1.86e-05, gnorm=1.224, train_wall=6, gb_free=12.1, wall=9407
2024-07-20 12:25:28 | INFO | train_inner | epoch 082:     13 / 19 loss=5.042, nll_loss=1.271, ppl=2.41, wps=1861.1, ups=0.34, wpb=5469, bsz=224, num_updates=1552, lr=1.8624e-05, gnorm=1.382, train_wall=6, gb_free=13.3, wall=9413
2024-07-20 12:25:33 | INFO | train_inner | epoch 082:     15 / 19 loss=4.926, nll_loss=1.096, ppl=2.14, wps=1844.6, ups=0.47, wpb=3924, bsz=185.5, num_updates=1554, lr=1.8648e-05, gnorm=1.358, train_wall=4, gb_free=16.5, wall=9417
2024-07-20 12:25:38 | INFO | train_inner | epoch 082:     17 / 19 loss=4.986, nll_loss=1.206, ppl=2.31, wps=2001.4, ups=0.41, wpb=4925, bsz=228, num_updates=1556, lr=1.8672e-05, gnorm=1.414, train_wall=5, gb_free=11.8, wall=9422
2024-07-20 12:25:43 | INFO | train_inner | epoch 082:     19 / 19 loss=5.02, nll_loss=1.287, ppl=2.44, wps=1694.2, ups=0.39, wpb=4330, bsz=216, num_updates=1558, lr=1.8696e-05, gnorm=1.492, train_wall=5, gb_free=16.4, wall=9427
2024-07-20 12:25:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36177.1171875Mb; avail=218852.83984375Mb
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36177.609375Mb; avail=218852.34765625Mb
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002101
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36177.609375Mb; avail=218852.34765625Mb
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001828
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004821
2024-07-20 12:25:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36177.609375Mb; avail=218852.34765625Mb
2024-07-20 12:25:45 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 5.973 | nll_loss 2.294 | ppl 4.9 | wps 4156.2 | wpb 1665.6 | bsz 74.4 | num_updates 1558 | best_loss 5.973
2024-07-20 12:25:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 1558 updates
2024-07-20 12:25:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 12:26:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt
2024-07-20 12:26:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_best.pt (epoch 82 @ 1558 updates, score 5.973) (writing took 65.96509015094489 seconds)
2024-07-20 12:26:51 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2024-07-20 12:26:51 | INFO | train | epoch 082 | loss 4.999 | nll_loss 1.226 | ppl 2.34 | wps 773.4 | ups 0.16 | wpb 4866.4 | bsz 219.1 | num_updates 1558 | lr 1.8696e-05 | gnorm 1.374 | train_wall 51 | gb_free 16.4 | wall 9495
2024-07-20 12:26:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:26:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:26:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:26:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000748
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=49341.84375Mb; avail=205690.2265625Mb
2024-07-20 12:26:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000708
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49322.15625Mb; avail=205713.359375Mb
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49312.3125Mb; avail=205717.7890625Mb
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000228
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001239
2024-07-20 12:26:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49312.8046875Mb; avail=205717.296875Mb
2024-07-20 12:26:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:26:51 | INFO | fairseq.trainer | begin training epoch 83
2024-07-20 12:26:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:26:56 | INFO | train_inner | epoch 083:      2 / 19 loss=5.007, nll_loss=1.267, ppl=2.41, wps=143, ups=0.03, wpb=5269, bsz=216, num_updates=1560, lr=1.872e-05, gnorm=1.502, train_wall=5, gb_free=12.8, wall=9500
2024-07-20 12:27:02 | INFO | train_inner | epoch 083:      4 / 19 loss=4.899, nll_loss=1.071, ppl=2.1, wps=1587.7, ups=0.35, wpb=4545, bsz=232, num_updates=1562, lr=1.8744e-05, gnorm=1.229, train_wall=6, gb_free=11.6, wall=9506
2024-07-20 12:27:06 | INFO | train_inner | epoch 083:      6 / 19 loss=4.841, nll_loss=1.003, ppl=2, wps=1322.7, ups=0.47, wpb=2804.5, bsz=137.5, num_updates=1564, lr=1.8768e-05, gnorm=1.64, train_wall=4, gb_free=12.7, wall=9510
2024-07-20 12:27:11 | INFO | train_inner | epoch 083:      8 / 19 loss=4.971, nll_loss=1.212, ppl=2.32, wps=2140, ups=0.41, wpb=5233, bsz=224, num_updates=1566, lr=1.8792e-05, gnorm=1.36, train_wall=5, gb_free=15.5, wall=9515
2024-07-20 12:27:17 | INFO | train_inner | epoch 083:     10 / 19 loss=4.884, nll_loss=1.087, ppl=2.12, wps=1484.8, ups=0.34, wpb=4304, bsz=180, num_updates=1568, lr=1.8816e-05, gnorm=1.248, train_wall=6, gb_free=12, wall=9521
2024-07-20 12:27:23 | INFO | train_inner | epoch 083:     12 / 19 loss=4.977, nll_loss=1.201, ppl=2.3, wps=2250.6, ups=0.36, wpb=6251, bsz=328, num_updates=1570, lr=1.884e-05, gnorm=1.126, train_wall=6, gb_free=11, wall=9527
2024-07-20 12:27:29 | INFO | train_inner | epoch 083:     14 / 19 loss=5.003, nll_loss=1.219, ppl=2.33, wps=1933.3, ups=0.32, wpb=5976, bsz=228, num_updates=1572, lr=1.8864e-05, gnorm=1.28, train_wall=6, gb_free=11.8, wall=9533
2024-07-20 12:27:34 | INFO | train_inner | epoch 083:     16 / 19 loss=5.011, nll_loss=1.255, ppl=2.39, wps=2053.6, ups=0.42, wpb=4930, bsz=228, num_updates=1574, lr=1.8888e-05, gnorm=1.253, train_wall=5, gb_free=13.4, wall=9538
2024-07-20 12:27:39 | INFO | train_inner | epoch 083:     18 / 19 loss=4.999, nll_loss=1.216, ppl=2.32, wps=2260.1, ups=0.38, wpb=5877.5, bsz=264, num_updates=1576, lr=1.8912e-05, gnorm=1.226, train_wall=5, gb_free=13.3, wall=9543
2024-07-20 12:27:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=48280.4296875Mb; avail=206749.546875Mb
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000552
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48280.4296875Mb; avail=206749.546875Mb
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002125
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48280.4296875Mb; avail=206749.546875Mb
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001862
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004856
2024-07-20 12:27:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48280.4296875Mb; avail=206749.546875Mb
2024-07-20 12:27:43 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 6.005 | nll_loss 2.283 | ppl 4.87 | wps 3053.1 | wpb 1665.6 | bsz 74.4 | num_updates 1577 | best_loss 5.973
2024-07-20 12:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 1577 updates
2024-07-20 12:27:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:28:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:28:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 83 @ 1577 updates, score 6.005) (writing took 54.209428555099294 seconds)
2024-07-20 12:28:38 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2024-07-20 12:28:38 | INFO | train | epoch 083 | loss 4.965 | nll_loss 1.184 | ppl 2.27 | wps 869.2 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1577 | lr 1.8924e-05 | gnorm 1.35 | train_wall 49 | gb_free 17.6 | wall 9602
2024-07-20 12:28:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:28:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:28:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:28:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000734
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35318.7890625Mb; avail=219711.31640625Mb
2024-07-20 12:28:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000081
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000767
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35318.7890625Mb; avail=219711.31640625Mb
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000039
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35318.7890625Mb; avail=219711.31640625Mb
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000214
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001307
2024-07-20 12:28:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35318.7890625Mb; avail=219711.31640625Mb
2024-07-20 12:28:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:28:38 | INFO | fairseq.trainer | begin training epoch 84
2024-07-20 12:28:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:28:40 | INFO | train_inner | epoch 084:      1 / 19 loss=4.906, nll_loss=1.117, ppl=2.17, wps=121, ups=0.03, wpb=3723.5, bsz=196, num_updates=1578, lr=1.8936e-05, gnorm=1.558, train_wall=4, gb_free=11.4, wall=9604
2024-07-20 12:28:45 | INFO | train_inner | epoch 084:      3 / 19 loss=4.961, nll_loss=1.172, ppl=2.25, wps=2075.4, ups=0.39, wpb=5380.5, bsz=216, num_updates=1580, lr=1.896e-05, gnorm=1.405, train_wall=5, gb_free=11, wall=9610
2024-07-20 12:28:51 | INFO | train_inner | epoch 084:      5 / 19 loss=5.006, nll_loss=1.259, ppl=2.39, wps=1971.5, ups=0.34, wpb=5763, bsz=244, num_updates=1582, lr=1.8984e-05, gnorm=1.331, train_wall=6, gb_free=13.4, wall=9615
2024-07-20 12:28:57 | INFO | train_inner | epoch 084:      7 / 19 loss=4.925, nll_loss=1.135, ppl=2.2, wps=1798.8, ups=0.34, wpb=5347, bsz=244, num_updates=1584, lr=1.9008e-05, gnorm=1.173, train_wall=6, gb_free=12.2, wall=9621
2024-07-20 12:29:03 | INFO | train_inner | epoch 084:      9 / 19 loss=4.863, nll_loss=1.042, ppl=2.06, wps=1841.9, ups=0.34, wpb=5366.5, bsz=276, num_updates=1586, lr=1.9032e-05, gnorm=1.129, train_wall=6, gb_free=12.1, wall=9627
2024-07-20 12:29:07 | INFO | train_inner | epoch 084:     11 / 19 loss=4.881, nll_loss=1.069, ppl=2.1, wps=1720.8, ups=0.47, wpb=3649.5, bsz=176, num_updates=1588, lr=1.9056e-05, gnorm=1.31, train_wall=4, gb_free=13.6, wall=9631
2024-07-20 12:29:12 | INFO | train_inner | epoch 084:     13 / 19 loss=4.991, nll_loss=1.255, ppl=2.39, wps=1790.9, ups=0.39, wpb=4606.5, bsz=213.5, num_updates=1590, lr=1.908e-05, gnorm=1.401, train_wall=5, gb_free=16, wall=9637
2024-07-20 12:29:18 | INFO | train_inner | epoch 084:     15 / 19 loss=4.976, nll_loss=1.197, ppl=2.29, wps=2243.7, ups=0.39, wpb=5793.5, bsz=252, num_updates=1592, lr=1.9104e-05, gnorm=1.276, train_wall=5, gb_free=12.7, wall=9642
2024-07-20 12:29:23 | INFO | train_inner | epoch 084:     17 / 19 loss=4.931, nll_loss=1.099, ppl=2.14, wps=1514.6, ups=0.35, wpb=4314.5, bsz=196, num_updates=1594, lr=1.9128e-05, gnorm=1.447, train_wall=6, gb_free=12.8, wall=9647
2024-07-20 12:29:27 | INFO | train_inner | epoch 084:     19 / 19 loss=5.004, nll_loss=1.207, ppl=2.31, wps=1784.9, ups=0.54, wpb=3327.5, bsz=112, num_updates=1596, lr=1.9152e-05, gnorm=1.501, train_wall=4, gb_free=18.4, wall=9651
2024-07-20 12:29:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42363.94921875Mb; avail=212665.6171875Mb
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42365.42578125Mb; avail=212664.6328125Mb
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002076
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42370.34765625Mb; avail=212659.7109375Mb
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001868
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004829
2024-07-20 12:29:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42374.77734375Mb; avail=212655.28125Mb
2024-07-20 12:29:30 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 5.998 | nll_loss 2.31 | ppl 4.96 | wps 4176.3 | wpb 1665.6 | bsz 74.4 | num_updates 1596 | best_loss 5.973
2024-07-20 12:29:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 1596 updates
2024-07-20 12:29:30 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:30:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:30:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 84 @ 1596 updates, score 5.998) (writing took 42.96089379303157 seconds)
2024-07-20 12:30:13 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2024-07-20 12:30:13 | INFO | train | epoch 084 | loss 4.945 | nll_loss 1.158 | ppl 2.23 | wps 973.9 | ups 0.2 | wpb 4866.4 | bsz 219.1 | num_updates 1596 | lr 1.9152e-05 | gnorm 1.324 | train_wall 49 | gb_free 18.4 | wall 9697
2024-07-20 12:30:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:30:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:30:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:30:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000767
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=47243.0625Mb; avail=207785.62890625Mb
2024-07-20 12:30:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000079
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000728
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47244.57421875Mb; avail=207783.625Mb
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47244.57421875Mb; avail=207784.1171875Mb
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001257
2024-07-20 12:30:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=47244.57421875Mb; avail=207783.08984375Mb
2024-07-20 12:30:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:30:13 | INFO | fairseq.trainer | begin training epoch 85
2024-07-20 12:30:13 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:30:19 | INFO | train_inner | epoch 085:      2 / 19 loss=5.013, nll_loss=1.276, ppl=2.42, wps=215.7, ups=0.04, wpb=5580.5, bsz=272, num_updates=1598, lr=1.9176e-05, gnorm=1.288, train_wall=6, gb_free=11.7, wall=9703
2024-07-20 12:30:24 | INFO | train_inner | epoch 085:      4 / 19 loss=4.936, nll_loss=1.137, ppl=2.2, wps=1828.9, ups=0.36, wpb=5117, bsz=196, num_updates=1600, lr=1.92e-05, gnorm=1.341, train_wall=6, gb_free=11.2, wall=9709
2024-07-20 12:30:30 | INFO | train_inner | epoch 085:      6 / 19 loss=4.89, nll_loss=1.071, ppl=2.1, wps=2122, ups=0.39, wpb=5454.5, bsz=248, num_updates=1602, lr=1.9224e-05, gnorm=1.214, train_wall=5, gb_free=11.2, wall=9714
2024-07-20 12:30:35 | INFO | train_inner | epoch 085:      8 / 19 loss=4.944, nll_loss=1.145, ppl=2.21, wps=1690.3, ups=0.35, wpb=4815, bsz=192, num_updates=1604, lr=1.9248e-05, gnorm=1.39, train_wall=6, gb_free=13.9, wall=9719
2024-07-20 12:30:40 | INFO | train_inner | epoch 085:     10 / 19 loss=4.918, nll_loss=1.139, ppl=2.2, wps=1791.5, ups=0.39, wpb=4579, bsz=236, num_updates=1606, lr=1.9272e-05, gnorm=1.444, train_wall=5, gb_free=11.7, wall=9725
2024-07-20 12:30:46 | INFO | train_inner | epoch 085:     12 / 19 loss=4.887, nll_loss=1.107, ppl=2.15, wps=1741, ups=0.36, wpb=4824.5, bsz=224, num_updates=1608, lr=1.9296e-05, gnorm=1.225, train_wall=6, gb_free=12, wall=9730
2024-07-20 12:30:51 | INFO | train_inner | epoch 085:     14 / 19 loss=4.906, nll_loss=1.093, ppl=2.13, wps=1942.1, ups=0.39, wpb=4935, bsz=232, num_updates=1610, lr=1.932e-05, gnorm=1.314, train_wall=5, gb_free=12, wall=9735
2024-07-20 12:30:57 | INFO | train_inner | epoch 085:     16 / 19 loss=4.896, nll_loss=1.1, ppl=2.14, wps=1996, ups=0.32, wpb=6245.5, bsz=280, num_updates=1612, lr=1.9344e-05, gnorm=1.074, train_wall=6, gb_free=13, wall=9741
2024-07-20 12:31:02 | INFO | train_inner | epoch 085:     18 / 19 loss=4.934, nll_loss=1.157, ppl=2.23, wps=1983.6, ups=0.44, wpb=4516, bsz=200, num_updates=1614, lr=1.9368e-05, gnorm=1.481, train_wall=5, gb_free=11.9, wall=9746
2024-07-20 12:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=64424.3671875Mb; avail=190605.5546875Mb
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64424.3671875Mb; avail=190605.0625Mb
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002091
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64424.859375Mb; avail=190605.5546875Mb
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001848
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004830
2024-07-20 12:31:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=64424.3671875Mb; avail=190605.5546875Mb
2024-07-20 12:31:05 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 6.012 | nll_loss 2.287 | ppl 4.88 | wps 4165 | wpb 1665.6 | bsz 74.4 | num_updates 1615 | best_loss 5.973
2024-07-20 12:31:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 1615 updates
2024-07-20 12:31:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:31:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:31:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 85 @ 1615 updates, score 6.012) (writing took 53.9477029859554 seconds)
2024-07-20 12:31:59 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2024-07-20 12:31:59 | INFO | train | epoch 085 | loss 4.925 | nll_loss 1.136 | ppl 2.2 | wps 871.1 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1615 | lr 1.938e-05 | gnorm 1.553 | train_wall 49 | gb_free 26.7 | wall 9803
2024-07-20 12:31:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:31:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:31:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:31:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000626
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23888.67578125Mb; avail=231141.41796875Mb
2024-07-20 12:31:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000093
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000727
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23888.67578125Mb; avail=231141.41796875Mb
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23888.67578125Mb; avail=231141.41796875Mb
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000207
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001234
2024-07-20 12:31:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23888.67578125Mb; avail=231141.41796875Mb
2024-07-20 12:31:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:31:59 | INFO | fairseq.trainer | begin training epoch 86
2024-07-20 12:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:32:02 | INFO | train_inner | epoch 086:      1 / 19 loss=4.796, nll_loss=0.946, ppl=1.93, wps=72.6, ups=0.03, wpb=2168.5, bsz=97.5, num_updates=1616, lr=1.9392e-05, gnorm=3.556, train_wall=3, gb_free=12.3, wall=9806
2024-07-20 12:32:06 | INFO | train_inner | epoch 086:      3 / 19 loss=4.983, nll_loss=1.192, ppl=2.29, wps=1971.5, ups=0.41, wpb=4766.5, bsz=172, num_updates=1618, lr=1.9416e-05, gnorm=1.467, train_wall=5, gb_free=16, wall=9811
2024-07-20 12:32:11 | INFO | train_inner | epoch 086:      5 / 19 loss=4.895, nll_loss=1.109, ppl=2.16, wps=1998.3, ups=0.48, wpb=4192.5, bsz=209.5, num_updates=1620, lr=1.944e-05, gnorm=1.414, train_wall=4, gb_free=11.5, wall=9815
2024-07-20 12:32:16 | INFO | train_inner | epoch 086:      7 / 19 loss=4.84, nll_loss=1.034, ppl=2.05, wps=1654.2, ups=0.34, wpb=4822.5, bsz=260, num_updates=1622, lr=1.9464e-05, gnorm=1.218, train_wall=6, gb_free=10.7, wall=9821
2024-07-20 12:32:21 | INFO | train_inner | epoch 086:      9 / 19 loss=4.935, nll_loss=1.138, ppl=2.2, wps=2001.6, ups=0.4, wpb=4962, bsz=192, num_updates=1624, lr=1.9488e-05, gnorm=1.332, train_wall=5, gb_free=13.4, wall=9825
2024-07-20 12:32:27 | INFO | train_inner | epoch 086:     11 / 19 loss=4.942, nll_loss=1.154, ppl=2.23, wps=1920.2, ups=0.34, wpb=5606, bsz=244, num_updates=1626, lr=1.9512e-05, gnorm=1.236, train_wall=6, gb_free=13.3, wall=9831
2024-07-20 12:32:32 | INFO | train_inner | epoch 086:     13 / 19 loss=4.893, nll_loss=1.089, ppl=2.13, wps=2069.9, ups=0.39, wpb=5263, bsz=236, num_updates=1628, lr=1.9536e-05, gnorm=1.233, train_wall=5, gb_free=12.7, wall=9836
2024-07-20 12:32:39 | INFO | train_inner | epoch 086:     15 / 19 loss=4.887, nll_loss=1.068, ppl=2.1, wps=1802, ups=0.32, wpb=5653, bsz=268, num_updates=1630, lr=1.956e-05, gnorm=1.498, train_wall=6, gb_free=12.4, wall=9843
2024-07-20 12:32:43 | INFO | train_inner | epoch 086:     17 / 19 loss=5.012, nll_loss=1.273, ppl=2.42, wps=2074.1, ups=0.41, wpb=5119, bsz=228, num_updates=1632, lr=1.9584e-05, gnorm=1.495, train_wall=5, gb_free=11.7, wall=9848
2024-07-20 12:32:48 | INFO | train_inner | epoch 086:     19 / 19 loss=4.935, nll_loss=1.142, ppl=2.21, wps=1652, ups=0.43, wpb=3842, bsz=176, num_updates=1634, lr=1.9608e-05, gnorm=1.5, train_wall=5, gb_free=16.9, wall=9852
2024-07-20 12:32:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41060.4375Mb; avail=213969.671875Mb
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000563
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41060.4375Mb; avail=213969.671875Mb
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002081
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41060.9296875Mb; avail=213969.671875Mb
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001845
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004804
2024-07-20 12:32:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41060.4375Mb; avail=213969.1796875Mb
2024-07-20 12:32:51 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 6.019 | nll_loss 2.288 | ppl 4.88 | wps 3560.9 | wpb 1665.6 | bsz 74.4 | num_updates 1634 | best_loss 5.973
2024-07-20 12:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 1634 updates
2024-07-20 12:32:51 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:33:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:33:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 86 @ 1634 updates, score 6.019) (writing took 42.879521088209 seconds)
2024-07-20 12:33:34 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2024-07-20 12:33:34 | INFO | train | epoch 086 | loss 4.918 | nll_loss 1.124 | ppl 2.18 | wps 970.8 | ups 0.2 | wpb 4866.4 | bsz 219.1 | num_updates 1634 | lr 1.9608e-05 | gnorm 1.365 | train_wall 49 | gb_free 16.9 | wall 9898
2024-07-20 12:33:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:33:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:33:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:33:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000727
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32158.92578125Mb; avail=222871.1640625Mb
2024-07-20 12:33:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000080
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000744
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32158.92578125Mb; avail=222871.1640625Mb
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32158.92578125Mb; avail=222871.1640625Mb
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000217
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001265
2024-07-20 12:33:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32158.92578125Mb; avail=222871.1640625Mb
2024-07-20 12:33:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:33:34 | INFO | fairseq.trainer | begin training epoch 87
2024-07-20 12:33:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:33:39 | INFO | train_inner | epoch 087:      2 / 19 loss=4.834, nll_loss=0.985, ppl=1.98, wps=160.1, ups=0.04, wpb=4103.5, bsz=196, num_updates=1636, lr=1.9632e-05, gnorm=1.295, train_wall=5, gb_free=12.6, wall=9904
2024-07-20 12:33:45 | INFO | train_inner | epoch 087:      4 / 19 loss=4.912, nll_loss=1.129, ppl=2.19, wps=2041.2, ups=0.33, wpb=6163.5, bsz=288, num_updates=1638, lr=1.9656e-05, gnorm=1.161, train_wall=6, gb_free=10.5, wall=9910
2024-07-20 12:33:51 | INFO | train_inner | epoch 087:      6 / 19 loss=4.922, nll_loss=1.15, ppl=2.22, wps=1944, ups=0.34, wpb=5698, bsz=292, num_updates=1640, lr=1.968e-05, gnorm=1.231, train_wall=6, gb_free=12.9, wall=9915
2024-07-20 12:33:56 | INFO | train_inner | epoch 087:      8 / 19 loss=4.792, nll_loss=0.95, ppl=1.93, wps=1606.4, ups=0.42, wpb=3807.5, bsz=184, num_updates=1642, lr=1.9704e-05, gnorm=1.306, train_wall=5, gb_free=11.8, wall=9920
2024-07-20 12:34:02 | INFO | train_inner | epoch 087:     10 / 19 loss=4.92, nll_loss=1.106, ppl=2.15, wps=1799.7, ups=0.35, wpb=5191.5, bsz=204, num_updates=1644, lr=1.9728e-05, gnorm=1.302, train_wall=6, gb_free=12.2, wall=9926
2024-07-20 12:34:07 | INFO | train_inner | epoch 087:     12 / 19 loss=4.933, nll_loss=1.145, ppl=2.21, wps=1987.7, ups=0.41, wpb=4830.5, bsz=180, num_updates=1646, lr=1.9752e-05, gnorm=1.312, train_wall=5, gb_free=12.4, wall=9931
2024-07-20 12:34:12 | INFO | train_inner | epoch 087:     14 / 19 loss=4.895, nll_loss=1.099, ppl=2.14, wps=2127.5, ups=0.39, wpb=5465.5, bsz=232, num_updates=1648, lr=1.9776e-05, gnorm=1.213, train_wall=5, gb_free=12.5, wall=9936
2024-07-20 12:34:18 | INFO | train_inner | epoch 087:     16 / 19 loss=4.875, nll_loss=1.07, ppl=2.1, wps=1925, ups=0.33, wpb=5863, bsz=264, num_updates=1650, lr=1.98e-05, gnorm=1.194, train_wall=6, gb_free=12.8, wall=9942
2024-07-20 12:34:22 | INFO | train_inner | epoch 087:     18 / 19 loss=4.872, nll_loss=1.072, ppl=2.1, wps=1933.6, ups=0.45, wpb=4261, bsz=217.5, num_updates=1652, lr=1.9824e-05, gnorm=1.362, train_wall=4, gb_free=11.1, wall=9946
2024-07-20 12:34:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42617.15234375Mb; avail=212412.38671875Mb
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42617.15234375Mb; avail=212412.38671875Mb
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002079
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42617.64453125Mb; avail=212412.38671875Mb
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001860
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004836
2024-07-20 12:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42617.64453125Mb; avail=212412.38671875Mb
2024-07-20 12:34:26 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 5.993 | nll_loss 2.297 | ppl 4.92 | wps 4169.3 | wpb 1665.6 | bsz 74.4 | num_updates 1653 | best_loss 5.973
2024-07-20 12:34:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 1653 updates
2024-07-20 12:34:26 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:35:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:35:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 87 @ 1653 updates, score 5.993) (writing took 52.87705275812186 seconds)
2024-07-20 12:35:19 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2024-07-20 12:35:19 | INFO | train | epoch 087 | loss 4.89 | nll_loss 1.086 | ppl 2.12 | wps 880.2 | ups 0.18 | wpb 4866.4 | bsz 219.1 | num_updates 1653 | lr 1.9836e-05 | gnorm 1.331 | train_wall 49 | gb_free 17 | wall 10003
2024-07-20 12:35:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:35:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:35:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:35:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000628
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23300.37109375Mb; avail=231729.77734375Mb
2024-07-20 12:35:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000078
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000721
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23300.37109375Mb; avail=231729.77734375Mb
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23300.37109375Mb; avail=231729.77734375Mb
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000215
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001264
2024-07-20 12:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23300.37109375Mb; avail=231729.77734375Mb
2024-07-20 12:35:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:35:19 | INFO | fairseq.trainer | begin training epoch 88
2024-07-20 12:35:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:35:22 | INFO | train_inner | epoch 088:      1 / 19 loss=4.952, nll_loss=1.163, ppl=2.24, wps=120.2, ups=0.03, wpb=3563.5, bsz=132, num_updates=1654, lr=1.9848e-05, gnorm=1.966, train_wall=4, gb_free=11.7, wall=10006
2024-07-20 12:35:27 | INFO | train_inner | epoch 088:      3 / 19 loss=4.807, nll_loss=0.983, ppl=1.98, wps=1619.4, ups=0.34, wpb=4719.5, bsz=224, num_updates=1656, lr=1.9872e-05, gnorm=1.167, train_wall=6, gb_free=10.7, wall=10012
2024-07-20 12:35:32 | INFO | train_inner | epoch 088:      5 / 19 loss=4.912, nll_loss=1.109, ppl=2.16, wps=2180.6, ups=0.4, wpb=5446, bsz=260, num_updates=1658, lr=1.9896e-05, gnorm=1.281, train_wall=5, gb_free=11.7, wall=10017
2024-07-20 12:35:37 | INFO | train_inner | epoch 088:      7 / 19 loss=4.846, nll_loss=1.022, ppl=2.03, wps=1831.3, ups=0.41, wpb=4417.5, bsz=196, num_updates=1660, lr=1.992e-05, gnorm=1.258, train_wall=5, gb_free=12.6, wall=10021
2024-07-20 12:35:42 | INFO | train_inner | epoch 088:      9 / 19 loss=4.94, nll_loss=1.153, ppl=2.22, wps=1808.3, ups=0.4, wpb=4533, bsz=168, num_updates=1662, lr=1.9944e-05, gnorm=1.37, train_wall=5, gb_free=13.7, wall=10026
2024-07-20 12:35:47 | INFO | train_inner | epoch 088:     11 / 19 loss=4.896, nll_loss=1.104, ppl=2.15, wps=2088.2, ups=0.39, wpb=5359.5, bsz=256, num_updates=1664, lr=1.9968e-05, gnorm=1.236, train_wall=5, gb_free=11.4, wall=10032
2024-07-20 12:35:53 | INFO | train_inner | epoch 088:     13 / 19 loss=4.934, nll_loss=1.123, ppl=2.18, wps=1775.8, ups=0.35, wpb=5029.5, bsz=204, num_updates=1666, lr=1.9992e-05, gnorm=1.261, train_wall=6, gb_free=13.6, wall=10037
2024-07-20 12:35:59 | INFO | train_inner | epoch 088:     15 / 19 loss=4.934, nll_loss=1.15, ppl=2.22, wps=2197.5, ups=0.36, wpb=6045, bsz=300, num_updates=1668, lr=2.0016e-05, gnorm=1.193, train_wall=5, gb_free=11.8, wall=10043
2024-07-20 12:36:03 | INFO | train_inner | epoch 088:     17 / 19 loss=4.821, nll_loss=0.987, ppl=1.98, wps=2043, ups=0.48, wpb=4219, bsz=189.5, num_updates=1670, lr=2.004e-05, gnorm=1.368, train_wall=4, gb_free=12.9, wall=10047
2024-07-20 12:36:07 | INFO | train_inner | epoch 088:     19 / 19 loss=4.777, nll_loss=0.951, ppl=1.93, wps=1579.8, ups=0.42, wpb=3745.5, bsz=176, num_updates=1672, lr=2.0064e-05, gnorm=1.374, train_wall=5, gb_free=16.5, wall=10052
2024-07-20 12:36:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28945.05859375Mb; avail=226084.99609375Mb
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000548
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28945.05859375Mb; avail=226084.99609375Mb
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002082
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28945.05859375Mb; avail=226084.99609375Mb
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001832
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004781
2024-07-20 12:36:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28945.05859375Mb; avail=226084.99609375Mb
2024-07-20 12:36:10 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 5.998 | nll_loss 2.329 | ppl 5.02 | wps 4175.1 | wpb 1665.6 | bsz 74.4 | num_updates 1672 | best_loss 5.973
2024-07-20 12:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 1672 updates
2024-07-20 12:36:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:36:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:36:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 88 @ 1672 updates, score 5.998) (writing took 42.32810190599412 seconds)
2024-07-20 12:36:52 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2024-07-20 12:36:52 | INFO | train | epoch 088 | loss 4.885 | nll_loss 1.079 | ppl 2.11 | wps 990.9 | ups 0.2 | wpb 4866.4 | bsz 219.1 | num_updates 1672 | lr 2.0064e-05 | gnorm 1.285 | train_wall 48 | gb_free 16.5 | wall 10096
2024-07-20 12:36:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:36:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:36:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:36:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000761
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36838.375Mb; avail=218191.6875Mb
2024-07-20 12:36:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000089
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000739
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36838.375Mb; avail=218191.6875Mb
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36838.375Mb; avail=218191.6875Mb
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000217
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001255
2024-07-20 12:36:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36838.375Mb; avail=218191.6875Mb
2024-07-20 12:36:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:36:52 | INFO | fairseq.trainer | begin training epoch 89
2024-07-20 12:36:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:36:58 | INFO | train_inner | epoch 089:      2 / 19 loss=4.899, nll_loss=1.132, ppl=2.19, wps=214.8, ups=0.04, wpb=5435, bsz=232, num_updates=1674, lr=2.0088e-05, gnorm=1.269, train_wall=6, gb_free=12.3, wall=10102
2024-07-20 12:37:03 | INFO | train_inner | epoch 089:      4 / 19 loss=4.815, nll_loss=0.983, ppl=1.98, wps=1758.4, ups=0.39, wpb=4509, bsz=192, num_updates=1676, lr=2.0112e-05, gnorm=1.292, train_wall=5, gb_free=12.1, wall=10107
2024-07-20 12:37:09 | INFO | train_inner | epoch 089:      6 / 19 loss=4.825, nll_loss=0.982, ppl=1.98, wps=2169.7, ups=0.37, wpb=5838.5, bsz=288, num_updates=1678, lr=2.0136e-05, gnorm=1.071, train_wall=5, gb_free=11.6, wall=10113
2024-07-20 12:37:14 | INFO | train_inner | epoch 089:      8 / 19 loss=4.863, nll_loss=1.079, ppl=2.11, wps=1944.9, ups=0.38, wpb=5088.5, bsz=220, num_updates=1680, lr=2.016e-05, gnorm=1.35, train_wall=5, gb_free=13, wall=10118
2024-07-20 12:37:19 | INFO | train_inner | epoch 089:     10 / 19 loss=4.862, nll_loss=1.066, ppl=2.09, wps=1860.6, ups=0.36, wpb=5211.5, bsz=248, num_updates=1682, lr=2.0184e-05, gnorm=1.264, train_wall=6, gb_free=11.4, wall=10124
2024-07-20 12:37:25 | INFO | train_inner | epoch 089:     12 / 19 loss=4.897, nll_loss=1.077, ppl=2.11, wps=1933.8, ups=0.33, wpb=5784, bsz=244, num_updates=1684, lr=2.0208e-05, gnorm=1.343, train_wall=6, gb_free=11.4, wall=10130
2024-07-20 12:37:30 | INFO | train_inner | epoch 089:     14 / 19 loss=4.794, nll_loss=0.972, ppl=1.96, wps=1999.9, ups=0.39, wpb=5071, bsz=264, num_updates=1686, lr=2.0232e-05, gnorm=1.629, train_wall=5, gb_free=14, wall=10135
2024-07-20 12:37:35 | INFO | train_inner | epoch 089:     16 / 19 loss=4.844, nll_loss=1.032, ppl=2.04, wps=1996.9, ups=0.4, wpb=5006, bsz=216, num_updates=1688, lr=2.0256e-05, gnorm=1.233, train_wall=5, gb_free=11.6, wall=10140
2024-07-20 12:37:41 | INFO | train_inner | epoch 089:     18 / 19 loss=4.827, nll_loss=1.011, ppl=2.02, wps=1570.8, ups=0.38, wpb=4123.5, bsz=176, num_updates=1690, lr=2.028e-05, gnorm=1.829, train_wall=5, gb_free=12.6, wall=10145
2024-07-20 12:37:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=49199.9765625Mb; avail=205830.02734375Mb
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49199.9765625Mb; avail=205830.02734375Mb
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002099
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49199.9765625Mb; avail=205830.02734375Mb
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001844
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004842
2024-07-20 12:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=49199.9765625Mb; avail=205830.02734375Mb
2024-07-20 12:37:44 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 6.03 | nll_loss 2.314 | ppl 4.97 | wps 4171.1 | wpb 1665.6 | bsz 74.4 | num_updates 1691 | best_loss 5.973
2024-07-20 12:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 1691 updates
2024-07-20 12:37:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:38:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:38:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 89 @ 1691 updates, score 6.03) (writing took 49.71995036187582 seconds)
2024-07-20 12:38:33 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2024-07-20 12:38:33 | INFO | train | epoch 089 | loss 4.85 | nll_loss 1.039 | ppl 2.06 | wps 913.7 | ups 0.19 | wpb 4866.4 | bsz 219.1 | num_updates 1691 | lr 2.0292e-05 | gnorm 1.676 | train_wall 49 | gb_free 27.1 | wall 10198
2024-07-20 12:38:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:38:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:38:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:38:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000600
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23767.69921875Mb; avail=231278.47265625Mb
2024-07-20 12:38:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000094
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000706
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23767.69921875Mb; avail=231278.47265625Mb
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23767.69921875Mb; avail=231278.47265625Mb
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000204
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001207
2024-07-20 12:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23767.69921875Mb; avail=231278.47265625Mb
2024-07-20 12:38:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:38:34 | INFO | fairseq.trainer | begin training epoch 90
2024-07-20 12:38:34 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:38:36 | INFO | train_inner | epoch 090:      1 / 19 loss=4.888, nll_loss=1.081, ppl=2.12, wps=122.3, ups=0.04, wpb=3401, bsz=133.5, num_updates=1692, lr=2.0304e-05, gnorm=4.304, train_wall=3, gb_free=11.4, wall=10200
2024-07-20 12:38:41 | INFO | train_inner | epoch 090:      3 / 19 loss=4.846, nll_loss=1.014, ppl=2.02, wps=2297.4, ups=0.39, wpb=5828.5, bsz=280, num_updates=1694, lr=2.0328e-05, gnorm=1.129, train_wall=5, gb_free=11.7, wall=10206
2024-07-20 12:38:47 | INFO | train_inner | epoch 090:      5 / 19 loss=4.805, nll_loss=0.986, ppl=1.98, wps=1602.1, ups=0.35, wpb=4522, bsz=204, num_updates=1696, lr=2.0352e-05, gnorm=1.226, train_wall=6, gb_free=12.6, wall=10211
2024-07-20 12:38:53 | INFO | train_inner | epoch 090:      7 / 19 loss=4.897, nll_loss=1.132, ppl=2.19, wps=1897.9, ups=0.32, wpb=5892.5, bsz=296, num_updates=1698, lr=2.0376e-05, gnorm=1.21, train_wall=6, gb_free=11.4, wall=10217
2024-07-20 12:38:58 | INFO | train_inner | epoch 090:      9 / 19 loss=4.828, nll_loss=1.009, ppl=2.01, wps=1685.2, ups=0.39, wpb=4316, bsz=169.5, num_updates=1700, lr=2.04e-05, gnorm=1.584, train_wall=5, gb_free=11.5, wall=10222
2024-07-20 12:39:03 | INFO | train_inner | epoch 090:     11 / 19 loss=4.817, nll_loss=0.978, ppl=1.97, wps=1958.2, ups=0.45, wpb=4349, bsz=208, num_updates=1702, lr=2.0424e-05, gnorm=1.597, train_wall=4, gb_free=11.2, wall=10227
2024-07-20 12:39:09 | INFO | train_inner | epoch 090:     13 / 19 loss=4.862, nll_loss=1.057, ppl=2.08, wps=1908.2, ups=0.34, wpb=5575.5, bsz=232, num_updates=1704, lr=2.0448e-05, gnorm=1.29, train_wall=6, gb_free=12.6, wall=10233
2024-07-20 12:39:14 | INFO | train_inner | epoch 090:     15 / 19 loss=4.903, nll_loss=1.096, ppl=2.14, wps=1684.9, ups=0.36, wpb=4622, bsz=196, num_updates=1706, lr=2.0472e-05, gnorm=1.451, train_wall=5, gb_free=13.4, wall=10238
2024-07-20 12:39:19 | INFO | train_inner | epoch 090:     17 / 19 loss=4.879, nll_loss=1.094, ppl=2.13, wps=1768.1, ups=0.38, wpb=4595.5, bsz=232, num_updates=1708, lr=2.0496e-05, gnorm=1.287, train_wall=5, gb_free=11.8, wall=10243
2024-07-20 12:39:23 | INFO | train_inner | epoch 090:     19 / 19 loss=4.862, nll_loss=1.041, ppl=2.06, wps=1853.1, ups=0.56, wpb=3293, bsz=132, num_updates=1710, lr=2.052e-05, gnorm=2.315, train_wall=4, gb_free=18.9, wall=10247
2024-07-20 12:39:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23819.76171875Mb; avail=231226.4140625Mb
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000566
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23820.25390625Mb; avail=231225.921875Mb
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002096
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23820.25390625Mb; avail=231225.921875Mb
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001848
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004819
2024-07-20 12:39:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23820.25390625Mb; avail=231225.921875Mb
2024-07-20 12:39:25 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 6.025 | nll_loss 2.309 | ppl 4.96 | wps 4179.3 | wpb 1665.6 | bsz 74.4 | num_updates 1710 | best_loss 5.973
2024-07-20 12:39:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 1710 updates
2024-07-20 12:39:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:40:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:40:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 90 @ 1710 updates, score 6.025) (writing took 38.98993069585413 seconds)
2024-07-20 12:40:04 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2024-07-20 12:40:04 | INFO | train | epoch 090 | loss 4.858 | nll_loss 1.05 | ppl 2.07 | wps 1016.5 | ups 0.21 | wpb 4866.4 | bsz 219.1 | num_updates 1710 | lr 2.052e-05 | gnorm 1.447 | train_wall 49 | gb_free 18.9 | wall 10289
2024-07-20 12:40:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:40:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:40:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:40:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000726
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32233.4296875Mb; avail=222812.74609375Mb
2024-07-20 12:40:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000076
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000728
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32233.42578125Mb; avail=222812.74609375Mb
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32233.42578125Mb; avail=222812.74609375Mb
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001250
2024-07-20 12:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32233.42578125Mb; avail=222812.74609375Mb
2024-07-20 12:40:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:40:05 | INFO | fairseq.trainer | begin training epoch 91
2024-07-20 12:40:05 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:40:10 | INFO | train_inner | epoch 091:      2 / 19 loss=4.87, nll_loss=1.06, ppl=2.08, wps=247.3, ups=0.04, wpb=5815, bsz=272, num_updates=1712, lr=2.0544e-05, gnorm=1.104, train_wall=5, gb_free=12.3, wall=10294
2024-07-20 12:40:16 | INFO | train_inner | epoch 091:      4 / 19 loss=4.804, nll_loss=0.973, ppl=1.96, wps=1920.5, ups=0.35, wpb=5472.5, bsz=248, num_updates=1714, lr=2.0568e-05, gnorm=1.127, train_wall=6, gb_free=13.1, wall=10300
2024-07-20 12:40:21 | INFO | train_inner | epoch 091:      6 / 19 loss=4.821, nll_loss=1.026, ppl=2.04, wps=1789.5, ups=0.34, wpb=5244.5, bsz=256, num_updates=1716, lr=2.0592e-05, gnorm=1.175, train_wall=6, gb_free=13, wall=10306
2024-07-20 12:40:27 | INFO | train_inner | epoch 091:      8 / 19 loss=4.814, nll_loss=0.965, ppl=1.95, wps=1522.3, ups=0.38, wpb=4048, bsz=184, num_updates=1718, lr=2.0616e-05, gnorm=1.365, train_wall=5, gb_free=16.9, wall=10311
2024-07-20 12:40:32 | INFO | train_inner | epoch 091:     10 / 19 loss=4.869, nll_loss=1.065, ppl=2.09, wps=2228.6, ups=0.39, wpb=5683.5, bsz=268, num_updates=1720, lr=2.064e-05, gnorm=1.245, train_wall=5, gb_free=11, wall=10316
2024-07-20 12:40:37 | INFO | train_inner | epoch 091:     12 / 19 loss=4.762, nll_loss=0.915, ppl=1.89, wps=1957.7, ups=0.4, wpb=4889, bsz=220, num_updates=1722, lr=2.0664e-05, gnorm=1.176, train_wall=5, gb_free=13, wall=10321
2024-07-20 12:40:52 | INFO | train_inner | epoch 091:     14 / 19 loss=4.86, nll_loss=1.058, ppl=2.08, wps=696, ups=0.13, wpb=5200, bsz=212, num_updates=1724, lr=2.0688e-05, gnorm=1.4, train_wall=15, gb_free=12.4, wall=10336
2024-07-20 12:40:56 | INFO | train_inner | epoch 091:     16 / 19 loss=4.773, nll_loss=0.94, ppl=1.92, wps=1884.5, ups=0.48, wpb=3962.5, bsz=185.5, num_updates=1726, lr=2.0712e-05, gnorm=1.254, train_wall=4, gb_free=17.4, wall=10340
2024-07-20 12:41:02 | INFO | train_inner | epoch 091:     18 / 19 loss=4.826, nll_loss=1.003, ppl=2, wps=1520.8, ups=0.35, wpb=4368, bsz=164, num_updates=1728, lr=2.0736e-05, gnorm=1.352, train_wall=6, gb_free=12.8, wall=10346
2024-07-20 12:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14428.3359375Mb; avail=240617.8828125Mb
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000489
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14428.3359375Mb; avail=240617.8828125Mb
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002079
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14428.3359375Mb; avail=240617.8828125Mb
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001828
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004709
2024-07-20 12:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14428.33203125Mb; avail=240617.8828125Mb
2024-07-20 12:41:06 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 6.031 | nll_loss 2.316 | ppl 4.98 | wps 3205.1 | wpb 1665.6 | bsz 74.4 | num_updates 1729 | best_loss 5.973
2024-07-20 12:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 1729 updates
2024-07-20 12:41:06 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:41:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:41:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 91 @ 1729 updates, score 6.031) (writing took 39.00616464414634 seconds)
2024-07-20 12:41:45 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2024-07-20 12:41:45 | INFO | train | epoch 091 | loss 4.828 | nll_loss 1.012 | ppl 2.02 | wps 916.9 | ups 0.19 | wpb 4866.4 | bsz 219.1 | num_updates 1729 | lr 2.0748e-05 | gnorm 1.281 | train_wall 58 | gb_free 17 | wall 10389
2024-07-20 12:41:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:hi-ne': 4163}; raw total size: 4163
2024-07-20 12:41:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:hi-ne': 4163}; resampled total size: 4163
2024-07-20 12:41:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:hi-ne': 1.0}
2024-07-20 12:41:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000698
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16786.5078125Mb; avail=238259.7109375Mb
2024-07-20 12:41:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000093
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000763
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16786.5078125Mb; avail=238259.7109375Mb
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000041
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16786.5078125Mb; avail=238259.7109375Mb
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000222
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001354
2024-07-20 12:41:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16786.5078125Mb; avail=238259.7109375Mb
2024-07-20 12:41:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 19
2024-07-20 12:41:45 | INFO | fairseq.trainer | begin training epoch 92
2024-07-20 12:41:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-20 12:41:48 | INFO | train_inner | epoch 092:      1 / 19 loss=4.845, nll_loss=1.052, ppl=2.07, wps=181.6, ups=0.04, wpb=4187.5, bsz=228, num_updates=1730, lr=2.076e-05, gnorm=1.57, train_wall=4, gb_free=13.3, wall=10392
2024-07-20 12:41:54 | INFO | train_inner | epoch 092:      3 / 19 loss=4.76, nll_loss=0.898, ppl=1.86, wps=1669.5, ups=0.35, wpb=4832.5, bsz=216, num_updates=1732, lr=2.0784e-05, gnorm=1.135, train_wall=6, gb_free=12, wall=10398
2024-07-20 12:41:59 | INFO | train_inner | epoch 092:      5 / 19 loss=4.807, nll_loss=0.971, ppl=1.96, wps=1936.3, ups=0.34, wpb=5632.5, bsz=228, num_updates=1734, lr=2.0808e-05, gnorm=1.133, train_wall=6, gb_free=12.2, wall=10404
2024-07-20 12:42:05 | INFO | train_inner | epoch 092:      7 / 19 loss=4.81, nll_loss=1.006, ppl=2.01, wps=1881, ups=0.39, wpb=4869, bsz=216, num_updates=1736, lr=2.0832e-05, gnorm=1.26, train_wall=5, gb_free=10.8, wall=10409
2024-07-20 12:42:08 | INFO | train_inner | epoch 092:      9 / 19 loss=4.817, nll_loss=0.988, ppl=1.98, wps=1972.7, ups=0.56, wpb=3526.5, bsz=125.5, num_updates=1738, lr=2.0856e-05, gnorm=1.421, train_wall=4, gb_free=17.2, wall=10412
2024-07-20 12:42:14 | INFO | train_inner | epoch 092:     11 / 19 loss=4.751, nll_loss=0.906, ppl=1.87, wps=1757.8, ups=0.33, wpb=5272.5, bsz=288, num_updates=1740, lr=2.088e-05, gnorm=1.083, train_wall=6, gb_free=11.2, wall=10418
2024-07-20 12:42:21 | INFO | train_inner | epoch 092:     13 / 19 loss=4.768, nll_loss=0.924, ppl=1.9, wps=1529, ups=0.29, wpb=5321.5, bsz=224, num_updates=1742, lr=2.0904e-05, gnorm=1.084, train_wall=7, gb_free=13.2, wall=10425
2024-07-20 12:42:27 | INFO | train_inner | epoch 092:     15 / 19 loss=4.832, nll_loss=1.006, ppl=2.01, wps=1715.9, ups=0.37, wpb=4675.5, bsz=168, num_updates=1744, lr=2.0928e-05, gnorm=1.279, train_wall=5, gb_free=11.3, wall=10431
2024-07-20 12:42:33 | INFO | train_inner | epoch 092:     17 / 19 loss=4.835, nll_loss=1.059, ppl=2.08, wps=1867, ups=0.32, wpb=5778, bsz=308, num_updates=1746, lr=2.0952e-05, gnorm=1.196, train_wall=6, gb_free=11.6, wall=10437
2024-07-20 12:42:37 | INFO | train_inner | epoch 092:     19 / 19 loss=4.78, nll_loss=0.943, ppl=1.92, wps=1827.1, ups=0.5, wpb=3683.5, bsz=152, num_updates=1748, lr=2.0976e-05, gnorm=1.615, train_wall=4, gb_free=16.3, wall=10441
2024-07-20 12:42:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16825.03125Mb; avail=238221.14453125Mb
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000576
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16825.03125Mb; avail=238221.14453125Mb
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002105
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16825.03125Mb; avail=238221.14453125Mb
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001854
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004853
2024-07-20 12:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16825.03125Mb; avail=238221.14453125Mb
2024-07-20 12:42:39 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 6.027 | nll_loss 2.314 | ppl 4.97 | wps 4167.1 | wpb 1665.6 | bsz 74.4 | num_updates 1748 | best_loss 5.973
2024-07-20 12:42:39 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2024-07-20 12:42:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 1748 updates
2024-07-20 12:42:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:43:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt
2024-07-20 12:43:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B/checkpoint_last.pt (epoch 92 @ 1748 updates, score 6.027) (writing took 38.53597626206465 seconds)
2024-07-20 12:43:18 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2024-07-20 12:43:18 | INFO | train | epoch 092 | loss 4.796 | nll_loss 0.968 | ppl 1.96 | wps 998 | ups 0.21 | wpb 4866.4 | bsz 219.1 | num_updates 1748 | lr 2.0976e-05 | gnorm 1.242 | train_wall 51 | gb_free 16.3 | wall 10482
2024-07-20 12:43:18 | INFO | fairseq_cli.train | done training in 10474.4 seconds
2024-07-29 10:00:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='ne-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='ne-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-07-29 10:00:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'ne']
2024-07-29 10:00:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ne] dictionary: 128112 types
2024-07-29 10:00:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-07-29 10:00:37 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-07-29 10:00:37 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-07-29 10:00:37 | INFO | fairseq_cli.train | model: TransformerModel
2024-07-29 10:00:37 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-07-29 10:00:37 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-07-29 10:00:37 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-07-29 10:00:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-07-29 10:00:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29901.234375Mb; avail=225180.96875Mb
2024-07-29 10:00:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-07-29 10:00:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:ne-hi': 1}
2024-07-29 10:00:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ne-hi src_langtok: 128066; tgt_langtok: 128036
2024-07-29 10:00:37 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.ne-hi.ne
2024-07-29 10:00:37 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.ne-hi.hi
2024-07-29 10:00:37 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin valid ne-hi 521 examples
2024-07-29 10:01:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='ne-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='ne-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-07-29 10:01:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'ne']
2024-07-29 10:01:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ne] dictionary: 128112 types
2024-07-29 10:01:54 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-07-29 10:02:02 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-07-29 10:02:02 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-07-29 10:02:02 | INFO | fairseq_cli.train | model: TransformerModel
2024-07-29 10:02:02 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-07-29 10:02:02 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-07-29 10:02:02 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-07-29 10:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-07-29 10:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27226.0390625Mb; avail=227856.12109375Mb
2024-07-29 10:02:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-07-29 10:02:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:ne-hi': 1}
2024-07-29 10:02:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ne-hi src_langtok: 128066; tgt_langtok: 128036
2024-07-29 10:02:02 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.ne-hi.ne
2024-07-29 10:02:02 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.ne-hi.hi
2024-07-29 10:02:02 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin valid ne-hi 521 examples
2024-07-29 10:02:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-07-29 10:02:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-07-29 10:02:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-07-29 10:02:03 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-07-29 10:02:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-07-29 10:02:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-07-29 10:02:03 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-07-29 10:02:03 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/krish/content/1.2B_last_checkpoint.pt: optimizer, lr scheduler, meters, dataloader will be reset
2024-07-29 10:02:03 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/1.2B_last_checkpoint.pt
2024-07-29 10:02:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-07-29 10:02:10 | INFO | fairseq.trainer | Loaded checkpoint /home/krish/content/1.2B_last_checkpoint.pt (epoch 81 @ 0 updates)
2024-07-29 10:02:10 | INFO | fairseq.trainer | loading train data for epoch 1
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15930.58984375Mb; avail=239143.66015625Mb
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:ne-hi': 1}
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ne-hi src_langtok: 128066; tgt_langtok: 128036
2024-07-29 10:02:10 | INFO | fairseq.data.data_utils | loaded 4,163 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/train.ne-hi.ne
2024-07-29 10:02:10 | INFO | fairseq.data.data_utils | loaded 4,163 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/train.ne-hi.hi
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin train ne-hi 4163 examples
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:ne-hi', 4163)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:ne-hi', 1.0)]
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000489
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=15930.58984375Mb; avail=239143.66015625Mb
2024-07-29 10:02:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000054
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000648
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15930.58984375Mb; avail=239143.66015625Mb
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15930.58984375Mb; avail=239143.66015625Mb
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000604
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001569
2024-07-29 10:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=15930.58984375Mb; avail=239143.66015625Mb
2024-07-29 10:02:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:02:10 | INFO | fairseq.trainer | begin training epoch 1
2024-07-29 10:02:10 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:02:16 | INFO | train_inner | epoch 001:      2 / 15 loss=8.826, nll_loss=5.974, ppl=62.86, wps=1773.4, ups=0.35, wpb=4646.5, bsz=284, num_updates=2, lr=2.4e-08, gnorm=6.205, train_wall=6, gb_free=11.4, wall=13
2024-07-29 10:02:22 | INFO | train_inner | epoch 001:      4 / 15 loss=8.741, nll_loss=5.862, ppl=58.17, wps=1520.6, ups=0.35, wpb=4399.5, bsz=248, num_updates=4, lr=4.8e-08, gnorm=6.209, train_wall=6, gb_free=9.9, wall=19
2024-07-29 10:02:28 | INFO | train_inner | epoch 001:      6 / 15 loss=8.614, nll_loss=5.699, ppl=51.96, wps=1559.1, ups=0.35, wpb=4462.5, bsz=312, num_updates=6, lr=7.2e-08, gnorm=5.922, train_wall=6, gb_free=8.9, wall=25
2024-07-29 10:02:33 | INFO | train_inner | epoch 001:      8 / 15 loss=8.561, nll_loss=5.632, ppl=49.59, wps=1531.2, ups=0.42, wpb=3631.5, bsz=261.5, num_updates=8, lr=9.6e-08, gnorm=5.974, train_wall=5, gb_free=9.5, wall=29
2024-07-29 10:02:38 | INFO | train_inner | epoch 001:     10 / 15 loss=8.766, nll_loss=5.892, ppl=59.37, wps=1467.9, ups=0.35, wpb=4195, bsz=296, num_updates=10, lr=1.2e-07, gnorm=6.281, train_wall=6, gb_free=10, wall=35
2024-07-29 10:02:44 | INFO | train_inner | epoch 001:     12 / 15 loss=8.726, nll_loss=5.849, ppl=57.62, wps=1463.3, ups=0.33, wpb=4447.5, bsz=256, num_updates=12, lr=1.44e-07, gnorm=6.444, train_wall=6, gb_free=9.9, wall=41
2024-07-29 10:02:50 | INFO | train_inner | epoch 001:     14 / 15 loss=8.679, nll_loss=5.791, ppl=55.38, wps=1481.6, ups=0.36, wpb=4144, bsz=276, num_updates=14, lr=1.68e-07, gnorm=5.903, train_wall=6, gb_free=12.4, wall=47
2024-07-29 10:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=31230.84375Mb; avail=223835.3984375Mb
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000483
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31230.3515625Mb; avail=223835.890625Mb
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002241
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31230.84375Mb; avail=223835.3984375Mb
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001844
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004920
2024-07-29 10:02:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=31230.84375Mb; avail=223835.3984375Mb
2024-07-29 10:02:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.09 | nll_loss 4.863 | ppl 29.09 | wps 3262.7 | wpb 1361.3 | bsz 86.8 | num_updates 15
2024-07-29 10:02:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 15 updates
2024-07-29 10:02:55 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:03:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:03:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 1 @ 15 updates, score 8.09) (writing took 49.18131017591804 seconds)
2024-07-29 10:03:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-07-29 10:03:44 | INFO | train | epoch 001 | loss 8.71 | nll_loss 5.825 | ppl 56.69 | wps 668.2 | ups 0.15 | wpb 4315.9 | bsz 277.5 | num_updates 15 | lr 1.8e-07 | gnorm 6.11 | train_wall 42 | gb_free 12.4 | wall 101
2024-07-29 10:03:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:03:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:03:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:03:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000725
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19713.17578125Mb; avail=235353.09765625Mb
2024-07-29 10:03:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000710
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19711.20703125Mb; avail=235354.9921875Mb
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19711.20703125Mb; avail=235354.9921875Mb
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000214
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001232
2024-07-29 10:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19711.20703125Mb; avail=235354.9921875Mb
2024-07-29 10:03:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:03:44 | INFO | fairseq.trainer | begin training epoch 2
2024-07-29 10:03:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:03:47 | INFO | train_inner | epoch 002:      1 / 15 loss=8.723, nll_loss=5.84, ppl=57.29, wps=156.6, ups=0.04, wpb=4452.5, bsz=292, num_updates=16, lr=1.92e-07, gnorm=5.609, train_wall=5, gb_free=10.7, wall=104
2024-07-29 10:03:53 | INFO | train_inner | epoch 002:      3 / 15 loss=8.771, nll_loss=5.903, ppl=59.83, wps=1509.9, ups=0.34, wpb=4478.5, bsz=268, num_updates=18, lr=2.16e-07, gnorm=5.689, train_wall=6, gb_free=11.7, wall=110
2024-07-29 10:03:58 | INFO | train_inner | epoch 002:      5 / 15 loss=8.663, nll_loss=5.774, ppl=54.74, wps=1641, ups=0.35, wpb=4649.5, bsz=272, num_updates=20, lr=2.4e-07, gnorm=5.619, train_wall=6, gb_free=10.1, wall=115
2024-07-29 10:04:05 | INFO | train_inner | epoch 002:      7 / 15 loss=8.542, nll_loss=5.614, ppl=48.98, wps=1520.9, ups=0.33, wpb=4660.5, bsz=328, num_updates=22, lr=2.64e-07, gnorm=5.646, train_wall=6, gb_free=9.1, wall=121
2024-07-29 10:04:09 | INFO | train_inner | epoch 002:      9 / 15 loss=8.474, nll_loss=5.529, ppl=46.18, wps=1626.1, ups=0.42, wpb=3912, bsz=245.5, num_updates=24, lr=2.88e-07, gnorm=5.602, train_wall=5, gb_free=14.2, wall=126
2024-07-29 10:04:15 | INFO | train_inner | epoch 002:     11 / 15 loss=8.543, nll_loss=5.622, ppl=49.26, wps=1411.9, ups=0.35, wpb=4044, bsz=316, num_updates=26, lr=3.12e-07, gnorm=5.537, train_wall=6, gb_free=10.8, wall=132
2024-07-29 10:04:21 | INFO | train_inner | epoch 002:     13 / 15 loss=8.627, nll_loss=5.734, ppl=53.23, wps=1564.6, ups=0.34, wpb=4620, bsz=280, num_updates=28, lr=3.36e-07, gnorm=5.422, train_wall=6, gb_free=11.7, wall=138
2024-07-29 10:04:27 | INFO | train_inner | epoch 002:     15 / 15 loss=8.531, nll_loss=5.606, ppl=48.71, wps=1365.2, ups=0.34, wpb=3995.5, bsz=228, num_updates=30, lr=3.6e-07, gnorm=5.279, train_wall=6, gb_free=12, wall=144
2024-07-29 10:04:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36397.34765625Mb; avail=218636.44921875Mb
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000585
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36397.83984375Mb; avail=218635.42578125Mb
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002103
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36401.28515625Mb; avail=218631.80859375Mb
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001880
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004922
2024-07-29 10:04:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36406.20703125Mb; avail=218628.21484375Mb
2024-07-29 10:04:29 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.98 | nll_loss 4.733 | ppl 26.59 | wps 3207 | wpb 1361.3 | bsz 86.8 | num_updates 30 | best_loss 7.98
2024-07-29 10:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 30 updates
2024-07-29 10:04:29 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:05:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:05:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 2 @ 30 updates, score 7.98) (writing took 63.081110302358866 seconds)
2024-07-29 10:05:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2024-07-29 10:05:32 | INFO | train | epoch 002 | loss 8.602 | nll_loss 5.694 | ppl 51.77 | wps 598.7 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 30 | lr 3.6e-07 | gnorm 5.535 | train_wall 43 | gb_free 12 | wall 209
2024-07-29 10:05:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:05:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:05:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:05:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000791
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28961.6796875Mb; avail=226104.453125Mb
2024-07-29 10:05:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000079
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000745
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28961.6796875Mb; avail=226104.20703125Mb
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28961.921875Mb; avail=226104.20703125Mb
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000223
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001302
2024-07-29 10:05:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28961.921875Mb; avail=226104.20703125Mb
2024-07-29 10:05:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:05:32 | INFO | fairseq.trainer | begin training epoch 3
2024-07-29 10:05:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:05:37 | INFO | train_inner | epoch 003:      2 / 15 loss=8.56, nll_loss=5.644, ppl=50.01, wps=88, ups=0.03, wpb=3080, bsz=193.5, num_updates=32, lr=3.84e-07, gnorm=5.608, train_wall=5, gb_free=16.8, wall=214
2024-07-29 10:05:43 | INFO | train_inner | epoch 003:      4 / 15 loss=8.509, nll_loss=5.588, ppl=48.11, wps=1549.1, ups=0.34, wpb=4542.5, bsz=236, num_updates=34, lr=4.08e-07, gnorm=5.153, train_wall=6, gb_free=11.8, wall=220
2024-07-29 10:05:49 | INFO | train_inner | epoch 003:      6 / 15 loss=8.471, nll_loss=5.535, ppl=46.36, wps=1528.1, ups=0.33, wpb=4586.5, bsz=320, num_updates=36, lr=4.32e-07, gnorm=4.818, train_wall=6, gb_free=11.5, wall=226
2024-07-29 10:05:55 | INFO | train_inner | epoch 003:      8 / 15 loss=8.573, nll_loss=5.667, ppl=50.79, wps=1460.7, ups=0.33, wpb=4364.5, bsz=244, num_updates=38, lr=4.56e-07, gnorm=4.712, train_wall=6, gb_free=9.6, wall=232
2024-07-29 10:06:11 | INFO | train_inner | epoch 003:     10 / 15 loss=8.399, nll_loss=5.451, ppl=43.74, wps=530.1, ups=0.13, wpb=4187, bsz=284, num_updates=40, lr=4.8e-07, gnorm=4.423, train_wall=16, gb_free=10.7, wall=247
2024-07-29 10:06:16 | INFO | train_inner | epoch 003:     12 / 15 loss=8.286, nll_loss=5.304, ppl=39.51, wps=1734.7, ups=0.35, wpb=4943.5, bsz=352, num_updates=42, lr=5.04e-07, gnorm=4.291, train_wall=6, gb_free=12.1, wall=253
2024-07-29 10:06:22 | INFO | train_inner | epoch 003:     14 / 15 loss=8.332, nll_loss=5.367, ppl=41.26, wps=1502.5, ups=0.32, wpb=4652.5, bsz=284, num_updates=44, lr=5.28e-07, gnorm=4.462, train_wall=6, gb_free=8, wall=259
2024-07-29 10:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25960.8359375Mb; avail=229104.84765625Mb
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000499
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25961.8203125Mb; avail=229103.86328125Mb
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002036
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25966.7421875Mb; avail=229098.94140625Mb
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001815
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004679
2024-07-29 10:06:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25971.6640625Mb; avail=229094.51171875Mb
2024-07-29 10:06:27 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.846 | nll_loss 4.578 | ppl 23.88 | wps 3202.7 | wpb 1361.3 | bsz 86.8 | num_updates 45 | best_loss 7.846
2024-07-29 10:06:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 45 updates
2024-07-29 10:06:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:07:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:07:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 3 @ 45 updates, score 7.846) (writing took 68.46862176898867 seconds)
2024-07-29 10:07:36 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2024-07-29 10:07:36 | INFO | train | epoch 003 | loss 8.426 | nll_loss 5.482 | ppl 44.69 | wps 522.9 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 45 | lr 5.4e-07 | gnorm 4.762 | train_wall 53 | gb_free 9.7 | wall 333
2024-07-29 10:07:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:07:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:07:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:07:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000749
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23060.24609375Mb; avail=232005.8828125Mb
2024-07-29 10:07:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000090
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000756
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23060.24609375Mb; avail=232005.8828125Mb
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000048
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23060.24609375Mb; avail=232005.8828125Mb
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000221
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001342
2024-07-29 10:07:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23060.24609375Mb; avail=232005.8828125Mb
2024-07-29 10:07:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:07:36 | INFO | fairseq.trainer | begin training epoch 4
2024-07-29 10:07:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:07:39 | INFO | train_inner | epoch 004:      1 / 15 loss=8.276, nll_loss=5.297, ppl=39.32, wps=117.5, ups=0.03, wpb=4491, bsz=332, num_updates=46, lr=5.52e-07, gnorm=4.326, train_wall=6, gb_free=11.9, wall=336
2024-07-29 10:07:44 | INFO | train_inner | epoch 004:      3 / 15 loss=8.316, nll_loss=5.35, ppl=40.79, wps=1538.6, ups=0.36, wpb=4222.5, bsz=260, num_updates=48, lr=5.76e-07, gnorm=3.969, train_wall=5, gb_free=11.4, wall=341
2024-07-29 10:07:49 | INFO | train_inner | epoch 004:      5 / 15 loss=8.197, nll_loss=5.204, ppl=36.86, wps=1597.3, ups=0.42, wpb=3771.5, bsz=225.5, num_updates=50, lr=6e-07, gnorm=4.15, train_wall=5, gb_free=11.4, wall=346
2024-07-29 10:07:55 | INFO | train_inner | epoch 004:      7 / 15 loss=8.432, nll_loss=5.508, ppl=45.52, wps=1458.4, ups=0.33, wpb=4414.5, bsz=252, num_updates=52, lr=6.24e-07, gnorm=3.915, train_wall=6, gb_free=8.6, wall=352
2024-07-29 10:08:01 | INFO | train_inner | epoch 004:      9 / 15 loss=8.315, nll_loss=5.357, ppl=40.98, wps=1445.2, ups=0.33, wpb=4330.5, bsz=328, num_updates=54, lr=6.48e-07, gnorm=3.753, train_wall=6, gb_free=9.5, wall=358
2024-07-29 10:08:07 | INFO | train_inner | epoch 004:     11 / 15 loss=8.321, nll_loss=5.368, ppl=41.28, wps=1408.3, ups=0.34, wpb=4137.5, bsz=260, num_updates=56, lr=6.72e-07, gnorm=3.633, train_wall=6, gb_free=8.5, wall=364
2024-07-29 10:08:13 | INFO | train_inner | epoch 004:     13 / 15 loss=8.174, nll_loss=5.181, ppl=36.28, wps=1534.5, ups=0.33, wpb=4648.5, bsz=284, num_updates=58, lr=6.96e-07, gnorm=3.592, train_wall=6, gb_free=10.3, wall=370
2024-07-29 10:08:19 | INFO | train_inner | epoch 004:     15 / 15 loss=8.146, nll_loss=5.148, ppl=35.46, wps=1477.6, ups=0.34, wpb=4366.5, bsz=308, num_updates=60, lr=7.2e-07, gnorm=3.428, train_wall=6, gb_free=11.4, wall=376
2024-07-29 10:08:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40245.484375Mb; avail=214820.60546875Mb
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40244.9921875Mb; avail=214821.09765625Mb
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002063
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40244.9921875Mb; avail=214820.60546875Mb
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001845
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004836
2024-07-29 10:08:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40245.9765625Mb; avail=214820.11328125Mb
2024-07-29 10:08:21 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.724 | nll_loss 4.437 | ppl 21.67 | wps 3193.9 | wpb 1361.3 | bsz 86.8 | num_updates 60 | best_loss 7.724
2024-07-29 10:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 60 updates
2024-07-29 10:08:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:09:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 4 @ 60 updates, score 7.724) (writing took 68.85316455876455 seconds)
2024-07-29 10:09:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2024-07-29 10:09:30 | INFO | train | epoch 004 | loss 8.275 | nll_loss 5.306 | ppl 39.56 | wps 567.8 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 60 | lr 7.2e-07 | gnorm 3.802 | train_wall 43 | gb_free 11.4 | wall 447
2024-07-29 10:09:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:09:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:09:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:09:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000811
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34349.45703125Mb; avail=220124.55078125Mb
2024-07-29 10:09:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000093
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000741
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34349.45703125Mb; avail=220124.55078125Mb
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34349.45703125Mb; avail=220124.55078125Mb
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000220
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001288
2024-07-29 10:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34349.45703125Mb; avail=220124.55078125Mb
2024-07-29 10:09:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:09:30 | INFO | fairseq.trainer | begin training epoch 5
2024-07-29 10:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:09:36 | INFO | train_inner | epoch 005:      2 / 15 loss=8.249, nll_loss=5.279, ppl=38.82, wps=115.1, ups=0.03, wpb=4437.5, bsz=252, num_updates=62, lr=7.44e-07, gnorm=3.267, train_wall=6, gb_free=8.8, wall=453
2024-07-29 10:09:42 | INFO | train_inner | epoch 005:      4 / 15 loss=8.107, nll_loss=5.102, ppl=34.33, wps=1530.2, ups=0.33, wpb=4571.5, bsz=292, num_updates=64, lr=7.68e-07, gnorm=3.178, train_wall=6, gb_free=9.6, wall=459
2024-07-29 10:09:48 | INFO | train_inner | epoch 005:      6 / 15 loss=8.181, nll_loss=5.203, ppl=36.84, wps=1562.7, ups=0.34, wpb=4662, bsz=304, num_updates=66, lr=7.92e-07, gnorm=3.1, train_wall=6, gb_free=9.5, wall=465
2024-07-29 10:09:54 | INFO | train_inner | epoch 005:      8 / 15 loss=8.15, nll_loss=5.163, ppl=35.83, wps=1560.1, ups=0.34, wpb=4570, bsz=252, num_updates=68, lr=8.16e-07, gnorm=3.13, train_wall=6, gb_free=11, wall=471
2024-07-29 10:10:09 | INFO | train_inner | epoch 005:     10 / 15 loss=8.115, nll_loss=5.11, ppl=34.55, wps=537.2, ups=0.13, wpb=4194, bsz=288, num_updates=70, lr=8.4e-07, gnorm=3.08, train_wall=16, gb_free=12.1, wall=486
2024-07-29 10:10:16 | INFO | train_inner | epoch 005:     12 / 15 loss=8.061, nll_loss=5.047, ppl=33.05, wps=1544, ups=0.33, wpb=4734, bsz=316, num_updates=72, lr=8.64e-07, gnorm=2.736, train_wall=6, gb_free=11.7, wall=492
2024-07-29 10:10:21 | INFO | train_inner | epoch 005:     14 / 15 loss=8.061, nll_loss=5.06, ppl=33.37, wps=1519.9, ups=0.36, wpb=4203.5, bsz=296, num_updates=74, lr=8.88e-07, gnorm=2.898, train_wall=6, gb_free=11.6, wall=498
2024-07-29 10:10:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34025.53125Mb; avail=220449.0859375Mb
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000487
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34025.53125Mb; avail=220448.59375Mb
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002040
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34025.53125Mb; avail=220448.59375Mb
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001838
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004710
2024-07-29 10:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34025.53125Mb; avail=220448.59375Mb
2024-07-29 10:10:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.63 | nll_loss 4.33 | ppl 20.11 | wps 3201.2 | wpb 1361.3 | bsz 86.8 | num_updates 75 | best_loss 7.63
2024-07-29 10:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 75 updates
2024-07-29 10:10:25 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:11:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 5 @ 75 updates, score 7.63) (writing took 65.32697843201458 seconds)
2024-07-29 10:11:30 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2024-07-29 10:11:30 | INFO | train | epoch 005 | loss 8.133 | nll_loss 5.139 | ppl 35.23 | wps 537.9 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 75 | lr 9e-07 | gnorm 3.102 | train_wall 53 | gb_free 11.3 | wall 567
2024-07-29 10:11:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:11:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:11:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:11:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000705
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27200.89453125Mb; avail=227273.234375Mb
2024-07-29 10:11:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000107
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000772
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27200.89453125Mb; avail=227273.234375Mb
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27200.89453125Mb; avail=227273.234375Mb
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000226
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001336
2024-07-29 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27200.89453125Mb; avail=227273.234375Mb
2024-07-29 10:11:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:11:30 | INFO | fairseq.trainer | begin training epoch 6
2024-07-29 10:11:30 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:11:33 | INFO | train_inner | epoch 006:      1 / 15 loss=7.944, nll_loss=4.9, ppl=29.86, wps=94.6, ups=0.03, wpb=3418, bsz=253.5, num_updates=76, lr=9.12e-07, gnorm=3.155, train_wall=5, gb_free=9.6, wall=570
2024-07-29 10:11:39 | INFO | train_inner | epoch 006:      3 / 15 loss=8.09, nll_loss=5.095, ppl=34.18, wps=1520.3, ups=0.36, wpb=4186, bsz=252, num_updates=78, lr=9.36e-07, gnorm=2.764, train_wall=5, gb_free=11.9, wall=576
2024-07-29 10:11:44 | INFO | train_inner | epoch 006:      5 / 15 loss=7.997, nll_loss=4.971, ppl=31.36, wps=1566.4, ups=0.36, wpb=4389, bsz=332, num_updates=80, lr=9.6e-07, gnorm=2.638, train_wall=6, gb_free=11.5, wall=581
2024-07-29 10:11:49 | INFO | train_inner | epoch 006:      7 / 15 loss=8.03, nll_loss=5.029, ppl=32.65, wps=1637.3, ups=0.43, wpb=3787.5, bsz=249.5, num_updates=82, lr=9.84e-07, gnorm=2.994, train_wall=5, gb_free=14, wall=586
2024-07-29 10:11:55 | INFO | train_inner | epoch 006:      9 / 15 loss=8.115, nll_loss=5.135, ppl=35.15, wps=1396.7, ups=0.35, wpb=4001.5, bsz=236, num_updates=84, lr=1.008e-06, gnorm=2.776, train_wall=6, gb_free=9.9, wall=592
2024-07-29 10:12:01 | INFO | train_inner | epoch 006:     11 / 15 loss=8.145, nll_loss=5.166, ppl=35.91, wps=1434.3, ups=0.32, wpb=4479.5, bsz=268, num_updates=86, lr=1.032e-06, gnorm=2.528, train_wall=6, gb_free=9.1, wall=598
2024-07-29 10:12:07 | INFO | train_inner | epoch 006:     13 / 15 loss=7.981, nll_loss=4.965, ppl=31.24, wps=1496.1, ups=0.33, wpb=4498.5, bsz=288, num_updates=88, lr=1.056e-06, gnorm=2.485, train_wall=6, gb_free=9.9, wall=604
2024-07-29 10:18:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 2, 'log_format': 'simple', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 222, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3600, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3600, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 40000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': '/home/krish/content/1.2B_last_checkpoint.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 5000, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='ne-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer_wmt_en_de_big'), 'task': Namespace(no_progress_bar=False, log_interval=2, log_format='simple', log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=222, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation_multi_simple_epoch', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3600, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3600, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer_wmt_en_de_big', max_epoch=0, max_update=40000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[2], lr=[3e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model='/home/krish/content/1.2B_last_checkpoint.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=5000, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, source_lang=None, target_lang=None, lang_pairs='ne-hi', keep_inference_langtok=False, sampling_method='temperature', sampling_temperature=1.5, data='/home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin', langs=['hi', 'ne'], lang_dict=None, source_dict=None, target_dict=None, lang_tok_style='multilingual', load_alignments=False, left_pad_source='True', left_pad_target='False', upsample_primary=1, truncate_source=False, encoder_langtok='src', decoder_langtok=True, lang_tok_replacing_bos_eos=False, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, extra_data=None, extra_lang_pairs=None, fixed_dictionary=None, langtoks_specs=['main'], langtoks=None, sampling_weights_from_file=None, sampling_weights=None, virtual_epoch_size=None, virtual_data_size=None, label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2500, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.3, attention_dropout=0.1, encoder_layers=24, decoder_layers=24, encoder_ffn_embed_dim=8192, decoder_ffn_embed_dim=8192, encoder_layerdrop=0.05, decoder_layerdrop=0.05, share_decoder_input_output_embed=True, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=1024, encoder_attention_heads=16, decoder_embed_dim=1024, decoder_attention_heads=16, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, max_source_positions=1024, max_target_positions=1024, _name='translation_multi_simple_epoch'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2500, 'warmup_init_lr': -1.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-07-29 10:18:14 | INFO | fairseq.data.multilingual.multilingual_data_manager | parsed the language list as they are ordered in the option: ['hi', 'ne']
2024-07-29 10:18:14 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ne] dictionary: 128112 types
2024-07-29 10:18:14 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hi] dictionary: 128112 types
2024-07-29 10:18:22 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-22): 23 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(128112, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): LayerDropModuleList(
      (0-20): 21 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=128112, bias=False)
  )
)
2024-07-29 10:18:22 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask
2024-07-29 10:18:22 | INFO | fairseq_cli.train | model: TransformerModel
2024-07-29 10:18:22 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2024-07-29 10:18:22 | INFO | fairseq_cli.train | num. shared model params: 1,743,106,048 (num. trained: 1,743,106,048)
2024-07-29 10:18:22 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-07-29 10:18:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None
2024-07-29 10:18:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16267.19140625Mb; avail=238223.0546875Mb
2024-07-29 10:18:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-07-29 10:18:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:ne-hi': 1}
2024-07-29 10:18:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ne-hi src_langtok: 128066; tgt_langtok: 128036
2024-07-29 10:18:22 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.ne-hi.ne
2024-07-29 10:18:22 | INFO | fairseq.data.data_utils | loaded 521 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/valid.ne-hi.hi
2024-07-29 10:18:22 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin valid ne-hi 521 examples
2024-07-29 10:18:23 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-07-29 10:18:23 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2024-07-29 10:18:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-07-29 10:18:23 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.529 GB ; name = NVIDIA RTX A6000                        
2024-07-29 10:18:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-07-29 10:18:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-07-29 10:18:23 | INFO | fairseq_cli.train | max tokens per device = 3600 and max sentences per device = None
2024-07-29 10:18:23 | INFO | fairseq.trainer | Preparing to load checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 10:18:31 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2024-07-29 10:18:32 | INFO | fairseq.trainer | Loaded checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 6 @ 75 updates)
2024-07-29 10:18:32 | INFO | fairseq.trainer | loading train data for epoch 6
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=6/None
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13624.36328125Mb; avail=240857.83984375Mb
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:ne-hi': 1}
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ne-hi src_langtok: 128066; tgt_langtok: 128036
2024-07-29 10:18:32 | INFO | fairseq.data.data_utils | loaded 4,163 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/train.ne-hi.ne
2024-07-29 10:18:32 | INFO | fairseq.data.data_utils | loaded 4,163 examples from: /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin/train.ne-hi.hi
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | /home/krish/content/Hindi_Nepali/wmt22_spm/wmt22_bin train ne-hi 4163 examples
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:ne-hi', 4163)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: temperature
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.multilingual_data_manager | | Upsample ratios: [('main:ne-hi', 1.0)]
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000479
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=13624.359375Mb; avail=240857.83984375Mb
2024-07-29 10:18:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000049
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000621
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13624.359375Mb; avail=240857.83984375Mb
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13624.359375Mb; avail=240857.83984375Mb
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000613
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001557
2024-07-29 10:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=13624.359375Mb; avail=240857.83984375Mb
2024-07-29 10:18:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:18:32 | INFO | fairseq.trainer | begin training epoch 6
2024-07-29 10:18:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:18:35 | INFO | train_inner | epoch 006:      1 / 15 loss=7.86, nll_loss=4.795, ppl=27.76, wps=0, ups=0, wpb=4842, bsz=344, num_updates=76, lr=9.12e-07, gnorm=2.555, train_wall=3, gb_free=9.7, wall=12
2024-07-29 10:18:41 | INFO | train_inner | epoch 006:      3 / 15 loss=8.09, nll_loss=5.095, ppl=34.18, wps=1533.7, ups=0.37, wpb=4186, bsz=252, num_updates=78, lr=9.36e-07, gnorm=2.764, train_wall=5, gb_free=11.9, wall=18
2024-07-29 10:18:46 | INFO | train_inner | epoch 006:      5 / 15 loss=7.997, nll_loss=4.971, ppl=31.36, wps=1578.9, ups=0.36, wpb=4389, bsz=332, num_updates=80, lr=9.6e-07, gnorm=2.638, train_wall=6, gb_free=11.5, wall=23
2024-07-29 10:18:51 | INFO | train_inner | epoch 006:      7 / 15 loss=8.03, nll_loss=5.029, ppl=32.65, wps=1609.6, ups=0.42, wpb=3787.5, bsz=249.5, num_updates=82, lr=9.84e-07, gnorm=2.994, train_wall=5, gb_free=14, wall=28
2024-07-29 10:18:57 | INFO | train_inner | epoch 006:      9 / 15 loss=8.115, nll_loss=5.135, ppl=35.15, wps=1406, ups=0.35, wpb=4001.5, bsz=236, num_updates=84, lr=1.008e-06, gnorm=2.776, train_wall=6, gb_free=9.8, wall=34
2024-07-29 10:19:03 | INFO | train_inner | epoch 006:     11 / 15 loss=8.145, nll_loss=5.166, ppl=35.91, wps=1480.2, ups=0.33, wpb=4479.5, bsz=268, num_updates=86, lr=1.032e-06, gnorm=2.528, train_wall=6, gb_free=9, wall=40
2024-07-29 10:19:09 | INFO | train_inner | epoch 006:     13 / 15 loss=7.981, nll_loss=4.965, ppl=31.24, wps=1505.8, ups=0.33, wpb=4498.5, bsz=288, num_updates=88, lr=1.056e-06, gnorm=2.485, train_wall=6, gb_free=9.9, wall=46
2024-07-29 10:19:15 | INFO | train_inner | epoch 006:     15 / 15 loss=8.03, nll_loss=5.026, ppl=32.59, wps=1522.7, ups=0.33, wpb=4606.5, bsz=284, num_updates=90, lr=1.08e-06, gnorm=2.504, train_wall=6, gb_free=9.4, wall=52
2024-07-29 10:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14130.515625Mb; avail=240343.66015625Mb
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000557
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14130.515625Mb; avail=240343.66015625Mb
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002272
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14130.515625Mb; avail=240343.66015625Mb
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001949
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005111
2024-07-29 10:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14130.515625Mb; avail=240343.66015625Mb
2024-07-29 10:19:17 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.559 | nll_loss 4.252 | ppl 19.05 | wps 3210.4 | wpb 1361.3 | bsz 86.8 | num_updates 90 | best_loss 7.559
2024-07-29 10:19:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 90 updates
2024-07-29 10:19:17 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:19:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:20:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 6 @ 90 updates, score 7.559) (writing took 64.45182268694043 seconds)
2024-07-29 10:20:22 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2024-07-29 10:20:22 | INFO | train | epoch 006 | loss 8.04 | nll_loss 5.035 | ppl 32.79 | wps 564.1 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 90 | lr 1.08e-06 | gnorm 2.662 | train_wall 43 | gb_free 9.4 | wall 119
2024-07-29 10:20:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:20:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:20:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:20:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000678
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=19458.4609375Mb; avail=235607.64453125Mb
2024-07-29 10:20:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000711
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19458.4609375Mb; avail=235607.64453125Mb
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19458.4609375Mb; avail=235607.64453125Mb
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000208
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001234
2024-07-29 10:20:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=19458.4609375Mb; avail=235607.64453125Mb
2024-07-29 10:20:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:20:22 | INFO | fairseq.trainer | begin training epoch 7
2024-07-29 10:20:22 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:20:28 | INFO | train_inner | epoch 007:      2 / 15 loss=7.964, nll_loss=4.935, ppl=30.59, wps=128.3, ups=0.03, wpb=4661, bsz=268, num_updates=92, lr=1.104e-06, gnorm=2.545, train_wall=6, gb_free=9.6, wall=125
2024-07-29 10:20:33 | INFO | train_inner | epoch 007:      4 / 15 loss=8.112, nll_loss=5.132, ppl=35.06, wps=1328.1, ups=0.34, wpb=3859, bsz=232, num_updates=94, lr=1.128e-06, gnorm=2.516, train_wall=6, gb_free=12.2, wall=130
2024-07-29 10:20:39 | INFO | train_inner | epoch 007:      6 / 15 loss=7.865, nll_loss=4.81, ppl=28.05, wps=1624.7, ups=0.34, wpb=4819.5, bsz=320, num_updates=96, lr=1.152e-06, gnorm=2.231, train_wall=6, gb_free=11.2, wall=136
2024-07-29 10:20:45 | INFO | train_inner | epoch 007:      8 / 15 loss=7.918, nll_loss=4.89, ppl=29.65, wps=1595.8, ups=0.34, wpb=4707, bsz=324, num_updates=98, lr=1.176e-06, gnorm=2.309, train_wall=6, gb_free=9.9, wall=142
2024-07-29 10:20:50 | INFO | train_inner | epoch 007:     10 / 15 loss=8.051, nll_loss=5.062, ppl=33.41, wps=1363.9, ups=0.44, wpb=3130, bsz=237.5, num_updates=100, lr=1.2e-06, gnorm=2.752, train_wall=5, gb_free=16.3, wall=147
2024-07-29 10:20:56 | INFO | train_inner | epoch 007:     12 / 15 loss=7.903, nll_loss=4.869, ppl=29.23, wps=1486.9, ups=0.32, wpb=4629, bsz=280, num_updates=102, lr=1.224e-06, gnorm=2.214, train_wall=6, gb_free=9.6, wall=153
2024-07-29 10:21:02 | INFO | train_inner | epoch 007:     14 / 15 loss=7.893, nll_loss=4.862, ppl=29.09, wps=1465.1, ups=0.34, wpb=4250, bsz=284, num_updates=104, lr=1.248e-06, gnorm=2.323, train_wall=6, gb_free=9.5, wall=159
2024-07-29 10:21:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=14684.90625Mb; avail=240381.3203125Mb
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14685.3984375Mb; avail=240380.828125Mb
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002188
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14685.3984375Mb; avail=240380.828125Mb
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001930
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005030
2024-07-29 10:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=14685.3984375Mb; avail=240380.828125Mb
2024-07-29 10:21:07 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.51 | nll_loss 4.2 | ppl 18.37 | wps 3186 | wpb 1361.3 | bsz 86.8 | num_updates 105 | best_loss 7.51
2024-07-29 10:21:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 105 updates
2024-07-29 10:21:07 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:21:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 7 @ 105 updates, score 7.51) (writing took 63.4811356770806 seconds)
2024-07-29 10:22:10 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-07-29 10:22:10 | INFO | train | epoch 007 | loss 7.953 | nll_loss 4.932 | ppl 30.52 | wps 594.5 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 105 | lr 1.26e-06 | gnorm 2.414 | train_wall 43 | gb_free 10.4 | wall 227
2024-07-29 10:22:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:22:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:22:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:22:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000762
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23452.9453125Mb; avail=231613.24609375Mb
2024-07-29 10:22:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000090
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000789
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23453.4375Mb; avail=231612.75390625Mb
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23453.9296875Mb; avail=231612.75390625Mb
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000236
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001347
2024-07-29 10:22:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23453.9296875Mb; avail=231612.75390625Mb
2024-07-29 10:22:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:22:11 | INFO | fairseq.trainer | begin training epoch 8
2024-07-29 10:22:11 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:22:13 | INFO | train_inner | epoch 008:      1 / 15 loss=7.935, nll_loss=4.918, ppl=30.23, wps=131.9, ups=0.03, wpb=4724.5, bsz=312, num_updates=106, lr=1.272e-06, gnorm=2.306, train_wall=6, gb_free=9.4, wall=230
2024-07-29 10:22:19 | INFO | train_inner | epoch 008:      3 / 15 loss=8.001, nll_loss=4.998, ppl=31.96, wps=1466.6, ups=0.35, wpb=4156.5, bsz=240, num_updates=108, lr=1.296e-06, gnorm=2.456, train_wall=6, gb_free=10, wall=236
2024-07-29 10:22:25 | INFO | train_inner | epoch 008:      5 / 15 loss=7.782, nll_loss=4.719, ppl=26.33, wps=1545.5, ups=0.33, wpb=4641, bsz=312, num_updates=110, lr=1.32e-06, gnorm=2.166, train_wall=6, gb_free=9.1, wall=242
2024-07-29 10:22:31 | INFO | train_inner | epoch 008:      7 / 15 loss=7.883, nll_loss=4.85, ppl=28.85, wps=1506.5, ups=0.35, wpb=4269.5, bsz=296, num_updates=112, lr=1.344e-06, gnorm=2.285, train_wall=6, gb_free=10.6, wall=248
2024-07-29 10:22:37 | INFO | train_inner | epoch 008:      9 / 15 loss=7.916, nll_loss=4.891, ppl=29.68, wps=1473.8, ups=0.33, wpb=4429.5, bsz=248, num_updates=114, lr=1.368e-06, gnorm=2.475, train_wall=6, gb_free=8.5, wall=254
2024-07-29 10:22:43 | INFO | train_inner | epoch 008:     11 / 15 loss=7.917, nll_loss=4.906, ppl=29.98, wps=1493.8, ups=0.34, wpb=4455.5, bsz=304, num_updates=116, lr=1.392e-06, gnorm=2.178, train_wall=6, gb_free=10.6, wall=260
2024-07-29 10:22:49 | INFO | train_inner | epoch 008:     13 / 15 loss=7.947, nll_loss=4.941, ppl=30.72, wps=1503.4, ups=0.33, wpb=4563, bsz=284, num_updates=118, lr=1.416e-06, gnorm=2.155, train_wall=6, gb_free=11.5, wall=266
2024-07-29 10:22:53 | INFO | train_inner | epoch 008:     15 / 15 loss=7.922, nll_loss=4.907, ppl=30, wps=1485.4, ups=0.43, wpb=3444, bsz=221.5, num_updates=120, lr=1.44e-06, gnorm=2.806, train_wall=5, gb_free=11.9, wall=270
2024-07-29 10:22:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:22:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23554.44921875Mb; avail=231511.69140625Mb
2024-07-29 10:22:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000585
2024-07-29 10:22:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23554.44921875Mb; avail=231511.69140625Mb
2024-07-29 10:22:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002103
2024-07-29 10:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23554.94140625Mb; avail=231511.19921875Mb
2024-07-29 10:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001901
2024-07-29 10:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004921
2024-07-29 10:22:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23554.94140625Mb; avail=231511.19921875Mb
2024-07-29 10:22:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.469 | nll_loss 4.156 | ppl 17.82 | wps 3190 | wpb 1361.3 | bsz 86.8 | num_updates 120 | best_loss 7.469
2024-07-29 10:22:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 120 updates
2024-07-29 10:22:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:23:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:24:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 8 @ 120 updates, score 7.469) (writing took 78.59909250214696 seconds)
2024-07-29 10:24:14 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-07-29 10:24:14 | INFO | train | epoch 008 | loss 7.905 | nll_loss 4.882 | ppl 29.48 | wps 522.7 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 120 | lr 1.44e-06 | gnorm 2.348 | train_wall 43 | gb_free 11.9 | wall 351
2024-07-29 10:24:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:24:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:24:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:24:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000619
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28472.53515625Mb; avail=226592.7578125Mb
2024-07-29 10:24:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000080
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000722
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28474.59765625Mb; avail=226590.6953125Mb
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28474.59765625Mb; avail=226591.1875Mb
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000217
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001258
2024-07-29 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28476.16015625Mb; avail=226588.1484375Mb
2024-07-29 10:24:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:24:14 | INFO | fairseq.trainer | begin training epoch 9
2024-07-29 10:24:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:24:20 | INFO | train_inner | epoch 009:      2 / 15 loss=7.916, nll_loss=4.906, ppl=29.97, wps=89.7, ups=0.02, wpb=3873, bsz=256, num_updates=122, lr=1.464e-06, gnorm=2.36, train_wall=5, gb_free=11.8, wall=357
2024-07-29 10:24:26 | INFO | train_inner | epoch 009:      4 / 15 loss=7.752, nll_loss=4.689, ppl=25.8, wps=1692, ups=0.34, wpb=4918, bsz=324, num_updates=124, lr=1.488e-06, gnorm=2.029, train_wall=6, gb_free=10.3, wall=363
2024-07-29 10:24:32 | INFO | train_inner | epoch 009:      6 / 15 loss=7.892, nll_loss=4.876, ppl=29.37, wps=1492.5, ups=0.33, wpb=4464, bsz=288, num_updates=126, lr=1.512e-06, gnorm=2.088, train_wall=6, gb_free=9.1, wall=369
2024-07-29 10:24:37 | INFO | train_inner | epoch 009:      8 / 15 loss=7.879, nll_loss=4.861, ppl=29.06, wps=1369.9, ups=0.35, wpb=3965, bsz=240, num_updates=128, lr=1.536e-06, gnorm=2.308, train_wall=6, gb_free=9.8, wall=374
2024-07-29 10:24:43 | INFO | train_inner | epoch 009:     10 / 15 loss=7.733, nll_loss=4.669, ppl=25.44, wps=1481, ups=0.39, wpb=3755.5, bsz=245.5, num_updates=130, lr=1.56e-06, gnorm=2.454, train_wall=5, gb_free=8.3, wall=379
2024-07-29 10:24:48 | INFO | train_inner | epoch 009:     12 / 15 loss=7.899, nll_loss=4.881, ppl=29.47, wps=1503.3, ups=0.34, wpb=4461, bsz=276, num_updates=132, lr=1.584e-06, gnorm=2.176, train_wall=6, gb_free=11, wall=385
2024-07-29 10:24:54 | INFO | train_inner | epoch 009:     14 / 15 loss=7.809, nll_loss=4.783, ppl=27.54, wps=1603.5, ups=0.34, wpb=4680, bsz=320, num_updates=134, lr=1.608e-06, gnorm=2.044, train_wall=6, gb_free=8.7, wall=391
2024-07-29 10:24:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28601.26171875Mb; avail=226464.796875Mb
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000599
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28601.75390625Mb; avail=226464.3046875Mb
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002106
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28601.75390625Mb; avail=226464.3046875Mb
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001919
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004956
2024-07-29 10:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28601.75390625Mb; avail=226464.3046875Mb
2024-07-29 10:25:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.425 | nll_loss 4.107 | ppl 17.24 | wps 3190.2 | wpb 1361.3 | bsz 86.8 | num_updates 135 | best_loss 7.425
2024-07-29 10:25:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 135 updates
2024-07-29 10:25:00 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:25:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:26:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 9 @ 135 updates, score 7.425) (writing took 62.243770704604685 seconds)
2024-07-29 10:26:02 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2024-07-29 10:26:02 | INFO | train | epoch 009 | loss 7.835 | nll_loss 4.803 | ppl 27.91 | wps 601.8 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 135 | lr 1.62e-06 | gnorm 2.203 | train_wall 43 | gb_free 9.5 | wall 459
2024-07-29 10:26:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:26:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:26:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:26:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000789
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28533.6171875Mb; avail=226512.5Mb
2024-07-29 10:26:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000096
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000795
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28533.6171875Mb; avail=226512.5Mb
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28533.6171875Mb; avail=226512.0078125Mb
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000216
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001344
2024-07-29 10:26:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28533.125Mb; avail=226512.9921875Mb
2024-07-29 10:26:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:26:02 | INFO | fairseq.trainer | begin training epoch 10
2024-07-29 10:26:02 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:26:05 | INFO | train_inner | epoch 010:      1 / 15 loss=7.757, nll_loss=4.709, ppl=26.15, wps=126.1, ups=0.03, wpb=4436.5, bsz=304, num_updates=136, lr=1.632e-06, gnorm=2.132, train_wall=6, gb_free=12, wall=462
2024-07-29 10:26:09 | INFO | train_inner | epoch 010:      3 / 15 loss=7.973, nll_loss=4.992, ppl=31.81, wps=1466.4, ups=0.42, wpb=3500.5, bsz=197.5, num_updates=138, lr=1.656e-06, gnorm=2.568, train_wall=5, gb_free=9.4, wall=466
2024-07-29 10:26:15 | INFO | train_inner | epoch 010:      5 / 15 loss=7.841, nll_loss=4.813, ppl=28.12, wps=1474.5, ups=0.36, wpb=4130.5, bsz=244, num_updates=140, lr=1.68e-06, gnorm=2.265, train_wall=6, gb_free=12.5, wall=472
2024-07-29 10:26:21 | INFO | train_inner | epoch 010:      7 / 15 loss=7.747, nll_loss=4.702, ppl=26.02, wps=1539.8, ups=0.35, wpb=4341.5, bsz=296, num_updates=142, lr=1.704e-06, gnorm=2.071, train_wall=6, gb_free=10.8, wall=478
2024-07-29 10:26:26 | INFO | train_inner | epoch 010:      9 / 15 loss=7.977, nll_loss=4.992, ppl=31.83, wps=1423.5, ups=0.34, wpb=4158.5, bsz=228, num_updates=144, lr=1.728e-06, gnorm=2.522, train_wall=6, gb_free=12.1, wall=483
2024-07-29 10:26:33 | INFO | train_inner | epoch 010:     11 / 15 loss=7.65, nll_loss=4.573, ppl=23.81, wps=1567.9, ups=0.32, wpb=4902.5, bsz=312, num_updates=146, lr=1.752e-06, gnorm=2.017, train_wall=6, gb_free=9.7, wall=490
2024-07-29 10:26:39 | INFO | train_inner | epoch 010:     13 / 15 loss=7.746, nll_loss=4.704, ppl=26.06, wps=1515.7, ups=0.33, wpb=4596, bsz=340, num_updates=148, lr=1.776e-06, gnorm=2.02, train_wall=6, gb_free=10.3, wall=496
2024-07-29 10:26:45 | INFO | train_inner | epoch 010:     15 / 15 loss=7.711, nll_loss=4.648, ppl=25.08, wps=1457.2, ups=0.32, wpb=4556.5, bsz=292, num_updates=150, lr=1.8e-06, gnorm=2.03, train_wall=6, gb_free=9.3, wall=502
2024-07-29 10:26:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28634.83203125Mb; avail=226411.2265625Mb
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000586
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28635.32421875Mb; avail=226410.734375Mb
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002151
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28635.32421875Mb; avail=226410.734375Mb
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001956
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005020
2024-07-29 10:26:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28635.3203125Mb; avail=226410.734375Mb
2024-07-29 10:26:47 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.393 | nll_loss 4.066 | ppl 16.75 | wps 3186.5 | wpb 1361.3 | bsz 86.8 | num_updates 150 | best_loss 7.393
2024-07-29 10:26:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 150 updates
2024-07-29 10:26:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:27:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:27:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 10 @ 150 updates, score 7.393) (writing took 59.88407436711714 seconds)
2024-07-29 10:27:47 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2024-07-29 10:27:47 | INFO | train | epoch 010 | loss 7.792 | nll_loss 4.757 | ppl 27.03 | wps 614.9 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 150 | lr 1.8e-06 | gnorm 2.208 | train_wall 43 | gb_free 9.3 | wall 564
2024-07-29 10:27:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:27:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:27:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:27:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000769
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38525.65234375Mb; avail=216543.359375Mb
2024-07-29 10:27:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000745
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38519.25390625Mb; avail=216546.3125Mb
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38520.23828125Mb; avail=216545.8203125Mb
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000214
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001284
2024-07-29 10:27:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38522.20703125Mb; avail=216543.359375Mb
2024-07-29 10:27:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:27:47 | INFO | fairseq.trainer | begin training epoch 11
2024-07-29 10:27:47 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:27:53 | INFO | train_inner | epoch 011:      2 / 15 loss=7.756, nll_loss=4.71, ppl=26.18, wps=125.8, ups=0.03, wpb=4272.5, bsz=264, num_updates=152, lr=1.824e-06, gnorm=2.212, train_wall=6, gb_free=11.5, wall=570
2024-07-29 10:27:59 | INFO | train_inner | epoch 011:      4 / 15 loss=7.787, nll_loss=4.758, ppl=27.06, wps=1623.1, ups=0.34, wpb=4736.5, bsz=304, num_updates=154, lr=1.848e-06, gnorm=2.119, train_wall=6, gb_free=9, wall=576
2024-07-29 10:28:20 | INFO | train_inner | epoch 011:      6 / 15 loss=7.77, nll_loss=4.736, ppl=26.65, wps=407, ups=0.1, wpb=4259, bsz=304, num_updates=156, lr=1.872e-06, gnorm=2.275, train_wall=21, gb_free=11.6, wall=597
2024-07-29 10:28:26 | INFO | train_inner | epoch 011:      8 / 15 loss=7.697, nll_loss=4.643, ppl=24.99, wps=1527.2, ups=0.33, wpb=4624.5, bsz=320, num_updates=158, lr=1.896e-06, gnorm=2.055, train_wall=6, gb_free=10.1, wall=603
2024-07-29 10:28:32 | INFO | train_inner | epoch 011:     10 / 15 loss=7.804, nll_loss=4.775, ppl=27.39, wps=1364.7, ups=0.35, wpb=3946, bsz=216, num_updates=160, lr=1.92e-06, gnorm=2.296, train_wall=6, gb_free=10.6, wall=609
2024-07-29 10:28:37 | INFO | train_inner | epoch 011:     12 / 15 loss=7.733, nll_loss=4.693, ppl=25.86, wps=1625.7, ups=0.34, wpb=4746.5, bsz=320, num_updates=162, lr=1.944e-06, gnorm=2.011, train_wall=6, gb_free=12.3, wall=614
2024-07-29 10:28:43 | INFO | train_inner | epoch 011:     14 / 15 loss=7.78, nll_loss=4.747, ppl=26.86, wps=1511.4, ups=0.34, wpb=4390, bsz=260, num_updates=164, lr=1.968e-06, gnorm=2.181, train_wall=6, gb_free=10.2, wall=620
2024-07-29 10:28:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29621.3359375Mb; avail=225444.89453125Mb
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000485
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29621.3359375Mb; avail=225444.89453125Mb
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002072
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29621.3359375Mb; avail=225444.89453125Mb
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001875
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004742
2024-07-29 10:28:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29621.3359375Mb; avail=225444.89453125Mb
2024-07-29 10:28:47 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.341 | nll_loss 4.007 | ppl 16.07 | wps 3192.6 | wpb 1361.3 | bsz 86.8 | num_updates 165 | best_loss 7.341
2024-07-29 10:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 165 updates
2024-07-29 10:28:47 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:29:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 11 @ 165 updates, score 7.341) (writing took 65.05951620591804 seconds)
2024-07-29 10:29:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2024-07-29 10:29:52 | INFO | train | epoch 011 | loss 7.759 | nll_loss 4.72 | ppl 26.35 | wps 517.7 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 165 | lr 1.98e-06 | gnorm 2.201 | train_wall 58 | gb_free 12 | wall 689
2024-07-29 10:29:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:29:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:29:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:29:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000661
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24381.875Mb; avail=230683.77734375Mb
2024-07-29 10:29:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000087
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000794
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24376.4609375Mb; avail=230689.19140625Mb
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24377.9375Mb; avail=230688.20703125Mb
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000233
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001354
2024-07-29 10:29:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24380.3984375Mb; avail=230685.25390625Mb
2024-07-29 10:29:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:29:52 | INFO | fairseq.trainer | begin training epoch 12
2024-07-29 10:29:52 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:29:55 | INFO | train_inner | epoch 012:      1 / 15 loss=7.722, nll_loss=4.666, ppl=25.39, wps=105.5, ups=0.03, wpb=3808.5, bsz=245.5, num_updates=166, lr=1.992e-06, gnorm=2.351, train_wall=5, gb_free=8.4, wall=692
2024-07-29 10:30:01 | INFO | train_inner | epoch 012:      3 / 15 loss=7.722, nll_loss=4.678, ppl=25.61, wps=1590.6, ups=0.35, wpb=4581.5, bsz=280, num_updates=168, lr=2.016e-06, gnorm=2.144, train_wall=6, gb_free=9.8, wall=698
2024-07-29 10:30:07 | INFO | train_inner | epoch 012:      5 / 15 loss=7.69, nll_loss=4.638, ppl=24.91, wps=1560.2, ups=0.34, wpb=4642, bsz=304, num_updates=170, lr=2.04e-06, gnorm=1.944, train_wall=6, gb_free=9, wall=704
2024-07-29 10:30:13 | INFO | train_inner | epoch 012:      7 / 15 loss=7.731, nll_loss=4.685, ppl=25.72, wps=1407.7, ups=0.36, wpb=3942, bsz=260, num_updates=172, lr=2.064e-06, gnorm=2.314, train_wall=6, gb_free=12.5, wall=710
2024-07-29 10:30:17 | INFO | train_inner | epoch 012:      9 / 15 loss=7.923, nll_loss=4.941, ppl=30.71, wps=1478.4, ups=0.43, wpb=3434.5, bsz=213.5, num_updates=174, lr=2.088e-06, gnorm=2.761, train_wall=5, gb_free=15.3, wall=714
2024-07-29 10:30:23 | INFO | train_inner | epoch 012:     11 / 15 loss=7.766, nll_loss=4.73, ppl=26.54, wps=1400.2, ups=0.33, wpb=4259.5, bsz=256, num_updates=176, lr=2.112e-06, gnorm=2.19, train_wall=6, gb_free=9.5, wall=720
2024-07-29 10:30:29 | INFO | train_inner | epoch 012:     13 / 15 loss=7.579, nll_loss=4.499, ppl=22.61, wps=1645.9, ups=0.34, wpb=4815.5, bsz=312, num_updates=178, lr=2.136e-06, gnorm=2.186, train_wall=6, gb_free=10.5, wall=726
2024-07-29 10:30:35 | INFO | train_inner | epoch 012:     15 / 15 loss=7.636, nll_loss=4.567, ppl=23.7, wps=1448.6, ups=0.34, wpb=4280.5, bsz=304, num_updates=180, lr=2.16e-06, gnorm=2.096, train_wall=6, gb_free=11, wall=732
2024-07-29 10:30:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24466.99609375Mb; avail=230599.0625Mb
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000598
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24466.99609375Mb; avail=230599.0625Mb
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002141
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24467.484375Mb; avail=230598.5703125Mb
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001911
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005054
2024-07-29 10:30:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24467.484375Mb; avail=230598.5703125Mb
2024-07-29 10:30:37 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.315 | nll_loss 3.967 | ppl 15.64 | wps 3179.6 | wpb 1361.3 | bsz 86.8 | num_updates 180 | best_loss 7.315
2024-07-29 10:30:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 180 updates
2024-07-29 10:30:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:31:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 12 @ 180 updates, score 7.315) (writing took 62.659521664027125 seconds)
2024-07-29 10:31:40 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2024-07-29 10:31:40 | INFO | train | epoch 012 | loss 7.712 | nll_loss 4.666 | ppl 25.38 | wps 599.9 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 180 | lr 2.16e-06 | gnorm 2.217 | train_wall 43 | gb_free 11 | wall 797
2024-07-29 10:31:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:31:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:31:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:31:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001142
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46768.10546875Mb; avail=208269.953125Mb
2024-07-29 10:31:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000103
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.001181
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46768.10546875Mb; avail=208269.953125Mb
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000065
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46768.10546875Mb; avail=208269.953125Mb
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000359
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.002085
2024-07-29 10:31:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46768.10546875Mb; avail=208269.953125Mb
2024-07-29 10:31:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:31:40 | INFO | fairseq.trainer | begin training epoch 13
2024-07-29 10:31:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:31:46 | INFO | train_inner | epoch 013:      2 / 15 loss=7.697, nll_loss=4.643, ppl=24.99, wps=116.4, ups=0.03, wpb=4115, bsz=244, num_updates=182, lr=2.184e-06, gnorm=2.196, train_wall=6, gb_free=11.2, wall=803
2024-07-29 10:31:51 | INFO | train_inner | epoch 013:      4 / 15 loss=7.658, nll_loss=4.586, ppl=24.02, wps=1609.6, ups=0.37, wpb=4335, bsz=280, num_updates=184, lr=2.208e-06, gnorm=2.092, train_wall=5, gb_free=12.2, wall=808
2024-07-29 10:32:07 | INFO | train_inner | epoch 013:      6 / 15 loss=7.637, nll_loss=4.563, ppl=23.63, wps=554.4, ups=0.13, wpb=4366, bsz=320, num_updates=186, lr=2.232e-06, gnorm=2.073, train_wall=16, gb_free=9.5, wall=824
2024-07-29 10:32:13 | INFO | train_inner | epoch 013:      8 / 15 loss=7.549, nll_loss=4.454, ppl=21.92, wps=1627.6, ups=0.33, wpb=4862, bsz=300, num_updates=188, lr=2.256e-06, gnorm=1.945, train_wall=6, gb_free=11.7, wall=830
2024-07-29 10:32:19 | INFO | train_inner | epoch 013:     10 / 15 loss=7.708, nll_loss=4.673, ppl=25.5, wps=1531.7, ups=0.33, wpb=4644, bsz=320, num_updates=190, lr=2.28e-06, gnorm=2.103, train_wall=6, gb_free=11.9, wall=836
2024-07-29 10:32:25 | INFO | train_inner | epoch 013:     12 / 15 loss=7.613, nll_loss=4.54, ppl=23.26, wps=1471.7, ups=0.34, wpb=4367, bsz=292, num_updates=192, lr=2.304e-06, gnorm=2.149, train_wall=6, gb_free=11.2, wall=842
2024-07-29 10:32:30 | INFO | train_inner | epoch 013:     14 / 15 loss=7.891, nll_loss=4.901, ppl=29.87, wps=1419, ups=0.42, wpb=3364.5, bsz=193.5, num_updates=194, lr=2.328e-06, gnorm=2.72, train_wall=5, gb_free=10.7, wall=847
2024-07-29 10:32:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28932.3125Mb; avail=226105.7890625Mb
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000479
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28932.3125Mb; avail=226105.7890625Mb
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002085
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28932.3125Mb; avail=226105.7890625Mb
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001950
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004836
2024-07-29 10:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28932.3125Mb; avail=226105.7890625Mb
2024-07-29 10:32:35 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.267 | nll_loss 3.908 | ppl 15.01 | wps 3183.2 | wpb 1361.3 | bsz 86.8 | num_updates 195 | best_loss 7.267
2024-07-29 10:32:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 195 updates
2024-07-29 10:32:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:33:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:33:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 13 @ 195 updates, score 7.267) (writing took 61.48859976278618 seconds)
2024-07-29 10:33:37 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2024-07-29 10:33:37 | INFO | train | epoch 013 | loss 7.669 | nll_loss 4.609 | ppl 24.4 | wps 556.2 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 195 | lr 2.34e-06 | gnorm 2.171 | train_wall 52 | gb_free 9.8 | wall 913
2024-07-29 10:33:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:33:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:33:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:33:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000717
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37822.11328125Mb; avail=217216.07421875Mb
2024-07-29 10:33:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000079
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000784
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37821.62109375Mb; avail=217216.56640625Mb
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37822.11328125Mb; avail=217216.07421875Mb
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000226
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001325
2024-07-29 10:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37822.11328125Mb; avail=217216.07421875Mb
2024-07-29 10:33:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:33:37 | INFO | fairseq.trainer | begin training epoch 14
2024-07-29 10:33:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:33:40 | INFO | train_inner | epoch 014:      1 / 15 loss=7.641, nll_loss=4.566, ppl=23.68, wps=133.1, ups=0.03, wpb=4641, bsz=268, num_updates=196, lr=2.352e-06, gnorm=1.967, train_wall=6, gb_free=11.4, wall=916
2024-07-29 10:33:44 | INFO | train_inner | epoch 014:      3 / 15 loss=7.562, nll_loss=4.484, ppl=22.38, wps=1428.4, ups=0.41, wpb=3481.5, bsz=229.5, num_updates=198, lr=2.376e-06, gnorm=2.251, train_wall=5, gb_free=9.4, wall=921
2024-07-29 10:33:51 | INFO | train_inner | epoch 014:      5 / 15 loss=7.659, nll_loss=4.6, ppl=24.26, wps=1429.8, ups=0.33, wpb=4384.5, bsz=280, num_updates=200, lr=2.4e-06, gnorm=1.981, train_wall=6, gb_free=8.8, wall=928
2024-07-29 10:33:56 | INFO | train_inner | epoch 014:      7 / 15 loss=7.478, nll_loss=4.366, ppl=20.62, wps=1614, ups=0.35, wpb=4564, bsz=296, num_updates=202, lr=2.424e-06, gnorm=1.997, train_wall=6, gb_free=11.5, wall=933
2024-07-29 10:34:02 | INFO | train_inner | epoch 014:      9 / 15 loss=7.642, nll_loss=4.571, ppl=23.77, wps=1489.6, ups=0.34, wpb=4412, bsz=292, num_updates=204, lr=2.448e-06, gnorm=2.083, train_wall=6, gb_free=9.1, wall=939
2024-07-29 10:34:08 | INFO | train_inner | epoch 014:     11 / 15 loss=7.535, nll_loss=4.441, ppl=21.73, wps=1597.7, ups=0.34, wpb=4748, bsz=316, num_updates=206, lr=2.472e-06, gnorm=1.918, train_wall=6, gb_free=8.7, wall=945
2024-07-29 10:34:14 | INFO | train_inner | epoch 014:     13 / 15 loss=7.769, nll_loss=4.735, ppl=26.63, wps=1347, ups=0.35, wpb=3867.5, bsz=188, num_updates=208, lr=2.496e-06, gnorm=2.376, train_wall=6, gb_free=11.3, wall=951
2024-07-29 10:34:20 | INFO | train_inner | epoch 014:     15 / 15 loss=7.539, nll_loss=4.446, ppl=21.8, wps=1567.9, ups=0.34, wpb=4587, bsz=344, num_updates=210, lr=2.52e-06, gnorm=1.968, train_wall=6, gb_free=10.6, wall=957
2024-07-29 10:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28425.49609375Mb; avail=226612.60546875Mb
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000576
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28425.98828125Mb; avail=226612.11328125Mb
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002108
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28425.98828125Mb; avail=226612.11328125Mb
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001914
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004918
2024-07-29 10:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28425.98828125Mb; avail=226612.11328125Mb
2024-07-29 10:34:22 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.221 | nll_loss 3.849 | ppl 14.41 | wps 3183.2 | wpb 1361.3 | bsz 86.8 | num_updates 210 | best_loss 7.221
2024-07-29 10:34:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 210 updates
2024-07-29 10:34:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:35:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:35:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 14 @ 210 updates, score 7.221) (writing took 66.3567243940197 seconds)
2024-07-29 10:35:28 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2024-07-29 10:35:28 | INFO | train | epoch 014 | loss 7.596 | nll_loss 4.518 | ppl 22.91 | wps 579.4 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 210 | lr 2.52e-06 | gnorm 2.072 | train_wall 43 | gb_free 10.6 | wall 1025
2024-07-29 10:35:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:35:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:35:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:35:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000743
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50032.609375Mb; avail=205005.4453125Mb
2024-07-29 10:35:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000085
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000787
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50032.609375Mb; avail=205005.4453125Mb
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50032.609375Mb; avail=205005.4453125Mb
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000313
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001478
2024-07-29 10:35:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50032.609375Mb; avail=205005.4453125Mb
2024-07-29 10:35:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:35:28 | INFO | fairseq.trainer | begin training epoch 15
2024-07-29 10:35:28 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:35:34 | INFO | train_inner | epoch 015:      2 / 15 loss=7.603, nll_loss=4.519, ppl=22.92, wps=110.5, ups=0.03, wpb=4105.5, bsz=228, num_updates=212, lr=2.544e-06, gnorm=2.146, train_wall=6, gb_free=9.4, wall=1031
2024-07-29 10:35:40 | INFO | train_inner | epoch 015:      4 / 15 loss=7.63, nll_loss=4.574, ppl=23.82, wps=1558.4, ups=0.36, wpb=4368, bsz=308, num_updates=214, lr=2.568e-06, gnorm=2.057, train_wall=6, gb_free=11.5, wall=1037
2024-07-29 10:35:46 | INFO | train_inner | epoch 015:      6 / 15 loss=7.527, nll_loss=4.424, ppl=21.47, wps=1481.7, ups=0.33, wpb=4453.5, bsz=268, num_updates=216, lr=2.592e-06, gnorm=2.137, train_wall=6, gb_free=11.5, wall=1043
2024-07-29 10:36:02 | INFO | train_inner | epoch 015:      8 / 15 loss=7.511, nll_loss=4.411, ppl=21.28, wps=603.7, ups=0.12, wpb=4860, bsz=316, num_updates=218, lr=2.616e-06, gnorm=2.045, train_wall=16, gb_free=8.9, wall=1059
2024-07-29 10:36:08 | INFO | train_inner | epoch 015:     10 / 15 loss=7.541, nll_loss=4.449, ppl=21.84, wps=1558.9, ups=0.34, wpb=4621.5, bsz=288, num_updates=220, lr=2.64e-06, gnorm=2.097, train_wall=6, gb_free=11.1, wall=1065
2024-07-29 10:36:12 | INFO | train_inner | epoch 015:     12 / 15 loss=7.662, nll_loss=4.611, ppl=24.43, wps=1383.3, ups=0.44, wpb=3177.5, bsz=201.5, num_updates=222, lr=2.664e-06, gnorm=2.511, train_wall=5, gb_free=12.4, wall=1069
2024-07-29 10:36:18 | INFO | train_inner | epoch 015:     14 / 15 loss=7.589, nll_loss=4.504, ppl=22.69, wps=1583.8, ups=0.35, wpb=4538.5, bsz=304, num_updates=224, lr=2.688e-06, gnorm=2.009, train_wall=6, gb_free=10.8, wall=1075
2024-07-29 10:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27178.6953125Mb; avail=227859.40625Mb
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000517
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27178.6953125Mb; avail=227859.40625Mb
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002105
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27178.6953125Mb; avail=227859.40625Mb
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001909
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004884
2024-07-29 10:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27178.6953125Mb; avail=227859.40625Mb
2024-07-29 10:36:23 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.184 | nll_loss 3.8 | ppl 13.93 | wps 3198.9 | wpb 1361.3 | bsz 86.8 | num_updates 225 | best_loss 7.184
2024-07-29 10:36:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 225 updates
2024-07-29 10:36:23 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:37:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:37:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 15 @ 225 updates, score 7.184) (writing took 61.90330751566216 seconds)
2024-07-29 10:37:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2024-07-29 10:37:25 | INFO | train | epoch 015 | loss 7.571 | nll_loss 4.487 | ppl 22.43 | wps 554.2 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 225 | lr 2.7e-06 | gnorm 2.125 | train_wall 53 | gb_free 9.5 | wall 1142
2024-07-29 10:37:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:37:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:37:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:37:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000724
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34475.19921875Mb; avail=220574.4609375Mb
2024-07-29 10:37:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000069
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000738
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34475.19921875Mb; avail=220574.953125Mb
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34475.19921875Mb; avail=220574.953125Mb
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001303
2024-07-29 10:37:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34475.19921875Mb; avail=220574.953125Mb
2024-07-29 10:37:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:37:25 | INFO | fairseq.trainer | begin training epoch 16
2024-07-29 10:37:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:37:28 | INFO | train_inner | epoch 016:      1 / 15 loss=7.533, nll_loss=4.437, ppl=21.66, wps=128.7, ups=0.03, wpb=4529.5, bsz=304, num_updates=226, lr=2.712e-06, gnorm=2.007, train_wall=6, gb_free=9.8, wall=1145
2024-07-29 10:37:34 | INFO | train_inner | epoch 016:      3 / 15 loss=7.601, nll_loss=4.526, ppl=23.04, wps=1420.2, ups=0.37, wpb=3870.5, bsz=260, num_updates=228, lr=2.736e-06, gnorm=2.199, train_wall=5, gb_free=11.5, wall=1151
2024-07-29 10:37:40 | INFO | train_inner | epoch 016:      5 / 15 loss=7.565, nll_loss=4.479, ppl=22.3, wps=1504.3, ups=0.34, wpb=4404.5, bsz=276, num_updates=230, lr=2.76e-06, gnorm=2.03, train_wall=6, gb_free=10.1, wall=1157
2024-07-29 10:37:45 | INFO | train_inner | epoch 016:      7 / 15 loss=7.478, nll_loss=4.361, ppl=20.55, wps=1625.7, ups=0.35, wpb=4645.5, bsz=288, num_updates=232, lr=2.784e-06, gnorm=1.987, train_wall=6, gb_free=12.5, wall=1162
2024-07-29 10:37:50 | INFO | train_inner | epoch 016:      9 / 15 loss=7.599, nll_loss=4.525, ppl=23.02, wps=1373.6, ups=0.45, wpb=3072.5, bsz=169.5, num_updates=234, lr=2.808e-06, gnorm=2.524, train_wall=4, gb_free=9.6, wall=1167
2024-07-29 10:37:56 | INFO | train_inner | epoch 016:     11 / 15 loss=7.468, nll_loss=4.353, ppl=20.44, wps=1527.4, ups=0.32, wpb=4720.5, bsz=312, num_updates=236, lr=2.832e-06, gnorm=1.929, train_wall=6, gb_free=9.3, wall=1173
2024-07-29 10:38:02 | INFO | train_inner | epoch 016:     13 / 15 loss=7.503, nll_loss=4.406, ppl=21.2, wps=1614.1, ups=0.34, wpb=4763, bsz=312, num_updates=238, lr=2.856e-06, gnorm=1.929, train_wall=6, gb_free=10.6, wall=1179
2024-07-29 10:38:08 | INFO | train_inner | epoch 016:     15 / 15 loss=7.417, nll_loss=4.293, ppl=19.6, wps=1498, ups=0.33, wpb=4608.5, bsz=328, num_updates=240, lr=2.88e-06, gnorm=1.962, train_wall=6, gb_free=10.1, wall=1185
2024-07-29 10:38:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23138.48828125Mb; avail=231911.5859375Mb
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23138.98046875Mb; avail=231911.09375Mb
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002110
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23138.98046875Mb; avail=231911.09375Mb
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001935
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004946
2024-07-29 10:38:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23138.98046875Mb; avail=231911.09375Mb
2024-07-29 10:38:10 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.139 | nll_loss 3.745 | ppl 13.4 | wps 3193.2 | wpb 1361.3 | bsz 86.8 | num_updates 240 | best_loss 7.139
2024-07-29 10:38:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 240 updates
2024-07-29 10:38:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:38:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:39:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 16 @ 240 updates, score 7.139) (writing took 66.72740500699729 seconds)
2024-07-29 10:39:17 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2024-07-29 10:39:17 | INFO | train | epoch 016 | loss 7.515 | nll_loss 4.415 | ppl 21.33 | wps 578.4 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 240 | lr 2.88e-06 | gnorm 2.084 | train_wall 43 | gb_free 10.1 | wall 1254
2024-07-29 10:39:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:39:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:39:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:39:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000769
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46152.3984375Mb; avail=208897.0859375Mb
2024-07-29 10:39:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000739
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46152.890625Mb; avail=208897.0859375Mb
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46152.890625Mb; avail=208897.578125Mb
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000215
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001291
2024-07-29 10:39:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46152.890625Mb; avail=208897.0859375Mb
2024-07-29 10:39:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:39:17 | INFO | fairseq.trainer | begin training epoch 17
2024-07-29 10:39:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:39:23 | INFO | train_inner | epoch 017:      2 / 15 loss=7.483, nll_loss=4.373, ppl=20.72, wps=106.8, ups=0.03, wpb=3995, bsz=212, num_updates=242, lr=2.904e-06, gnorm=2.17, train_wall=6, gb_free=9.5, wall=1260
2024-07-29 10:39:29 | INFO | train_inner | epoch 017:      4 / 15 loss=7.467, nll_loss=4.36, ppl=20.53, wps=1728.2, ups=0.35, wpb=4876.5, bsz=308, num_updates=244, lr=2.928e-06, gnorm=2.084, train_wall=6, gb_free=9.9, wall=1265
2024-07-29 10:39:35 | INFO | train_inner | epoch 017:      6 / 15 loss=7.586, nll_loss=4.513, ppl=22.83, wps=1418.1, ups=0.33, wpb=4333.5, bsz=260, num_updates=246, lr=2.952e-06, gnorm=2.018, train_wall=6, gb_free=9.6, wall=1272
2024-07-29 10:39:41 | INFO | train_inner | epoch 017:      8 / 15 loss=7.472, nll_loss=4.362, ppl=20.56, wps=1584.5, ups=0.33, wpb=4802, bsz=308, num_updates=248, lr=2.976e-06, gnorm=1.937, train_wall=6, gb_free=9.8, wall=1278
2024-07-29 10:39:46 | INFO | train_inner | epoch 017:     10 / 15 loss=7.369, nll_loss=4.207, ppl=18.47, wps=1568.1, ups=0.35, wpb=4532.5, bsz=288, num_updates=250, lr=3e-06, gnorm=1.967, train_wall=6, gb_free=9.8, wall=1283
2024-07-29 10:40:02 | INFO | train_inner | epoch 017:     12 / 15 loss=7.479, nll_loss=4.375, ppl=20.76, wps=545, ups=0.13, wpb=4326.5, bsz=300, num_updates=252, lr=3.024e-06, gnorm=2.152, train_wall=16, gb_free=12.2, wall=1299
2024-07-29 10:40:07 | INFO | train_inner | epoch 017:     14 / 15 loss=7.376, nll_loss=4.249, ppl=19.01, wps=1447.9, ups=0.41, wpb=3564, bsz=273.5, num_updates=254, lr=3.048e-06, gnorm=2.317, train_wall=5, gb_free=14.2, wall=1304
2024-07-29 10:40:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25882.16015625Mb; avail=229167.9140625Mb
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000484
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25882.16015625Mb; avail=229167.9140625Mb
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002092
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25882.16015625Mb; avail=229167.9140625Mb
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001929
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004834
2024-07-29 10:40:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25882.16015625Mb; avail=229167.9140625Mb
2024-07-29 10:40:12 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.099 | nll_loss 3.691 | ppl 12.92 | wps 3200 | wpb 1361.3 | bsz 86.8 | num_updates 255 | best_loss 7.099
2024-07-29 10:40:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 255 updates
2024-07-29 10:40:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:40:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:41:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 17 @ 255 updates, score 7.099) (writing took 63.77706592716277 seconds)
2024-07-29 10:41:16 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2024-07-29 10:41:16 | INFO | train | epoch 017 | loss 7.459 | nll_loss 4.344 | ppl 20.31 | wps 543.5 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 255 | lr 3.06e-06 | gnorm 2.088 | train_wall 53 | gb_free 11.7 | wall 1373
2024-07-29 10:41:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:41:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:41:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:41:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.002883
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40483.08203125Mb; avail=214582.60546875Mb
2024-07-29 10:41:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000747
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40483.08203125Mb; avail=214583.09765625Mb
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40483.5703125Mb; avail=214582.60546875Mb
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000222
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001297
2024-07-29 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40483.078125Mb; avail=214582.60546875Mb
2024-07-29 10:41:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:41:16 | INFO | fairseq.trainer | begin training epoch 18
2024-07-29 10:41:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:41:19 | INFO | train_inner | epoch 018:      1 / 15 loss=7.347, nll_loss=4.199, ppl=18.37, wps=120.9, ups=0.03, wpb=4341.5, bsz=324, num_updates=256, lr=3.072e-06, gnorm=1.938, train_wall=6, gb_free=12.2, wall=1376
2024-07-29 10:41:25 | INFO | train_inner | epoch 018:      3 / 15 loss=7.546, nll_loss=4.456, ppl=21.95, wps=1351.6, ups=0.36, wpb=3706, bsz=232, num_updates=258, lr=3.096e-06, gnorm=2.237, train_wall=5, gb_free=13.2, wall=1382
2024-07-29 10:41:30 | INFO | train_inner | epoch 018:      5 / 15 loss=7.442, nll_loss=4.327, ppl=20.08, wps=1565.9, ups=0.34, wpb=4604, bsz=292, num_updates=260, lr=3.12e-06, gnorm=2.229, train_wall=6, gb_free=8.9, wall=1387
2024-07-29 10:41:36 | INFO | train_inner | epoch 018:      7 / 15 loss=7.471, nll_loss=4.371, ppl=20.69, wps=1338, ups=0.34, wpb=3898.5, bsz=248, num_updates=262, lr=3.144e-06, gnorm=2.219, train_wall=6, gb_free=11.1, wall=1393
2024-07-29 10:41:42 | INFO | train_inner | epoch 018:      9 / 15 loss=7.279, nll_loss=4.117, ppl=17.36, wps=1592.2, ups=0.33, wpb=4755, bsz=284, num_updates=264, lr=3.168e-06, gnorm=2.066, train_wall=6, gb_free=10.4, wall=1399
2024-07-29 10:41:47 | INFO | train_inner | epoch 018:     11 / 15 loss=7.589, nll_loss=4.519, ppl=22.92, wps=1400.8, ups=0.41, wpb=3413.5, bsz=201.5, num_updates=266, lr=3.192e-06, gnorm=2.519, train_wall=5, gb_free=10.1, wall=1404
2024-07-29 10:41:53 | INFO | train_inner | epoch 018:     13 / 15 loss=7.332, nll_loss=4.19, ppl=18.25, wps=1571.1, ups=0.33, wpb=4748, bsz=316, num_updates=268, lr=3.216e-06, gnorm=1.895, train_wall=6, gb_free=11.2, wall=1410
2024-07-29 10:41:59 | INFO | train_inner | epoch 018:     15 / 15 loss=7.423, nll_loss=4.313, ppl=19.87, wps=1645.9, ups=0.34, wpb=4842.5, bsz=316, num_updates=270, lr=3.24e-06, gnorm=1.91, train_wall=6, gb_free=12, wall=1416
2024-07-29 10:41:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25824.12890625Mb; avail=229241.9765625Mb
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25824.12890625Mb; avail=229241.9765625Mb
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002098
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25824.12890625Mb; avail=229241.9765625Mb
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001889
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004880
2024-07-29 10:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25824.62109375Mb; avail=229241.484375Mb
2024-07-29 10:42:01 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.061 | nll_loss 3.647 | ppl 12.53 | wps 3185.3 | wpb 1361.3 | bsz 86.8 | num_updates 270 | best_loss 7.061
2024-07-29 10:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 270 updates
2024-07-29 10:42:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:42:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:43:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 18 @ 270 updates, score 7.061) (writing took 64.8298543910496 seconds)
2024-07-29 10:43:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2024-07-29 10:43:06 | INFO | train | epoch 018 | loss 7.42 | nll_loss 4.302 | ppl 19.73 | wps 588.3 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 270 | lr 3.24e-06 | gnorm 2.134 | train_wall 43 | gb_free 12 | wall 1483
2024-07-29 10:43:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:43:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:43:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:43:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000754
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50353.0390625Mb; avail=204713.03515625Mb
2024-07-29 10:43:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000092
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000762
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50353.0390625Mb; avail=204713.03515625Mb
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50353.0390625Mb; avail=204713.03515625Mb
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000216
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001368
2024-07-29 10:43:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50353.0390625Mb; avail=204713.03515625Mb
2024-07-29 10:43:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:43:06 | INFO | fairseq.trainer | begin training epoch 19
2024-07-29 10:43:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:43:12 | INFO | train_inner | epoch 019:      2 / 15 loss=7.438, nll_loss=4.319, ppl=19.96, wps=109.2, ups=0.03, wpb=3987, bsz=240, num_updates=272, lr=3.264e-06, gnorm=2.28, train_wall=6, gb_free=12.9, wall=1489
2024-07-29 10:43:18 | INFO | train_inner | epoch 019:      4 / 15 loss=7.405, nll_loss=4.282, ppl=19.45, wps=1491.3, ups=0.34, wpb=4403.5, bsz=276, num_updates=274, lr=3.288e-06, gnorm=2.03, train_wall=6, gb_free=10.2, wall=1495
2024-07-29 10:43:24 | INFO | train_inner | epoch 019:      6 / 15 loss=7.308, nll_loss=4.152, ppl=17.77, wps=1602.4, ups=0.34, wpb=4722, bsz=316, num_updates=276, lr=3.312e-06, gnorm=1.877, train_wall=6, gb_free=11.9, wall=1501
2024-07-29 10:43:29 | INFO | train_inner | epoch 019:      8 / 15 loss=7.39, nll_loss=4.252, ppl=19.05, wps=1419.3, ups=0.35, wpb=4002.5, bsz=272, num_updates=278, lr=3.336e-06, gnorm=2.019, train_wall=6, gb_free=11.7, wall=1506
2024-07-29 10:43:45 | INFO | train_inner | epoch 019:     10 / 15 loss=7.261, nll_loss=4.1, ppl=17.15, wps=602.5, ups=0.13, wpb=4787.5, bsz=312, num_updates=280, lr=3.36e-06, gnorm=2.08, train_wall=16, gb_free=10.3, wall=1522
2024-07-29 10:43:50 | INFO | train_inner | epoch 019:     12 / 15 loss=7.382, nll_loss=4.252, ppl=19.05, wps=1597, ups=0.44, wpb=3645, bsz=249.5, num_updates=282, lr=3.384e-06, gnorm=2.165, train_wall=5, gb_free=9.3, wall=1527
2024-07-29 10:43:56 | INFO | train_inner | epoch 019:     14 / 15 loss=7.309, nll_loss=4.145, ppl=17.69, wps=1528.2, ups=0.33, wpb=4678, bsz=288, num_updates=284, lr=3.408e-06, gnorm=1.836, train_wall=6, gb_free=9.6, wall=1533
2024-07-29 10:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26851.671875Mb; avail=228214.43359375Mb
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000473
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26851.671875Mb; avail=228214.43359375Mb
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002094
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26851.671875Mb; avail=228214.43359375Mb
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001875
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004759
2024-07-29 10:43:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26851.671875Mb; avail=228214.43359375Mb
2024-07-29 10:44:01 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.021 | nll_loss 3.588 | ppl 12.02 | wps 3201.6 | wpb 1361.3 | bsz 86.8 | num_updates 285 | best_loss 7.021
2024-07-29 10:44:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 285 updates
2024-07-29 10:44:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 19 @ 285 updates, score 7.021) (writing took 62.80457938928157 seconds)
2024-07-29 10:45:04 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2024-07-29 10:45:04 | INFO | train | epoch 019 | loss 7.354 | nll_loss 4.212 | ppl 18.54 | wps 548.2 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 285 | lr 3.42e-06 | gnorm 2.033 | train_wall 53 | gb_free 9.3 | wall 1601
2024-07-29 10:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000682
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43171.390625Mb; avail=211862.72265625Mb
2024-07-29 10:45:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000746
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43171.390625Mb; avail=211862.72265625Mb
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43171.390625Mb; avail=211862.72265625Mb
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000232
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001332
2024-07-29 10:45:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43171.390625Mb; avail=211862.72265625Mb
2024-07-29 10:45:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:45:04 | INFO | fairseq.trainer | begin training epoch 20
2024-07-29 10:45:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:45:08 | INFO | train_inner | epoch 020:      1 / 15 loss=7.449, nll_loss=4.327, ppl=20.07, wps=122.2, ups=0.03, wpb=4365, bsz=256, num_updates=286, lr=3.432e-06, gnorm=1.995, train_wall=6, gb_free=8.9, wall=1604
2024-07-29 10:45:13 | INFO | train_inner | epoch 020:      3 / 15 loss=7.422, nll_loss=4.287, ppl=19.52, wps=1404.9, ups=0.37, wpb=3817.5, bsz=228, num_updates=288, lr=3.456e-06, gnorm=2.049, train_wall=5, gb_free=11.4, wall=1610
2024-07-29 10:45:18 | INFO | train_inner | epoch 020:      5 / 15 loss=7.247, nll_loss=4.075, ppl=16.85, wps=1323.7, ups=0.42, wpb=3145.5, bsz=213.5, num_updates=290, lr=3.48e-06, gnorm=2.173, train_wall=5, gb_free=13.4, wall=1615
2024-07-29 10:45:24 | INFO | train_inner | epoch 020:      7 / 15 loss=7.32, nll_loss=4.176, ppl=18.08, wps=1585.7, ups=0.34, wpb=4624, bsz=324, num_updates=292, lr=3.504e-06, gnorm=1.935, train_wall=6, gb_free=9.6, wall=1620
2024-07-29 10:45:29 | INFO | train_inner | epoch 020:      9 / 15 loss=7.462, nll_loss=4.358, ppl=20.51, wps=1474.1, ups=0.34, wpb=4346, bsz=248, num_updates=294, lr=3.528e-06, gnorm=2.357, train_wall=6, gb_free=10.1, wall=1626
2024-07-29 10:45:36 | INFO | train_inner | epoch 020:     11 / 15 loss=7.149, nll_loss=3.948, ppl=15.43, wps=1562.6, ups=0.33, wpb=4755, bsz=284, num_updates=296, lr=3.552e-06, gnorm=1.943, train_wall=6, gb_free=10.6, wall=1632
2024-07-29 10:45:41 | INFO | train_inner | epoch 020:     13 / 15 loss=7.213, nll_loss=4.04, ppl=16.45, wps=1663.4, ups=0.34, wpb=4849, bsz=340, num_updates=298, lr=3.576e-06, gnorm=1.825, train_wall=6, gb_free=11.7, wall=1638
2024-07-29 10:45:47 | INFO | train_inner | epoch 020:     15 / 15 loss=7.183, nll_loss=3.999, ppl=15.99, wps=1503, ups=0.33, wpb=4611.5, bsz=316, num_updates=300, lr=3.6e-06, gnorm=1.877, train_wall=6, gb_free=8.7, wall=1644
2024-07-29 10:45:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28488.19140625Mb; avail=226545.421875Mb
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28488.68359375Mb; avail=226545.421875Mb
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002116
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28488.68359375Mb; avail=226545.421875Mb
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001883
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004891
2024-07-29 10:45:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28488.68359375Mb; avail=226545.421875Mb
2024-07-29 10:45:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.999 | nll_loss 3.561 | ppl 11.8 | wps 3185.5 | wpb 1361.3 | bsz 86.8 | num_updates 300 | best_loss 6.999
2024-07-29 10:45:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 300 updates
2024-07-29 10:45:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:46:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:46:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 20 @ 300 updates, score 6.999) (writing took 62.94210045412183 seconds)
2024-07-29 10:46:53 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2024-07-29 10:46:53 | INFO | train | epoch 020 | loss 7.296 | nll_loss 4.14 | ppl 17.62 | wps 596.6 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 300 | lr 3.6e-06 | gnorm 2.026 | train_wall 43 | gb_free 8.7 | wall 1710
2024-07-29 10:46:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:46:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:46:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:46:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000781
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=52994.78125Mb; avail=202039.27734375Mb
2024-07-29 10:46:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000736
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52994.78125Mb; avail=202039.27734375Mb
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52994.78125Mb; avail=202039.27734375Mb
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000230
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001281
2024-07-29 10:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52994.78125Mb; avail=202039.27734375Mb
2024-07-29 10:46:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:46:53 | INFO | fairseq.trainer | begin training epoch 21
2024-07-29 10:46:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:46:59 | INFO | train_inner | epoch 021:      2 / 15 loss=7.323, nll_loss=4.16, ppl=17.87, wps=122.4, ups=0.03, wpb=4351.5, bsz=280, num_updates=302, lr=3.624e-06, gnorm=1.95, train_wall=6, gb_free=11.9, wall=1716
2024-07-29 10:47:04 | INFO | train_inner | epoch 021:      4 / 15 loss=7.16, nll_loss=3.967, ppl=15.64, wps=1648.3, ups=0.34, wpb=4844, bsz=332, num_updates=304, lr=3.648e-06, gnorm=1.886, train_wall=6, gb_free=9, wall=1721
2024-07-29 10:47:10 | INFO | train_inner | epoch 021:      6 / 15 loss=7.292, nll_loss=4.135, ppl=17.57, wps=1360, ups=0.35, wpb=3929.5, bsz=240, num_updates=306, lr=3.672e-06, gnorm=2.095, train_wall=6, gb_free=12.4, wall=1727
2024-07-29 10:47:16 | INFO | train_inner | epoch 021:      8 / 15 loss=7.178, nll_loss=3.99, ppl=15.89, wps=1421.2, ups=0.34, wpb=4200, bsz=272, num_updates=308, lr=3.696e-06, gnorm=1.933, train_wall=6, gb_free=9.3, wall=1733
2024-07-29 10:47:22 | INFO | train_inner | epoch 021:     10 / 15 loss=7.328, nll_loss=4.175, ppl=18.06, wps=1498.7, ups=0.33, wpb=4551, bsz=272, num_updates=310, lr=3.72e-06, gnorm=1.963, train_wall=6, gb_free=9.7, wall=1739
2024-07-29 10:47:27 | INFO | train_inner | epoch 021:     12 / 15 loss=7.196, nll_loss=4.019, ppl=16.21, wps=1465.6, ups=0.41, wpb=3588, bsz=229.5, num_updates=312, lr=3.744e-06, gnorm=2.241, train_wall=5, gb_free=9.2, wall=1744
2024-07-29 10:47:43 | INFO | train_inner | epoch 021:     14 / 15 loss=7.239, nll_loss=4.083, ppl=16.95, wps=598.5, ups=0.13, wpb=4765, bsz=340, num_updates=314, lr=3.768e-06, gnorm=2.125, train_wall=16, gb_free=9.3, wall=1760
2024-07-29 10:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=27124.0234375Mb; avail=227910.08203125Mb
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000470
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27124.0234375Mb; avail=227910.08203125Mb
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002083
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27124.0234375Mb; avail=227910.08203125Mb
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001899
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004767
2024-07-29 10:47:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=27124.0234375Mb; avail=227910.08203125Mb
2024-07-29 10:47:48 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.949 | nll_loss 3.503 | ppl 11.34 | wps 3205.2 | wpb 1361.3 | bsz 86.8 | num_updates 315 | best_loss 6.949
2024-07-29 10:47:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 315 updates
2024-07-29 10:47:48 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:48:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:48:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 21 @ 315 updates, score 6.949) (writing took 65.87403316702694 seconds)
2024-07-29 10:48:54 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2024-07-29 10:48:54 | INFO | train | epoch 021 | loss 7.249 | nll_loss 4.081 | ppl 16.93 | wps 533.4 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 315 | lr 3.78e-06 | gnorm 2.033 | train_wall 53 | gb_free 9.6 | wall 1831
2024-07-29 10:48:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:48:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:48:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:48:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000684
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43573.80859375Mb; avail=211452.34765625Mb
2024-07-29 10:48:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000088
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000782
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43574.30078125Mb; avail=211451.85546875Mb
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43573.80859375Mb; avail=211452.34765625Mb
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000223
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001325
2024-07-29 10:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43573.80859375Mb; avail=211452.34765625Mb
2024-07-29 10:48:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:48:54 | INFO | fairseq.trainer | begin training epoch 22
2024-07-29 10:48:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:48:57 | INFO | train_inner | epoch 022:      1 / 15 loss=7.198, nll_loss=4.011, ppl=16.12, wps=123.1, ups=0.03, wpb=4561.5, bsz=288, num_updates=316, lr=3.792e-06, gnorm=1.947, train_wall=6, gb_free=10.1, wall=1834
2024-07-29 10:49:03 | INFO | train_inner | epoch 022:      3 / 15 loss=7.227, nll_loss=4.046, ppl=16.51, wps=1525.1, ups=0.33, wpb=4553, bsz=276, num_updates=318, lr=3.816e-06, gnorm=1.953, train_wall=6, gb_free=10.2, wall=1840
2024-07-29 10:49:09 | INFO | train_inner | epoch 022:      5 / 15 loss=7.185, nll_loss=4.009, ppl=16.1, wps=1530.5, ups=0.35, wpb=4391, bsz=348, num_updates=320, lr=3.84e-06, gnorm=1.861, train_wall=6, gb_free=10.2, wall=1846
2024-07-29 10:49:15 | INFO | train_inner | epoch 022:      7 / 15 loss=7.175, nll_loss=3.963, ppl=15.59, wps=1603.9, ups=0.34, wpb=4719, bsz=268, num_updates=322, lr=3.864e-06, gnorm=2.134, train_wall=6, gb_free=9.9, wall=1852
2024-07-29 10:49:21 | INFO | train_inner | epoch 022:      9 / 15 loss=7.214, nll_loss=4.032, ppl=16.36, wps=1687.3, ups=0.35, wpb=4833.5, bsz=316, num_updates=324, lr=3.888e-06, gnorm=1.895, train_wall=6, gb_free=9.8, wall=1857
2024-07-29 10:49:26 | INFO | train_inner | epoch 022:     11 / 15 loss=7.269, nll_loss=4.111, ppl=17.28, wps=1332.1, ups=0.34, wpb=3906.5, bsz=260, num_updates=326, lr=3.912e-06, gnorm=2.301, train_wall=6, gb_free=9.7, wall=1863
2024-07-29 10:49:31 | INFO | train_inner | epoch 022:     13 / 15 loss=7.207, nll_loss=4.031, ppl=16.34, wps=1511, ups=0.42, wpb=3638, bsz=225.5, num_updates=328, lr=3.936e-06, gnorm=2.134, train_wall=5, gb_free=11.3, wall=1868
2024-07-29 10:49:37 | INFO | train_inner | epoch 022:     15 / 15 loss=7.254, nll_loss=4.087, ppl=16.99, wps=1285.1, ups=0.33, wpb=3907.5, bsz=216, num_updates=330, lr=3.96e-06, gnorm=2.245, train_wall=6, gb_free=8.8, wall=1874
2024-07-29 10:49:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34003.41015625Mb; avail=221022.23046875Mb
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000579
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34003.90234375Mb; avail=221022.23046875Mb
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002095
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34003.90234375Mb; avail=221022.23046875Mb
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001909
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004907
2024-07-29 10:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34003.90234375Mb; avail=221022.23046875Mb
2024-07-29 10:49:40 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.917 | nll_loss 3.457 | ppl 10.98 | wps 3186.3 | wpb 1361.3 | bsz 86.8 | num_updates 330 | best_loss 6.917
2024-07-29 10:49:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 330 updates
2024-07-29 10:49:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:50:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:50:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 22 @ 330 updates, score 6.917) (writing took 65.46734867524356 seconds)
2024-07-29 10:50:45 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2024-07-29 10:50:45 | INFO | train | epoch 022 | loss 7.208 | nll_loss 4.025 | ppl 16.28 | wps 584 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 330 | lr 3.96e-06 | gnorm 2.055 | train_wall 43 | gb_free 8.8 | wall 1942
2024-07-29 10:50:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:50:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:50:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:50:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000796
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=58493.66796875Mb; avail=196532.421875Mb
2024-07-29 10:50:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000087
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000741
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58493.66796875Mb; avail=196532.421875Mb
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58493.66796875Mb; avail=196532.421875Mb
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000212
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001264
2024-07-29 10:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=58493.66796875Mb; avail=196532.421875Mb
2024-07-29 10:50:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:50:45 | INFO | fairseq.trainer | begin training epoch 23
2024-07-29 10:50:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:50:51 | INFO | train_inner | epoch 023:      2 / 15 loss=7.175, nll_loss=3.994, ppl=15.94, wps=117.7, ups=0.03, wpb=4347.5, bsz=304, num_updates=332, lr=3.984e-06, gnorm=1.942, train_wall=6, gb_free=9.2, wall=1948
2024-07-29 10:50:57 | INFO | train_inner | epoch 023:      4 / 15 loss=7.311, nll_loss=4.162, ppl=17.9, wps=1267.2, ups=0.37, wpb=3461, bsz=184, num_updates=334, lr=4.008e-06, gnorm=2.583, train_wall=5, gb_free=9.7, wall=1954
2024-07-29 10:51:01 | INFO | train_inner | epoch 023:      6 / 15 loss=7.116, nll_loss=3.9, ppl=14.93, wps=1610.4, ups=0.43, wpb=3762.5, bsz=221.5, num_updates=336, lr=4.032e-06, gnorm=2.253, train_wall=5, gb_free=15, wall=1958
2024-07-29 10:51:07 | INFO | train_inner | epoch 023:      8 / 15 loss=7.297, nll_loss=4.131, ppl=17.52, wps=1552.4, ups=0.35, wpb=4391, bsz=300, num_updates=338, lr=4.056e-06, gnorm=1.951, train_wall=6, gb_free=12.5, wall=1964
2024-07-29 10:51:13 | INFO | train_inner | epoch 023:     10 / 15 loss=7.101, nll_loss=3.894, ppl=14.87, wps=1606, ups=0.34, wpb=4672, bsz=308, num_updates=340, lr=4.08e-06, gnorm=1.916, train_wall=6, gb_free=11.8, wall=1970
2024-07-29 10:51:29 | INFO | train_inner | epoch 023:     12 / 15 loss=7.132, nll_loss=3.929, ppl=15.23, wps=598, ups=0.13, wpb=4738.5, bsz=308, num_updates=342, lr=4.104e-06, gnorm=1.814, train_wall=16, gb_free=8.7, wall=1986
2024-07-29 10:51:35 | INFO | train_inner | epoch 023:     14 / 15 loss=7.049, nll_loss=3.825, ppl=14.17, wps=1470.2, ups=0.32, wpb=4623.5, bsz=308, num_updates=344, lr=4.128e-06, gnorm=1.919, train_wall=6, gb_free=8.9, wall=1992
2024-07-29 10:51:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35852.10546875Mb; avail=219214.0234375Mb
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000484
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35852.10546875Mb; avail=219214.0234375Mb
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002097
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35852.10546875Mb; avail=219214.0234375Mb
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001875
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004791
2024-07-29 10:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35852.10546875Mb; avail=219214.0234375Mb
2024-07-29 10:51:40 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.882 | nll_loss 3.412 | ppl 10.65 | wps 3195.8 | wpb 1361.3 | bsz 86.8 | num_updates 345 | best_loss 6.882
2024-07-29 10:51:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 345 updates
2024-07-29 10:51:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:52:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 23 @ 345 updates, score 6.882) (writing took 62.582424208987504 seconds)
2024-07-29 10:52:43 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2024-07-29 10:52:43 | INFO | train | epoch 023 | loss 7.164 | nll_loss 3.97 | ppl 15.67 | wps 550 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 345 | lr 4.14e-06 | gnorm 2.04 | train_wall 53 | gb_free 10.2 | wall 2060
2024-07-29 10:52:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:52:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:52:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:52:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000705
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=60126.30078125Mb; avail=194936.3828125Mb
2024-07-29 10:52:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000741
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60123.8984375Mb; avail=194940.7578125Mb
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60124.8828125Mb; avail=194938.7890625Mb
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001330
2024-07-29 10:52:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=60119.01171875Mb; avail=194945.15234375Mb
2024-07-29 10:52:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:52:43 | INFO | fairseq.trainer | begin training epoch 24
2024-07-29 10:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:52:45 | INFO | train_inner | epoch 024:      1 / 15 loss=7.238, nll_loss=4.054, ppl=16.61, wps=119.6, ups=0.03, wpb=4216.5, bsz=240, num_updates=346, lr=4.152e-06, gnorm=2.102, train_wall=6, gb_free=12, wall=2062
2024-07-29 10:52:51 | INFO | train_inner | epoch 024:      3 / 15 loss=7.044, nll_loss=3.819, ppl=14.11, wps=1548.6, ups=0.33, wpb=4695, bsz=300, num_updates=348, lr=4.176e-06, gnorm=1.877, train_wall=6, gb_free=9.8, wall=2068
2024-07-29 10:52:57 | INFO | train_inner | epoch 024:      5 / 15 loss=7.088, nll_loss=3.874, ppl=14.66, wps=1557.6, ups=0.35, wpb=4390.5, bsz=280, num_updates=350, lr=4.2e-06, gnorm=1.875, train_wall=6, gb_free=12, wall=2074
2024-07-29 10:53:03 | INFO | train_inner | epoch 024:      7 / 15 loss=7.222, nll_loss=4.051, ppl=16.58, wps=1476.1, ups=0.33, wpb=4434.5, bsz=260, num_updates=352, lr=4.224e-06, gnorm=2.051, train_wall=6, gb_free=10.1, wall=2080
2024-07-29 10:53:09 | INFO | train_inner | epoch 024:      9 / 15 loss=7.085, nll_loss=3.865, ppl=14.57, wps=1615.3, ups=0.34, wpb=4771.5, bsz=304, num_updates=354, lr=4.248e-06, gnorm=1.86, train_wall=6, gb_free=12.3, wall=2086
2024-07-29 10:53:15 | INFO | train_inner | epoch 024:     11 / 15 loss=7.027, nll_loss=3.796, ppl=13.89, wps=1556.9, ups=0.33, wpb=4665.5, bsz=340, num_updates=356, lr=4.272e-06, gnorm=2.015, train_wall=6, gb_free=11, wall=2092
2024-07-29 10:53:21 | INFO | train_inner | epoch 024:     13 / 15 loss=7.262, nll_loss=4.102, ppl=17.17, wps=1313.1, ups=0.34, wpb=3860, bsz=252, num_updates=358, lr=4.296e-06, gnorm=2.244, train_wall=6, gb_free=10.9, wall=2098
2024-07-29 10:53:26 | INFO | train_inner | epoch 024:     15 / 15 loss=7.013, nll_loss=3.775, ppl=13.69, wps=1545.7, ups=0.42, wpb=3709.5, bsz=253.5, num_updates=360, lr=4.32e-06, gnorm=2.123, train_wall=5, gb_free=10.3, wall=2103
2024-07-29 10:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50338.23828125Mb; avail=204727.80859375Mb
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000574
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50338.23828125Mb; avail=204727.80859375Mb
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002098
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50338.23828125Mb; avail=204727.80859375Mb
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001911
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004909
2024-07-29 10:53:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50338.73046875Mb; avail=204727.31640625Mb
2024-07-29 10:53:28 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.854 | nll_loss 3.372 | ppl 10.36 | wps 3189.1 | wpb 1361.3 | bsz 86.8 | num_updates 360 | best_loss 6.854
2024-07-29 10:53:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 360 updates
2024-07-29 10:53:28 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:54:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:54:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 24 @ 360 updates, score 6.854) (writing took 60.9374160990119 seconds)
2024-07-29 10:54:29 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2024-07-29 10:54:29 | INFO | train | epoch 024 | loss 7.116 | nll_loss 3.91 | ppl 15.03 | wps 609.6 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 360 | lr 4.32e-06 | gnorm 2.03 | train_wall 43 | gb_free 10.3 | wall 2166
2024-07-29 10:54:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:54:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:54:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:54:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000786
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=52139.61328125Mb; avail=202926.01171875Mb
2024-07-29 10:54:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000744
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52139.61328125Mb; avail=202926.50390625Mb
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52139.61328125Mb; avail=202926.01171875Mb
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000225
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001284
2024-07-29 10:54:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52139.61328125Mb; avail=202926.50390625Mb
2024-07-29 10:54:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:54:29 | INFO | fairseq.trainer | begin training epoch 25
2024-07-29 10:54:29 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:54:34 | INFO | train_inner | epoch 025:      2 / 15 loss=7.162, nll_loss=3.971, ppl=15.68, wps=96.5, ups=0.03, wpb=3287.5, bsz=201.5, num_updates=362, lr=4.344e-06, gnorm=2.413, train_wall=5, gb_free=14.1, wall=2171
2024-07-29 10:54:40 | INFO | train_inner | epoch 025:      4 / 15 loss=7.088, nll_loss=3.872, ppl=14.64, wps=1460.3, ups=0.32, wpb=4510.5, bsz=268, num_updates=364, lr=4.368e-06, gnorm=1.944, train_wall=6, gb_free=9.1, wall=2177
2024-07-29 10:54:46 | INFO | train_inner | epoch 025:      6 / 15 loss=7.042, nll_loss=3.818, ppl=14.1, wps=1619.7, ups=0.34, wpb=4703.5, bsz=316, num_updates=366, lr=4.392e-06, gnorm=1.828, train_wall=6, gb_free=12.1, wall=2183
2024-07-29 10:54:52 | INFO | train_inner | epoch 025:      8 / 15 loss=6.898, nll_loss=3.633, ppl=12.4, wps=1599.8, ups=0.33, wpb=4783.5, bsz=316, num_updates=368, lr=4.416e-06, gnorm=1.821, train_wall=6, gb_free=9.2, wall=2189
2024-07-29 10:54:58 | INFO | train_inner | epoch 025:     10 / 15 loss=7.148, nll_loss=3.949, ppl=15.44, wps=1407.1, ups=0.34, wpb=4079, bsz=292, num_updates=370, lr=4.44e-06, gnorm=1.971, train_wall=6, gb_free=11.4, wall=2195
2024-07-29 10:55:03 | INFO | train_inner | epoch 025:     12 / 15 loss=6.962, nll_loss=3.717, ppl=13.15, wps=1543.2, ups=0.35, wpb=4361, bsz=272, num_updates=372, lr=4.464e-06, gnorm=2.075, train_wall=6, gb_free=12.1, wall=2200
2024-07-29 10:55:09 | INFO | train_inner | epoch 025:     14 / 15 loss=7.07, nll_loss=3.86, ppl=14.52, wps=1547.8, ups=0.36, wpb=4280, bsz=272, num_updates=374, lr=4.488e-06, gnorm=2.005, train_wall=6, gb_free=12.4, wall=2206
2024-07-29 10:55:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40320.80859375Mb; avail=214745.28125Mb
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40321.296875Mb; avail=214744.7890625Mb
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002101
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40321.296875Mb; avail=214744.7890625Mb
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001912
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004914
2024-07-29 10:55:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40321.296875Mb; avail=214744.7890625Mb
2024-07-29 10:55:14 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.826 | nll_loss 3.333 | ppl 10.08 | wps 3179.6 | wpb 1361.3 | bsz 86.8 | num_updates 375 | best_loss 6.826
2024-07-29 10:55:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 375 updates
2024-07-29 10:55:14 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:56:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:56:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 25 @ 375 updates, score 6.826) (writing took 77.97874602675438 seconds)
2024-07-29 10:56:32 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2024-07-29 10:56:32 | INFO | train | epoch 025 | loss 7.047 | nll_loss 3.824 | ppl 14.17 | wps 525.9 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 375 | lr 4.5e-06 | gnorm 2.005 | train_wall 43 | gb_free 11.7 | wall 2289
2024-07-29 10:56:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:56:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:56:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:56:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000627
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=51464.25390625Mb; avail=203601.8515625Mb
2024-07-29 10:56:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000089
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000777
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51464.25390625Mb; avail=203601.8515625Mb
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51464.25390625Mb; avail=203601.8515625Mb
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000202
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001351
2024-07-29 10:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=51464.74609375Mb; avail=203601.359375Mb
2024-07-29 10:56:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:56:32 | INFO | fairseq.trainer | begin training epoch 26
2024-07-29 10:56:32 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:56:35 | INFO | train_inner | epoch 026:      1 / 15 loss=7.035, nll_loss=3.811, ppl=14.03, wps=101.6, ups=0.02, wpb=4377.5, bsz=312, num_updates=376, lr=4.512e-06, gnorm=1.942, train_wall=6, gb_free=11.4, wall=2292
2024-07-29 10:56:41 | INFO | train_inner | epoch 026:      3 / 15 loss=6.884, nll_loss=3.616, ppl=12.26, wps=1641.5, ups=0.34, wpb=4803, bsz=320, num_updates=378, lr=4.536e-06, gnorm=1.823, train_wall=6, gb_free=10.1, wall=2298
2024-07-29 10:56:47 | INFO | train_inner | epoch 026:      5 / 15 loss=7.098, nll_loss=3.878, ppl=14.7, wps=1390.7, ups=0.34, wpb=4142.5, bsz=252, num_updates=380, lr=4.56e-06, gnorm=2.036, train_wall=6, gb_free=9.6, wall=2304
2024-07-29 10:56:53 | INFO | train_inner | epoch 026:      7 / 15 loss=6.926, nll_loss=3.672, ppl=12.74, wps=1563.2, ups=0.33, wpb=4675.5, bsz=308, num_updates=382, lr=4.584e-06, gnorm=2.006, train_wall=6, gb_free=10.5, wall=2310
2024-07-29 10:56:59 | INFO | train_inner | epoch 026:      9 / 15 loss=7.015, nll_loss=3.773, ppl=13.67, wps=1449.4, ups=0.35, wpb=4175.5, bsz=240, num_updates=384, lr=4.608e-06, gnorm=2.119, train_wall=6, gb_free=10.6, wall=2315
2024-07-29 10:57:03 | INFO | train_inner | epoch 026:     11 / 15 loss=6.957, nll_loss=3.707, ppl=13.06, wps=1528.9, ups=0.43, wpb=3576, bsz=265.5, num_updates=386, lr=4.632e-06, gnorm=2.196, train_wall=5, gb_free=16, wall=2320
2024-07-29 10:57:09 | INFO | train_inner | epoch 026:     13 / 15 loss=6.97, nll_loss=3.712, ppl=13.1, wps=1557.3, ups=0.33, wpb=4698, bsz=296, num_updates=388, lr=4.656e-06, gnorm=1.774, train_wall=6, gb_free=10.2, wall=2326
2024-07-29 10:57:15 | INFO | train_inner | epoch 026:     15 / 15 loss=7.112, nll_loss=3.901, ppl=14.94, wps=1429.8, ups=0.33, wpb=4286, bsz=232, num_updates=390, lr=4.68e-06, gnorm=2.162, train_wall=6, gb_free=9.1, wall=2332
2024-07-29 10:57:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39655.19921875Mb; avail=215410.84375Mb
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000583
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39655.19921875Mb; avail=215410.84375Mb
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002107
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39655.19921875Mb; avail=215410.84375Mb
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001905
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004926
2024-07-29 10:57:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39655.19921875Mb; avail=215410.84375Mb
2024-07-29 10:57:17 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.796 | nll_loss 3.294 | ppl 9.81 | wps 3185.9 | wpb 1361.3 | bsz 86.8 | num_updates 390 | best_loss 6.796
2024-07-29 10:57:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 390 updates
2024-07-29 10:57:17 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:57:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:58:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 26 @ 390 updates, score 6.796) (writing took 63.19888108596206 seconds)
2024-07-29 10:58:21 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2024-07-29 10:58:21 | INFO | train | epoch 026 | loss 6.992 | nll_loss 3.749 | ppl 13.44 | wps 595.6 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 390 | lr 4.68e-06 | gnorm 2.01 | train_wall 43 | gb_free 9.1 | wall 2398
2024-07-29 10:58:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 10:58:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 10:58:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 10:58:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000744
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=52882.19140625Mb; avail=202183.96484375Mb
2024-07-29 10:58:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000088
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000777
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52882.19140625Mb; avail=202183.96484375Mb
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52882.19140625Mb; avail=202183.96484375Mb
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000232
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001329
2024-07-29 10:58:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=52882.19140625Mb; avail=202183.96484375Mb
2024-07-29 10:58:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 10:58:21 | INFO | fairseq.trainer | begin training epoch 27
2024-07-29 10:58:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 10:58:27 | INFO | train_inner | epoch 027:      2 / 15 loss=7.063, nll_loss=3.836, ppl=14.28, wps=110.7, ups=0.03, wpb=3952.5, bsz=212, num_updates=392, lr=4.704e-06, gnorm=2.142, train_wall=6, gb_free=9.4, wall=2404
2024-07-29 10:58:31 | INFO | train_inner | epoch 027:      4 / 15 loss=6.902, nll_loss=3.643, ppl=12.49, wps=1579.3, ups=0.45, wpb=3478.5, bsz=237.5, num_updates=394, lr=4.728e-06, gnorm=2.301, train_wall=4, gb_free=10.2, wall=2408
2024-07-29 10:58:37 | INFO | train_inner | epoch 027:      6 / 15 loss=6.958, nll_loss=3.706, ppl=13.05, wps=1563.1, ups=0.34, wpb=4631.5, bsz=288, num_updates=396, lr=4.752e-06, gnorm=1.877, train_wall=6, gb_free=9.3, wall=2414
2024-07-29 10:58:43 | INFO | train_inner | epoch 027:      8 / 15 loss=6.912, nll_loss=3.652, ppl=12.57, wps=1543, ups=0.33, wpb=4648.5, bsz=300, num_updates=398, lr=4.776e-06, gnorm=1.95, train_wall=6, gb_free=9.7, wall=2420
2024-07-29 10:58:49 | INFO | train_inner | epoch 027:     10 / 15 loss=6.86, nll_loss=3.589, ppl=12.03, wps=1596.8, ups=0.34, wpb=4728, bsz=332, num_updates=400, lr=4.8e-06, gnorm=1.847, train_wall=6, gb_free=10.6, wall=2426
2024-07-29 10:58:55 | INFO | train_inner | epoch 027:     12 / 15 loss=6.916, nll_loss=3.653, ppl=12.58, wps=1438.8, ups=0.34, wpb=4277.5, bsz=308, num_updates=402, lr=4.824e-06, gnorm=2.024, train_wall=6, gb_free=10.2, wall=2432
2024-07-29 10:59:01 | INFO | train_inner | epoch 027:     14 / 15 loss=6.945, nll_loss=3.697, ppl=12.97, wps=1505.7, ups=0.33, wpb=4542.5, bsz=296, num_updates=404, lr=4.848e-06, gnorm=1.854, train_wall=6, gb_free=9.4, wall=2438
2024-07-29 10:59:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41068.12109375Mb; avail=213998.01171875Mb
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000581
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41068.12109375Mb; avail=213998.01171875Mb
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002122
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41068.12109375Mb; avail=213998.01171875Mb
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001909
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004945
2024-07-29 10:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41068.12109375Mb; avail=213998.01171875Mb
2024-07-29 10:59:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.762 | nll_loss 3.255 | ppl 9.55 | wps 3179.8 | wpb 1361.3 | bsz 86.8 | num_updates 405 | best_loss 6.762
2024-07-29 10:59:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 405 updates
2024-07-29 10:59:06 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 10:59:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:00:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 27 @ 405 updates, score 6.762) (writing took 77.7649381798692 seconds)
2024-07-29 11:00:24 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2024-07-29 11:00:24 | INFO | train | epoch 027 | loss 6.949 | nll_loss 3.698 | ppl 12.98 | wps 525 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 405 | lr 4.86e-06 | gnorm 2.028 | train_wall 43 | gb_free 9.4 | wall 2521
2024-07-29 11:00:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:00:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:00:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:00:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000646
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=45909.83984375Mb; avail=209156.3125Mb
2024-07-29 11:00:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000070
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000781
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45909.83984375Mb; avail=209156.3125Mb
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000037
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45909.83984375Mb; avail=209156.3125Mb
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000211
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001338
2024-07-29 11:00:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=45910.33203125Mb; avail=209155.8203125Mb
2024-07-29 11:00:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:00:24 | INFO | fairseq.trainer | begin training epoch 28
2024-07-29 11:00:24 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:00:27 | INFO | train_inner | epoch 028:      1 / 15 loss=7.065, nll_loss=3.85, ppl=14.42, wps=100.6, ups=0.02, wpb=4333.5, bsz=256, num_updates=406, lr=4.872e-06, gnorm=2.123, train_wall=6, gb_free=10.1, wall=2524
2024-07-29 11:00:33 | INFO | train_inner | epoch 028:      3 / 15 loss=6.889, nll_loss=3.634, ppl=12.41, wps=1658.4, ups=0.34, wpb=4844, bsz=348, num_updates=408, lr=4.896e-06, gnorm=1.842, train_wall=6, gb_free=8.8, wall=2530
2024-07-29 11:00:39 | INFO | train_inner | epoch 028:      5 / 15 loss=6.852, nll_loss=3.576, ppl=11.93, wps=1556.1, ups=0.34, wpb=4522, bsz=296, num_updates=410, lr=4.92e-06, gnorm=1.868, train_wall=6, gb_free=12.2, wall=2536
2024-07-29 11:00:44 | INFO | train_inner | epoch 028:      7 / 15 loss=6.979, nll_loss=3.743, ppl=13.39, wps=1531.7, ups=0.37, wpb=4174, bsz=272, num_updates=412, lr=4.944e-06, gnorm=2.151, train_wall=5, gb_free=11, wall=2541
2024-07-29 11:00:50 | INFO | train_inner | epoch 028:      9 / 15 loss=6.943, nll_loss=3.67, ppl=12.73, wps=1543.3, ups=0.34, wpb=4556, bsz=260, num_updates=414, lr=4.968e-06, gnorm=1.936, train_wall=6, gb_free=9.6, wall=2547
2024-07-29 11:00:56 | INFO | train_inner | epoch 028:     11 / 15 loss=6.915, nll_loss=3.649, ppl=12.54, wps=1403.7, ups=0.33, wpb=4220.5, bsz=260, num_updates=416, lr=4.992e-06, gnorm=1.981, train_wall=6, gb_free=8.8, wall=2553
2024-07-29 11:01:01 | INFO | train_inner | epoch 028:     13 / 15 loss=6.835, nll_loss=3.548, ppl=11.7, wps=1492.8, ups=0.42, wpb=3579.5, bsz=217.5, num_updates=418, lr=5.016e-06, gnorm=2.208, train_wall=5, gb_free=10.5, wall=2558
2024-07-29 11:01:07 | INFO | train_inner | epoch 028:     15 / 15 loss=6.932, nll_loss=3.677, ppl=12.79, wps=1444.8, ups=0.34, wpb=4250.5, bsz=280, num_updates=420, lr=5.04e-06, gnorm=1.932, train_wall=6, gb_free=9.6, wall=2564
2024-07-29 11:01:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36724.9375Mb; avail=218341.1484375Mb
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000573
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36724.9375Mb; avail=218341.1484375Mb
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002115
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36724.9375Mb; avail=218341.1484375Mb
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001902
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004936
2024-07-29 11:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36725.4296875Mb; avail=218340.65625Mb
2024-07-29 11:01:09 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.737 | nll_loss 3.215 | ppl 9.29 | wps 3189.8 | wpb 1361.3 | bsz 86.8 | num_updates 420 | best_loss 6.737
2024-07-29 11:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 420 updates
2024-07-29 11:01:09 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:01:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 28 @ 420 updates, score 6.737) (writing took 64.51485399529338 seconds)
2024-07-29 11:02:13 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2024-07-29 11:02:13 | INFO | train | epoch 028 | loss 6.912 | nll_loss 3.651 | ppl 12.56 | wps 591.4 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 420 | lr 5.04e-06 | gnorm 1.977 | train_wall 43 | gb_free 9.6 | wall 2630
2024-07-29 11:02:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:02:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:02:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:02:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000784
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46211.7578125Mb; avail=208854.4140625Mb
2024-07-29 11:02:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000153
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000813
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46211.7578125Mb; avail=208854.4140625Mb
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46211.7578125Mb; avail=208853.921875Mb
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000230
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001360
2024-07-29 11:02:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46212.25Mb; avail=208853.921875Mb
2024-07-29 11:02:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:02:14 | INFO | fairseq.trainer | begin training epoch 29
2024-07-29 11:02:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:02:19 | INFO | train_inner | epoch 029:      2 / 15 loss=6.913, nll_loss=3.649, ppl=12.55, wps=112.6, ups=0.03, wpb=4076.5, bsz=252, num_updates=422, lr=5.064e-06, gnorm=2.057, train_wall=6, gb_free=11.6, wall=2636
2024-07-29 11:02:24 | INFO | train_inner | epoch 029:      4 / 15 loss=6.907, nll_loss=3.642, ppl=12.49, wps=1537.6, ups=0.44, wpb=3534, bsz=229.5, num_updates=424, lr=5.088e-06, gnorm=2.135, train_wall=5, gb_free=10.6, wall=2641
2024-07-29 11:02:30 | INFO | train_inner | epoch 029:      6 / 15 loss=6.879, nll_loss=3.609, ppl=12.2, wps=1476.3, ups=0.34, wpb=4306.5, bsz=312, num_updates=426, lr=5.112e-06, gnorm=1.984, train_wall=6, gb_free=12.2, wall=2647
2024-07-29 11:02:36 | INFO | train_inner | epoch 029:      8 / 15 loss=6.901, nll_loss=3.634, ppl=12.42, wps=1572.5, ups=0.33, wpb=4755.5, bsz=284, num_updates=428, lr=5.136e-06, gnorm=2.01, train_wall=6, gb_free=11.7, wall=2653
2024-07-29 11:02:42 | INFO | train_inner | epoch 029:     10 / 15 loss=6.744, nll_loss=3.421, ppl=10.71, wps=1613.1, ups=0.34, wpb=4813, bsz=316, num_updates=430, lr=5.16e-06, gnorm=1.781, train_wall=6, gb_free=9.5, wall=2659
2024-07-29 11:02:48 | INFO | train_inner | epoch 029:     12 / 15 loss=6.866, nll_loss=3.588, ppl=12.03, wps=1480.7, ups=0.33, wpb=4447, bsz=276, num_updates=432, lr=5.184e-06, gnorm=1.973, train_wall=6, gb_free=10.9, wall=2665
2024-07-29 11:02:53 | INFO | train_inner | epoch 029:     14 / 15 loss=6.842, nll_loss=3.555, ppl=11.76, wps=1475.1, ups=0.36, wpb=4127, bsz=272, num_updates=434, lr=5.208e-06, gnorm=2.053, train_wall=6, gb_free=9.9, wall=2670
2024-07-29 11:02:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34352.20703125Mb; avail=220713.92578125Mb
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000568
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34352.69921875Mb; avail=220713.43359375Mb
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002095
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34352.69921875Mb; avail=220713.43359375Mb
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001910
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004896
2024-07-29 11:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34352.69921875Mb; avail=220713.43359375Mb
2024-07-29 11:02:58 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.717 | nll_loss 3.194 | ppl 9.15 | wps 3187.5 | wpb 1361.3 | bsz 86.8 | num_updates 435 | best_loss 6.717
2024-07-29 11:02:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 435 updates
2024-07-29 11:02:58 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:03:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:04:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 29 @ 435 updates, score 6.717) (writing took 77.69450112478808 seconds)
2024-07-29 11:04:16 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2024-07-29 11:04:16 | INFO | train | epoch 029 | loss 6.863 | nll_loss 3.583 | ppl 11.98 | wps 527.9 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 435 | lr 5.22e-06 | gnorm 1.999 | train_wall 43 | gb_free 9.8 | wall 2753
2024-07-29 11:04:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:04:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:04:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:04:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000647
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41918.78515625Mb; avail=213147.3828125Mb
2024-07-29 11:04:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000076
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000702
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41919.27734375Mb; avail=213146.890625Mb
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41919.27734375Mb; avail=213146.890625Mb
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000205
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001218
2024-07-29 11:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41919.27734375Mb; avail=213146.890625Mb
2024-07-29 11:04:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:04:16 | INFO | fairseq.trainer | begin training epoch 30
2024-07-29 11:04:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:04:19 | INFO | train_inner | epoch 030:      1 / 15 loss=6.877, nll_loss=3.607, ppl=12.18, wps=103.4, ups=0.02, wpb=4443, bsz=296, num_updates=436, lr=5.232e-06, gnorm=1.952, train_wall=6, gb_free=10.9, wall=2756
2024-07-29 11:04:25 | INFO | train_inner | epoch 030:      3 / 15 loss=6.883, nll_loss=3.621, ppl=12.3, wps=1367.7, ups=0.35, wpb=3869, bsz=272, num_updates=438, lr=5.256e-06, gnorm=2.047, train_wall=6, gb_free=12, wall=2762
2024-07-29 11:04:30 | INFO | train_inner | epoch 030:      5 / 15 loss=6.913, nll_loss=3.661, ppl=12.65, wps=1507.7, ups=0.35, wpb=4288.5, bsz=268, num_updates=440, lr=5.28e-06, gnorm=1.941, train_wall=6, gb_free=9.3, wall=2767
2024-07-29 11:04:35 | INFO | train_inner | epoch 030:      7 / 15 loss=6.752, nll_loss=3.449, ppl=10.92, wps=1600.6, ups=0.43, wpb=3690, bsz=245.5, num_updates=442, lr=5.304e-06, gnorm=2.091, train_wall=5, gb_free=10.8, wall=2772
2024-07-29 11:04:41 | INFO | train_inner | epoch 030:      9 / 15 loss=6.71, nll_loss=3.393, ppl=10.51, wps=1657.6, ups=0.34, wpb=4941.5, bsz=304, num_updates=444, lr=5.328e-06, gnorm=1.919, train_wall=6, gb_free=9.6, wall=2778
2024-07-29 11:04:47 | INFO | train_inner | epoch 030:     11 / 15 loss=6.835, nll_loss=3.563, ppl=11.82, wps=1530.6, ups=0.34, wpb=4472.5, bsz=288, num_updates=446, lr=5.352e-06, gnorm=1.948, train_wall=6, gb_free=12.1, wall=2784
2024-07-29 11:04:53 | INFO | train_inner | epoch 030:     13 / 15 loss=6.801, nll_loss=3.493, ppl=11.26, wps=1582.2, ups=0.34, wpb=4636, bsz=272, num_updates=448, lr=5.376e-06, gnorm=1.93, train_wall=6, gb_free=11.2, wall=2790
2024-07-29 11:04:59 | INFO | train_inner | epoch 030:     15 / 15 loss=6.783, nll_loss=3.48, ppl=11.16, wps=1384, ups=0.32, wpb=4339, bsz=276, num_updates=450, lr=5.4e-06, gnorm=2.032, train_wall=6, gb_free=8.9, wall=2796
2024-07-29 11:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30076.20703125Mb; avail=224989.390625Mb
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000574
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30076.69921875Mb; avail=224989.390625Mb
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002098
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30076.69921875Mb; avail=224989.390625Mb
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001936
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004937
2024-07-29 11:04:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30076.69921875Mb; avail=224989.390625Mb
2024-07-29 11:05:01 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.696 | nll_loss 3.154 | ppl 8.9 | wps 3187.8 | wpb 1361.3 | bsz 86.8 | num_updates 450 | best_loss 6.696
2024-07-29 11:05:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 450 updates
2024-07-29 11:05:01 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:05:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:06:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 30 @ 450 updates, score 6.696) (writing took 64.5297169322148 seconds)
2024-07-29 11:06:06 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2024-07-29 11:06:06 | INFO | train | epoch 030 | loss 6.813 | nll_loss 3.526 | ppl 11.52 | wps 590.3 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 450 | lr 5.4e-06 | gnorm 1.981 | train_wall 43 | gb_free 8.9 | wall 2863
2024-07-29 11:06:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:06:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:06:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:06:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000793
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=38301.6875Mb; avail=216764.953125Mb
2024-07-29 11:06:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000761
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38300.6953125Mb; avail=216765.453125Mb
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38301.1875Mb; avail=216765.453125Mb
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001328
2024-07-29 11:06:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=38301.1875Mb; avail=216764.9609375Mb
2024-07-29 11:06:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:06:06 | INFO | fairseq.trainer | begin training epoch 31
2024-07-29 11:06:06 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:06:12 | INFO | train_inner | epoch 031:      2 / 15 loss=6.903, nll_loss=3.6, ppl=12.12, wps=118.7, ups=0.03, wpb=4317, bsz=272, num_updates=452, lr=5.424e-06, gnorm=2.056, train_wall=6, gb_free=10.4, wall=2869
2024-07-29 11:06:17 | INFO | train_inner | epoch 031:      4 / 15 loss=6.772, nll_loss=3.473, ppl=11.1, wps=1481.7, ups=0.36, wpb=4063, bsz=288, num_updates=454, lr=5.448e-06, gnorm=2.02, train_wall=5, gb_free=11.3, wall=2874
2024-07-29 11:06:23 | INFO | train_inner | epoch 031:      6 / 15 loss=6.686, nll_loss=3.371, ppl=10.35, wps=1669.6, ups=0.35, wpb=4745, bsz=348, num_updates=456, lr=5.472e-06, gnorm=1.987, train_wall=6, gb_free=10.9, wall=2880
2024-07-29 11:06:29 | INFO | train_inner | epoch 031:      8 / 15 loss=6.84, nll_loss=3.549, ppl=11.7, wps=1500.9, ups=0.32, wpb=4621.5, bsz=292, num_updates=458, lr=5.496e-06, gnorm=1.925, train_wall=6, gb_free=8.8, wall=2886
2024-07-29 11:06:35 | INFO | train_inner | epoch 031:     10 / 15 loss=6.791, nll_loss=3.505, ppl=11.36, wps=1615.2, ups=0.34, wpb=4683.5, bsz=300, num_updates=460, lr=5.52e-06, gnorm=1.913, train_wall=6, gb_free=12.1, wall=2892
2024-07-29 11:06:40 | INFO | train_inner | epoch 031:     12 / 15 loss=6.774, nll_loss=3.472, ppl=11.09, wps=1506.6, ups=0.42, wpb=3591.5, bsz=209.5, num_updates=462, lr=5.544e-06, gnorm=2.252, train_wall=5, gb_free=12, wall=2897
2024-07-29 11:06:45 | INFO | train_inner | epoch 031:     14 / 15 loss=6.826, nll_loss=3.534, ppl=11.59, wps=1420.1, ups=0.36, wpb=3974.5, bsz=224, num_updates=464, lr=5.568e-06, gnorm=2.146, train_wall=6, gb_free=13.2, wall=2902
2024-07-29 11:06:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28456.84765625Mb; avail=226609.28515625Mb
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000589
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28456.84765625Mb; avail=226609.28515625Mb
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002107
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28456.84765625Mb; avail=226609.28515625Mb
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001920
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004945
2024-07-29 11:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28456.84765625Mb; avail=226609.28515625Mb
2024-07-29 11:06:50 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.677 | nll_loss 3.133 | ppl 8.77 | wps 3180.3 | wpb 1361.3 | bsz 86.8 | num_updates 465 | best_loss 6.677
2024-07-29 11:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 465 updates
2024-07-29 11:06:50 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:07:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:07:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 31 @ 465 updates, score 6.677) (writing took 65.51919660717249 seconds)
2024-07-29 11:07:56 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2024-07-29 11:07:56 | INFO | train | epoch 031 | loss 6.797 | nll_loss 3.497 | ppl 11.29 | wps 587.7 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 465 | lr 5.58e-06 | gnorm 2.033 | train_wall 42 | gb_free 11.8 | wall 2973
2024-07-29 11:07:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:07:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:07:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:07:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000748
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=48542.11328125Mb; avail=206523.9296875Mb
2024-07-29 11:07:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000744
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48542.11328125Mb; avail=206523.9296875Mb
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000034
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48542.11328125Mb; avail=206523.9296875Mb
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001284
2024-07-29 11:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=48542.11328125Mb; avail=206523.9296875Mb
2024-07-29 11:07:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:07:56 | INFO | fairseq.trainer | begin training epoch 32
2024-07-29 11:07:56 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:07:59 | INFO | train_inner | epoch 032:      1 / 15 loss=6.762, nll_loss=3.441, ppl=10.86, wps=123.9, ups=0.03, wpb=4568.5, bsz=268, num_updates=466, lr=5.592e-06, gnorm=1.919, train_wall=6, gb_free=9.7, wall=2976
2024-07-29 11:08:20 | INFO | train_inner | epoch 032:      3 / 15 loss=6.803, nll_loss=3.499, ppl=11.31, wps=423.9, ups=0.1, wpb=4367.5, bsz=284, num_updates=468, lr=5.616e-06, gnorm=1.902, train_wall=21, gb_free=11.6, wall=2997
2024-07-29 11:08:25 | INFO | train_inner | epoch 032:      5 / 15 loss=6.798, nll_loss=3.504, ppl=11.35, wps=1397.5, ups=0.34, wpb=4054, bsz=228, num_updates=470, lr=5.64e-06, gnorm=2.095, train_wall=6, gb_free=12.2, wall=3002
2024-07-29 11:08:31 | INFO | train_inner | epoch 032:      7 / 15 loss=6.749, nll_loss=3.444, ppl=10.88, wps=1622.9, ups=0.34, wpb=4756, bsz=336, num_updates=472, lr=5.664e-06, gnorm=1.797, train_wall=6, gb_free=8.6, wall=3008
2024-07-29 11:08:37 | INFO | train_inner | epoch 032:      9 / 15 loss=6.601, nll_loss=3.259, ppl=9.58, wps=1457.8, ups=0.34, wpb=4264, bsz=296, num_updates=474, lr=5.688e-06, gnorm=2.041, train_wall=6, gb_free=10, wall=3014
2024-07-29 11:08:43 | INFO | train_inner | epoch 032:     11 / 15 loss=6.801, nll_loss=3.512, ppl=11.41, wps=1364, ups=0.32, wpb=4225, bsz=236, num_updates=476, lr=5.712e-06, gnorm=2.05, train_wall=6, gb_free=9.2, wall=3020
2024-07-29 11:08:49 | INFO | train_inner | epoch 032:     13 / 15 loss=6.699, nll_loss=3.391, ppl=10.49, wps=1667.9, ups=0.35, wpb=4806.5, bsz=332, num_updates=478, lr=5.736e-06, gnorm=1.804, train_wall=6, gb_free=11.8, wall=3026
2024-07-29 11:08:54 | INFO | train_inner | epoch 032:     15 / 15 loss=6.652, nll_loss=3.313, ppl=9.94, wps=1555.4, ups=0.42, wpb=3701.5, bsz=249.5, num_updates=480, lr=5.76e-06, gnorm=2.143, train_wall=5, gb_free=9.9, wall=3031
2024-07-29 11:08:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26981.08984375Mb; avail=228084.82421875Mb
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000465
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26981.08984375Mb; avail=228084.82421875Mb
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002104
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26981.08984375Mb; avail=228084.82421875Mb
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001898
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004779
2024-07-29 11:08:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26981.08984375Mb; avail=228084.82421875Mb
2024-07-29 11:08:56 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.659 | nll_loss 3.107 | ppl 8.62 | wps 3184.9 | wpb 1361.3 | bsz 86.8 | num_updates 480 | best_loss 6.659
2024-07-29 11:08:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 480 updates
2024-07-29 11:08:56 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:09:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:09:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 32 @ 480 updates, score 6.659) (writing took 61.64472905173898 seconds)
2024-07-29 11:09:58 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2024-07-29 11:09:58 | INFO | train | epoch 032 | loss 6.731 | nll_loss 3.419 | ppl 10.7 | wps 531.7 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 480 | lr 5.76e-06 | gnorm 1.974 | train_wall 58 | gb_free 9.9 | wall 3095
2024-07-29 11:09:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:09:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:09:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:09:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000700
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34477.73828125Mb; avail=220588.3828125Mb
2024-07-29 11:09:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000091
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000766
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34478.23046875Mb; avail=220587.890625Mb
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34477.73828125Mb; avail=220588.3828125Mb
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000227
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001316
2024-07-29 11:09:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34477.73828125Mb; avail=220588.3828125Mb
2024-07-29 11:09:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:09:58 | INFO | fairseq.trainer | begin training epoch 33
2024-07-29 11:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:10:03 | INFO | train_inner | epoch 033:      2 / 15 loss=6.693, nll_loss=3.372, ppl=10.35, wps=131.6, ups=0.03, wpb=4583, bsz=288, num_updates=482, lr=5.784e-06, gnorm=1.817, train_wall=6, gb_free=11.7, wall=3100
2024-07-29 11:10:09 | INFO | train_inner | epoch 033:      4 / 15 loss=6.644, nll_loss=3.308, ppl=9.9, wps=1443, ups=0.34, wpb=4185, bsz=316, num_updates=484, lr=5.808e-06, gnorm=2.024, train_wall=6, gb_free=11.3, wall=3106
2024-07-29 11:10:15 | INFO | train_inner | epoch 033:      6 / 15 loss=6.657, nll_loss=3.321, ppl=9.99, wps=1549.7, ups=0.33, wpb=4684, bsz=296, num_updates=486, lr=5.832e-06, gnorm=1.734, train_wall=6, gb_free=10.1, wall=3112
2024-07-29 11:10:20 | INFO | train_inner | epoch 033:      8 / 15 loss=6.717, nll_loss=3.398, ppl=10.54, wps=1389.9, ups=0.4, wpb=3446.5, bsz=201.5, num_updates=488, lr=5.856e-06, gnorm=2.311, train_wall=5, gb_free=13.8, wall=3117
2024-07-29 11:10:26 | INFO | train_inner | epoch 033:     10 / 15 loss=6.652, nll_loss=3.316, ppl=9.96, wps=1563, ups=0.35, wpb=4479.5, bsz=304, num_updates=490, lr=5.88e-06, gnorm=1.919, train_wall=6, gb_free=10.9, wall=3123
2024-07-29 11:10:32 | INFO | train_inner | epoch 033:     12 / 15 loss=6.807, nll_loss=3.506, ppl=11.36, wps=1332.9, ups=0.34, wpb=3964.5, bsz=220, num_updates=492, lr=5.904e-06, gnorm=2.113, train_wall=6, gb_free=10, wall=3129
2024-07-29 11:10:38 | INFO | train_inner | epoch 033:     14 / 15 loss=6.683, nll_loss=3.358, ppl=10.25, wps=1519.8, ups=0.33, wpb=4545.5, bsz=300, num_updates=494, lr=5.928e-06, gnorm=1.849, train_wall=6, gb_free=9.5, wall=3135
2024-07-29 11:10:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21957.32421875Mb; avail=233108.31640625Mb
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21957.81640625Mb; avail=233108.31640625Mb
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002110
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21957.81640625Mb; avail=233108.31640625Mb
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001921
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004936
2024-07-29 11:10:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21957.81640625Mb; avail=233108.31640625Mb
2024-07-29 11:10:43 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.629 | nll_loss 3.07 | ppl 8.4 | wps 3172.1 | wpb 1361.3 | bsz 86.8 | num_updates 495 | best_loss 6.629
2024-07-29 11:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 495 updates
2024-07-29 11:10:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:11:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 33 @ 495 updates, score 6.629) (writing took 65.55062897410244 seconds)
2024-07-29 11:11:49 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2024-07-29 11:11:49 | INFO | train | epoch 033 | loss 6.676 | nll_loss 3.345 | ppl 10.16 | wps 583.2 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 495 | lr 5.94e-06 | gnorm 1.95 | train_wall 43 | gb_free 9.5 | wall 3206
2024-07-29 11:11:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:11:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:11:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:11:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000791
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44371.6953125Mb; avail=210693.87109375Mb
2024-07-29 11:11:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000754
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44372.6796875Mb; avail=210693.87109375Mb
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44372.1875Mb; avail=210693.87109375Mb
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000214
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001292
2024-07-29 11:11:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44372.1875Mb; avail=210693.87109375Mb
2024-07-29 11:11:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:11:49 | INFO | fairseq.trainer | begin training epoch 34
2024-07-29 11:11:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:11:52 | INFO | train_inner | epoch 034:      1 / 15 loss=6.606, nll_loss=3.246, ppl=9.49, wps=128.6, ups=0.03, wpb=4743, bsz=284, num_updates=496, lr=5.952e-06, gnorm=1.816, train_wall=6, gb_free=9.8, wall=3209
2024-07-29 11:11:57 | INFO | train_inner | epoch 034:      3 / 15 loss=6.599, nll_loss=3.247, ppl=9.49, wps=1294.5, ups=0.36, wpb=3591, bsz=244, num_updates=498, lr=5.976e-06, gnorm=2.03, train_wall=6, gb_free=11.1, wall=3214
2024-07-29 11:12:13 | INFO | train_inner | epoch 034:      5 / 15 loss=6.696, nll_loss=3.372, ppl=10.35, wps=613, ups=0.13, wpb=4860, bsz=300, num_updates=500, lr=6e-06, gnorm=1.794, train_wall=16, gb_free=10.4, wall=3230
2024-07-29 11:12:18 | INFO | train_inner | epoch 034:      7 / 15 loss=6.561, nll_loss=3.202, ppl=9.2, wps=1523.2, ups=0.41, wpb=3673.5, bsz=233.5, num_updates=502, lr=6.024e-06, gnorm=2.068, train_wall=5, gb_free=9, wall=3235
2024-07-29 11:12:24 | INFO | train_inner | epoch 034:      9 / 15 loss=6.648, nll_loss=3.312, ppl=9.93, wps=1412, ups=0.32, wpb=4463, bsz=272, num_updates=504, lr=6.048e-06, gnorm=1.873, train_wall=6, gb_free=9.6, wall=3241
2024-07-29 11:12:30 | INFO | train_inner | epoch 034:     11 / 15 loss=6.577, nll_loss=3.221, ppl=9.33, wps=1514, ups=0.33, wpb=4557.5, bsz=284, num_updates=506, lr=6.072e-06, gnorm=1.811, train_wall=6, gb_free=10.1, wall=3247
2024-07-29 11:12:36 | INFO | train_inner | epoch 034:     13 / 15 loss=6.644, nll_loss=3.316, ppl=9.96, wps=1521, ups=0.33, wpb=4617, bsz=296, num_updates=508, lr=6.096e-06, gnorm=1.893, train_wall=6, gb_free=9.9, wall=3253
2024-07-29 11:12:42 | INFO | train_inner | epoch 034:     15 / 15 loss=6.675, nll_loss=3.355, ppl=10.23, wps=1520, ups=0.35, wpb=4346, bsz=324, num_updates=510, lr=6.12e-06, gnorm=1.896, train_wall=6, gb_free=12, wall=3259
2024-07-29 11:12:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25883.43359375Mb; avail=229182.6953125Mb
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000478
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25883.43359375Mb; avail=229182.6953125Mb
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002087
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25883.43359375Mb; avail=229182.6953125Mb
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001901
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004835
2024-07-29 11:12:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25883.43359375Mb; avail=229182.6953125Mb
2024-07-29 11:12:44 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.61 | nll_loss 3.048 | ppl 8.27 | wps 3175.7 | wpb 1361.3 | bsz 86.8 | num_updates 510 | best_loss 6.61
2024-07-29 11:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 510 updates
2024-07-29 11:12:44 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:13:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:13:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 34 @ 510 updates, score 6.61) (writing took 65.38085307879373 seconds)
2024-07-29 11:13:50 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2024-07-29 11:13:50 | INFO | train | epoch 034 | loss 6.639 | nll_loss 3.301 | ppl 9.85 | wps 535.1 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 510 | lr 6.12e-06 | gnorm 1.909 | train_wall 53 | gb_free 12 | wall 3327
2024-07-29 11:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000696
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35600.96484375Mb; avail=219465.23828125Mb
2024-07-29 11:13:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000091
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000759
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35600.96484375Mb; avail=219465.23828125Mb
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35600.96484375Mb; avail=219465.23828125Mb
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000212
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001298
2024-07-29 11:13:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35600.47265625Mb; avail=219465.23828125Mb
2024-07-29 11:13:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:13:50 | INFO | fairseq.trainer | begin training epoch 35
2024-07-29 11:13:50 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:13:56 | INFO | train_inner | epoch 035:      2 / 15 loss=6.713, nll_loss=3.396, ppl=10.53, wps=110.6, ups=0.03, wpb=4073.5, bsz=260, num_updates=512, lr=6.144e-06, gnorm=2.008, train_wall=6, gb_free=9.9, wall=3333
2024-07-29 11:14:02 | INFO | train_inner | epoch 035:      4 / 15 loss=6.604, nll_loss=3.25, ppl=9.51, wps=1452.6, ups=0.33, wpb=4342, bsz=280, num_updates=514, lr=6.168e-06, gnorm=1.881, train_wall=6, gb_free=10.9, wall=3339
2024-07-29 11:14:08 | INFO | train_inner | epoch 035:      6 / 15 loss=6.52, nll_loss=3.144, ppl=8.84, wps=1625.8, ups=0.34, wpb=4787.5, bsz=332, num_updates=516, lr=6.192e-06, gnorm=1.64, train_wall=6, gb_free=11.4, wall=3345
2024-07-29 11:14:12 | INFO | train_inner | epoch 035:      8 / 15 loss=6.678, nll_loss=3.349, ppl=10.19, wps=1473.4, ups=0.42, wpb=3549.5, bsz=213.5, num_updates=518, lr=6.216e-06, gnorm=2.174, train_wall=5, gb_free=9, wall=3349
2024-07-29 11:14:18 | INFO | train_inner | epoch 035:     10 / 15 loss=6.598, nll_loss=3.249, ppl=9.51, wps=1478.1, ups=0.34, wpb=4405, bsz=324, num_updates=520, lr=6.24e-06, gnorm=1.887, train_wall=6, gb_free=10.7, wall=3355
2024-07-29 11:14:24 | INFO | train_inner | epoch 035:     12 / 15 loss=6.567, nll_loss=3.205, ppl=9.22, wps=1431.2, ups=0.34, wpb=4272, bsz=236, num_updates=522, lr=6.264e-06, gnorm=1.966, train_wall=6, gb_free=12.5, wall=3361
2024-07-29 11:14:30 | INFO | train_inner | epoch 035:     14 / 15 loss=6.576, nll_loss=3.213, ppl=9.27, wps=1623.7, ups=0.35, wpb=4701, bsz=296, num_updates=524, lr=6.288e-06, gnorm=1.808, train_wall=6, gb_free=9.9, wall=3367
2024-07-29 11:14:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22963.0078125Mb; avail=232103.16796875Mb
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000593
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22963.0078125Mb; avail=232103.16796875Mb
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002117
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22963.0078125Mb; avail=232103.16796875Mb
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001925
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004990
2024-07-29 11:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22963.5Mb; avail=232102.67578125Mb
2024-07-29 11:14:35 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.585 | nll_loss 3.012 | ppl 8.07 | wps 3175.1 | wpb 1361.3 | bsz 86.8 | num_updates 525 | best_loss 6.585
2024-07-29 11:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 525 updates
2024-07-29 11:14:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:15:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:15:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 35 @ 525 updates, score 6.585) (writing took 65.66977472696453 seconds)
2024-07-29 11:15:41 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2024-07-29 11:15:41 | INFO | train | epoch 035 | loss 6.596 | nll_loss 3.241 | ppl 9.46 | wps 581.3 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 525 | lr 6.3e-06 | gnorm 1.904 | train_wall 43 | gb_free 10.1 | wall 3438
2024-07-29 11:15:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:15:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:15:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:15:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000765
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44566.41796875Mb; avail=210499.64453125Mb
2024-07-29 11:15:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000092
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000774
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44566.41796875Mb; avail=210499.64453125Mb
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44566.41796875Mb; avail=210499.64453125Mb
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000221
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001312
2024-07-29 11:15:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44566.41796875Mb; avail=210499.64453125Mb
2024-07-29 11:15:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:15:41 | INFO | fairseq.trainer | begin training epoch 36
2024-07-29 11:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:15:44 | INFO | train_inner | epoch 036:      1 / 15 loss=6.569, nll_loss=3.201, ppl=9.2, wps=120.7, ups=0.03, wpb=4478.5, bsz=280, num_updates=526, lr=6.312e-06, gnorm=1.808, train_wall=6, gb_free=8.9, wall=3441
2024-07-29 11:15:59 | INFO | train_inner | epoch 036:      3 / 15 loss=6.62, nll_loss=3.27, ppl=9.64, wps=480.5, ups=0.13, wpb=3576.5, bsz=185.5, num_updates=528, lr=6.336e-06, gnorm=2.499, train_wall=15, gb_free=9.2, wall=3456
2024-07-29 11:16:05 | INFO | train_inner | epoch 036:      5 / 15 loss=6.61, nll_loss=3.266, ppl=9.62, wps=1455.8, ups=0.34, wpb=4300, bsz=292, num_updates=530, lr=6.36e-06, gnorm=1.906, train_wall=6, gb_free=10, wall=3462
2024-07-29 11:16:11 | INFO | train_inner | epoch 036:      7 / 15 loss=6.536, nll_loss=3.156, ppl=8.92, wps=1527.4, ups=0.35, wpb=4368, bsz=272, num_updates=532, lr=6.384e-06, gnorm=1.851, train_wall=6, gb_free=11.1, wall=3468
2024-07-29 11:16:16 | INFO | train_inner | epoch 036:      9 / 15 loss=6.63, nll_loss=3.273, ppl=9.67, wps=1469.2, ups=0.36, wpb=4039.5, bsz=228, num_updates=534, lr=6.408e-06, gnorm=2.104, train_wall=5, gb_free=10.7, wall=3473
2024-07-29 11:16:22 | INFO | train_inner | epoch 036:     11 / 15 loss=6.537, nll_loss=3.175, ppl=9.03, wps=1620.7, ups=0.34, wpb=4753.5, bsz=336, num_updates=536, lr=6.432e-06, gnorm=1.752, train_wall=6, gb_free=10.4, wall=3479
2024-07-29 11:16:28 | INFO | train_inner | epoch 036:     13 / 15 loss=6.603, nll_loss=3.249, ppl=9.51, wps=1564.3, ups=0.35, wpb=4533, bsz=296, num_updates=538, lr=6.456e-06, gnorm=1.872, train_wall=6, gb_free=10.6, wall=3485
2024-07-29 11:16:34 | INFO | train_inner | epoch 036:     15 / 15 loss=6.489, nll_loss=3.126, ppl=8.73, wps=1481.4, ups=0.32, wpb=4559.5, bsz=332, num_updates=540, lr=6.48e-06, gnorm=1.833, train_wall=6, gb_free=11.2, wall=3491
2024-07-29 11:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23278.11328125Mb; avail=231788.01953125Mb
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000462
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23278.11328125Mb; avail=231788.01953125Mb
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002072
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23278.11328125Mb; avail=231788.01953125Mb
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001885
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004725
2024-07-29 11:16:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23278.11328125Mb; avail=231788.01953125Mb
2024-07-29 11:16:36 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.568 | nll_loss 3.005 | ppl 8.03 | wps 3178.9 | wpb 1361.3 | bsz 86.8 | num_updates 540 | best_loss 6.568
2024-07-29 11:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 540 updates
2024-07-29 11:16:36 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:17:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 36 @ 540 updates, score 6.568) (writing took 65.28030623495579 seconds)
2024-07-29 11:17:42 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2024-07-29 11:17:42 | INFO | train | epoch 036 | loss 6.578 | nll_loss 3.22 | ppl 9.32 | wps 536.8 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 540 | lr 6.48e-06 | gnorm 1.961 | train_wall 53 | gb_free 11.2 | wall 3559
2024-07-29 11:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000700
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34325.68359375Mb; avail=220740.51171875Mb
2024-07-29 11:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000761
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34326.17578125Mb; avail=220740.51171875Mb
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34325.68359375Mb; avail=220740.51171875Mb
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000214
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001299
2024-07-29 11:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34326.17578125Mb; avail=220740.01953125Mb
2024-07-29 11:17:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:17:42 | INFO | fairseq.trainer | begin training epoch 37
2024-07-29 11:17:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:17:48 | INFO | train_inner | epoch 037:      2 / 15 loss=6.539, nll_loss=3.179, ppl=9.06, wps=120.7, ups=0.03, wpb=4445, bsz=280, num_updates=542, lr=6.504e-06, gnorm=1.858, train_wall=6, gb_free=9, wall=3565
2024-07-29 11:17:53 | INFO | train_inner | epoch 037:      4 / 15 loss=6.577, nll_loss=3.233, ppl=9.4, wps=1443.6, ups=0.37, wpb=3922.5, bsz=268, num_updates=544, lr=6.528e-06, gnorm=2.074, train_wall=5, gb_free=13, wall=3570
2024-07-29 11:17:59 | INFO | train_inner | epoch 037:      6 / 15 loss=6.55, nll_loss=3.179, ppl=9.06, wps=1611.8, ups=0.34, wpb=4726, bsz=300, num_updates=546, lr=6.552e-06, gnorm=1.838, train_wall=6, gb_free=11.8, wall=3576
2024-07-29 11:18:04 | INFO | train_inner | epoch 037:      8 / 15 loss=6.407, nll_loss=3.005, ppl=8.03, wps=1577.4, ups=0.42, wpb=3743, bsz=233.5, num_updates=548, lr=6.576e-06, gnorm=2.089, train_wall=5, gb_free=11, wall=3581
2024-07-29 11:18:10 | INFO | train_inner | epoch 037:     10 / 15 loss=6.515, nll_loss=3.138, ppl=8.81, wps=1442.8, ups=0.33, wpb=4436, bsz=284, num_updates=550, lr=6.6e-06, gnorm=1.79, train_wall=6, gb_free=9.1, wall=3587
2024-07-29 11:18:16 | INFO | train_inner | epoch 037:     12 / 15 loss=6.539, nll_loss=3.17, ppl=9, wps=1582.6, ups=0.34, wpb=4682.5, bsz=336, num_updates=552, lr=6.624e-06, gnorm=1.843, train_wall=6, gb_free=9.8, wall=3593
2024-07-29 11:18:22 | INFO | train_inner | epoch 037:     14 / 15 loss=6.474, nll_loss=3.079, ppl=8.45, wps=1466.1, ups=0.32, wpb=4605.5, bsz=272, num_updates=554, lr=6.648e-06, gnorm=1.784, train_wall=6, gb_free=9.5, wall=3599
2024-07-29 11:18:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20521.625Mb; avail=234544.50390625Mb
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000600
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20521.625Mb; avail=234544.50390625Mb
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002100
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20522.1171875Mb; avail=234544.01171875Mb
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001932
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004977
2024-07-29 11:18:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20522.1171875Mb; avail=234544.01171875Mb
2024-07-29 11:18:27 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.549 | nll_loss 2.972 | ppl 7.84 | wps 3182.9 | wpb 1361.3 | bsz 86.8 | num_updates 555 | best_loss 6.549
2024-07-29 11:18:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 555 updates
2024-07-29 11:18:27 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 37 @ 555 updates, score 6.549) (writing took 69.53797151474282 seconds)
2024-07-29 11:19:37 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2024-07-29 11:19:37 | INFO | train | epoch 037 | loss 6.524 | nll_loss 3.153 | ppl 8.89 | wps 562.4 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 555 | lr 6.66e-06 | gnorm 1.921 | train_wall 43 | gb_free 11.5 | wall 3674
2024-07-29 11:19:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:19:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:19:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:19:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000717
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36124.47265625Mb; avail=218941.2421875Mb
2024-07-29 11:19:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000089
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000733
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36130.37890625Mb; avail=218935.3359375Mb
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36127.91796875Mb; avail=218941.2421875Mb
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000200
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001245
2024-07-29 11:19:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36122.99609375Mb; avail=218942.71875Mb
2024-07-29 11:19:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:19:37 | INFO | fairseq.trainer | begin training epoch 38
2024-07-29 11:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:19:40 | INFO | train_inner | epoch 038:      1 / 15 loss=6.584, nll_loss=3.227, ppl=9.36, wps=108.1, ups=0.03, wpb=4198, bsz=236, num_updates=556, lr=6.672e-06, gnorm=2.119, train_wall=6, gb_free=10, wall=3677
2024-07-29 11:19:46 | INFO | train_inner | epoch 038:      3 / 15 loss=6.498, nll_loss=3.127, ppl=8.73, wps=1670.1, ups=0.34, wpb=4856, bsz=340, num_updates=558, lr=6.696e-06, gnorm=1.795, train_wall=6, gb_free=8.5, wall=3683
2024-07-29 11:19:51 | INFO | train_inner | epoch 038:      5 / 15 loss=6.517, nll_loss=3.157, ppl=8.92, wps=1475.1, ups=0.36, wpb=4122, bsz=256, num_updates=560, lr=6.72e-06, gnorm=1.983, train_wall=6, gb_free=12.5, wall=3688
2024-07-29 11:19:57 | INFO | train_inner | epoch 038:      7 / 15 loss=6.55, nll_loss=3.176, ppl=9.04, wps=1516.5, ups=0.34, wpb=4467.5, bsz=260, num_updates=562, lr=6.744e-06, gnorm=2.009, train_wall=6, gb_free=9.6, wall=3694
2024-07-29 11:20:03 | INFO | train_inner | epoch 038:      9 / 15 loss=6.521, nll_loss=3.144, ppl=8.84, wps=1507.5, ups=0.33, wpb=4588, bsz=304, num_updates=564, lr=6.768e-06, gnorm=1.799, train_wall=6, gb_free=9.3, wall=3700
2024-07-29 11:20:09 | INFO | train_inner | epoch 038:     11 / 15 loss=6.509, nll_loss=3.138, ppl=8.8, wps=1353.1, ups=0.35, wpb=3817, bsz=308, num_updates=566, lr=6.792e-06, gnorm=1.895, train_wall=6, gb_free=10.7, wall=3706
2024-07-29 11:20:14 | INFO | train_inner | epoch 038:     13 / 15 loss=6.509, nll_loss=3.118, ppl=8.68, wps=1438.4, ups=0.41, wpb=3489, bsz=193.5, num_updates=568, lr=6.816e-06, gnorm=2.231, train_wall=5, gb_free=9, wall=3711
2024-07-29 11:20:20 | INFO | train_inner | epoch 038:     15 / 15 loss=6.553, nll_loss=3.19, ppl=9.12, wps=1605.8, ups=0.35, wpb=4641, bsz=292, num_updates=570, lr=6.84e-06, gnorm=2.012, train_wall=6, gb_free=9.8, wall=3716
2024-07-29 11:20:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23511.57421875Mb; avail=231554.55859375Mb
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000587
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23512.06640625Mb; avail=231554.06640625Mb
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002125
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23512.06640625Mb; avail=231554.06640625Mb
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001914
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004966
2024-07-29 11:20:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23512.06640625Mb; avail=231554.06640625Mb
2024-07-29 11:20:22 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.539 | nll_loss 2.956 | ppl 7.76 | wps 3183.8 | wpb 1361.3 | bsz 86.8 | num_updates 570 | best_loss 6.539
2024-07-29 11:20:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 570 updates
2024-07-29 11:20:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:21:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:21:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 38 @ 570 updates, score 6.539) (writing took 63.6615104037337 seconds)
2024-07-29 11:21:25 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2024-07-29 11:21:25 | INFO | train | epoch 038 | loss 6.523 | nll_loss 3.151 | ppl 8.88 | wps 596 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 570 | lr 6.84e-06 | gnorm 1.961 | train_wall 43 | gb_free 9.8 | wall 3782
2024-07-29 11:21:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:21:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:21:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:21:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000792
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43628.33203125Mb; avail=211437.80078125Mb
2024-07-29 11:21:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000756
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43628.33203125Mb; avail=211437.80078125Mb
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43628.33203125Mb; avail=211437.80078125Mb
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001303
2024-07-29 11:21:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43628.33203125Mb; avail=211437.80078125Mb
2024-07-29 11:21:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:21:25 | INFO | fairseq.trainer | begin training epoch 39
2024-07-29 11:21:25 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:21:31 | INFO | train_inner | epoch 039:      2 / 15 loss=6.437, nll_loss=3.045, ppl=8.25, wps=132.1, ups=0.03, wpb=4740.5, bsz=300, num_updates=572, lr=6.864e-06, gnorm=1.861, train_wall=6, gb_free=11.1, wall=3788
2024-07-29 11:21:36 | INFO | train_inner | epoch 039:      4 / 15 loss=6.576, nll_loss=3.202, ppl=9.21, wps=1277.3, ups=0.44, wpb=2907.5, bsz=149.5, num_updates=574, lr=6.888e-06, gnorm=3.055, train_wall=5, gb_free=17, wall=3793
2024-07-29 11:21:41 | INFO | train_inner | epoch 039:      6 / 15 loss=6.522, nll_loss=3.153, ppl=8.89, wps=1703, ups=0.36, wpb=4769.5, bsz=324, num_updates=576, lr=6.912e-06, gnorm=1.866, train_wall=6, gb_free=12.3, wall=3798
2024-07-29 11:21:47 | INFO | train_inner | epoch 039:      8 / 15 loss=6.528, nll_loss=3.161, ppl=8.95, wps=1567.8, ups=0.34, wpb=4600.5, bsz=308, num_updates=578, lr=6.936e-06, gnorm=1.939, train_wall=6, gb_free=10.6, wall=3804
2024-07-29 11:21:53 | INFO | train_inner | epoch 039:     10 / 15 loss=6.489, nll_loss=3.094, ppl=8.54, wps=1391.4, ups=0.33, wpb=4246.5, bsz=244, num_updates=580, lr=6.96e-06, gnorm=1.997, train_wall=6, gb_free=9.4, wall=3810
2024-07-29 11:21:59 | INFO | train_inner | epoch 039:     12 / 15 loss=6.44, nll_loss=3.052, ppl=8.3, wps=1448.3, ups=0.33, wpb=4372, bsz=308, num_updates=582, lr=6.984e-06, gnorm=1.748, train_wall=6, gb_free=11, wall=3816
2024-07-29 11:22:05 | INFO | train_inner | epoch 039:     14 / 15 loss=6.36, nll_loss=2.941, ppl=7.68, wps=1640.9, ups=0.34, wpb=4879.5, bsz=320, num_updates=584, lr=7.008e-06, gnorm=1.898, train_wall=6, gb_free=9.5, wall=3822
2024-07-29 11:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24445.96484375Mb; avail=230620.16796875Mb
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000600
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24445.96484375Mb; avail=230620.16796875Mb
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002128
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24446.45703125Mb; avail=230619.67578125Mb
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001893
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004947
2024-07-29 11:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24446.45703125Mb; avail=230619.67578125Mb
2024-07-29 11:22:10 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.52 | nll_loss 2.934 | ppl 7.64 | wps 3187.4 | wpb 1361.3 | bsz 86.8 | num_updates 585 | best_loss 6.52
2024-07-29 11:22:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 585 updates
2024-07-29 11:22:10 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:22:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 39 @ 585 updates, score 6.52) (writing took 61.840333031956106 seconds)
2024-07-29 11:23:12 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2024-07-29 11:23:12 | INFO | train | epoch 039 | loss 6.483 | nll_loss 3.1 | ppl 8.57 | wps 605.8 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 585 | lr 7.02e-06 | gnorm 2.043 | train_wall 43 | gb_free 11.6 | wall 3889
2024-07-29 11:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000779
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39969.10546875Mb; avail=215097.0625Mb
2024-07-29 11:23:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000763
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39969.10546875Mb; avail=215097.0625Mb
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39969.59765625Mb; avail=215096.5703125Mb
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000224
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001297
2024-07-29 11:23:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39969.59765625Mb; avail=215096.5703125Mb
2024-07-29 11:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:23:12 | INFO | fairseq.trainer | begin training epoch 40
2024-07-29 11:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:23:15 | INFO | train_inner | epoch 040:      1 / 15 loss=6.495, nll_loss=3.121, ppl=8.7, wps=115.7, ups=0.03, wpb=4047.5, bsz=276, num_updates=586, lr=7.032e-06, gnorm=1.846, train_wall=6, gb_free=9.7, wall=3892
2024-07-29 11:23:20 | INFO | train_inner | epoch 040:      3 / 15 loss=6.446, nll_loss=3.043, ppl=8.24, wps=1394.1, ups=0.41, wpb=3367.5, bsz=205.5, num_updates=588, lr=7.056e-06, gnorm=2.048, train_wall=5, gb_free=9.3, wall=3897
2024-07-29 11:23:26 | INFO | train_inner | epoch 040:      5 / 15 loss=6.404, nll_loss=3.006, ppl=8.03, wps=1636, ups=0.34, wpb=4883.5, bsz=332, num_updates=590, lr=7.08e-06, gnorm=1.73, train_wall=6, gb_free=8.7, wall=3903
2024-07-29 11:23:47 | INFO | train_inner | epoch 040:      7 / 15 loss=6.423, nll_loss=3.031, ppl=8.17, wps=431.2, ups=0.1, wpb=4498.5, bsz=328, num_updates=592, lr=7.104e-06, gnorm=1.759, train_wall=21, gb_free=11.4, wall=3924
2024-07-29 11:23:53 | INFO | train_inner | epoch 040:      9 / 15 loss=6.578, nll_loss=3.208, ppl=9.24, wps=1322.9, ups=0.36, wpb=3662, bsz=212, num_updates=594, lr=7.128e-06, gnorm=2.057, train_wall=6, gb_free=9.5, wall=3929
2024-07-29 11:23:59 | INFO | train_inner | epoch 040:     11 / 15 loss=6.402, nll_loss=2.992, ppl=7.95, wps=1490.4, ups=0.33, wpb=4506, bsz=288, num_updates=596, lr=7.152e-06, gnorm=1.83, train_wall=6, gb_free=9.3, wall=3936
2024-07-29 11:24:04 | INFO | train_inner | epoch 040:     13 / 15 loss=6.354, nll_loss=2.925, ppl=7.6, wps=1557.6, ups=0.35, wpb=4503, bsz=276, num_updates=598, lr=7.176e-06, gnorm=1.86, train_wall=6, gb_free=10.1, wall=3941
2024-07-29 11:24:10 | INFO | train_inner | epoch 040:     15 / 15 loss=6.464, nll_loss=3.07, ppl=8.4, wps=1613.6, ups=0.34, wpb=4755, bsz=292, num_updates=600, lr=7.2e-06, gnorm=1.868, train_wall=6, gb_free=11.7, wall=3947
2024-07-29 11:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18553.875Mb; avail=236512.28515625Mb
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000493
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18553.875Mb; avail=236512.28515625Mb
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002105
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18553.875Mb; avail=236512.28515625Mb
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001873
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004819
2024-07-29 11:24:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18553.875Mb; avail=236512.28515625Mb
2024-07-29 11:24:12 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.514 | nll_loss 2.926 | ppl 7.6 | wps 3186.8 | wpb 1361.3 | bsz 86.8 | num_updates 600 | best_loss 6.514
2024-07-29 11:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 600 updates
2024-07-29 11:24:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:24:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:25:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 40 @ 600 updates, score 6.514) (writing took 63.457475564908236 seconds)
2024-07-29 11:25:16 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2024-07-29 11:25:16 | INFO | train | epoch 040 | loss 6.429 | nll_loss 3.028 | ppl 8.16 | wps 523.5 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 600 | lr 7.2e-06 | gnorm 1.872 | train_wall 58 | gb_free 11.7 | wall 4013
2024-07-29 11:25:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:25:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:25:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:25:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000680
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34693.2265625Mb; avail=220372.4453125Mb
2024-07-29 11:25:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000107
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000770
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34694.2109375Mb; avail=220372.4453125Mb
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34693.71875Mb; avail=220372.4453125Mb
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000219
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001314
2024-07-29 11:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34693.71875Mb; avail=220372.4453125Mb
2024-07-29 11:25:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:25:16 | INFO | fairseq.trainer | begin training epoch 41
2024-07-29 11:25:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:25:22 | INFO | train_inner | epoch 041:      2 / 15 loss=6.331, nll_loss=2.915, ppl=7.54, wps=128.2, ups=0.03, wpb=4594, bsz=324, num_updates=602, lr=7.224e-06, gnorm=1.889, train_wall=6, gb_free=9.9, wall=4019
2024-07-29 11:25:27 | INFO | train_inner | epoch 041:      4 / 15 loss=6.419, nll_loss=3.026, ppl=8.15, wps=1578.8, ups=0.43, wpb=3693.5, bsz=229.5, num_updates=604, lr=7.248e-06, gnorm=2.12, train_wall=5, gb_free=9.6, wall=4024
2024-07-29 11:25:32 | INFO | train_inner | epoch 041:      6 / 15 loss=6.432, nll_loss=3.04, ppl=8.22, wps=1542.2, ups=0.35, wpb=4418.5, bsz=304, num_updates=606, lr=7.272e-06, gnorm=1.762, train_wall=6, gb_free=11.4, wall=4029
2024-07-29 11:25:38 | INFO | train_inner | epoch 041:      8 / 15 loss=6.358, nll_loss=2.938, ppl=7.66, wps=1482.5, ups=0.33, wpb=4551.5, bsz=280, num_updates=608, lr=7.296e-06, gnorm=1.768, train_wall=6, gb_free=8.8, wall=4035
2024-07-29 11:25:45 | INFO | train_inner | epoch 041:     10 / 15 loss=6.394, nll_loss=2.991, ppl=7.95, wps=1577.5, ups=0.33, wpb=4782, bsz=308, num_updates=610, lr=7.32e-06, gnorm=1.743, train_wall=6, gb_free=9.5, wall=4041
2024-07-29 11:25:50 | INFO | train_inner | epoch 041:     12 / 15 loss=6.371, nll_loss=2.937, ppl=7.66, wps=1353, ups=0.34, wpb=4032, bsz=224, num_updates=612, lr=7.344e-06, gnorm=1.938, train_wall=6, gb_free=12.6, wall=4047
2024-07-29 11:25:56 | INFO | train_inner | epoch 041:     14 / 15 loss=6.341, nll_loss=2.909, ppl=7.51, wps=1456.5, ups=0.34, wpb=4246.5, bsz=288, num_updates=614, lr=7.368e-06, gnorm=1.878, train_wall=6, gb_free=10.1, wall=4053
2024-07-29 11:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20618.3359375Mb; avail=234447.796875Mb
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000608
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20618.3359375Mb; avail=234447.796875Mb
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002120
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20618.828125Mb; avail=234447.3046875Mb
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001918
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004973
2024-07-29 11:25:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20618.828125Mb; avail=234447.3046875Mb
2024-07-29 11:26:02 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.498 | nll_loss 2.894 | ppl 7.43 | wps 3183.4 | wpb 1361.3 | bsz 86.8 | num_updates 615 | best_loss 6.498
2024-07-29 11:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 615 updates
2024-07-29 11:26:02 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:26:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:27:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 41 @ 615 updates, score 6.498) (writing took 72.52378525165841 seconds)
2024-07-29 11:27:14 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2024-07-29 11:27:14 | INFO | train | epoch 041 | loss 6.383 | nll_loss 2.971 | ppl 7.84 | wps 547.9 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 615 | lr 7.38e-06 | gnorm 1.876 | train_wall 43 | gb_free 10.6 | wall 4131
2024-07-29 11:27:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:27:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:27:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:27:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000637
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39261.54296875Mb; avail=215804.1015625Mb
2024-07-29 11:27:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000714
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39262.03515625Mb; avail=215804.59375Mb
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39261.54296875Mb; avail=215804.1015625Mb
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000210
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001268
2024-07-29 11:27:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39262.03515625Mb; avail=215804.1015625Mb
2024-07-29 11:27:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:27:14 | INFO | fairseq.trainer | begin training epoch 42
2024-07-29 11:27:14 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:27:17 | INFO | train_inner | epoch 042:      1 / 15 loss=6.462, nll_loss=3.055, ppl=8.31, wps=104.3, ups=0.02, wpb=4205, bsz=232, num_updates=616, lr=7.392e-06, gnorm=1.952, train_wall=6, gb_free=12.6, wall=4134
2024-07-29 11:27:23 | INFO | train_inner | epoch 042:      3 / 15 loss=6.391, nll_loss=2.974, ppl=7.86, wps=1619, ups=0.34, wpb=4713, bsz=296, num_updates=618, lr=7.416e-06, gnorm=1.796, train_wall=6, gb_free=11.1, wall=4140
2024-07-29 11:27:29 | INFO | train_inner | epoch 042:      5 / 15 loss=6.36, nll_loss=2.947, ppl=7.71, wps=1557.3, ups=0.34, wpb=4636.5, bsz=316, num_updates=620, lr=7.44e-06, gnorm=1.735, train_wall=6, gb_free=9.9, wall=4146
2024-07-29 11:27:34 | INFO | train_inner | epoch 042:      7 / 15 loss=6.476, nll_loss=3.091, ppl=8.52, wps=1364.3, ups=0.36, wpb=3828, bsz=204, num_updates=622, lr=7.464e-06, gnorm=2.219, train_wall=6, gb_free=11.5, wall=4151
2024-07-29 11:27:39 | INFO | train_inner | epoch 042:      9 / 15 loss=6.338, nll_loss=2.916, ppl=7.55, wps=1473.6, ups=0.41, wpb=3591.5, bsz=209.5, num_updates=624, lr=7.488e-06, gnorm=2.107, train_wall=5, gb_free=13.9, wall=4156
2024-07-29 11:27:45 | INFO | train_inner | epoch 042:     11 / 15 loss=6.355, nll_loss=2.937, ppl=7.66, wps=1576.8, ups=0.34, wpb=4610.5, bsz=356, num_updates=626, lr=7.512e-06, gnorm=1.724, train_wall=6, gb_free=9, wall=4162
2024-07-29 11:27:51 | INFO | train_inner | epoch 042:     13 / 15 loss=6.3, nll_loss=2.861, ppl=7.26, wps=1523.1, ups=0.33, wpb=4556.5, bsz=312, num_updates=628, lr=7.536e-06, gnorm=1.752, train_wall=6, gb_free=11.5, wall=4168
2024-07-29 11:27:57 | INFO | train_inner | epoch 042:     15 / 15 loss=6.251, nll_loss=2.805, ppl=6.99, wps=1435.2, ups=0.34, wpb=4280, bsz=280, num_updates=630, lr=7.56e-06, gnorm=1.817, train_wall=6, gb_free=8.4, wall=4174
2024-07-29 11:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24531.1640625Mb; avail=230535.01171875Mb
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000588
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24531.1640625Mb; avail=230535.01171875Mb
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002119
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24531.1640625Mb; avail=230535.01171875Mb
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001921
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004943
2024-07-29 11:27:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24531.65625Mb; avail=230534.51953125Mb
2024-07-29 11:27:59 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.482 | nll_loss 2.879 | ppl 7.36 | wps 3191.5 | wpb 1361.3 | bsz 86.8 | num_updates 630 | best_loss 6.482
2024-07-29 11:27:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 630 updates
2024-07-29 11:27:59 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:29:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 42 @ 630 updates, score 6.482) (writing took 64.9818900320679 seconds)
2024-07-29 11:29:04 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2024-07-29 11:29:04 | INFO | train | epoch 042 | loss 6.359 | nll_loss 2.938 | ppl 7.67 | wps 587.7 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 630 | lr 7.56e-06 | gnorm 1.884 | train_wall 43 | gb_free 8.4 | wall 4241
2024-07-29 11:29:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:29:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:29:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:29:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000762
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=46124.15625Mb; avail=208941.99609375Mb
2024-07-29 11:29:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000723
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46124.15625Mb; avail=208941.99609375Mb
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46124.15625Mb; avail=208941.99609375Mb
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000213
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001236
2024-07-29 11:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=46124.15625Mb; avail=208941.99609375Mb
2024-07-29 11:29:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:29:04 | INFO | fairseq.trainer | begin training epoch 43
2024-07-29 11:29:04 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:29:09 | INFO | train_inner | epoch 043:      2 / 15 loss=6.317, nll_loss=2.889, ppl=7.41, wps=96.3, ups=0.03, wpb=3461.5, bsz=221.5, num_updates=632, lr=7.584e-06, gnorm=2.19, train_wall=5, gb_free=14.5, wall=4246
2024-07-29 11:29:15 | INFO | train_inner | epoch 043:      4 / 15 loss=6.364, nll_loss=2.94, ppl=7.67, wps=1497.4, ups=0.34, wpb=4463.5, bsz=244, num_updates=634, lr=7.608e-06, gnorm=1.883, train_wall=6, gb_free=9.9, wall=4252
2024-07-29 11:29:21 | INFO | train_inner | epoch 043:      6 / 15 loss=6.456, nll_loss=3.057, ppl=8.32, wps=1467.9, ups=0.35, wpb=4177, bsz=260, num_updates=636, lr=7.632e-06, gnorm=1.978, train_wall=6, gb_free=10.1, wall=4258
2024-07-29 11:29:27 | INFO | train_inner | epoch 043:      8 / 15 loss=6.33, nll_loss=2.904, ppl=7.49, wps=1612.9, ups=0.34, wpb=4765.5, bsz=320, num_updates=638, lr=7.656e-06, gnorm=1.679, train_wall=6, gb_free=12.5, wall=4263
2024-07-29 11:29:32 | INFO | train_inner | epoch 043:     10 / 15 loss=6.348, nll_loss=2.911, ppl=7.52, wps=1367.3, ups=0.35, wpb=3932.5, bsz=236, num_updates=640, lr=7.68e-06, gnorm=2.03, train_wall=6, gb_free=11.4, wall=4269
2024-07-29 11:29:38 | INFO | train_inner | epoch 043:     12 / 15 loss=6.35, nll_loss=2.935, ppl=7.65, wps=1739.4, ups=0.35, wpb=4937, bsz=356, num_updates=642, lr=7.704e-06, gnorm=1.857, train_wall=6, gb_free=12.4, wall=4275
2024-07-29 11:29:44 | INFO | train_inner | epoch 043:     14 / 15 loss=6.326, nll_loss=2.897, ppl=7.45, wps=1542.1, ups=0.35, wpb=4380, bsz=308, num_updates=644, lr=7.728e-06, gnorm=1.885, train_wall=6, gb_free=11.2, wall=4281
2024-07-29 11:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21460.5703125Mb; avail=233605.5625Mb
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000589
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.5703125Mb; avail=233605.5625Mb
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002120
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21460.5703125Mb; avail=233605.5625Mb
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001937
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004966
2024-07-29 11:29:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21461.0625Mb; avail=233605.0703125Mb
2024-07-29 11:29:49 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.482 | nll_loss 2.872 | ppl 7.32 | wps 3182.7 | wpb 1361.3 | bsz 86.8 | num_updates 645 | best_loss 6.482
2024-07-29 11:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 645 updates
2024-07-29 11:29:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:30:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 43 @ 645 updates, score 6.482) (writing took 65.00296633597463 seconds)
2024-07-29 11:30:54 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2024-07-29 11:30:54 | INFO | train | epoch 043 | loss 6.354 | nll_loss 2.93 | ppl 7.62 | wps 590.1 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 645 | lr 7.74e-06 | gnorm 1.927 | train_wall 42 | gb_free 10.2 | wall 4351
2024-07-29 11:30:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:30:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:30:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:30:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000765
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=44423.94140625Mb; avail=210641.6953125Mb
2024-07-29 11:30:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000757
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44429.35546875Mb; avail=210635.7890625Mb
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44431.32421875Mb; avail=210634.8046875Mb
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000229
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001383
2024-07-29 11:30:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=44423.44921875Mb; avail=210642.1875Mb
2024-07-29 11:30:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:30:54 | INFO | fairseq.trainer | begin training epoch 44
2024-07-29 11:30:54 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:30:57 | INFO | train_inner | epoch 044:      1 / 15 loss=6.358, nll_loss=2.934, ppl=7.64, wps=128.1, ups=0.03, wpb=4689, bsz=316, num_updates=646, lr=7.752e-06, gnorm=1.832, train_wall=6, gb_free=12.4, wall=4354
2024-07-29 11:31:03 | INFO | train_inner | epoch 044:      3 / 15 loss=6.254, nll_loss=2.8, ppl=6.97, wps=1639.9, ups=0.34, wpb=4765, bsz=284, num_updates=648, lr=7.776e-06, gnorm=1.777, train_wall=6, gb_free=11.7, wall=4360
2024-07-29 11:31:08 | INFO | train_inner | epoch 044:      5 / 15 loss=6.298, nll_loss=2.868, ppl=7.3, wps=1501.3, ups=0.34, wpb=4360, bsz=332, num_updates=650, lr=7.8e-06, gnorm=1.666, train_wall=6, gb_free=10.5, wall=4365
2024-07-29 11:31:14 | INFO | train_inner | epoch 044:      7 / 15 loss=6.368, nll_loss=2.949, ppl=7.72, wps=1560.1, ups=0.34, wpb=4635.5, bsz=276, num_updates=652, lr=7.824e-06, gnorm=1.748, train_wall=6, gb_free=9.4, wall=4371
2024-07-29 11:31:19 | INFO | train_inner | epoch 044:      9 / 15 loss=6.317, nll_loss=2.911, ppl=7.52, wps=1634.6, ups=0.43, wpb=3758, bsz=265.5, num_updates=654, lr=7.848e-06, gnorm=2.134, train_wall=5, gb_free=11.5, wall=4376
2024-07-29 11:31:25 | INFO | train_inner | epoch 044:     11 / 15 loss=6.274, nll_loss=2.833, ppl=7.13, wps=1494.4, ups=0.34, wpb=4444.5, bsz=300, num_updates=656, lr=7.872e-06, gnorm=1.824, train_wall=6, gb_free=8.7, wall=4382
2024-07-29 11:31:31 | INFO | train_inner | epoch 044:     13 / 15 loss=6.295, nll_loss=2.854, ppl=7.23, wps=1440.8, ups=0.35, wpb=4161, bsz=260, num_updates=658, lr=7.896e-06, gnorm=1.928, train_wall=6, gb_free=12.1, wall=4388
2024-07-29 11:31:52 | INFO | train_inner | epoch 044:     15 / 15 loss=6.334, nll_loss=2.878, ppl=7.35, wps=364.9, ups=0.1, wpb=3809, bsz=184, num_updates=660, lr=7.92e-06, gnorm=2.168, train_wall=21, gb_free=7.9, wall=4409
2024-07-29 11:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29446.55859375Mb; avail=225619.53125Mb
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000504
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29447.54296875Mb; avail=225618.546875Mb
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002099
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29452.46484375Mb; avail=225613.1328125Mb
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001890
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004804
2024-07-29 11:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29457.38671875Mb; avail=225608.703125Mb
2024-07-29 11:31:54 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.48 | nll_loss 2.858 | ppl 7.25 | wps 3208.8 | wpb 1361.3 | bsz 86.8 | num_updates 660 | best_loss 6.48
2024-07-29 11:31:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 660 updates
2024-07-29 11:31:54 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:32:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:33:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 44 @ 660 updates, score 6.48) (writing took 69.55664163874462 seconds)
2024-07-29 11:33:03 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2024-07-29 11:33:03 | INFO | train | epoch 044 | loss 6.311 | nll_loss 2.877 | ppl 7.35 | wps 500.1 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 660 | lr 7.92e-06 | gnorm 1.884 | train_wall 57 | gb_free 7.9 | wall 4480
2024-07-29 11:33:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:33:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:33:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:33:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000736
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22002.640625Mb; avail=233063.53515625Mb
2024-07-29 11:33:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000097
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000811
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22002.640625Mb; avail=233063.53515625Mb
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22002.640625Mb; avail=233063.53515625Mb
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000231
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001356
2024-07-29 11:33:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22002.640625Mb; avail=233063.53515625Mb
2024-07-29 11:33:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:33:03 | INFO | fairseq.trainer | begin training epoch 45
2024-07-29 11:33:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:33:08 | INFO | train_inner | epoch 045:      2 / 15 loss=6.221, nll_loss=2.752, ppl=6.74, wps=82.1, ups=0.03, wpb=3148, bsz=213.5, num_updates=662, lr=7.944e-06, gnorm=2.024, train_wall=5, gb_free=14, wall=4485
2024-07-29 11:33:14 | INFO | train_inner | epoch 045:      4 / 15 loss=6.28, nll_loss=2.847, ppl=7.2, wps=1459.3, ups=0.34, wpb=4319, bsz=320, num_updates=664, lr=7.968e-06, gnorm=1.805, train_wall=6, gb_free=9.1, wall=4491
2024-07-29 11:33:20 | INFO | train_inner | epoch 045:      6 / 15 loss=6.197, nll_loss=2.73, ppl=6.64, wps=1650.9, ups=0.34, wpb=4852.5, bsz=308, num_updates=666, lr=7.992e-06, gnorm=1.801, train_wall=6, gb_free=9.5, wall=4497
2024-07-29 11:33:26 | INFO | train_inner | epoch 045:      8 / 15 loss=6.264, nll_loss=2.828, ppl=7.1, wps=1576.4, ups=0.33, wpb=4716.5, bsz=320, num_updates=668, lr=8.016e-06, gnorm=1.731, train_wall=6, gb_free=9.8, wall=4503
2024-07-29 11:33:32 | INFO | train_inner | epoch 045:     10 / 15 loss=6.25, nll_loss=2.808, ppl=7, wps=1505, ups=0.34, wpb=4477, bsz=264, num_updates=670, lr=8.04e-06, gnorm=1.78, train_wall=6, gb_free=10, wall=4509
2024-07-29 11:33:38 | INFO | train_inner | epoch 045:     12 / 15 loss=6.244, nll_loss=2.793, ppl=6.93, wps=1455.8, ups=0.33, wpb=4396, bsz=280, num_updates=672, lr=8.064e-06, gnorm=1.752, train_wall=6, gb_free=10.1, wall=4515
2024-07-29 11:33:44 | INFO | train_inner | epoch 045:     14 / 15 loss=6.261, nll_loss=2.803, ppl=6.98, wps=1443.8, ups=0.34, wpb=4189, bsz=220, num_updates=674, lr=8.088e-06, gnorm=1.902, train_wall=6, gb_free=13, wall=4521
2024-07-29 11:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35194.51171875Mb; avail=219871.62109375Mb
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000593
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35195.98828125Mb; avail=219869.65234375Mb
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002104
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35201.40234375Mb; avail=219864.73046875Mb
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001919
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004933
2024-07-29 11:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35206.32421875Mb; avail=219859.31640625Mb
2024-07-29 11:33:49 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.455 | nll_loss 2.832 | ppl 7.12 | wps 3186 | wpb 1361.3 | bsz 86.8 | num_updates 675 | best_loss 6.455
2024-07-29 11:33:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 675 updates
2024-07-29 11:33:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:34:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:34:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 45 @ 675 updates, score 6.455) (writing took 64.1844960344024 seconds)
2024-07-29 11:34:53 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2024-07-29 11:34:53 | INFO | train | epoch 045 | loss 6.245 | nll_loss 2.794 | ppl 6.93 | wps 589.9 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 675 | lr 8.1e-06 | gnorm 1.824 | train_wall 43 | gb_free 12.4 | wall 4590
2024-07-29 11:34:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:34:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:34:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:34:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000801
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30684.90625Mb; avail=224381.1875Mb
2024-07-29 11:34:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000873
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30684.90625Mb; avail=224381.1875Mb
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30684.90625Mb; avail=224381.1875Mb
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000241
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001436
2024-07-29 11:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30684.90625Mb; avail=224381.1875Mb
2024-07-29 11:34:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:34:53 | INFO | fairseq.trainer | begin training epoch 46
2024-07-29 11:34:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:34:56 | INFO | train_inner | epoch 046:      1 / 15 loss=6.258, nll_loss=2.799, ppl=6.96, wps=127.6, ups=0.03, wpb=4620.5, bsz=308, num_updates=676, lr=8.112e-06, gnorm=1.696, train_wall=6, gb_free=9.7, wall=4593
2024-07-29 11:35:02 | INFO | train_inner | epoch 046:      3 / 15 loss=6.227, nll_loss=2.765, ppl=6.8, wps=1573.3, ups=0.34, wpb=4600, bsz=324, num_updates=678, lr=8.136e-06, gnorm=1.834, train_wall=6, gb_free=9.9, wall=4599
2024-07-29 11:35:08 | INFO | train_inner | epoch 046:      5 / 15 loss=6.316, nll_loss=2.866, ppl=7.29, wps=1333.8, ups=0.34, wpb=3901, bsz=212, num_updates=680, lr=8.16e-06, gnorm=2.043, train_wall=6, gb_free=8.8, wall=4605
2024-07-29 11:35:14 | INFO | train_inner | epoch 046:      7 / 15 loss=6.21, nll_loss=2.738, ppl=6.67, wps=1513.9, ups=0.35, wpb=4380.5, bsz=264, num_updates=682, lr=8.184e-06, gnorm=1.969, train_wall=6, gb_free=11.3, wall=4611
2024-07-29 11:35:19 | INFO | train_inner | epoch 046:      9 / 15 loss=6.235, nll_loss=2.784, ppl=6.89, wps=1456.5, ups=0.42, wpb=3478.5, bsz=205.5, num_updates=684, lr=8.208e-06, gnorm=2.251, train_wall=5, gb_free=9.5, wall=4615
2024-07-29 11:35:34 | INFO | train_inner | epoch 046:     11 / 15 loss=6.181, nll_loss=2.734, ppl=6.65, wps=578.9, ups=0.13, wpb=4623.5, bsz=344, num_updates=686, lr=8.232e-06, gnorm=1.611, train_wall=16, gb_free=10.8, wall=4631
2024-07-29 11:35:40 | INFO | train_inner | epoch 046:     13 / 15 loss=6.253, nll_loss=2.821, ppl=7.06, wps=1520.4, ups=0.34, wpb=4407.5, bsz=296, num_updates=688, lr=8.256e-06, gnorm=2.453, train_wall=6, gb_free=9.6, wall=4637
2024-07-29 11:35:46 | INFO | train_inner | epoch 046:     15 / 15 loss=6.208, nll_loss=2.743, ppl=6.7, wps=1540.5, ups=0.33, wpb=4629.5, bsz=284, num_updates=690, lr=8.28e-06, gnorm=1.827, train_wall=6, gb_free=11.4, wall=4643
2024-07-29 11:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=39042.03125Mb; avail=216024.0625Mb
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000506
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39042.03125Mb; avail=216024.0625Mb
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002092
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39042.03125Mb; avail=216024.0625Mb
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001924
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004840
2024-07-29 11:35:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=39042.5234375Mb; avail=216023.5703125Mb
2024-07-29 11:35:48 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.44 | nll_loss 2.815 | ppl 7.03 | wps 3189.6 | wpb 1361.3 | bsz 86.8 | num_updates 690 | best_loss 6.44
2024-07-29 11:35:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 690 updates
2024-07-29 11:35:48 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:36:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 46 @ 690 updates, score 6.44) (writing took 69.37933728471398 seconds)
2024-07-29 11:36:58 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2024-07-29 11:36:58 | INFO | train | epoch 046 | loss 6.235 | nll_loss 2.78 | ppl 6.87 | wps 519 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 690 | lr 8.28e-06 | gnorm 1.974 | train_wall 53 | gb_free 11.4 | wall 4715
2024-07-29 11:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000706
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24591.79296875Mb; avail=230474.4296875Mb
2024-07-29 11:36:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000089
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000794
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24591.79296875Mb; avail=230474.4296875Mb
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000049
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24591.79296875Mb; avail=230474.4296875Mb
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000215
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001337
2024-07-29 11:36:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24591.79296875Mb; avail=230474.4296875Mb
2024-07-29 11:36:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:36:58 | INFO | fairseq.trainer | begin training epoch 47
2024-07-29 11:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:37:04 | INFO | train_inner | epoch 047:      2 / 15 loss=6.172, nll_loss=2.689, ppl=6.45, wps=123.7, ups=0.03, wpb=4795, bsz=320, num_updates=692, lr=8.304e-06, gnorm=1.696, train_wall=6, gb_free=9.2, wall=4721
2024-07-29 11:37:10 | INFO | train_inner | epoch 047:      4 / 15 loss=6.153, nll_loss=2.668, ppl=6.35, wps=1652.8, ups=0.35, wpb=4722, bsz=324, num_updates=694, lr=8.328e-06, gnorm=1.615, train_wall=6, gb_free=11, wall=4726
2024-07-29 11:37:16 | INFO | train_inner | epoch 047:      6 / 15 loss=6.251, nll_loss=2.783, ppl=6.88, wps=1377.3, ups=0.32, wpb=4320.5, bsz=248, num_updates=696, lr=8.352e-06, gnorm=1.858, train_wall=6, gb_free=9.3, wall=4733
2024-07-29 11:37:22 | INFO | train_inner | epoch 047:      8 / 15 loss=6.195, nll_loss=2.726, ppl=6.62, wps=1487.9, ups=0.33, wpb=4463.5, bsz=312, num_updates=698, lr=8.376e-06, gnorm=1.652, train_wall=6, gb_free=9.2, wall=4739
2024-07-29 11:37:28 | INFO | train_inner | epoch 047:     10 / 15 loss=6.186, nll_loss=2.727, ppl=6.62, wps=1606.7, ups=0.34, wpb=4685, bsz=308, num_updates=700, lr=8.4e-06, gnorm=1.701, train_wall=6, gb_free=10.9, wall=4745
2024-07-29 11:37:32 | INFO | train_inner | epoch 047:     12 / 15 loss=6.225, nll_loss=2.77, ppl=6.82, wps=1472.8, ups=0.41, wpb=3575, bsz=213.5, num_updates=702, lr=8.424e-06, gnorm=1.969, train_wall=5, gb_free=11.3, wall=4749
2024-07-29 11:37:38 | INFO | train_inner | epoch 047:     14 / 15 loss=6.173, nll_loss=2.71, ppl=6.54, wps=1435.2, ups=0.34, wpb=4239.5, bsz=292, num_updates=704, lr=8.448e-06, gnorm=1.824, train_wall=6, gb_free=11.2, wall=4755
2024-07-29 11:37:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37569.6640625Mb; avail=217496.4921875Mb
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37569.171875Mb; avail=217496.984375Mb
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002116
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37569.171875Mb; avail=217496.4921875Mb
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001883
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004898
2024-07-29 11:37:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37569.6640625Mb; avail=217496.4921875Mb
2024-07-29 11:37:43 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.434 | nll_loss 2.807 | ppl 7 | wps 3185.6 | wpb 1361.3 | bsz 86.8 | num_updates 705 | best_loss 6.434
2024-07-29 11:37:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 705 updates
2024-07-29 11:37:43 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:38:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:38:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 47 @ 705 updates, score 6.434) (writing took 67.5083663072437 seconds)
2024-07-29 11:38:51 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2024-07-29 11:38:51 | INFO | train | epoch 047 | loss 6.194 | nll_loss 2.725 | ppl 6.61 | wps 573.2 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 705 | lr 8.46e-06 | gnorm 1.801 | train_wall 43 | gb_free 12.9 | wall 4828
2024-07-29 11:38:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:38:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:38:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:38:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000790
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=30602.80859375Mb; avail=224463.28125Mb
2024-07-29 11:38:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000087
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000772
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30602.80859375Mb; avail=224463.28125Mb
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30602.80859375Mb; avail=224463.28125Mb
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000216
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001309
2024-07-29 11:38:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=30602.80859375Mb; avail=224463.28125Mb
2024-07-29 11:38:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:38:51 | INFO | fairseq.trainer | begin training epoch 48
2024-07-29 11:38:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:38:54 | INFO | train_inner | epoch 048:      1 / 15 loss=6.182, nll_loss=2.702, ppl=6.51, wps=103.7, ups=0.03, wpb=3912.5, bsz=212, num_updates=706, lr=8.472e-06, gnorm=2.063, train_wall=6, gb_free=11.3, wall=4831
2024-07-29 11:38:59 | INFO | train_inner | epoch 048:      3 / 15 loss=6.155, nll_loss=2.656, ppl=6.3, wps=1530.4, ups=0.35, wpb=4316.5, bsz=236, num_updates=708, lr=8.496e-06, gnorm=1.876, train_wall=6, gb_free=10, wall=4836
2024-07-29 11:39:05 | INFO | train_inner | epoch 048:      5 / 15 loss=6.18, nll_loss=2.708, ppl=6.54, wps=1612.6, ups=0.35, wpb=4660.5, bsz=296, num_updates=710, lr=8.52e-06, gnorm=1.818, train_wall=6, gb_free=9.5, wall=4842
2024-07-29 11:39:11 | INFO | train_inner | epoch 048:      7 / 15 loss=6.157, nll_loss=2.684, ppl=6.43, wps=1567, ups=0.34, wpb=4578.5, bsz=352, num_updates=712, lr=8.544e-06, gnorm=1.763, train_wall=6, gb_free=10.1, wall=4848
2024-07-29 11:39:16 | INFO | train_inner | epoch 048:      9 / 15 loss=6.194, nll_loss=2.71, ppl=6.54, wps=1371.6, ups=0.41, wpb=3355.5, bsz=189.5, num_updates=714, lr=8.568e-06, gnorm=2.291, train_wall=5, gb_free=9.2, wall=4853
2024-07-29 11:39:22 | INFO | train_inner | epoch 048:     11 / 15 loss=6.213, nll_loss=2.745, ppl=6.7, wps=1485.4, ups=0.33, wpb=4547.5, bsz=296, num_updates=716, lr=8.592e-06, gnorm=2.077, train_wall=6, gb_free=10.2, wall=4859
2024-07-29 11:39:38 | INFO | train_inner | epoch 048:     13 / 15 loss=6.216, nll_loss=2.757, ppl=6.76, wps=582, ups=0.13, wpb=4596.5, bsz=296, num_updates=718, lr=8.616e-06, gnorm=1.833, train_wall=16, gb_free=11.1, wall=4875
2024-07-29 11:39:44 | INFO | train_inner | epoch 048:     15 / 15 loss=6.137, nll_loss=2.659, ppl=6.32, wps=1388, ups=0.35, wpb=3971, bsz=268, num_updates=720, lr=8.64e-06, gnorm=1.878, train_wall=6, gb_free=10.8, wall=4881
2024-07-29 11:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24936.27734375Mb; avail=230129.859375Mb
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000461
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24936.27734375Mb; avail=230129.859375Mb
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002073
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24936.27734375Mb; avail=230129.859375Mb
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001870
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004720
2024-07-29 11:39:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24936.27734375Mb; avail=230129.859375Mb
2024-07-29 11:39:46 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.424 | nll_loss 2.803 | ppl 6.98 | wps 3194.8 | wpb 1361.3 | bsz 86.8 | num_updates 720 | best_loss 6.424
2024-07-29 11:39:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 720 updates
2024-07-29 11:39:46 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:40:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 48 @ 720 updates, score 6.424) (writing took 63.130578491836786 seconds)
2024-07-29 11:40:49 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2024-07-29 11:40:49 | INFO | train | epoch 048 | loss 6.177 | nll_loss 2.701 | ppl 6.5 | wps 548 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 720 | lr 8.64e-06 | gnorm 1.921 | train_wall 53 | gb_free 10.8 | wall 4946
2024-07-29 11:40:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:40:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:40:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:40:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000727
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36061.703125Mb; avail=219004.51171875Mb
2024-07-29 11:40:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000736
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36061.703125Mb; avail=219004.51171875Mb
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000046
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36061.703125Mb; avail=219004.51171875Mb
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000214
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001350
2024-07-29 11:40:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36061.703125Mb; avail=219004.51171875Mb
2024-07-29 11:40:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:40:49 | INFO | fairseq.trainer | begin training epoch 49
2024-07-29 11:40:49 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:40:55 | INFO | train_inner | epoch 049:      2 / 15 loss=6.168, nll_loss=2.698, ppl=6.49, wps=125, ups=0.03, wpb=4467, bsz=292, num_updates=722, lr=8.664e-06, gnorm=1.849, train_wall=6, gb_free=9.7, wall=4952
2024-07-29 11:41:01 | INFO | train_inner | epoch 049:      4 / 15 loss=6.109, nll_loss=2.617, ppl=6.14, wps=1567.9, ups=0.35, wpb=4534.5, bsz=276, num_updates=724, lr=8.688e-06, gnorm=1.828, train_wall=6, gb_free=12.1, wall=4958
2024-07-29 11:41:07 | INFO | train_inner | epoch 049:      6 / 15 loss=6.135, nll_loss=2.651, ppl=6.28, wps=1612.1, ups=0.34, wpb=4762.5, bsz=300, num_updates=726, lr=8.712e-06, gnorm=1.729, train_wall=6, gb_free=9.7, wall=4964
2024-07-29 11:41:13 | INFO | train_inner | epoch 049:      8 / 15 loss=6.165, nll_loss=2.689, ppl=6.45, wps=1568.4, ups=0.34, wpb=4604.5, bsz=280, num_updates=728, lr=8.736e-06, gnorm=1.819, train_wall=6, gb_free=9.1, wall=4970
2024-07-29 11:41:19 | INFO | train_inner | epoch 049:     10 / 15 loss=6.225, nll_loss=2.762, ppl=6.78, wps=1465.2, ups=0.33, wpb=4418.5, bsz=288, num_updates=730, lr=8.76e-06, gnorm=1.817, train_wall=6, gb_free=9, wall=4976
2024-07-29 11:41:24 | INFO | train_inner | epoch 049:     12 / 15 loss=6.089, nll_loss=2.578, ppl=5.97, wps=1511.8, ups=0.35, wpb=4311, bsz=256, num_updates=732, lr=8.784e-06, gnorm=1.924, train_wall=6, gb_free=9.6, wall=4981
2024-07-29 11:41:29 | INFO | train_inner | epoch 049:     14 / 15 loss=6.191, nll_loss=2.701, ppl=6.5, wps=1272.8, ups=0.44, wpb=2869.5, bsz=197.5, num_updates=734, lr=8.808e-06, gnorm=2.216, train_wall=4, gb_free=11.5, wall=4986
2024-07-29 11:41:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21357.85546875Mb; avail=233708.28125Mb
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000598
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.85546875Mb; avail=233708.28125Mb
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002115
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.85546875Mb; avail=233708.28125Mb
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001921
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004953
2024-07-29 11:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21357.85546875Mb; avail=233708.28125Mb
2024-07-29 11:41:34 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.417 | nll_loss 2.798 | ppl 6.95 | wps 3178.6 | wpb 1361.3 | bsz 86.8 | num_updates 735 | best_loss 6.417
2024-07-29 11:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 735 updates
2024-07-29 11:41:34 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:42:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:42:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 49 @ 735 updates, score 6.417) (writing took 63.75428752740845 seconds)
2024-07-29 11:42:38 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2024-07-29 11:42:38 | INFO | train | epoch 049 | loss 6.146 | nll_loss 2.661 | ppl 6.33 | wps 594.5 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 735 | lr 8.82e-06 | gnorm 1.869 | train_wall 43 | gb_free 11.7 | wall 5055
2024-07-29 11:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000762
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41413.81640625Mb; avail=213651.33203125Mb
2024-07-29 11:42:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000070
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000834
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41409.87890625Mb; avail=213656.25390625Mb
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41410.86328125Mb; avail=213654.77734375Mb
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000233
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001376
2024-07-29 11:42:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41413.81640625Mb; avail=213652.31640625Mb
2024-07-29 11:42:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:42:38 | INFO | fairseq.trainer | begin training epoch 50
2024-07-29 11:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:42:41 | INFO | train_inner | epoch 050:      1 / 15 loss=6.088, nll_loss=2.593, ppl=6.03, wps=123.7, ups=0.03, wpb=4453.5, bsz=316, num_updates=736, lr=8.832e-06, gnorm=1.821, train_wall=6, gb_free=9.4, wall=5058
2024-07-29 11:42:47 | INFO | train_inner | epoch 050:      3 / 15 loss=6.165, nll_loss=2.698, ppl=6.49, wps=1605.7, ups=0.35, wpb=4529.5, bsz=272, num_updates=738, lr=8.856e-06, gnorm=1.789, train_wall=6, gb_free=10.4, wall=5064
2024-07-29 11:42:52 | INFO | train_inner | epoch 050:      5 / 15 loss=6.13, nll_loss=2.657, ppl=6.31, wps=1376.4, ups=0.35, wpb=3923.5, bsz=224, num_updates=740, lr=8.88e-06, gnorm=2.109, train_wall=6, gb_free=11, wall=5069
2024-07-29 11:43:07 | INFO | train_inner | epoch 050:      7 / 15 loss=6.077, nll_loss=2.591, ppl=6.03, wps=467.9, ups=0.13, wpb=3466.5, bsz=241.5, num_updates=742, lr=8.904e-06, gnorm=2.073, train_wall=15, gb_free=14.2, wall=5084
2024-07-29 11:43:13 | INFO | train_inner | epoch 050:      9 / 15 loss=6.114, nll_loss=2.624, ppl=6.16, wps=1661.3, ups=0.36, wpb=4658, bsz=328, num_updates=744, lr=8.928e-06, gnorm=1.655, train_wall=6, gb_free=11.7, wall=5090
2024-07-29 11:43:19 | INFO | train_inner | epoch 050:     11 / 15 loss=6.116, nll_loss=2.617, ppl=6.13, wps=1478.5, ups=0.35, wpb=4283.5, bsz=284, num_updates=746, lr=8.952e-06, gnorm=1.839, train_wall=6, gb_free=11.1, wall=5095
2024-07-29 11:43:25 | INFO | train_inner | epoch 050:     13 / 15 loss=6.112, nll_loss=2.608, ppl=6.1, wps=1494, ups=0.32, wpb=4689, bsz=296, num_updates=748, lr=8.976e-06, gnorm=1.798, train_wall=6, gb_free=8.5, wall=5102
2024-07-29 11:43:31 | INFO | train_inner | epoch 050:     15 / 15 loss=6.146, nll_loss=2.656, ppl=6.3, wps=1581.4, ups=0.33, wpb=4768, bsz=312, num_updates=750, lr=9e-06, gnorm=1.832, train_wall=6, gb_free=8.4, wall=5108
2024-07-29 11:43:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=11056.828125Mb; avail=244009.3828125Mb
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000497
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11056.828125Mb; avail=244009.3828125Mb
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002094
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11056.828125Mb; avail=244009.3828125Mb
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001912
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004814
2024-07-29 11:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=11056.828125Mb; avail=244009.3828125Mb
2024-07-29 11:43:33 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.419 | nll_loss 2.78 | ppl 6.87 | wps 3181.3 | wpb 1361.3 | bsz 86.8 | num_updates 750 | best_loss 6.417
2024-07-29 11:43:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 750 updates
2024-07-29 11:43:33 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:44:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 50 @ 750 updates, score 6.419) (writing took 43.6339500057511 seconds)
2024-07-29 11:44:17 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2024-07-29 11:44:17 | INFO | train | epoch 050 | loss 6.124 | nll_loss 2.637 | ppl 6.22 | wps 655.2 | ups 0.15 | wpb 4315.9 | bsz 277.5 | num_updates 750 | lr 9e-06 | gnorm 1.878 | train_wall 53 | gb_free 8.4 | wall 5154
2024-07-29 11:44:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:44:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:44:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:44:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.001202
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29050.78515625Mb; avail=226017.09375Mb
2024-07-29 11:44:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000073
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000716
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29027.65234375Mb; avail=226036.78125Mb
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29027.65234375Mb; avail=226037.2734375Mb
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000209
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001233
2024-07-29 11:44:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29018.3046875Mb; avail=226044.5390625Mb
2024-07-29 11:44:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:44:17 | INFO | fairseq.trainer | begin training epoch 51
2024-07-29 11:44:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:44:23 | INFO | train_inner | epoch 051:      2 / 15 loss=6.099, nll_loss=2.583, ppl=5.99, wps=178.3, ups=0.04, wpb=4653, bsz=276, num_updates=752, lr=9.024e-06, gnorm=1.702, train_wall=6, gb_free=10, wall=5160
2024-07-29 11:44:29 | INFO | train_inner | epoch 051:      4 / 15 loss=6.111, nll_loss=2.641, ppl=6.24, wps=1591.5, ups=0.34, wpb=4644.5, bsz=340, num_updates=754, lr=9.048e-06, gnorm=1.856, train_wall=6, gb_free=10.1, wall=5166
2024-07-29 11:44:35 | INFO | train_inner | epoch 051:      6 / 15 loss=6.043, nll_loss=2.556, ppl=5.88, wps=1415.6, ups=0.34, wpb=4150, bsz=292, num_updates=756, lr=9.072e-06, gnorm=1.727, train_wall=6, gb_free=9.8, wall=5172
2024-07-29 11:44:41 | INFO | train_inner | epoch 051:      8 / 15 loss=6.049, nll_loss=2.547, ppl=5.84, wps=1469.8, ups=0.34, wpb=4372, bsz=284, num_updates=758, lr=9.096e-06, gnorm=1.716, train_wall=6, gb_free=12.4, wall=5178
2024-07-29 11:44:45 | INFO | train_inner | epoch 051:     10 / 15 loss=6.114, nll_loss=2.627, ppl=6.18, wps=1568.5, ups=0.41, wpb=3790.5, bsz=245.5, num_updates=760, lr=9.12e-06, gnorm=1.956, train_wall=5, gb_free=9.3, wall=5182
2024-07-29 11:44:51 | INFO | train_inner | epoch 051:     12 / 15 loss=6.034, nll_loss=2.518, ppl=5.73, wps=1554.6, ups=0.35, wpb=4479.5, bsz=304, num_updates=762, lr=9.144e-06, gnorm=1.688, train_wall=6, gb_free=11.5, wall=5188
2024-07-29 11:44:57 | INFO | train_inner | epoch 051:     14 / 15 loss=6.038, nll_loss=2.501, ppl=5.66, wps=1382.7, ups=0.34, wpb=4085, bsz=216, num_updates=764, lr=9.168e-06, gnorm=1.887, train_wall=6, gb_free=10.1, wall=5194
2024-07-29 11:45:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20860.0390625Mb; avail=234206.09375Mb
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000584
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20860.53125Mb; avail=234205.6015625Mb
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002116
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20860.53125Mb; avail=234205.6015625Mb
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001892
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004925
2024-07-29 11:45:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20860.53125Mb; avail=234205.6015625Mb
2024-07-29 11:45:02 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.422 | nll_loss 2.775 | ppl 6.84 | wps 3127.9 | wpb 1361.3 | bsz 86.8 | num_updates 765 | best_loss 6.417
2024-07-29 11:45:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 765 updates
2024-07-29 11:45:02 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:45:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 51 @ 765 updates, score 6.422) (writing took 38.774569678120315 seconds)
2024-07-29 11:45:41 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2024-07-29 11:45:41 | INFO | train | epoch 051 | loss 6.093 | nll_loss 2.597 | ppl 6.05 | wps 766.5 | ups 0.18 | wpb 4315.9 | bsz 277.5 | num_updates 765 | lr 9.18e-06 | gnorm 1.836 | train_wall 43 | gb_free 12.2 | wall 5238
2024-07-29 11:45:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:45:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:45:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:45:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000728
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40880.203125Mb; avail=214185.91015625Mb
2024-07-29 11:45:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000744
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40880.203125Mb; avail=214185.91015625Mb
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40880.203125Mb; avail=214185.91015625Mb
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000218
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001273
2024-07-29 11:45:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40880.203125Mb; avail=214185.91015625Mb
2024-07-29 11:45:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:45:41 | INFO | fairseq.trainer | begin training epoch 52
2024-07-29 11:45:41 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:45:43 | INFO | train_inner | epoch 052:      1 / 15 loss=6.287, nll_loss=2.842, ppl=7.17, wps=154.7, ups=0.04, wpb=3556, bsz=209.5, num_updates=766, lr=9.192e-06, gnorm=2.372, train_wall=5, gb_free=11.3, wall=5240
2024-07-29 11:45:49 | INFO | train_inner | epoch 052:      3 / 15 loss=6.041, nll_loss=2.519, ppl=5.73, wps=1644.1, ups=0.35, wpb=4759.5, bsz=320, num_updates=768, lr=9.216e-06, gnorm=1.606, train_wall=6, gb_free=11.6, wall=5246
2024-07-29 11:45:55 | INFO | train_inner | epoch 052:      5 / 15 loss=6.118, nll_loss=2.634, ppl=6.21, wps=1614.7, ups=0.34, wpb=4722, bsz=340, num_updates=770, lr=9.24e-06, gnorm=1.639, train_wall=6, gb_free=9.4, wall=5252
2024-07-29 11:46:01 | INFO | train_inner | epoch 052:      7 / 15 loss=6.063, nll_loss=2.548, ppl=5.85, wps=1445.8, ups=0.34, wpb=4261.5, bsz=268, num_updates=772, lr=9.264e-06, gnorm=1.744, train_wall=6, gb_free=9.8, wall=5258
2024-07-29 11:46:07 | INFO | train_inner | epoch 052:      9 / 15 loss=6.034, nll_loss=2.517, ppl=5.72, wps=1384.7, ups=0.33, wpb=4206.5, bsz=264, num_updates=774, lr=9.288e-06, gnorm=1.843, train_wall=6, gb_free=11.1, wall=5264
2024-07-29 11:46:23 | INFO | train_inner | epoch 052:     11 / 15 loss=5.971, nll_loss=2.447, ppl=5.45, wps=615.2, ups=0.13, wpb=4911.5, bsz=352, num_updates=776, lr=9.312e-06, gnorm=1.534, train_wall=16, gb_free=9.8, wall=5280
2024-07-29 11:46:28 | INFO | train_inner | epoch 052:     13 / 15 loss=6.082, nll_loss=2.557, ppl=5.89, wps=1344.7, ups=0.35, wpb=3856.5, bsz=192, num_updates=778, lr=9.336e-06, gnorm=2.04, train_wall=6, gb_free=10.3, wall=5285
2024-07-29 11:46:34 | INFO | train_inner | epoch 052:     15 / 15 loss=6.164, nll_loss=2.69, ppl=6.45, wps=1438.9, ups=0.34, wpb=4291, bsz=260, num_updates=780, lr=9.36e-06, gnorm=1.804, train_wall=6, gb_free=8.7, wall=5291
2024-07-29 11:46:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21812.90625Mb; avail=233253.23046875Mb
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000502
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21812.90625Mb; avail=233253.23046875Mb
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002103
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21812.90625Mb; avail=233253.23046875Mb
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001893
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004816
2024-07-29 11:46:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21812.90625Mb; avail=233253.23046875Mb
2024-07-29 11:46:37 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.394 | nll_loss 2.776 | ppl 6.85 | wps 3185 | wpb 1361.3 | bsz 86.8 | num_updates 780 | best_loss 6.394
2024-07-29 11:46:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 780 updates
2024-07-29 11:46:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:47:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 52 @ 780 updates, score 6.394) (writing took 65.66752171004191 seconds)
2024-07-29 11:47:42 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2024-07-29 11:47:42 | INFO | train | epoch 052 | loss 6.067 | nll_loss 2.558 | ppl 5.89 | wps 534.1 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 780 | lr 9.36e-06 | gnorm 1.779 | train_wall 53 | gb_free 8.7 | wall 5359
2024-07-29 11:47:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:47:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:47:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:47:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000696
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=35533.07421875Mb; avail=219533.109375Mb
2024-07-29 11:47:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000090
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000760
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35533.07421875Mb; avail=219533.109375Mb
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35533.07421875Mb; avail=219533.109375Mb
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000216
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001296
2024-07-29 11:47:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=35533.07421875Mb; avail=219533.109375Mb
2024-07-29 11:47:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:47:42 | INFO | fairseq.trainer | begin training epoch 53
2024-07-29 11:47:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:47:48 | INFO | train_inner | epoch 053:      2 / 15 loss=5.946, nll_loss=2.419, ppl=5.35, wps=107.1, ups=0.03, wpb=3936.5, bsz=228, num_updates=782, lr=9.384e-06, gnorm=1.945, train_wall=5, gb_free=11.1, wall=5365
2024-07-29 11:47:53 | INFO | train_inner | epoch 053:      4 / 15 loss=6.023, nll_loss=2.515, ppl=5.72, wps=1510.9, ups=0.42, wpb=3635.5, bsz=241.5, num_updates=784, lr=9.408e-06, gnorm=1.989, train_wall=5, gb_free=10.2, wall=5370
2024-07-29 11:47:58 | INFO | train_inner | epoch 053:      6 / 15 loss=5.999, nll_loss=2.476, ppl=5.56, wps=1615.2, ups=0.35, wpb=4630, bsz=316, num_updates=786, lr=9.432e-06, gnorm=1.655, train_wall=6, gb_free=12.6, wall=5375
2024-07-29 11:48:04 | INFO | train_inner | epoch 053:      8 / 15 loss=6.056, nll_loss=2.542, ppl=5.82, wps=1580.7, ups=0.34, wpb=4600.5, bsz=284, num_updates=788, lr=9.456e-06, gnorm=1.954, train_wall=6, gb_free=10.2, wall=5381
2024-07-29 11:48:10 | INFO | train_inner | epoch 053:     10 / 15 loss=6.015, nll_loss=2.478, ppl=5.57, wps=1543.5, ups=0.34, wpb=4603.5, bsz=300, num_updates=790, lr=9.48e-06, gnorm=1.679, train_wall=6, gb_free=11.4, wall=5387
2024-07-29 11:48:16 | INFO | train_inner | epoch 053:     12 / 15 loss=6.038, nll_loss=2.51, ppl=5.7, wps=1503, ups=0.32, wpb=4631.5, bsz=292, num_updates=792, lr=9.504e-06, gnorm=1.768, train_wall=6, gb_free=12.1, wall=5393
2024-07-29 11:48:22 | INFO | train_inner | epoch 053:     14 / 15 loss=6.052, nll_loss=2.546, ppl=5.84, wps=1441.9, ups=0.34, wpb=4284, bsz=312, num_updates=794, lr=9.528e-06, gnorm=1.704, train_wall=6, gb_free=9, wall=5399
2024-07-29 11:48:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=20866.81640625Mb; avail=234199.3203125Mb
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20866.81640625Mb; avail=234199.3203125Mb
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002098
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20866.81640625Mb; avail=234199.3203125Mb
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001911
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004908
2024-07-29 11:48:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=20866.81640625Mb; avail=234199.3203125Mb
2024-07-29 11:48:28 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.393 | nll_loss 2.774 | ppl 6.84 | wps 3183.3 | wpb 1361.3 | bsz 86.8 | num_updates 795 | best_loss 6.393
2024-07-29 11:48:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 795 updates
2024-07-29 11:48:28 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:49:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:49:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 53 @ 795 updates, score 6.393) (writing took 68.22076025325805 seconds)
2024-07-29 11:49:36 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2024-07-29 11:49:36 | INFO | train | epoch 053 | loss 6.02 | nll_loss 2.499 | ppl 5.65 | wps 570.6 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 795 | lr 9.54e-06 | gnorm 1.826 | train_wall 43 | gb_free 10.6 | wall 5473
2024-07-29 11:49:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:49:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:49:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:49:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000765
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43685.1015625Mb; avail=211380.54296875Mb
2024-07-29 11:49:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000746
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43690.515625Mb; avail=211375.62109375Mb
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43691.5Mb; avail=211374.63671875Mb
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000227
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001285
2024-07-29 11:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43693.9609375Mb; avail=211376.11328125Mb
2024-07-29 11:49:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:49:36 | INFO | fairseq.trainer | begin training epoch 54
2024-07-29 11:49:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:49:39 | INFO | train_inner | epoch 054:      1 / 15 loss=5.981, nll_loss=2.466, ppl=5.53, wps=117.5, ups=0.03, wpb=4490, bsz=280, num_updates=796, lr=9.552e-06, gnorm=1.754, train_wall=6, gb_free=10.2, wall=5476
2024-07-29 11:49:45 | INFO | train_inner | epoch 054:      3 / 15 loss=5.956, nll_loss=2.437, ppl=5.42, wps=1532.5, ups=0.33, wpb=4588, bsz=296, num_updates=798, lr=9.576e-06, gnorm=2.979, train_wall=6, gb_free=11.5, wall=5482
2024-07-29 11:49:51 | INFO | train_inner | epoch 054:      5 / 15 loss=6.037, nll_loss=2.53, ppl=5.78, wps=1455.7, ups=0.35, wpb=4206.5, bsz=256, num_updates=800, lr=9.6e-06, gnorm=1.784, train_wall=6, gb_free=11.8, wall=5488
2024-07-29 11:49:55 | INFO | train_inner | epoch 054:      7 / 15 loss=6.046, nll_loss=2.531, ppl=5.78, wps=1266.2, ups=0.43, wpb=2952, bsz=157.5, num_updates=802, lr=9.624e-06, gnorm=2.444, train_wall=5, gb_free=13.1, wall=5492
2024-07-29 11:50:01 | INFO | train_inner | epoch 054:      9 / 15 loss=5.941, nll_loss=2.384, ppl=5.22, wps=1473.7, ups=0.33, wpb=4411.5, bsz=280, num_updates=804, lr=9.648e-06, gnorm=1.746, train_wall=6, gb_free=10, wall=5498
2024-07-29 11:50:07 | INFO | train_inner | epoch 054:     11 / 15 loss=6.044, nll_loss=2.503, ppl=5.67, wps=1547.2, ups=0.35, wpb=4437, bsz=296, num_updates=806, lr=9.672e-06, gnorm=1.79, train_wall=6, gb_free=9.6, wall=5504
2024-07-29 11:50:13 | INFO | train_inner | epoch 054:     13 / 15 loss=5.984, nll_loss=2.436, ppl=5.41, wps=1553.8, ups=0.34, wpb=4571.5, bsz=312, num_updates=808, lr=9.696e-06, gnorm=1.613, train_wall=6, gb_free=9.9, wall=5510
2024-07-29 11:50:19 | INFO | train_inner | epoch 054:     15 / 15 loss=5.986, nll_loss=2.455, ppl=5.48, wps=1607.6, ups=0.34, wpb=4761, bsz=312, num_updates=810, lr=9.72e-06, gnorm=1.764, train_wall=6, gb_free=10.8, wall=5516
2024-07-29 11:50:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21182.046875Mb; avail=233883.91796875Mb
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000585
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21182.5390625Mb; avail=233883.42578125Mb
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002111
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21182.5390625Mb; avail=233883.42578125Mb
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001952
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004971
2024-07-29 11:50:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21182.5390625Mb; avail=233883.42578125Mb
2024-07-29 11:50:21 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.387 | nll_loss 2.76 | ppl 6.77 | wps 3184.6 | wpb 1361.3 | bsz 86.8 | num_updates 810 | best_loss 6.387
2024-07-29 11:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 810 updates
2024-07-29 11:50:21 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:51:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 54 @ 810 updates, score 6.387) (writing took 75.33964041993022 seconds)
2024-07-29 11:51:36 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2024-07-29 11:51:36 | INFO | train | epoch 054 | loss 5.993 | nll_loss 2.462 | ppl 5.51 | wps 537 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 810 | lr 9.72e-06 | gnorm 1.984 | train_wall 43 | gb_free 10.8 | wall 5593
2024-07-29 11:51:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:51:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:51:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:51:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000602
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=37611.06640625Mb; avail=217455.15234375Mb
2024-07-29 11:51:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000090
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000735
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37611.06640625Mb; avail=217455.15234375Mb
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37611.06640625Mb; avail=217455.15234375Mb
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000198
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001249
2024-07-29 11:51:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=37611.06640625Mb; avail=217455.15234375Mb
2024-07-29 11:51:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:51:36 | INFO | fairseq.trainer | begin training epoch 55
2024-07-29 11:51:36 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:51:42 | INFO | train_inner | epoch 055:      2 / 15 loss=5.927, nll_loss=2.404, ppl=5.29, wps=116.1, ups=0.02, wpb=4847, bsz=292, num_updates=812, lr=9.744e-06, gnorm=1.724, train_wall=6, gb_free=11.5, wall=5599
2024-07-29 11:51:48 | INFO | train_inner | epoch 055:      4 / 15 loss=5.912, nll_loss=2.391, ppl=5.25, wps=1490, ups=0.34, wpb=4321.5, bsz=328, num_updates=814, lr=9.768e-06, gnorm=1.784, train_wall=6, gb_free=9.5, wall=5605
2024-07-29 11:51:54 | INFO | train_inner | epoch 055:      6 / 15 loss=6.002, nll_loss=2.497, ppl=5.65, wps=1590.4, ups=0.34, wpb=4640.5, bsz=304, num_updates=816, lr=9.792e-06, gnorm=1.74, train_wall=6, gb_free=12.6, wall=5611
2024-07-29 11:52:00 | INFO | train_inner | epoch 055:      8 / 15 loss=5.985, nll_loss=2.458, ppl=5.49, wps=1490.6, ups=0.34, wpb=4360.5, bsz=288, num_updates=818, lr=9.816e-06, gnorm=1.633, train_wall=6, gb_free=8.8, wall=5617
2024-07-29 11:52:06 | INFO | train_inner | epoch 055:     10 / 15 loss=5.967, nll_loss=2.419, ppl=5.35, wps=1460.6, ups=0.34, wpb=4352.5, bsz=256, num_updates=820, lr=9.84e-06, gnorm=1.825, train_wall=6, gb_free=10.1, wall=5623
2024-07-29 11:52:10 | INFO | train_inner | epoch 055:     12 / 15 loss=5.88, nll_loss=2.306, ppl=4.94, wps=1335.5, ups=0.42, wpb=3171, bsz=181.5, num_updates=822, lr=9.864e-06, gnorm=2.195, train_wall=5, gb_free=15, wall=5627
2024-07-29 11:52:16 | INFO | train_inner | epoch 055:     14 / 15 loss=6.034, nll_loss=2.499, ppl=5.65, wps=1491.6, ups=0.33, wpb=4498.5, bsz=280, num_updates=824, lr=9.888e-06, gnorm=1.85, train_wall=6, gb_free=10.2, wall=5633
2024-07-29 11:52:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22881.18359375Mb; avail=232184.953125Mb
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000574
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22881.67578125Mb; avail=232184.4609375Mb
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002094
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22881.67578125Mb; avail=232184.4609375Mb
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001917
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004907
2024-07-29 11:52:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22881.67578125Mb; avail=232184.4609375Mb
2024-07-29 11:52:22 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.401 | nll_loss 2.752 | ppl 6.73 | wps 3185.7 | wpb 1361.3 | bsz 86.8 | num_updates 825 | best_loss 6.387
2024-07-29 11:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 825 updates
2024-07-29 11:52:22 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:52:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 55 @ 825 updates, score 6.401) (writing took 38.40582966711372 seconds)
2024-07-29 11:53:00 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2024-07-29 11:53:00 | INFO | train | epoch 055 | loss 5.962 | nll_loss 2.431 | ppl 5.39 | wps 773.4 | ups 0.18 | wpb 4315.9 | bsz 277.5 | num_updates 825 | lr 9.9e-06 | gnorm 1.82 | train_wall 43 | gb_free 10.5 | wall 5677
2024-07-29 11:53:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:53:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:53:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:53:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000728
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42017.9140625Mb; avail=213047.73046875Mb
2024-07-29 11:53:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000089
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000754
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42019.8828125Mb; avail=213046.25390625Mb
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42019.8828125Mb; avail=213046.25390625Mb
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000215
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001275
2024-07-29 11:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42020.8671875Mb; avail=213045.26953125Mb
2024-07-29 11:53:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:53:00 | INFO | fairseq.trainer | begin training epoch 56
2024-07-29 11:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:53:03 | INFO | train_inner | epoch 056:      1 / 15 loss=5.935, nll_loss=2.385, ppl=5.22, wps=187.7, ups=0.04, wpb=4389, bsz=276, num_updates=826, lr=9.912e-06, gnorm=1.828, train_wall=6, gb_free=9.4, wall=5680
2024-07-29 11:53:09 | INFO | train_inner | epoch 056:      3 / 15 loss=5.962, nll_loss=2.43, ppl=5.39, wps=1615.7, ups=0.35, wpb=4640, bsz=296, num_updates=828, lr=9.936e-06, gnorm=2.06, train_wall=6, gb_free=11.8, wall=5686
2024-07-29 11:53:15 | INFO | train_inner | epoch 056:      5 / 15 loss=5.836, nll_loss=2.264, ppl=4.8, wps=1457.2, ups=0.32, wpb=4530, bsz=272, num_updates=830, lr=9.96e-06, gnorm=1.664, train_wall=6, gb_free=9.1, wall=5692
2024-07-29 11:53:21 | INFO | train_inner | epoch 056:      7 / 15 loss=5.919, nll_loss=2.38, ppl=5.21, wps=1433.2, ups=0.35, wpb=4078.5, bsz=268, num_updates=832, lr=9.984e-06, gnorm=1.821, train_wall=6, gb_free=10.1, wall=5698
2024-07-29 11:53:27 | INFO | train_inner | epoch 056:      9 / 15 loss=5.97, nll_loss=2.433, ppl=5.4, wps=1459.2, ups=0.32, wpb=4586, bsz=280, num_updates=834, lr=1.0008e-05, gnorm=1.7, train_wall=6, gb_free=8.9, wall=5704
2024-07-29 11:53:32 | INFO | train_inner | epoch 056:     11 / 15 loss=5.925, nll_loss=2.373, ppl=5.18, wps=1526.2, ups=0.45, wpb=3359, bsz=217.5, num_updates=836, lr=1.0032e-05, gnorm=2.137, train_wall=4, gb_free=12.4, wall=5709
2024-07-29 11:53:37 | INFO | train_inner | epoch 056:     13 / 15 loss=5.986, nll_loss=2.46, ppl=5.5, wps=1578.8, ups=0.34, wpb=4663.5, bsz=312, num_updates=838, lr=1.0056e-05, gnorm=1.705, train_wall=6, gb_free=10.7, wall=5714
2024-07-29 11:53:43 | INFO | train_inner | epoch 056:     15 / 15 loss=6.041, nll_loss=2.539, ppl=5.81, wps=1469.1, ups=0.34, wpb=4301.5, bsz=312, num_updates=840, lr=1.008e-05, gnorm=1.834, train_wall=6, gb_free=11.7, wall=5720
2024-07-29 11:53:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=34774.01171875Mb; avail=220292.0390625Mb
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000574
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34774.01171875Mb; avail=220292.0390625Mb
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002112
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34774.01171875Mb; avail=220292.0390625Mb
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001908
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004920
2024-07-29 11:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=34774.50390625Mb; avail=220291.546875Mb
2024-07-29 11:53:46 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.384 | nll_loss 2.735 | ppl 6.66 | wps 3163.9 | wpb 1361.3 | bsz 86.8 | num_updates 840 | best_loss 6.384
2024-07-29 11:53:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 840 updates
2024-07-29 11:53:46 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:54:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 56 @ 840 updates, score 6.384) (writing took 74.44581303792074 seconds)
2024-07-29 11:55:00 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2024-07-29 11:55:00 | INFO | train | epoch 056 | loss 5.946 | nll_loss 2.407 | ppl 5.31 | wps 539.6 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 840 | lr 1.008e-05 | gnorm 1.847 | train_wall 43 | gb_free 11.7 | wall 5797
2024-07-29 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000622
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41241.28125Mb; avail=213824.8984375Mb
2024-07-29 11:55:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000074
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000697
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41241.28125Mb; avail=213825.390625Mb
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41241.28125Mb; avail=213824.8984375Mb
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000202
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001195
2024-07-29 11:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41241.28125Mb; avail=213824.8984375Mb
2024-07-29 11:55:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:55:00 | INFO | fairseq.trainer | begin training epoch 57
2024-07-29 11:55:00 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:55:06 | INFO | train_inner | epoch 057:      2 / 15 loss=5.914, nll_loss=2.359, ppl=5.13, wps=114.4, ups=0.02, wpb=4742, bsz=308, num_updates=842, lr=1.0104e-05, gnorm=1.687, train_wall=6, gb_free=11.6, wall=5803
2024-07-29 11:55:12 | INFO | train_inner | epoch 057:      4 / 15 loss=5.775, nll_loss=2.169, ppl=4.5, wps=1418.4, ups=0.35, wpb=4019.5, bsz=228, num_updates=844, lr=1.0128e-05, gnorm=1.675, train_wall=6, gb_free=9.4, wall=5809
2024-07-29 11:55:16 | INFO | train_inner | epoch 057:      6 / 15 loss=6.001, nll_loss=2.483, ppl=5.59, wps=1499.6, ups=0.43, wpb=3453, bsz=221.5, num_updates=846, lr=1.0152e-05, gnorm=2.047, train_wall=5, gb_free=10.2, wall=5813
2024-07-29 11:55:22 | INFO | train_inner | epoch 057:      8 / 15 loss=5.95, nll_loss=2.425, ppl=5.37, wps=1470.2, ups=0.35, wpb=4239.5, bsz=284, num_updates=848, lr=1.0176e-05, gnorm=1.724, train_wall=6, gb_free=11, wall=5819
2024-07-29 11:55:28 | INFO | train_inner | epoch 057:     10 / 15 loss=5.893, nll_loss=2.35, ppl=5.1, wps=1500.1, ups=0.32, wpb=4627, bsz=312, num_updates=850, lr=1.02e-05, gnorm=1.688, train_wall=6, gb_free=10.1, wall=5825
2024-07-29 11:55:34 | INFO | train_inner | epoch 057:     12 / 15 loss=5.909, nll_loss=2.372, ppl=5.18, wps=1672.9, ups=0.35, wpb=4809.5, bsz=340, num_updates=852, lr=1.0224e-05, gnorm=1.646, train_wall=6, gb_free=10.3, wall=5831
2024-07-29 11:55:40 | INFO | train_inner | epoch 057:     14 / 15 loss=5.805, nll_loss=2.22, ppl=4.66, wps=1412.7, ups=0.34, wpb=4102.5, bsz=244, num_updates=854, lr=1.0248e-05, gnorm=1.739, train_wall=6, gb_free=11.8, wall=5837
2024-07-29 11:55:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=26553.45703125Mb; avail=228512.6796875Mb
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000587
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26553.45703125Mb; avail=228512.6796875Mb
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002125
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26553.45703125Mb; avail=228512.6796875Mb
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001915
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004955
2024-07-29 11:55:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=26553.45703125Mb; avail=228512.6796875Mb
2024-07-29 11:55:45 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.395 | nll_loss 2.735 | ppl 6.66 | wps 3186.2 | wpb 1361.3 | bsz 86.8 | num_updates 855 | best_loss 6.384
2024-07-29 11:55:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 855 updates
2024-07-29 11:55:45 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:56:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:56:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 57 @ 855 updates, score 6.395) (writing took 41.040816785767674 seconds)
2024-07-29 11:56:26 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2024-07-29 11:56:26 | INFO | train | epoch 057 | loss 5.898 | nll_loss 2.346 | ppl 5.09 | wps 749.8 | ups 0.17 | wpb 4315.9 | bsz 277.5 | num_updates 855 | lr 1.026e-05 | gnorm 1.745 | train_wall 43 | gb_free 12.1 | wall 5883
2024-07-29 11:56:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:56:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:56:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:56:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000690
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=50236.4296875Mb; avail=204829.65625Mb
2024-07-29 11:56:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000077
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000741
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50236.4296875Mb; avail=204829.65625Mb
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000035
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50236.4296875Mb; avail=204829.65625Mb
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000210
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001262
2024-07-29 11:56:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=50236.4296875Mb; avail=204829.65625Mb
2024-07-29 11:56:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:56:26 | INFO | fairseq.trainer | begin training epoch 58
2024-07-29 11:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:56:29 | INFO | train_inner | epoch 058:      1 / 15 loss=5.944, nll_loss=2.393, ppl=5.25, wps=192, ups=0.04, wpb=4736.5, bsz=296, num_updates=856, lr=1.0272e-05, gnorm=1.729, train_wall=6, gb_free=12.5, wall=5886
2024-07-29 11:56:34 | INFO | train_inner | epoch 058:      3 / 15 loss=5.855, nll_loss=2.278, ppl=4.85, wps=1462.2, ups=0.43, wpb=3387.5, bsz=201.5, num_updates=858, lr=1.0296e-05, gnorm=2.151, train_wall=5, gb_free=9.7, wall=5891
2024-07-29 11:56:40 | INFO | train_inner | epoch 058:      5 / 15 loss=5.867, nll_loss=2.28, ppl=4.86, wps=1393.2, ups=0.33, wpb=4217, bsz=256, num_updates=860, lr=1.032e-05, gnorm=1.766, train_wall=6, gb_free=10.6, wall=5897
2024-07-29 11:56:46 | INFO | train_inner | epoch 058:      7 / 15 loss=5.825, nll_loss=2.237, ppl=4.71, wps=1320.1, ups=0.34, wpb=3891.5, bsz=228, num_updates=862, lr=1.0344e-05, gnorm=1.989, train_wall=6, gb_free=12.4, wall=5903
2024-07-29 11:56:52 | INFO | train_inner | epoch 058:      9 / 15 loss=5.95, nll_loss=2.412, ppl=5.32, wps=1617.6, ups=0.34, wpb=4779.5, bsz=320, num_updates=864, lr=1.0368e-05, gnorm=1.848, train_wall=6, gb_free=8.7, wall=5909
2024-07-29 11:56:58 | INFO | train_inner | epoch 058:     11 / 15 loss=5.86, nll_loss=2.307, ppl=4.95, wps=1630.9, ups=0.34, wpb=4743, bsz=304, num_updates=866, lr=1.0392e-05, gnorm=1.6, train_wall=6, gb_free=11.4, wall=5915
2024-07-29 11:57:04 | INFO | train_inner | epoch 058:     13 / 15 loss=5.887, nll_loss=2.352, ppl=5.1, wps=1554.1, ups=0.33, wpb=4669, bsz=304, num_updates=868, lr=1.0416e-05, gnorm=1.691, train_wall=6, gb_free=10.1, wall=5921
2024-07-29 11:57:10 | INFO | train_inner | epoch 058:     15 / 15 loss=5.908, nll_loss=2.373, ppl=5.18, wps=1439.8, ups=0.33, wpb=4322, bsz=316, num_updates=870, lr=1.044e-05, gnorm=1.619, train_wall=6, gb_free=9.3, wall=5927
2024-07-29 11:57:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=32268.80859375Mb; avail=222797.328125Mb
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000582
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32268.80859375Mb; avail=222797.328125Mb
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002137
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32268.80859375Mb; avail=222797.328125Mb
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001923
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004972
2024-07-29 11:57:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=32269.30078125Mb; avail=222796.8359375Mb
2024-07-29 11:57:12 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.388 | nll_loss 2.744 | ppl 6.7 | wps 3169.6 | wpb 1361.3 | bsz 86.8 | num_updates 870 | best_loss 6.384
2024-07-29 11:57:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 870 updates
2024-07-29 11:57:12 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:58:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 11:58:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 58 @ 870 updates, score 6.388) (writing took 50.67122999904677 seconds)
2024-07-29 11:58:03 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2024-07-29 11:58:03 | INFO | train | epoch 058 | loss 5.883 | nll_loss 2.325 | ppl 5.01 | wps 672.7 | ups 0.16 | wpb 4315.9 | bsz 277.5 | num_updates 870 | lr 1.044e-05 | gnorm 1.801 | train_wall 43 | gb_free 9.3 | wall 5980
2024-07-29 11:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000598
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42850.9453125Mb; avail=212215.27734375Mb
2024-07-29 11:58:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000705
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42850.453125Mb; avail=212215.76953125Mb
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42850.9453125Mb; avail=212215.27734375Mb
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000202
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001209
2024-07-29 11:58:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42850.453125Mb; avail=212215.76953125Mb
2024-07-29 11:58:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:58:03 | INFO | fairseq.trainer | begin training epoch 59
2024-07-29 11:58:03 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:58:09 | INFO | train_inner | epoch 059:      2 / 15 loss=5.84, nll_loss=2.275, ppl=4.84, wps=151.2, ups=0.03, wpb=4468.5, bsz=300, num_updates=872, lr=1.0464e-05, gnorm=1.648, train_wall=6, gb_free=10.4, wall=5986
2024-07-29 11:58:14 | INFO | train_inner | epoch 059:      4 / 15 loss=5.858, nll_loss=2.288, ppl=4.88, wps=1332.1, ups=0.36, wpb=3724.5, bsz=248, num_updates=874, lr=1.0488e-05, gnorm=1.811, train_wall=6, gb_free=11.3, wall=5991
2024-07-29 11:58:19 | INFO | train_inner | epoch 059:      6 / 15 loss=5.827, nll_loss=2.232, ppl=4.7, wps=1525.1, ups=0.41, wpb=3677, bsz=217.5, num_updates=876, lr=1.0512e-05, gnorm=2.01, train_wall=5, gb_free=14.1, wall=5996
2024-07-29 11:58:25 | INFO | train_inner | epoch 059:      8 / 15 loss=5.787, nll_loss=2.193, ppl=4.57, wps=1505.5, ups=0.33, wpb=4608, bsz=284, num_updates=878, lr=1.0536e-05, gnorm=1.678, train_wall=6, gb_free=9.8, wall=6002
2024-07-29 11:58:31 | INFO | train_inner | epoch 059:     10 / 15 loss=5.789, nll_loss=2.215, ppl=4.64, wps=1587.3, ups=0.33, wpb=4868.5, bsz=348, num_updates=880, lr=1.056e-05, gnorm=1.584, train_wall=6, gb_free=9.9, wall=6008
2024-07-29 11:58:37 | INFO | train_inner | epoch 059:     12 / 15 loss=5.767, nll_loss=2.177, ppl=4.52, wps=1506.1, ups=0.34, wpb=4488.5, bsz=272, num_updates=882, lr=1.0584e-05, gnorm=1.78, train_wall=6, gb_free=10.7, wall=6014
2024-07-29 11:58:43 | INFO | train_inner | epoch 059:     14 / 15 loss=5.863, nll_loss=2.297, ppl=4.91, wps=1480.4, ups=0.34, wpb=4339.5, bsz=280, num_updates=884, lr=1.0608e-05, gnorm=1.68, train_wall=6, gb_free=9.5, wall=6020
2024-07-29 11:58:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28163.625Mb; avail=226902.51171875Mb
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000603
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28163.625Mb; avail=226902.51171875Mb
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002098
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28163.625Mb; avail=226902.51171875Mb
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001932
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004962
2024-07-29 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28164.1171875Mb; avail=226902.01953125Mb
2024-07-29 11:58:49 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.379 | nll_loss 2.735 | ppl 6.66 | wps 3172.4 | wpb 1361.3 | bsz 86.8 | num_updates 885 | best_loss 6.379
2024-07-29 11:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 885 updates
2024-07-29 11:58:49 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:59:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 11:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 59 @ 885 updates, score 6.379) (writing took 62.803711561951786 seconds)
2024-07-29 11:59:51 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2024-07-29 11:59:51 | INFO | train | epoch 059 | loss 5.826 | nll_loss 2.25 | ppl 4.76 | wps 594.9 | ups 0.14 | wpb 4315.9 | bsz 277.5 | num_updates 885 | lr 1.062e-05 | gnorm 1.763 | train_wall 44 | gb_free 8.9 | wall 6088
2024-07-29 11:59:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 11:59:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 11:59:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 11:59:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000750
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=36636.96484375Mb; avail=218429.12890625Mb
2024-07-29 11:59:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000768
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36636.96484375Mb; avail=218429.12890625Mb
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36636.96484375Mb; avail=218429.12890625Mb
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000219
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001319
2024-07-29 11:59:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=36636.96484375Mb; avail=218429.12890625Mb
2024-07-29 11:59:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 11:59:51 | INFO | fairseq.trainer | begin training epoch 60
2024-07-29 11:59:51 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 11:59:55 | INFO | train_inner | epoch 060:      1 / 15 loss=5.843, nll_loss=2.283, ppl=4.87, wps=127.8, ups=0.03, wpb=4557, bsz=292, num_updates=886, lr=1.0632e-05, gnorm=1.783, train_wall=6, gb_free=8.3, wall=6092
2024-07-29 12:00:00 | INFO | train_inner | epoch 060:      3 / 15 loss=5.826, nll_loss=2.25, ppl=4.76, wps=1481.8, ups=0.34, wpb=4395.5, bsz=252, num_updates=888, lr=1.0656e-05, gnorm=1.898, train_wall=6, gb_free=10.3, wall=6097
2024-07-29 12:00:06 | INFO | train_inner | epoch 060:      5 / 15 loss=5.84, nll_loss=2.256, ppl=4.78, wps=1621.7, ups=0.35, wpb=4686.5, bsz=300, num_updates=890, lr=1.068e-05, gnorm=1.654, train_wall=6, gb_free=10.2, wall=6103
2024-07-29 12:00:11 | INFO | train_inner | epoch 060:      7 / 15 loss=5.749, nll_loss=2.142, ppl=4.41, wps=1448.8, ups=0.45, wpb=3243, bsz=189.5, num_updates=892, lr=1.0704e-05, gnorm=1.958, train_wall=4, gb_free=10.3, wall=6108
2024-07-29 12:00:17 | INFO | train_inner | epoch 060:      9 / 15 loss=5.83, nll_loss=2.261, ppl=4.79, wps=1559.8, ups=0.33, wpb=4767.5, bsz=336, num_updates=894, lr=1.0728e-05, gnorm=1.642, train_wall=6, gb_free=9.9, wall=6114
2024-07-29 12:00:23 | INFO | train_inner | epoch 060:     11 / 15 loss=5.833, nll_loss=2.263, ppl=4.8, wps=1529.4, ups=0.33, wpb=4661.5, bsz=296, num_updates=896, lr=1.0752e-05, gnorm=1.629, train_wall=6, gb_free=9.3, wall=6120
2024-07-29 12:00:29 | INFO | train_inner | epoch 060:     13 / 15 loss=5.861, nll_loss=2.301, ppl=4.93, wps=1475.7, ups=0.35, wpb=4170, bsz=280, num_updates=898, lr=1.0776e-05, gnorm=1.784, train_wall=6, gb_free=12.7, wall=6126
2024-07-29 12:00:35 | INFO | train_inner | epoch 060:     15 / 15 loss=5.797, nll_loss=2.22, ppl=4.66, wps=1328.5, ups=0.33, wpb=4083.5, bsz=268, num_updates=900, lr=1.08e-05, gnorm=1.697, train_wall=6, gb_free=9.7, wall=6132
2024-07-29 12:00:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=28987.4296875Mb; avail=226078.70703125Mb
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000570
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28987.4296875Mb; avail=226078.70703125Mb
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002113
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28987.4296875Mb; avail=226078.70703125Mb
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001909
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004913
2024-07-29 12:00:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=28987.4296875Mb; avail=226078.70703125Mb
2024-07-29 12:00:37 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.376 | nll_loss 2.731 | ppl 6.64 | wps 3178.5 | wpb 1361.3 | bsz 86.8 | num_updates 900 | best_loss 6.376
2024-07-29 12:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 900 updates
2024-07-29 12:00:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 12:01:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 12:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 60 @ 900 updates, score 6.376) (writing took 77.74599629966542 seconds)
2024-07-29 12:01:55 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2024-07-29 12:01:55 | INFO | train | epoch 060 | loss 5.817 | nll_loss 2.239 | ppl 4.72 | wps 525 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 900 | lr 1.08e-05 | gnorm 1.735 | train_wall 43 | gb_free 9.7 | wall 6212
2024-07-29 12:01:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:01:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:01:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:01:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000619
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=25237.0234375Mb; avail=229829.15625Mb
2024-07-29 12:01:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000082
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000716
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25237.0234375Mb; avail=229829.15625Mb
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25237.0234375Mb; avail=229829.15625Mb
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000206
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001273
2024-07-29 12:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=25237.0234375Mb; avail=229829.15625Mb
2024-07-29 12:01:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:01:55 | INFO | fairseq.trainer | begin training epoch 61
2024-07-29 12:01:55 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:01:59 | INFO | train_inner | epoch 061:      2 / 15 loss=5.784, nll_loss=2.19, ppl=4.56, wps=74.4, ups=0.02, wpb=3149, bsz=189.5, num_updates=902, lr=1.0824e-05, gnorm=2.065, train_wall=5, gb_free=11.1, wall=6216
2024-07-29 12:02:05 | INFO | train_inner | epoch 061:      4 / 15 loss=5.835, nll_loss=2.273, ppl=4.83, wps=1501.2, ups=0.34, wpb=4421.5, bsz=296, num_updates=904, lr=1.0848e-05, gnorm=1.95, train_wall=6, gb_free=10.3, wall=6222
2024-07-29 12:02:11 | INFO | train_inner | epoch 061:      6 / 15 loss=5.804, nll_loss=2.224, ppl=4.67, wps=1507.4, ups=0.33, wpb=4541.5, bsz=288, num_updates=906, lr=1.0872e-05, gnorm=1.582, train_wall=6, gb_free=10.3, wall=6228
2024-07-29 12:02:17 | INFO | train_inner | epoch 061:      8 / 15 loss=5.769, nll_loss=2.171, ppl=4.5, wps=1536.3, ups=0.33, wpb=4672.5, bsz=300, num_updates=908, lr=1.0896e-05, gnorm=1.635, train_wall=6, gb_free=10, wall=6234
2024-07-29 12:02:23 | INFO | train_inner | epoch 061:     10 / 15 loss=5.85, nll_loss=2.281, ppl=4.86, wps=1404.8, ups=0.36, wpb=3930.5, bsz=248, num_updates=910, lr=1.092e-05, gnorm=1.859, train_wall=6, gb_free=12.8, wall=6240
2024-07-29 12:02:29 | INFO | train_inner | epoch 061:     12 / 15 loss=5.79, nll_loss=2.198, ppl=4.59, wps=1614.6, ups=0.35, wpb=4610.5, bsz=280, num_updates=912, lr=1.0944e-05, gnorm=1.681, train_wall=6, gb_free=9.3, wall=6246
2024-07-29 12:02:35 | INFO | train_inner | epoch 061:     14 / 15 loss=5.791, nll_loss=2.217, ppl=4.65, wps=1473.4, ups=0.33, wpb=4519, bsz=312, num_updates=914, lr=1.0968e-05, gnorm=1.62, train_wall=6, gb_free=9.3, wall=6252
2024-07-29 12:02:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=42420.77734375Mb; avail=212645.359375Mb
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000571
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42421.26953125Mb; avail=212644.8671875Mb
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002127
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42421.26953125Mb; avail=212644.8671875Mb
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001888
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004908
2024-07-29 12:02:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=42421.26953125Mb; avail=212644.8671875Mb
2024-07-29 12:02:40 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.378 | nll_loss 2.735 | ppl 6.66 | wps 3181.1 | wpb 1361.3 | bsz 86.8 | num_updates 915 | best_loss 6.376
2024-07-29 12:02:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 915 updates
2024-07-29 12:02:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:03:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 61 @ 915 updates, score 6.378) (writing took 42.709623818751425 seconds)
2024-07-29 12:03:23 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2024-07-29 12:03:23 | INFO | train | epoch 061 | loss 5.801 | nll_loss 2.221 | ppl 4.66 | wps 735.4 | ups 0.17 | wpb 4315.9 | bsz 277.5 | num_updates 915 | lr 1.098e-05 | gnorm 1.763 | train_wall 43 | gb_free 11.4 | wall 6300
2024-07-29 12:03:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:03:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:03:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:03:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000716
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=23816.13671875Mb; avail=231250.04296875Mb
2024-07-29 12:03:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000070
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000723
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23816.13671875Mb; avail=231250.04296875Mb
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23816.13671875Mb; avail=231250.04296875Mb
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000213
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001232
2024-07-29 12:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=23816.13671875Mb; avail=231250.04296875Mb
2024-07-29 12:03:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:03:23 | INFO | fairseq.trainer | begin training epoch 62
2024-07-29 12:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:03:26 | INFO | train_inner | epoch 062:      1 / 15 loss=5.755, nll_loss=2.178, ppl=4.52, wps=189.5, ups=0.04, wpb=4834.5, bsz=332, num_updates=916, lr=1.0992e-05, gnorm=1.605, train_wall=6, gb_free=10.1, wall=6303
2024-07-29 12:03:31 | INFO | train_inner | epoch 062:      3 / 15 loss=5.769, nll_loss=2.181, ppl=4.53, wps=1516.7, ups=0.36, wpb=4240, bsz=244, num_updates=918, lr=1.1016e-05, gnorm=1.815, train_wall=6, gb_free=12.1, wall=6308
2024-07-29 12:03:38 | INFO | train_inner | epoch 062:      5 / 15 loss=5.813, nll_loss=2.229, ppl=4.69, wps=1547, ups=0.33, wpb=4754.5, bsz=316, num_updates=920, lr=1.104e-05, gnorm=1.691, train_wall=6, gb_free=8.9, wall=6315
2024-07-29 12:03:44 | INFO | train_inner | epoch 062:      7 / 15 loss=5.719, nll_loss=2.091, ppl=4.26, wps=1382.9, ups=0.32, wpb=4260, bsz=256, num_updates=922, lr=1.1064e-05, gnorm=1.679, train_wall=6, gb_free=9.4, wall=6321
2024-07-29 12:03:50 | INFO | train_inner | epoch 062:      9 / 15 loss=5.822, nll_loss=2.243, ppl=4.73, wps=1519.6, ups=0.34, wpb=4488, bsz=292, num_updates=924, lr=1.1088e-05, gnorm=1.666, train_wall=6, gb_free=10.9, wall=6327
2024-07-29 12:03:55 | INFO | train_inner | epoch 062:     11 / 15 loss=5.774, nll_loss=2.169, ppl=4.5, wps=1502.5, ups=0.35, wpb=4298, bsz=264, num_updates=926, lr=1.1112e-05, gnorm=1.776, train_wall=6, gb_free=9.8, wall=6332
2024-07-29 12:04:01 | INFO | train_inner | epoch 062:     13 / 15 loss=5.627, nll_loss=1.989, ppl=3.97, wps=1350.8, ups=0.39, wpb=3452.5, bsz=205.5, num_updates=928, lr=1.1136e-05, gnorm=1.794, train_wall=5, gb_free=8.7, wall=6337
2024-07-29 12:04:06 | INFO | train_inner | epoch 062:     15 / 15 loss=5.79, nll_loss=2.231, ppl=4.7, wps=1591.7, ups=0.35, wpb=4567, bsz=340, num_updates=930, lr=1.116e-05, gnorm=1.563, train_wall=6, gb_free=9.4, wall=6343
2024-07-29 12:04:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=41002.38671875Mb; avail=214063.21484375Mb
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000577
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41002.38671875Mb; avail=214063.70703125Mb
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002096
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41002.87890625Mb; avail=214062.72265625Mb
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001901
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004891
2024-07-29 12:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=41002.87890625Mb; avail=214063.21484375Mb
2024-07-29 12:04:09 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.372 | nll_loss 2.724 | ppl 6.61 | wps 3163 | wpb 1361.3 | bsz 86.8 | num_updates 930 | best_loss 6.372
2024-07-29 12:04:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 930 updates
2024-07-29 12:04:09 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 12:04:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 12:05:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 62 @ 930 updates, score 6.372) (writing took 66.34020912693813 seconds)
2024-07-29 12:05:15 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2024-07-29 12:05:15 | INFO | train | epoch 062 | loss 5.762 | nll_loss 2.167 | ppl 4.49 | wps 577.5 | ups 0.13 | wpb 4315.9 | bsz 277.5 | num_updates 930 | lr 1.116e-05 | gnorm 1.701 | train_wall 43 | gb_free 9.4 | wall 6412
2024-07-29 12:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000749
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=43984.9296875Mb; avail=211081.1640625Mb
2024-07-29 12:05:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000098
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000772
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43985.9140625Mb; avail=211080.1796875Mb
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43986.40625Mb; avail=211079.6875Mb
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000220
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001301
2024-07-29 12:05:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=43986.8984375Mb; avail=211079.1953125Mb
2024-07-29 12:05:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:05:15 | INFO | fairseq.trainer | begin training epoch 63
2024-07-29 12:05:15 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:05:21 | INFO | train_inner | epoch 063:      2 / 15 loss=5.762, nll_loss=2.173, ppl=4.51, wps=105.3, ups=0.03, wpb=3910, bsz=224, num_updates=932, lr=1.1184e-05, gnorm=1.919, train_wall=6, gb_free=8.9, wall=6418
2024-07-29 12:05:26 | INFO | train_inner | epoch 063:      4 / 15 loss=5.735, nll_loss=2.135, ppl=4.39, wps=1574.1, ups=0.35, wpb=4538, bsz=320, num_updates=934, lr=1.1208e-05, gnorm=1.544, train_wall=6, gb_free=10.2, wall=6423
2024-07-29 12:05:32 | INFO | train_inner | epoch 063:      6 / 15 loss=5.758, nll_loss=2.16, ppl=4.47, wps=1638.9, ups=0.34, wpb=4757.5, bsz=312, num_updates=936, lr=1.1232e-05, gnorm=1.547, train_wall=6, gb_free=11.8, wall=6429
2024-07-29 12:05:53 | INFO | train_inner | epoch 063:      8 / 15 loss=5.723, nll_loss=2.11, ppl=4.32, wps=453.5, ups=0.1, wpb=4752.5, bsz=316, num_updates=938, lr=1.1256e-05, gnorm=1.838, train_wall=21, gb_free=9.7, wall=6450
2024-07-29 12:05:59 | INFO | train_inner | epoch 063:     10 / 15 loss=5.867, nll_loss=2.27, ppl=4.82, wps=1414.5, ups=0.33, wpb=4282.5, bsz=228, num_updates=940, lr=1.128e-05, gnorm=1.942, train_wall=6, gb_free=9, wall=6456
2024-07-29 12:06:04 | INFO | train_inner | epoch 063:     12 / 15 loss=5.726, nll_loss=2.117, ppl=4.34, wps=1480.3, ups=0.41, wpb=3625, bsz=225.5, num_updates=942, lr=1.1304e-05, gnorm=1.776, train_wall=5, gb_free=9, wall=6461
2024-07-29 12:06:10 | INFO | train_inner | epoch 063:     14 / 15 loss=5.713, nll_loss=2.122, ppl=4.35, wps=1447, ups=0.35, wpb=4166, bsz=280, num_updates=944, lr=1.1328e-05, gnorm=1.618, train_wall=6, gb_free=11.4, wall=6467
2024-07-29 12:06:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=33041.31640625Mb; avail=222024.7734375Mb
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000485
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33041.31640625Mb; avail=222024.7734375Mb
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002104
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33041.31640625Mb; avail=222024.7734375Mb
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001888
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004788
2024-07-29 12:06:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=33041.31640625Mb; avail=222024.7734375Mb
2024-07-29 12:06:15 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.365 | nll_loss 2.727 | ppl 6.62 | wps 3193.8 | wpb 1361.3 | bsz 86.8 | num_updates 945 | best_loss 6.365
2024-07-29 12:06:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 945 updates
2024-07-29 12:06:15 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 12:06:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt
2024-07-29 12:07:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_best.pt (epoch 63 @ 945 updates, score 6.365) (writing took 65.45450092107058 seconds)
2024-07-29 12:07:21 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2024-07-29 12:07:21 | INFO | train | epoch 063 | loss 5.753 | nll_loss 2.154 | ppl 4.45 | wps 515 | ups 0.12 | wpb 4315.9 | bsz 277.5 | num_updates 945 | lr 1.134e-05 | gnorm 1.723 | train_wall 58 | gb_free 9.7 | wall 6538
2024-07-29 12:07:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:07:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:07:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:07:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000693
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=40973.25Mb; avail=214092.9296875Mb
2024-07-29 12:07:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000070
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000829
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40973.25Mb; avail=214092.9296875Mb
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000036
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40973.25Mb; avail=214092.9296875Mb
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000213
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001366
2024-07-29 12:07:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=40973.25Mb; avail=214092.9296875Mb
2024-07-29 12:07:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:07:21 | INFO | fairseq.trainer | begin training epoch 64
2024-07-29 12:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:07:23 | INFO | train_inner | epoch 064:      1 / 15 loss=5.693, nll_loss=2.085, ppl=4.24, wps=108.7, ups=0.03, wpb=3999.5, bsz=248, num_updates=946, lr=1.1352e-05, gnorm=1.791, train_wall=6, gb_free=10.1, wall=6540
2024-07-29 12:07:29 | INFO | train_inner | epoch 064:      3 / 15 loss=5.693, nll_loss=2.085, ppl=4.24, wps=1538.1, ups=0.33, wpb=4644, bsz=288, num_updates=948, lr=1.1376e-05, gnorm=1.649, train_wall=6, gb_free=9.3, wall=6546
2024-07-29 12:07:35 | INFO | train_inner | epoch 064:      5 / 15 loss=5.709, nll_loss=2.117, ppl=4.34, wps=1511.1, ups=0.35, wpb=4349, bsz=328, num_updates=950, lr=1.14e-05, gnorm=1.561, train_wall=6, gb_free=11.7, wall=6552
2024-07-29 12:07:41 | INFO | train_inner | epoch 064:      7 / 15 loss=5.779, nll_loss=2.201, ppl=4.6, wps=1525.4, ups=0.35, wpb=4334.5, bsz=300, num_updates=952, lr=1.1424e-05, gnorm=1.63, train_wall=6, gb_free=10.4, wall=6558
2024-07-29 12:07:47 | INFO | train_inner | epoch 064:      9 / 15 loss=5.701, nll_loss=2.072, ppl=4.21, wps=1531.6, ups=0.34, wpb=4490, bsz=280, num_updates=954, lr=1.1448e-05, gnorm=1.64, train_wall=6, gb_free=9.9, wall=6564
2024-07-29 12:07:52 | INFO | train_inner | epoch 064:     11 / 15 loss=5.762, nll_loss=2.157, ppl=4.46, wps=1422, ups=0.41, wpb=3464.5, bsz=205.5, num_updates=956, lr=1.1472e-05, gnorm=2.006, train_wall=5, gb_free=9, wall=6569
2024-07-29 12:07:58 | INFO | train_inner | epoch 064:     13 / 15 loss=5.724, nll_loss=2.109, ppl=4.31, wps=1584.5, ups=0.34, wpb=4716.5, bsz=304, num_updates=958, lr=1.1496e-05, gnorm=1.669, train_wall=6, gb_free=11.8, wall=6575
2024-07-29 12:08:04 | INFO | train_inner | epoch 064:     15 / 15 loss=5.709, nll_loss=2.104, ppl=4.3, wps=1587.3, ups=0.34, wpb=4709.5, bsz=304, num_updates=960, lr=1.152e-05, gnorm=1.574, train_wall=6, gb_free=11.2, wall=6580
2024-07-29 12:08:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12621.96484375Mb; avail=242459.74609375Mb
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000599
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12622.45703125Mb; avail=242459.74609375Mb
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002101
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12622.45703125Mb; avail=242459.74609375Mb
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001905
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004929
2024-07-29 12:08:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12622.45703125Mb; avail=242459.74609375Mb
2024-07-29 12:08:06 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.388 | nll_loss 2.734 | ppl 6.65 | wps 3114.3 | wpb 1361.3 | bsz 86.8 | num_updates 960 | best_loss 6.365
2024-07-29 12:08:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 960 updates
2024-07-29 12:08:06 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:08:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:08:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 64 @ 960 updates, score 6.388) (writing took 39.03812708193436 seconds)
2024-07-29 12:08:45 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2024-07-29 12:08:45 | INFO | train | epoch 064 | loss 5.72 | nll_loss 2.113 | ppl 4.33 | wps 768.2 | ups 0.18 | wpb 4315.9 | bsz 277.5 | num_updates 960 | lr 1.152e-05 | gnorm 1.704 | train_wall 43 | gb_free 11.2 | wall 6622
2024-07-29 12:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000740
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16957.21875Mb; avail=238124.98046875Mb
2024-07-29 12:08:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000734
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16957.21875Mb; avail=238124.98046875Mb
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16957.21875Mb; avail=238124.98046875Mb
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000214
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001248
2024-07-29 12:08:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16957.21875Mb; avail=238124.98046875Mb
2024-07-29 12:08:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:08:45 | INFO | fairseq.trainer | begin training epoch 65
2024-07-29 12:08:45 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:08:49 | INFO | train_inner | epoch 065:      2 / 15 loss=5.666, nll_loss=2.059, ppl=4.17, wps=125.2, ups=0.04, wpb=2873, bsz=221.5, num_updates=962, lr=1.1544e-05, gnorm=2.009, train_wall=4, gb_free=11.5, wall=6626
2024-07-29 12:08:55 | INFO | train_inner | epoch 065:      4 / 15 loss=5.697, nll_loss=2.091, ppl=4.26, wps=1582.8, ups=0.34, wpb=4706.5, bsz=320, num_updates=964, lr=1.1568e-05, gnorm=1.648, train_wall=6, gb_free=10, wall=6632
2024-07-29 12:09:01 | INFO | train_inner | epoch 065:      6 / 15 loss=5.702, nll_loss=2.085, ppl=4.24, wps=1485.6, ups=0.33, wpb=4444.5, bsz=280, num_updates=966, lr=1.1592e-05, gnorm=1.673, train_wall=6, gb_free=10.2, wall=6638
2024-07-29 12:09:07 | INFO | train_inner | epoch 065:      8 / 15 loss=5.76, nll_loss=2.159, ppl=4.47, wps=1629.1, ups=0.35, wpb=4627, bsz=288, num_updates=968, lr=1.1616e-05, gnorm=1.751, train_wall=6, gb_free=12.6, wall=6644
2024-07-29 12:09:23 | INFO | train_inner | epoch 065:     10 / 15 loss=5.71, nll_loss=2.09, ppl=4.26, wps=549, ups=0.12, wpb=4491, bsz=264, num_updates=970, lr=1.164e-05, gnorm=1.605, train_wall=16, gb_free=8.3, wall=6660
2024-07-29 12:09:29 | INFO | train_inner | epoch 065:     12 / 15 loss=5.712, nll_loss=2.113, ppl=4.33, wps=1671.4, ups=0.35, wpb=4842.5, bsz=312, num_updates=972, lr=1.1664e-05, gnorm=1.601, train_wall=6, gb_free=11, wall=6666
2024-07-29 12:09:35 | INFO | train_inner | epoch 065:     14 / 15 loss=5.701, nll_loss=2.078, ppl=4.22, wps=1424.6, ups=0.35, wpb=4040.5, bsz=252, num_updates=974, lr=1.1688e-05, gnorm=1.705, train_wall=6, gb_free=12.1, wall=6672
2024-07-29 12:09:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9256.88671875Mb; avail=245825.40234375Mb
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000481
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9256.88671875Mb; avail=245825.40234375Mb
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002099
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9256.88671875Mb; avail=245825.40234375Mb
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001862
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004754
2024-07-29 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9256.88671875Mb; avail=245825.40234375Mb
2024-07-29 12:09:40 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.39 | nll_loss 2.725 | ppl 6.61 | wps 3144.3 | wpb 1361.3 | bsz 86.8 | num_updates 975 | best_loss 6.365
2024-07-29 12:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 975 updates
2024-07-29 12:09:40 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:10:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:10:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 65 @ 975 updates, score 6.39) (writing took 39.225002001971006 seconds)
2024-07-29 12:10:19 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2024-07-29 12:10:19 | INFO | train | epoch 065 | loss 5.706 | nll_loss 2.094 | ppl 4.27 | wps 685.2 | ups 0.16 | wpb 4315.9 | bsz 277.5 | num_updates 975 | lr 1.17e-05 | gnorm 1.705 | train_wall 53 | gb_free 9.6 | wall 6716
2024-07-29 12:10:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:10:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:10:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:10:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000677
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12899.6640625Mb; avail=242182.625Mb
2024-07-29 12:10:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000092
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000746
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12899.6640625Mb; avail=242182.625Mb
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12899.6640625Mb; avail=242182.625Mb
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000209
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001263
2024-07-29 12:10:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12899.6640625Mb; avail=242182.625Mb
2024-07-29 12:10:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:10:19 | INFO | fairseq.trainer | begin training epoch 66
2024-07-29 12:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:10:22 | INFO | train_inner | epoch 066:      1 / 15 loss=5.648, nll_loss=2.011, ppl=4.03, wps=201, ups=0.04, wpb=4777.5, bsz=292, num_updates=976, lr=1.1712e-05, gnorm=1.583, train_wall=6, gb_free=10.1, wall=6719
2024-07-29 12:10:28 | INFO | train_inner | epoch 066:      3 / 15 loss=5.602, nll_loss=1.954, ppl=3.87, wps=1421.5, ups=0.34, wpb=4231.5, bsz=280, num_updates=978, lr=1.1736e-05, gnorm=1.542, train_wall=6, gb_free=10.9, wall=6725
2024-07-29 12:10:34 | INFO | train_inner | epoch 066:      5 / 15 loss=5.629, nll_loss=1.995, ppl=3.99, wps=1517.7, ups=0.33, wpb=4623, bsz=296, num_updates=980, lr=1.176e-05, gnorm=1.675, train_wall=6, gb_free=8.9, wall=6731
2024-07-29 12:10:41 | INFO | train_inner | epoch 066:      7 / 15 loss=5.676, nll_loss=2.067, ppl=4.19, wps=1462.2, ups=0.32, wpb=4534, bsz=296, num_updates=982, lr=1.1784e-05, gnorm=1.526, train_wall=6, gb_free=9.7, wall=6738
2024-07-29 12:10:47 | INFO | train_inner | epoch 066:      9 / 15 loss=5.704, nll_loss=2.115, ppl=4.33, wps=1604.4, ups=0.34, wpb=4720.5, bsz=316, num_updates=984, lr=1.1808e-05, gnorm=1.698, train_wall=6, gb_free=11.6, wall=6744
2024-07-29 12:10:51 | INFO | train_inner | epoch 066:     11 / 15 loss=5.699, nll_loss=2.087, ppl=4.25, wps=1513.6, ups=0.43, wpb=3556, bsz=197.5, num_updates=986, lr=1.1832e-05, gnorm=2.025, train_wall=5, gb_free=10.4, wall=6748
2024-07-29 12:10:57 | INFO | train_inner | epoch 066:     13 / 15 loss=5.634, nll_loss=1.984, ppl=3.96, wps=1501.2, ups=0.32, wpb=4645.5, bsz=304, num_updates=988, lr=1.1856e-05, gnorm=1.555, train_wall=6, gb_free=9.6, wall=6754
2024-07-29 12:11:03 | INFO | train_inner | epoch 066:     15 / 15 loss=5.619, nll_loss=1.967, ppl=3.91, wps=1356.3, ups=0.37, wpb=3626, bsz=244, num_updates=990, lr=1.188e-05, gnorm=1.859, train_wall=5, gb_free=10.1, wall=6760
2024-07-29 12:11:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12938.09375Mb; avail=242144.15625Mb
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000589
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12938.5859375Mb; avail=242143.6640625Mb
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002118
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12938.5859375Mb; avail=242143.6640625Mb
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001933
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.005017
2024-07-29 12:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12938.5859375Mb; avail=242143.6640625Mb
2024-07-29 12:11:05 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.394 | nll_loss 2.719 | ppl 6.58 | wps 3179.5 | wpb 1361.3 | bsz 86.8 | num_updates 990 | best_loss 6.365
2024-07-29 12:11:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 990 updates
2024-07-29 12:11:05 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:11:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 66 @ 990 updates, score 6.394) (writing took 38.68646632274613 seconds)
2024-07-29 12:11:44 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2024-07-29 12:11:44 | INFO | train | epoch 066 | loss 5.65 | nll_loss 2.022 | ppl 4.06 | wps 767.4 | ups 0.18 | wpb 4315.9 | bsz 277.5 | num_updates 990 | lr 1.188e-05 | gnorm 1.689 | train_wall 43 | gb_free 10.1 | wall 6801
2024-07-29 12:11:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:11:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:11:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:11:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000724
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=18068.63671875Mb; avail=237013.61328125Mb
2024-07-29 12:11:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000745
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18068.63671875Mb; avail=237013.61328125Mb
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18068.63671875Mb; avail=237013.61328125Mb
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000213
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001262
2024-07-29 12:11:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=18068.63671875Mb; avail=237013.61328125Mb
2024-07-29 12:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:11:44 | INFO | fairseq.trainer | begin training epoch 67
2024-07-29 12:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:11:50 | INFO | train_inner | epoch 067:      2 / 15 loss=5.614, nll_loss=1.977, ppl=3.94, wps=208.6, ups=0.04, wpb=4904.5, bsz=332, num_updates=992, lr=1.1904e-05, gnorm=1.637, train_wall=6, gb_free=8.9, wall=6807
2024-07-29 12:11:56 | INFO | train_inner | epoch 067:      4 / 15 loss=5.699, nll_loss=2.088, ppl=4.25, wps=1633.9, ups=0.35, wpb=4720, bsz=292, num_updates=994, lr=1.1928e-05, gnorm=1.941, train_wall=6, gb_free=10.1, wall=6813
2024-07-29 12:12:00 | INFO | train_inner | epoch 067:      6 / 15 loss=5.735, nll_loss=2.133, ppl=4.39, wps=1359.6, ups=0.45, wpb=3004, bsz=181.5, num_updates=996, lr=1.1952e-05, gnorm=2.478, train_wall=4, gb_free=14.5, wall=6817
2024-07-29 12:12:16 | INFO | train_inner | epoch 067:      8 / 15 loss=5.69, nll_loss=2.091, ppl=4.26, wps=522.8, ups=0.13, wpb=4169, bsz=284, num_updates=998, lr=1.1976e-05, gnorm=1.704, train_wall=16, gb_free=10.6, wall=6833
2024-07-29 12:12:22 | INFO | train_inner | epoch 067:     10 / 15 loss=5.619, nll_loss=1.989, ppl=3.97, wps=1456.4, ups=0.35, wpb=4151, bsz=260, num_updates=1000, lr=1.2e-05, gnorm=1.784, train_wall=6, gb_free=10.2, wall=6839
2024-07-29 12:12:28 | INFO | train_inner | epoch 067:     12 / 15 loss=5.611, nll_loss=1.959, ppl=3.89, wps=1475.9, ups=0.33, wpb=4430, bsz=240, num_updates=1002, lr=1.2024e-05, gnorm=1.718, train_wall=6, gb_free=10.5, wall=6845
2024-07-29 12:12:34 | INFO | train_inner | epoch 067:     14 / 15 loss=5.586, nll_loss=1.934, ppl=3.82, wps=1457.6, ups=0.32, wpb=4487, bsz=324, num_updates=1004, lr=1.2048e-05, gnorm=1.494, train_wall=6, gb_free=9.1, wall=6851
2024-07-29 12:12:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=10056.56640625Mb; avail=245025.7265625Mb
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000487
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10056.56640625Mb; avail=245025.7265625Mb
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002119
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10056.56640625Mb; avail=245025.7265625Mb
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001887
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004803
2024-07-29 12:12:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=10056.56640625Mb; avail=245025.7265625Mb
2024-07-29 12:12:39 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.392 | nll_loss 2.726 | ppl 6.61 | wps 3186.7 | wpb 1361.3 | bsz 86.8 | num_updates 1005 | best_loss 6.365
2024-07-29 12:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 1005 updates
2024-07-29 12:12:39 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:13:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:13:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 67 @ 1005 updates, score 6.392) (writing took 39.15158559801057 seconds)
2024-07-29 12:13:18 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2024-07-29 12:13:18 | INFO | train | epoch 067 | loss 5.647 | nll_loss 2.02 | ppl 4.05 | wps 685.5 | ups 0.16 | wpb 4315.9 | bsz 277.5 | num_updates 1005 | lr 1.206e-05 | gnorm 1.798 | train_wall 53 | gb_free 11.4 | wall 6895
2024-07-29 12:13:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:13:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:13:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:13:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000657
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16551.25390625Mb; avail=238531.03515625Mb
2024-07-29 12:13:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000072
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000738
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16551.25390625Mb; avail=238531.03515625Mb
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16551.25390625Mb; avail=238531.03515625Mb
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000209
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001247
2024-07-29 12:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16551.25390625Mb; avail=238531.03515625Mb
2024-07-29 12:13:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:13:18 | INFO | fairseq.trainer | begin training epoch 68
2024-07-29 12:13:18 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:13:21 | INFO | train_inner | epoch 068:      1 / 15 loss=5.669, nll_loss=2.042, ppl=4.12, wps=202.1, ups=0.04, wpb=4805.5, bsz=312, num_updates=1006, lr=1.2072e-05, gnorm=1.541, train_wall=6, gb_free=9.4, wall=6898
2024-07-29 12:13:27 | INFO | train_inner | epoch 068:      3 / 15 loss=5.69, nll_loss=2.075, ppl=4.21, wps=1701.1, ups=0.35, wpb=4820, bsz=300, num_updates=1008, lr=1.2096e-05, gnorm=1.751, train_wall=6, gb_free=11.7, wall=6904
2024-07-29 12:13:33 | INFO | train_inner | epoch 068:      5 / 15 loss=5.575, nll_loss=1.929, ppl=3.81, wps=1565.6, ups=0.34, wpb=4606, bsz=280, num_updates=1010, lr=1.212e-05, gnorm=1.582, train_wall=6, gb_free=9.8, wall=6910
2024-07-29 12:13:38 | INFO | train_inner | epoch 068:      7 / 15 loss=5.606, nll_loss=1.969, ppl=3.92, wps=1374.9, ups=0.4, wpb=3457, bsz=225.5, num_updates=1012, lr=1.2144e-05, gnorm=1.882, train_wall=5, gb_free=8.2, wall=6915
2024-07-29 12:13:44 | INFO | train_inner | epoch 068:      9 / 15 loss=5.576, nll_loss=1.928, ppl=3.8, wps=1435.2, ups=0.33, wpb=4312.5, bsz=240, num_updates=1014, lr=1.2168e-05, gnorm=1.709, train_wall=6, gb_free=9.9, wall=6921
2024-07-29 12:13:50 | INFO | train_inner | epoch 068:     11 / 15 loss=5.637, nll_loss=2.023, ppl=4.06, wps=1613.1, ups=0.34, wpb=4811.5, bsz=356, num_updates=1016, lr=1.2192e-05, gnorm=1.556, train_wall=6, gb_free=10, wall=6927
2024-07-29 12:13:56 | INFO | train_inner | epoch 068:     13 / 15 loss=5.546, nll_loss=1.886, ppl=3.7, wps=1293.9, ups=0.35, wpb=3709, bsz=244, num_updates=1018, lr=1.2216e-05, gnorm=1.686, train_wall=6, gb_free=12.7, wall=6933
2024-07-29 12:14:01 | INFO | train_inner | epoch 068:     15 / 15 loss=5.623, nll_loss=2.006, ppl=4.02, wps=1522.8, ups=0.35, wpb=4352, bsz=292, num_updates=1020, lr=1.224e-05, gnorm=1.731, train_wall=6, gb_free=11.1, wall=6938
2024-07-29 12:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=16593.20703125Mb; avail=238489.0390625Mb
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000580
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16593.69921875Mb; avail=238488.546875Mb
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002104
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16593.69921875Mb; avail=238488.546875Mb
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001914
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004925
2024-07-29 12:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=16593.69921875Mb; avail=238488.546875Mb
2024-07-29 12:14:04 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.384 | nll_loss 2.723 | ppl 6.6 | wps 3184.5 | wpb 1361.3 | bsz 86.8 | num_updates 1020 | best_loss 6.365
2024-07-29 12:14:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 1020 updates
2024-07-29 12:14:04 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:14:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:14:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 68 @ 1020 updates, score 6.384) (writing took 39.026408485136926 seconds)
2024-07-29 12:14:43 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2024-07-29 12:14:43 | INFO | train | epoch 068 | loss 5.615 | nll_loss 1.983 | ppl 3.95 | wps 766 | ups 0.18 | wpb 4315.9 | bsz 277.5 | num_updates 1020 | lr 1.224e-05 | gnorm 1.695 | train_wall 43 | gb_free 11.1 | wall 6980
2024-07-29 12:14:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:14:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:14:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:14:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000731
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=24414.05078125Mb; avail=230668.1953125Mb
2024-07-29 12:14:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000075
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000730
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24414.05078125Mb; avail=230668.1953125Mb
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24414.05078125Mb; avail=230668.1953125Mb
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000212
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001248
2024-07-29 12:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=24414.05078125Mb; avail=230668.1953125Mb
2024-07-29 12:14:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:14:43 | INFO | fairseq.trainer | begin training epoch 69
2024-07-29 12:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:14:49 | INFO | train_inner | epoch 069:      2 / 15 loss=5.532, nll_loss=1.855, ppl=3.62, wps=168.5, ups=0.04, wpb=3980, bsz=228, num_updates=1022, lr=1.2264e-05, gnorm=1.689, train_wall=6, gb_free=9.9, wall=6986
2024-07-29 12:14:53 | INFO | train_inner | epoch 069:      4 / 15 loss=5.571, nll_loss=1.911, ppl=3.76, wps=1476.1, ups=0.44, wpb=3381.5, bsz=225.5, num_updates=1024, lr=1.2288e-05, gnorm=1.755, train_wall=5, gb_free=11.3, wall=6990
2024-07-29 12:15:09 | INFO | train_inner | epoch 069:      6 / 15 loss=5.527, nll_loss=1.874, ppl=3.67, wps=554.1, ups=0.13, wpb=4384.5, bsz=276, num_updates=1026, lr=1.2312e-05, gnorm=1.694, train_wall=16, gb_free=11.2, wall=7006
2024-07-29 12:15:15 | INFO | train_inner | epoch 069:      8 / 15 loss=5.548, nll_loss=1.906, ppl=3.75, wps=1684.6, ups=0.34, wpb=4890, bsz=320, num_updates=1028, lr=1.2336e-05, gnorm=1.854, train_wall=6, gb_free=9.8, wall=7012
2024-07-29 12:15:21 | INFO | train_inner | epoch 069:     10 / 15 loss=5.606, nll_loss=1.984, ppl=3.95, wps=1479.5, ups=0.35, wpb=4269, bsz=276, num_updates=1030, lr=1.236e-05, gnorm=1.771, train_wall=6, gb_free=12.1, wall=7018
2024-07-29 12:15:27 | INFO | train_inner | epoch 069:     12 / 15 loss=5.586, nll_loss=1.937, ppl=3.83, wps=1535.6, ups=0.34, wpb=4547, bsz=288, num_updates=1032, lr=1.2384e-05, gnorm=1.588, train_wall=6, gb_free=11.9, wall=7023
2024-07-29 12:15:33 | INFO | train_inner | epoch 069:     14 / 15 loss=5.605, nll_loss=1.945, ppl=3.85, wps=1426.6, ups=0.32, wpb=4481, bsz=288, num_updates=1034, lr=1.2408e-05, gnorm=1.556, train_wall=6, gb_free=10.5, wall=7030
2024-07-29 12:15:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12763.14453125Mb; avail=242319.1484375Mb
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000503
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12763.14453125Mb; avail=242319.1484375Mb
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002076
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12763.14453125Mb; avail=242319.1484375Mb
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001886
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004788
2024-07-29 12:15:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12763.14453125Mb; avail=242319.1484375Mb
2024-07-29 12:15:38 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.381 | nll_loss 2.714 | ppl 6.56 | wps 3189.5 | wpb 1361.3 | bsz 86.8 | num_updates 1035 | best_loss 6.365
2024-07-29 12:15:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 1035 updates
2024-07-29 12:15:38 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:16:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:16:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 69 @ 1035 updates, score 6.381) (writing took 39.283881067764014 seconds)
2024-07-29 12:16:17 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2024-07-29 12:16:17 | INFO | train | epoch 069 | loss 5.573 | nll_loss 1.923 | ppl 3.79 | wps 684.6 | ups 0.16 | wpb 4315.9 | bsz 277.5 | num_updates 1035 | lr 1.242e-05 | gnorm 1.689 | train_wall 53 | gb_free 12.2 | wall 7074
2024-07-29 12:16:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:16:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:16:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:16:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000661
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21970.2890625Mb; avail=233112.00390625Mb
2024-07-29 12:16:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000723
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21970.2890625Mb; avail=233112.00390625Mb
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000030
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21970.2890625Mb; avail=233112.00390625Mb
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000208
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001230
2024-07-29 12:16:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21970.28515625Mb; avail=233112.00390625Mb
2024-07-29 12:16:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:16:17 | INFO | fairseq.trainer | begin training epoch 70
2024-07-29 12:16:17 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:16:20 | INFO | train_inner | epoch 070:      1 / 15 loss=5.623, nll_loss=1.978, ppl=3.94, wps=203.1, ups=0.04, wpb=4822.5, bsz=336, num_updates=1036, lr=1.2432e-05, gnorm=1.516, train_wall=6, gb_free=11.7, wall=7077
2024-07-29 12:16:26 | INFO | train_inner | epoch 070:      3 / 15 loss=5.549, nll_loss=1.891, ppl=3.71, wps=1425.8, ups=0.35, wpb=4017, bsz=220, num_updates=1038, lr=1.2456e-05, gnorm=1.712, train_wall=6, gb_free=10.8, wall=7083
2024-07-29 12:16:32 | INFO | train_inner | epoch 070:      5 / 15 loss=5.601, nll_loss=1.985, ppl=3.96, wps=1601.4, ups=0.34, wpb=4774.5, bsz=292, num_updates=1040, lr=1.248e-05, gnorm=1.647, train_wall=6, gb_free=9.9, wall=7089
2024-07-29 12:16:37 | INFO | train_inner | epoch 070:      7 / 15 loss=5.544, nll_loss=1.882, ppl=3.68, wps=1522.6, ups=0.41, wpb=3686.5, bsz=253.5, num_updates=1042, lr=1.2504e-05, gnorm=1.687, train_wall=5, gb_free=15.8, wall=7094
2024-07-29 12:16:42 | INFO | train_inner | epoch 070:      9 / 15 loss=5.507, nll_loss=1.831, ppl=3.56, wps=1483, ups=0.37, wpb=4053, bsz=268, num_updates=1044, lr=1.2528e-05, gnorm=1.635, train_wall=5, gb_free=11.6, wall=7099
2024-07-29 12:16:48 | INFO | train_inner | epoch 070:     11 / 15 loss=5.568, nll_loss=1.924, ppl=3.79, wps=1423, ups=0.33, wpb=4258.5, bsz=288, num_updates=1046, lr=1.2552e-05, gnorm=1.686, train_wall=6, gb_free=10.5, wall=7105
2024-07-29 12:16:54 | INFO | train_inner | epoch 070:     13 / 15 loss=5.511, nll_loss=1.85, ppl=3.6, wps=1453.6, ups=0.32, wpb=4546, bsz=316, num_updates=1048, lr=1.2576e-05, gnorm=1.714, train_wall=6, gb_free=9.3, wall=7111
2024-07-29 12:17:00 | INFO | train_inner | epoch 070:     15 / 15 loss=5.658, nll_loss=2.05, ppl=4.14, wps=1561.3, ups=0.34, wpb=4648, bsz=288, num_updates=1050, lr=1.26e-05, gnorm=1.711, train_wall=6, gb_free=10, wall=7117
2024-07-29 12:17:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=22028.59375Mb; avail=233053.1640625Mb
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000569
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22029.0859375Mb; avail=233053.1640625Mb
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002108
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22029.0859375Mb; avail=233053.1640625Mb
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001891
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004885
2024-07-29 12:17:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=22029.0859375Mb; avail=233053.1640625Mb
2024-07-29 12:17:03 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.387 | nll_loss 2.727 | ppl 6.62 | wps 3186.3 | wpb 1361.3 | bsz 86.8 | num_updates 1050 | best_loss 6.365
2024-07-29 12:17:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 1050 updates
2024-07-29 12:17:03 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:17:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 70 @ 1050 updates, score 6.387) (writing took 39.16374129196629 seconds)
2024-07-29 12:17:42 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2024-07-29 12:17:42 | INFO | train | epoch 070 | loss 5.568 | nll_loss 1.922 | ppl 3.79 | wps 765.3 | ups 0.18 | wpb 4315.9 | bsz 277.5 | num_updates 1050 | lr 1.26e-05 | gnorm 1.673 | train_wall 43 | gb_free 10 | wall 7159
2024-07-29 12:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000739
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=29837.5078125Mb; avail=225244.7421875Mb
2024-07-29 12:17:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000071
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000715
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29837.5078125Mb; avail=225244.7421875Mb
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000031
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29837.5078125Mb; avail=225244.7421875Mb
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000208
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001216
2024-07-29 12:17:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=29837.5078125Mb; avail=225244.7421875Mb
2024-07-29 12:17:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:17:42 | INFO | fairseq.trainer | begin training epoch 71
2024-07-29 12:17:42 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:17:48 | INFO | train_inner | epoch 071:      2 / 15 loss=5.577, nll_loss=1.928, ppl=3.81, wps=190.3, ups=0.04, wpb=4524.5, bsz=296, num_updates=1052, lr=1.2624e-05, gnorm=1.684, train_wall=6, gb_free=9.8, wall=7165
2024-07-29 12:17:54 | INFO | train_inner | epoch 071:      4 / 15 loss=5.527, nll_loss=1.852, ppl=3.61, wps=1411.6, ups=0.34, wpb=4174, bsz=260, num_updates=1054, lr=1.2648e-05, gnorm=1.638, train_wall=6, gb_free=9.9, wall=7171
2024-07-29 12:18:00 | INFO | train_inner | epoch 071:      6 / 15 loss=5.547, nll_loss=1.872, ppl=3.66, wps=1576.9, ups=0.34, wpb=4593, bsz=276, num_updates=1056, lr=1.2672e-05, gnorm=1.674, train_wall=6, gb_free=11.8, wall=7177
2024-07-29 12:18:15 | INFO | train_inner | epoch 071:      8 / 15 loss=5.484, nll_loss=1.816, ppl=3.52, wps=576.5, ups=0.13, wpb=4555.5, bsz=300, num_updates=1058, lr=1.2696e-05, gnorm=1.517, train_wall=16, gb_free=12, wall=7192
2024-07-29 12:18:20 | INFO | train_inner | epoch 071:     10 / 15 loss=5.537, nll_loss=1.892, ppl=3.71, wps=1467.9, ups=0.44, wpb=3340, bsz=237.5, num_updates=1060, lr=1.272e-05, gnorm=1.784, train_wall=5, gb_free=14.7, wall=7197
2024-07-29 12:18:26 | INFO | train_inner | epoch 071:     12 / 15 loss=5.515, nll_loss=1.865, ppl=3.64, wps=1459.1, ups=0.34, wpb=4230.5, bsz=276, num_updates=1062, lr=1.2744e-05, gnorm=1.641, train_wall=6, gb_free=12.8, wall=7203
2024-07-29 12:18:32 | INFO | train_inner | epoch 071:     14 / 15 loss=5.542, nll_loss=1.876, ppl=3.67, wps=1581.9, ups=0.34, wpb=4713, bsz=296, num_updates=1064, lr=1.2768e-05, gnorm=1.57, train_wall=6, gb_free=10.7, wall=7209
2024-07-29 12:18:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=12769.6328125Mb; avail=242312.65625Mb
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000490
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12769.6328125Mb; avail=242312.65625Mb
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002163
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12769.6328125Mb; avail=242312.65625Mb
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001897
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004856
2024-07-29 12:18:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=12769.6328125Mb; avail=242312.65625Mb
2024-07-29 12:18:37 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.395 | nll_loss 2.726 | ppl 6.61 | wps 3189.6 | wpb 1361.3 | bsz 86.8 | num_updates 1065 | best_loss 6.365
2024-07-29 12:18:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 1065 updates
2024-07-29 12:18:37 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:19:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:19:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 71 @ 1065 updates, score 6.395) (writing took 39.097557329107076 seconds)
2024-07-29 12:19:16 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2024-07-29 12:19:16 | INFO | train | epoch 071 | loss 5.533 | nll_loss 1.87 | ppl 3.66 | wps 686.2 | ups 0.16 | wpb 4315.9 | bsz 277.5 | num_updates 1065 | lr 1.278e-05 | gnorm 1.64 | train_wall 53 | gb_free 10.2 | wall 7253
2024-07-29 12:19:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:19:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:19:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:19:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000680
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17061.375Mb; avail=238020.9140625Mb
2024-07-29 12:19:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000070
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000721
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17061.375Mb; avail=238020.9140625Mb
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000032
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17061.375Mb; avail=238020.9140625Mb
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000207
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001229
2024-07-29 12:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17061.375Mb; avail=238020.9140625Mb
2024-07-29 12:19:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:19:16 | INFO | fairseq.trainer | begin training epoch 72
2024-07-29 12:19:16 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:19:19 | INFO | train_inner | epoch 072:      1 / 15 loss=5.507, nll_loss=1.829, ppl=3.55, wps=185.6, ups=0.04, wpb=4408, bsz=288, num_updates=1066, lr=1.2792e-05, gnorm=1.527, train_wall=6, gb_free=9.9, wall=7256
2024-07-29 12:19:25 | INFO | train_inner | epoch 072:      3 / 15 loss=5.517, nll_loss=1.848, ppl=3.6, wps=1520.1, ups=0.36, wpb=4271, bsz=264, num_updates=1068, lr=1.2816e-05, gnorm=1.582, train_wall=6, gb_free=9.7, wall=7262
2024-07-29 12:19:31 | INFO | train_inner | epoch 072:      5 / 15 loss=5.483, nll_loss=1.81, ppl=3.51, wps=1427.6, ups=0.33, wpb=4270.5, bsz=308, num_updates=1070, lr=1.284e-05, gnorm=1.463, train_wall=6, gb_free=9.4, wall=7268
2024-07-29 12:19:36 | INFO | train_inner | epoch 072:      7 / 15 loss=5.474, nll_loss=1.805, ppl=3.49, wps=1435.4, ups=0.41, wpb=3543.5, bsz=229.5, num_updates=1072, lr=1.2864e-05, gnorm=1.826, train_wall=5, gb_free=11.7, wall=7273
2024-07-29 12:19:42 | INFO | train_inner | epoch 072:      9 / 15 loss=5.499, nll_loss=1.84, ppl=3.58, wps=1588.9, ups=0.33, wpb=4771, bsz=300, num_updates=1074, lr=1.2888e-05, gnorm=1.569, train_wall=6, gb_free=9.7, wall=7279
2024-07-29 12:19:48 | INFO | train_inner | epoch 072:     11 / 15 loss=5.497, nll_loss=1.843, ppl=3.59, wps=1506.5, ups=0.35, wpb=4301.5, bsz=304, num_updates=1076, lr=1.2912e-05, gnorm=1.746, train_wall=6, gb_free=9.5, wall=7285
2024-07-29 12:19:53 | INFO | train_inner | epoch 072:     13 / 15 loss=5.531, nll_loss=1.845, ppl=3.59, wps=1536.7, ups=0.34, wpb=4553.5, bsz=256, num_updates=1078, lr=1.2936e-05, gnorm=1.575, train_wall=6, gb_free=12, wall=7290
2024-07-29 12:19:59 | INFO | train_inner | epoch 072:     15 / 15 loss=5.521, nll_loss=1.843, ppl=3.59, wps=1519.5, ups=0.34, wpb=4489.5, bsz=272, num_updates=1080, lr=1.296e-05, gnorm=1.639, train_wall=6, gb_free=9.7, wall=7296
2024-07-29 12:19:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=17108.2734375Mb; avail=237973.96875Mb
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000599
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17108.765625Mb; avail=237973.4765625Mb
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002119
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17108.765625Mb; avail=237973.4765625Mb
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001910
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004983
2024-07-29 12:19:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=17108.765625Mb; avail=237973.4765625Mb
2024-07-29 12:20:02 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 6.396 | nll_loss 2.73 | ppl 6.63 | wps 3186.1 | wpb 1361.3 | bsz 86.8 | num_updates 1080 | best_loss 6.365
2024-07-29 12:20:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 1080 updates
2024-07-29 12:20:02 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:20:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:20:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 72 @ 1080 updates, score 6.396) (writing took 38.75266069127247 seconds)
2024-07-29 12:20:40 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2024-07-29 12:20:40 | INFO | train | epoch 072 | loss 5.503 | nll_loss 1.832 | ppl 3.56 | wps 768.7 | ups 0.18 | wpb 4315.9 | bsz 277.5 | num_updates 1080 | lr 1.296e-05 | gnorm 1.618 | train_wall 43 | gb_free 9.7 | wall 7337
2024-07-29 12:20:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:ne-hi': 4163}; raw total size: 4163
2024-07-29 12:20:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:ne-hi': 4163}; resampled total size: 4163
2024-07-29 12:20:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Upsampling ratios: {'main:ne-hi': 1.0}
2024-07-29 12:20:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.000722
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=21355.80078125Mb; avail=233726.44921875Mb
2024-07-29 12:20:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.000103
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.000798
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.80078125Mb; avail=233726.44921875Mb
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.000033
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.80078125Mb; avail=233726.44921875Mb
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.000222
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.001390
2024-07-29 12:20:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=21355.80078125Mb; avail=233726.44921875Mb
2024-07-29 12:20:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 15
2024-07-29 12:20:40 | INFO | fairseq.trainer | begin training epoch 73
2024-07-29 12:20:40 | INFO | fairseq_cli.train | Start iterating over samples
2024-07-29 12:20:46 | INFO | train_inner | epoch 073:      2 / 15 loss=5.495, nll_loss=1.819, ppl=3.53, wps=198.8, ups=0.04, wpb=4677.5, bsz=316, num_updates=1082, lr=1.2984e-05, gnorm=1.484, train_wall=6, gb_free=11, wall=7343
2024-07-29 12:20:52 | INFO | train_inner | epoch 073:      4 / 15 loss=5.477, nll_loss=1.799, ppl=3.48, wps=1515.1, ups=0.33, wpb=4555, bsz=280, num_updates=1084, lr=1.3008e-05, gnorm=1.475, train_wall=6, gb_free=11.1, wall=7349
2024-07-29 12:20:58 | INFO | train_inner | epoch 073:      6 / 15 loss=5.528, nll_loss=1.889, ppl=3.7, wps=1559.1, ups=0.34, wpb=4580, bsz=316, num_updates=1086, lr=1.3032e-05, gnorm=1.609, train_wall=6, gb_free=10.2, wall=7355
2024-07-29 12:21:04 | INFO | train_inner | epoch 073:      8 / 15 loss=5.48, nll_loss=1.81, ppl=3.51, wps=1431.8, ups=0.36, wpb=4021, bsz=212, num_updates=1088, lr=1.3056e-05, gnorm=1.734, train_wall=6, gb_free=11.8, wall=7361
2024-07-29 12:21:19 | INFO | train_inner | epoch 073:     10 / 15 loss=5.518, nll_loss=1.862, ppl=3.64, wps=610.6, ups=0.13, wpb=4736, bsz=312, num_updates=1090, lr=1.308e-05, gnorm=1.656, train_wall=16, gb_free=11.1, wall=7376
2024-07-29 12:21:24 | INFO | train_inner | epoch 073:     12 / 15 loss=5.494, nll_loss=1.789, ppl=3.46, wps=1384.6, ups=0.42, wpb=3262.5, bsz=201.5, num_updates=1092, lr=1.3104e-05, gnorm=1.757, train_wall=5, gb_free=11.2, wall=7381
2024-07-29 12:21:30 | INFO | train_inner | epoch 073:     14 / 15 loss=5.583, nll_loss=1.928, ppl=3.81, wps=1491.1, ups=0.34, wpb=4363, bsz=296, num_updates=1094, lr=1.3128e-05, gnorm=1.766, train_wall=6, gb_free=10.9, wall=7387
2024-07-29 12:21:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: used=9169.61328125Mb; avail=245912.6796875Mb
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.000484
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9169.61328125Mb; avail=245912.6796875Mb
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.002104
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9169.61328125Mb; avail=245912.6796875Mb
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.001888
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.004787
2024-07-29 12:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: used=9169.61328125Mb; avail=245912.6796875Mb
2024-07-29 12:21:35 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 6.393 | nll_loss 2.736 | ppl 6.66 | wps 3191.4 | wpb 1361.3 | bsz 86.8 | num_updates 1095 | best_loss 6.365
2024-07-29 12:21:35 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2024-07-29 12:21:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 1095 updates
2024-07-29 12:21:35 | INFO | fairseq.trainer | Saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:22:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt
2024-07-29 12:22:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/krish/content/Hindi_Nepali/checkpoint1.2B_Ne-Hi/checkpoint_last.pt (epoch 73 @ 1095 updates, score 6.393) (writing took 39.285426502116024 seconds)
2024-07-29 12:22:14 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2024-07-29 12:22:14 | INFO | train | epoch 073 | loss 5.515 | nll_loss 1.849 | ppl 3.6 | wps 689.1 | ups 0.16 | wpb 4315.9 | bsz 277.5 | num_updates 1095 | lr 1.314e-05 | gnorm 1.643 | train_wall 52 | gb_free 11.7 | wall 7431
2024-07-29 12:22:14 | INFO | fairseq_cli.train | done training in 7422.3 seconds
